t al., 2018)</ref>, OpenAI GPT <ref type="bibr" target="#b15">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>. It has been shown that language modeling target="#b2">(Castro et al., 2018;</ref><ref type="bibr" target="#b1">de Araujo et al., 2018;</ref><ref type="bibr" target="#b4">Fernandes et al., 2018)</ref>. The model is composed of two bidirectio ained word embeddings were explored by <ref type="bibr" target="#b2">Castro et al. (2018)</ref> and <ref type="bibr" target="#b4">Fernandes et al. (2018)</ref> compared it to 3 other architectures. Th mming. During evaluation, the most likely sequence is obtained by Viterbi decoding. As described in <ref type="bibr" target="#b4">Devlin et al. (2018)</ref>, WordPiece tokenization requires prediction ead of using only the last hidden representation layer of BERT, we sum the last 4 layers, following <ref type="bibr" target="#b4">Devlin et al. (2018)</ref>. The resulting architecture resembles the L , we use document context for input examples instead of sentence context. Following the approach of <ref type="bibr" target="#b4">Devlin et al. (2018)</ref> on the SQuAD dataset, examples larger than pment set comprised of 10% of the First HAREM training set. We use the customized Adam optimizer of <ref type="bibr" target="#b4">Devlin et al. (2018)</ref>.</p><p>For the feature-based approach, we u

)</ref> has been commonly used in NER task <ref type="bibr" target="#b2">(Castro et al., 2018;</ref><ref type="bibr" target="#b1">de Araujo et al., 2018;</ref><ref type="bibr" target="#b4">Fernandes e
)</ref> has been commonly used in NER task <ref type="bibr" target="#b2">(Castro et al., 2018;</ref><ref type="bibr" target="#b1">de Araujo et al., 2018;</ref><ref type="bibr" target="#b4">Fernandes e
cture <ref type="bibr" target="#b10">(Lample et al., 2016)</ref> has been commonly used in NER task <ref type="bibr" target="#b2">(Castro et al., 2018;</ref><ref type="bibr" target="#b1">de Araujo et sification is then performed by the CRF layer. Several pre-trained word embeddings were explored by <ref type="bibr" target="#b2">Castro et al. (2018)</ref> and <ref type="bibr" target="#b4">Fernandes l 10 classes. This is the same setup used by <ref type="bibr">Santos and Guimaraes (2015)</ref> and <ref type="bibr" target="#b2">Castro et al. (2018)</ref>.</p><p>Vagueness and indeterminacy: some te narios (total and selective) to the works of <ref type="bibr">Santos and Guimaraes (2015)</ref> and <ref type="bibr" target="#b2">Castro et al. (2018)</ref>. To make the results comparable to both wor f the fine-tuning approach, as expected. BERT-LSTM-CRF outperforms the reported results of LSTM-CRF <ref type="bibr" target="#b2">(Castro et al., 2018)</ref> by about 1.5 points on the selective scena
to the minimal feature engineering requirements, which contributes to a higher domain independence <ref type="bibr" target="#b19">(Yadav and Bethard, 2018)</ref>. The CharWNN model <ref type="bibr">(
anguage processing tasks and also reduces the amount of labeled data needed for supervised learning <ref type="bibr" target="#b9">(Howard and Ruder, 2018)</ref>.</p><p>For Portuguese NER, there are fe
are many definitions of named entity and evaluation criteria, introducing evaluation complications <ref type="bibr" target="#b12">(Marrero et al., 2013)</ref>.</p><p>Current state-of-the-art NER syst
eira (2014) created a CRF model using 15 features extracted from the central and surrounding words. <ref type="bibr" target="#b14">(Pirovani and Oliveira, 2018</ref>) combined a CRF model with Local G
cture <ref type="bibr" target="#b10">(Lample et al., 2016)</ref> has been commonly used in NER task <ref type="bibr" target="#b2">(Castro et al., 2018;</ref><ref type="bibr" target="#b1">de Araujo et sification is then performed by the CRF layer. Several pre-trained word embeddings were explored by <ref type="bibr" target="#b2">Castro et al. (2018)</ref> and <ref type="bibr" target="#b4">Fernandes l 10 classes. This is the same setup used by <ref type="bibr">Santos and Guimaraes (2015)</ref> and <ref type="bibr" target="#b2">Castro et al. (2018)</ref>.</p><p>Vagueness and indeterminacy: some te narios (total and selective) to the works of <ref type="bibr">Santos and Guimaraes (2015)</ref> and <ref type="bibr" target="#b2">Castro et al. (2018)</ref>. To make the results comparable to both wor f the fine-tuning approach, as expected. BERT-LSTM-CRF outperforms the reported results of LSTM-CRF <ref type="bibr" target="#b2">(Castro et al., 2018)</ref> by about 1.5 points on the selective scena
word embeddings and then used to perform sequential classification.</p><p>The LSTM-CRF architecture <ref type="bibr" target="#b10">(Lample et al., 2016)</ref> has been commonly used in NER task <ref t om tag i to tag j. A includes 2 additional states: start and end of sequence.</p><p>As described by <ref type="bibr" target="#b10">Lample et al. (2016)</ref>, for an input sequence X = (x 1 , x 2 , .. r" target="#b4">Devlin et al. (2018)</ref>. The resulting architecture resembles the LSTM-CRF model <ref type="bibr" target="#b10">Lample et al. (2016)</ref> replacing its embedding techniques by BERT
ems employ neural architectures that have been pre-trained on language modeling tasks, such as ELMo <ref type="bibr" target="#b13">(Peters et al., 2018)</ref>, OpenAI GPT <ref type="bibr" target="#b15
)</ref> has been commonly used in NER task <ref type="bibr" target="#b2">(Castro et al., 2018;</ref><ref type="bibr" target="#b1">de Araujo et al., 2018;</ref><ref type="bibr" target="#b4">Fernandes e
tration of the evaluation procedure. Given an input document, the text is tokenized using WordPiece <ref type="bibr" target="#b18">(Wu et al., 2016)</ref> and the tokenized document is split into over
r bias term of the "O" tag with value of 6 in order to promote a better stability in early training <ref type="bibr" target="#b11">(Lin et al., 2017)</ref>. We also use a weight of 0.01 for "O" tag lo
</ref>) combined a CRF model with Local Grammars, following a similar approach.</p><p>Starting with <ref type="bibr" target="#b3">Collobert et al. (2011)</ref>, neural network NER systems have become )</ref>. The CharWNN model <ref type="bibr">(Santos and Guimaraes, 2015)</ref> extended the work of <ref type="bibr" target="#b3">Collobert et al. (2011)</ref> by employing a convolutional layer to ex
word embeddings and then used to perform sequential classification.</p><p>The LSTM-CRF architecture <ref type="bibr" target="#b10">(Lample et al., 2016)</ref> has been commonly used in NER task <ref t om tag i to tag j. A includes 2 additional states: start and end of sequence.</p><p>As described by <ref type="bibr" target="#b10">Lample et al. (2016)</ref>, for an input sequence X = (x 1 , x 2 , .. r" target="#b4">Devlin et al. (2018)</ref>. The resulting architecture resembles the LSTM-CRF model <ref type="bibr" target="#b10">Lample et al. (2016)</ref> replacing its embedding techniques by BERT
tration of the evaluation procedure. Given an input document, the text is tokenized using WordPiece <ref type="bibr" target="#b18">(Wu et al., 2016)</ref> and the tokenized document is split into over
to the minimal feature engineering requirements, which contributes to a higher domain independence <ref type="bibr" target="#b19">(Yadav and Bethard, 2018)</ref>. The CharWNN model <ref type="bibr">(
ems employ neural architectures that have been pre-trained on language modeling tasks, such as ELMo <ref type="bibr" target="#b13">(Peters et al., 2018)</ref>, OpenAI GPT <ref type="bibr" target="#b15
eira (2014) created a CRF model using 15 features extracted from the central and surrounding words. <ref type="bibr" target="#b14">(Pirovani and Oliveira, 2018</ref>) combined a CRF model with Local G
ems employ neural architectures that have been pre-trained on language modeling tasks, such as ELMo <ref type="bibr" target="#b13">(Peters et al., 2018)</ref>, OpenAI GPT <ref type="bibr" target="#b15
tle as possible. We will elaborate.</p><p>There are previous works that achieve the first condition <ref type="bibr" target="#b10">(Cisse et al., 2017;</ref><ref type="bibr" target="#b17">Hein &amp; A ef><ref type="bibr" target="#b16">Haber &amp; Ruthotto, 2017)</ref>. For example, Parseval networks <ref type="bibr" target="#b10">(Cisse et al., 2017)</ref> bound the Lipschitz constant by requiring leads to degradation in nominal accuracy, average confidence gap and robustness. Parseval networks <ref type="bibr" target="#b10">(Cisse et al., 2017)</ref> can be viewed as models without L c term, weight matrices and shows its effect in reducing generalization gap. The work on Parseval networks <ref type="bibr" target="#b10">(Cisse et al., 2017)</ref> shows that it is possible to control Lipsc ing the first condition, allows greater degrees of freedom in parameter training than the scheme in <ref type="bibr" target="#b10">Cisse et al. (2017)</ref>; a new loss function is specially designed |M i,j | (2)</formula><p>The above is where our linear and convolution layers differ from those in <ref type="bibr" target="#b10">Cisse et al. (2017)</ref>: they require W W T to be an identity matri s to be 1; they also propose to restrict aggregation operations. The reported robustness results of <ref type="bibr" target="#b10">Cisse et al. (2017)</ref>, however, are much weaker than those by adv common parameter with (9).</p><p>ResNet-like reconvergence is referred to as aggregation layers in <ref type="bibr" target="#b10">Cisse et al. (2017)</ref> and a different formula was used:</p><formu abel></formula><p>) where α ∈ [0, 1] is a trainable parameter. Because splitting is not modified in <ref type="bibr" target="#b10">Cisse et al. (2017)</ref>, their scheme may seem approximately equiva preserve distances. In contrast, because splitting is not modified, at reconvergence the scheme of <ref type="bibr" target="#b10">Cisse et al. (2017)</ref> must apply the shrinking factor of 1 − α on ents. We can also have a different t per channel or even per entry.</p><p>To be fair, the scheme of <ref type="bibr" target="#b10">Cisse et al. (2017)</ref> has an advantage of being nonexpansive with
alized margins as an indicator of generalization, which are strongly related to our confidence gap. <ref type="bibr" target="#b23">Pascanu et al. (2013)</ref> studies the role of the spectral radius o



rsarial training<ref type="bibr" target="#b30">(Tsipras et al., 2019)</ref> and adversarial polytope<ref type="bibr" target="#b31">(Wong et al., 2018)</ref>. It remains an open question whether such t

rsarial training<ref type="bibr" target="#b30">(Tsipras et al., 2019)</ref> and adversarial polytope<ref type="bibr" target="#b31">(Wong et al., 2018)</ref>. It remains an open question whether such t
a classifier itself are used in training <ref type="bibr" target="#b29">(Tramèr et al., 2017;</ref><ref type="bibr" target="#b33">Zantedeschi et al., 2017)</ref>. The work of <ref type="bibr" target=
vely. Similar trade-offs have been reported by other robustness works including adversarial training<ref type="bibr" target="#b30">(Tsipras et al., 2019)</ref> and adversarial polytope<ref type="bibr" adversaries<ref type="bibr" target="#b21">(Madry et al., 2017)</ref>. To the best of our knowledge,<ref type="bibr" target="#b30">Tsipras et al. (2019)</ref> from the same lab is the only paper in th


x) ≈ x (e.g., such a g(•) may perform image denoising to remove the adversarial perturbation, as in <ref type="bibr" target="#b9">Guo et al. (2018)</ref>). If g(•) is smooth and differentiable, then c mlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">INPUT TRANSFORMATIONS</head><p>Defense Details. <ref type="bibr" target="#b9">Guo et al. (2018)</ref> propose five input transformations to counter accuracy drops to 0% for the strongest defense under the smallest perturbation budget considered in <ref type="bibr" target="#b9">Guo et al. (2018)</ref>, a root-mean-square perturbation of 0.05 (and 18)</ref>, a root-mean-square perturbation of 0.05 (and a "normalized" 2 perturbation as defined in <ref type="bibr" target="#b9">Guo et al. (2018)</ref> of 0.01).</p></div> <div xmlns="http://www.tei "bibr" target="#b15">(Madry et al., 2018;</ref><ref type="bibr" target="#b14">Ma et al., 2018;</ref><ref type="bibr" target="#b9">Guo et al., 2018;</ref><ref type="bibr" target="#b28">Xie et al., 2018
//www.tei-c.org/ns/1.0"><head n="5.3.2.">MITIGATING THROUGH RANDOMIZATION</head><p>Defense Details. <ref type="bibr" target="#b28">Xie et al. (2018)</ref> propose to defend against adversarial example uld be "computationally impossible" (emphasis ours) and that such an attack "may not even converge" <ref type="bibr" target="#b28">(Xie et al., 2018)</ref>.</p><p>Evaluation. We find the authors' ense ype="bibr" target="#b14">Ma et al., 2018;</ref><ref type="bibr" target="#b9">Guo et al., 2018;</ref><ref type="bibr" target="#b28">Xie et al., 2018)</ref>.</p></div> <div xmlns="http://www.tei-c.org/n
pe="bibr" target="#b10">He et al., 2016)</ref> to 95% accuracy. For ImageNet we use the InceptionV3 <ref type="bibr" target="#b26">(Szegedy et al., 2016)</ref> network which reaches 78.0% top-1 and 93
lack-box security <ref type="bibr" target="#b27">(Tramèr et al., 2018)</ref>. We include one paper, <ref type="bibr" target="#b14">Ma et al. (2018)</ref>, that was not proposed as a defense per se, bu /head><p>LID is a general-purpose metric that measures the distance from an input to its neighbors. <ref type="bibr" target="#b14">Ma et al. (2018)</ref> propose using LID to characterize properties o study made complete source code available <ref type="bibr" target="#b15">(Madry et al., 2018;</ref><ref type="bibr" target="#b14">Ma et al., 2018;</ref><ref type="bibr" target="#b9">Guo et al., 2018;
rcumvent one) that rely on obfuscated gradients. We omit two defenses with provable security claims <ref type="bibr" target="#b18">(Raghunathan et al., 2018;</ref><ref type="bibr" target="#b23">Sinha
FAR-10 we train a wide ResNet <ref type="bibr" target="#b29">(Zagoruyko &amp; Komodakis, 2016;</ref><ref type="bibr" target="#b10">He et al., 2016)</ref> to 95% accuracy. For ImageNet we use the Incep
//www.tei-c.org/ns/1.0"><head n="5.3.2.">MITIGATING THROUGH RANDOMIZATION</head><p>Defense Details. <ref type="bibr" target="#b28">Xie et al. (2018)</ref> propose to defend against adversarial example uld be "computationally impossible" (emphasis ours) and that such an attack "may not even converge" <ref type="bibr" target="#b28">(Xie et al., 2018)</ref>.</p><p>Evaluation. We find the authors' ense ype="bibr" target="#b14">Ma et al., 2018;</ref><ref type="bibr" target="#b9">Guo et al., 2018;</ref><ref type="bibr" target="#b28">Xie et al., 2018)</ref>.</p></div> <div xmlns="http://www.tei-c.org/n
neural networks to adversarial examples <ref type="bibr" target="#b25">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b2">Biggio et al., 2013)</ref>, there has been significant interest recent
tive optimization-based attacks (e.g., <ref type="bibr" target="#b12">Kurakin et al. (2016a)</ref>; <ref type="bibr" target="#b15">Madry et al. (2018)</ref>; <ref type="bibr" target="#b6">Carlini &amp contrast, a defender must show no attack can succeed. We study the adversarial training approach of <ref type="bibr" target="#b15">Madry et al. (2018)</ref> which for a given -ball solves</p><formula rete thermometer encoded inputs. Using this attack, the authors perform the adversarial training of <ref type="bibr" target="#b15">Madry et al. (2018)</ref> on thermometer encoded networks.</p><p>On C . However, our attack reduces model accuracy to 30%. This is significantly weaker than the original <ref type="bibr" target="#b15">Madry et al. (2018)</ref> model that does not use thermometer encodin cy with = .015, compared to over 70% at the same perturbation budget with adversarial training as in<ref type="bibr" target="#b15">Madry et al. (2018)</ref>.</figDesc><table><row><cell>5.2. Gradient S choice of optimizer is far less important than choosing to use iterative optimization-based methods <ref type="bibr" target="#b15">(Madry et al., 2018)</ref>.</p></div> <div xmlns="http://www.tei-c.or ccuracy to 9% with a maximum ∞ perturbation of = 0.031. We find that combining adversarial training <ref type="bibr" target="#b15">(Madry et al., 2018)</ref> with PixelDefend provides no additional ro ot" n="4" xml:id="foot_2">That is, a thermometer encoded model that is trained using the approach of<ref type="bibr" target="#b15">(Madry et al., 2018)</ref>.</note> 			<note xmlns="http://www.tei-c.o w has been discussed in prior work <ref type="bibr" target="#b4">(Carlini &amp; Wagner, 2017a;</ref><ref type="bibr" target="#b15">Madry et al., 2018)</ref>; we repeat these points here and offer our t the time of writing this paper, four of the defenses we study made complete source code available <ref type="bibr" target="#b15">(Madry et al., 2018;</ref><ref type="bibr" target="#b14">Ma et al., 2
p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">DEFENSE-GAN</head><p>Defense-GAN <ref type="bibr" target="#b21">(Samangouei et al., 2018)</ref> uses a Generative Adversarial Network
ral angle is a commonly used distance metric to measure the difference between two spectral vectors <ref type="bibr" target="#b22">(Kruse et al., 1993)</ref>. The reflectance of individual pixel is re
entation <ref type="bibr" target="#b37">(Wang et al., 2018)</ref> and multi-resolution segmentation <ref type="bibr" target="#b0">(Benz et al., 2004)</ref> have been widely used to generate image obje nitial segmentation results for HR images. MSEG is developed from the multi-resolution segmentation <ref type="bibr" target="#b0">(Benz et al., 2004)</ref>. It initially generates an oversegmentation ough the segmentation algorithm used in this study is based on multi-resolution segmentation (MSEG) <ref type="bibr" target="#b0">(Benz et al. (2004)</ref>; <ref type="bibr" target="#b36">Tzotsos and
2.1.">Multiscale segmentation used</head><p>A region based multiscale image segmentation named MSEG <ref type="bibr" target="#b36">(Tzotsos and Argialas, 2006</ref>) is used to generate the initial se D have a size of × 500 500 pixels. The multiscale segmentation used in this study is MSEG algorithm <ref type="bibr" target="#b36">(Tzotsos and Argialas, 2006)</ref>, which is implemented under C++ en ed on multi-resolution segmentation (MSEG) <ref type="bibr" target="#b0">(Benz et al. (2004)</ref>; <ref type="bibr" target="#b36">Tzotsos and Argialas, 2006)</ref>, the proposed method is implemented

gmentation results at different scales <ref type="bibr" target="#b10">(Espindola et al., 2006;</ref><ref type="bibr" target="#b21">Karl and Maurer, 2010;</ref><ref type="bibr" target="#b31">Stein and
timal scale can be selected from the maximum value. Similar to other global scale selection methods <ref type="bibr" target="#b38">(Wang et al., 2019;</ref><ref type="bibr" target="#b43">Yang et al., pectral homogeneity and heterogeneity calculation can help decide the optimal scale of segmentation <ref type="bibr" target="#b38">(Wang et al., 2019)</ref>.</p><p>This study proposed a local spectral
le optimization methods usually can be divided into two groups: supervised and unsupervised methods <ref type="bibr" target="#b15">(Grybas et al., 2017)</ref>. Supervised scale optimization methods ar
et="#b21">Karl and Maurer, 2010;</ref><ref type="bibr" target="#b31">Stein and de Beurs, 2005;</ref><ref type="bibr" target="#b49">Zhou et al., 2017;</ref><ref type="bibr" target="#b12">Georganos et a
target="#b49">Zhou et al., 2017;</ref><ref type="bibr" target="#b12">Georganos et al., 2018b;</ref><ref type="bibr" target="#b17">Hu et al., 2016)</ref>. Therefore, it is a key step to design an appr
been applied in segmentation evaluation <ref type="bibr" target="#b20">(Johnson and Xie, 2011;</ref><ref type="bibr" target="#b47">Zhang et al., 2012)</ref>. Recently, a region-based precision and rec
an appropriate scale for segmentation, researchers proposed a variety of scale optimization methods <ref type="bibr" target="#b11">(Georganos et al., 2018a;</ref><ref type="bibr" target="#b19">Hu et a
n algorithms proposed and utilized in GEOBIA <ref type="bibr" target="#b24">(Liu et al., 2015;</ref><ref type="bibr" target="#b14">Grinias et al., 2016;</ref><ref type="bibr" target="#b35">Troya-Galvi
been applied in segmentation evaluation <ref type="bibr" target="#b20">(Johnson and Xie, 2011;</ref><ref type="bibr" target="#b47">Zhang et al., 2012)</ref>. Recently, a region-based precision and rec
get="#b10">(Espindola et al., 2006;</ref><ref type="bibr" target="#b21">Karl and Maurer, 2010;</ref><ref type="bibr" target="#b31">Stein and de Beurs, 2005;</ref><ref type="bibr" target="#b49">Zhou et

"bibr" target="#b47">Zhang et al., 2012)</ref>. Recently, a region-based precision and recall index <ref type="bibr" target="#b46">(Zhang et al., 2015a)</ref> is specially proposed for evaluating the while the recall is higher in comparison with other methods. In fact, according to the analysis in <ref type="bibr" target="#b46">Zhang et al. (2015a)</ref>, if an image is over-segmented, the precis
zation. One solution is to introduce land-cover categories to guide the selection of optimal scales <ref type="bibr" target="#b29">(Myint et al., 2011)</ref>. The optimal scales are obtained in differ
zation. One solution is to introduce land-cover categories to guide the selection of optimal scales <ref type="bibr" target="#b29">(Myint et al., 2011)</ref>. The optimal scales are obtained in differ
dress the complicated spatial and hierarchical relationships in the HR image classification process <ref type="bibr" target="#b26">(Ma et al., 2015;</ref><ref type="bibr" target="#b9">Dronova, 2015;</
gmentation results at different scales <ref type="bibr" target="#b10">(Espindola et al., 2006;</ref><ref type="bibr" target="#b21">Karl and Maurer, 2010;</ref><ref type="bibr" target="#b31">Stein and
ritical effect on the later steps of GEOBIA, including object feature extraction and classification <ref type="bibr" target="#b13">(Gonçalves et al., 2019)</ref>.</p><p>There are a number of HR image selection methods have drawn more attention <ref type="bibr" target="#b2">(Böck et al., 2017;</ref><ref type="bibr" target="#b13">Gonçalves et al., 2019)</ref>. Many studies consider that the spectra

/ref>.</p><p>There are a number of HR image segmentation algorithms proposed and utilized in GEOBIA <ref type="bibr" target="#b24">(Liu et al., 2015;</ref><ref type="bibr" target="#b14">Grinias et al.

used in remote sensing image processing by distinguishing land covers with the spectral reflectance <ref type="bibr" target="#b30">(Pu and Landry, 2012)</ref>. However, the pixel-based methods fail to
et="#b14">Grinias et al., 2016;</ref><ref type="bibr" target="#b35">Troya-Galvis et al., 2018;</ref><ref type="bibr" target="#b6">Comaniciu and Meer, 2002)</ref>. For example, watershed segmentation < ed independently and can be applied in other multiscale segmentation algorithms, such as mean shift <ref type="bibr" target="#b6">(Comaniciu and Meer, 2002)</ref> and watershed segmentation <ref type=
fine the segmentation result. Other studies try to optimize segmentation at several distinct scales <ref type="bibr" target="#b44">(Yang et al., 2014)</ref>. For instance, in <ref type="bibr" target="
used in remote sensing image processing by distinguishing land covers with the spectral reflectance <ref type="bibr" target="#b30">(Pu and Landry, 2012)</ref>. However, the pixel-based methods fail to
/ref>.</p><p>There are a number of HR image segmentation algorithms proposed and utilized in GEOBIA <ref type="bibr" target="#b24">(Liu et al., 2015;</ref><ref type="bibr" target="#b14">Grinias et al.
ts and tend to design a measure to evaluate the quality of segmentation results at different scales <ref type="bibr" target="#b10">(Espindola et al., 2006;</ref><ref type="bibr" target="#b21">Karl and
been applied in segmentation evaluation <ref type="bibr" target="#b20">(Johnson and Xie, 2011;</ref><ref type="bibr" target="#b47">Zhang et al., 2012)</ref>. Recently, a region-based precision and rec

ritical effect on the later steps of GEOBIA, including object feature extraction and classification <ref type="bibr" target="#b13">(Gonçalves et al., 2019)</ref>.</p><p>There are a number of HR image selection methods have drawn more attention <ref type="bibr" target="#b2">(Böck et al., 2017;</ref><ref type="bibr" target="#b13">Gonçalves et al., 2019)</ref>. Many studies consider that the spectra
mory disambiguation for largewindow processors, in which thousands of instructions may be in flight <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" ta
recent older instruction first <ref type="bibr" target="#b28">(28)</ref> and then then next oldest <ref type="bibr" target="#b25">(25)</ref> and so on. In some cases, circuit implementations exploit LSQ, but instead of reserving a slot in each of the LSQ partitions, they use memory bank predictors <ref type="bibr" target="#b25">[25]</ref> to predict a target bank and reserve a slot there. If the
s may be in flight <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b24">24]</ref>, by exploiting the ability to address interleave LSQ banks.
s may be in flight <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b24">24]</ref>, by exploiting the ability to address interleave LSQ banks.
Cain and Lipasti's scheme, to reduce the number of loads that have to perform commit time checking <ref type="bibr" target="#b20">[20]</ref>. These mechanisms eliminate the CAM from the LQ but requir
n propose two-level store buffers, each of which are centralized, fully associative and age ordered <ref type="bibr" target="#b5">[5]</ref>. Stores first enter the L1 store buffer, and when it overflo
centralized forwarding buffer, but address-partitioned unordered structures for violation detection <ref type="bibr" target="#b10">[10]</ref>. Both these schemes increase the area required for memory based filters <ref type="bibr" target="#b22">[22]</ref> or the small associative forwarding buffers <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" targ
s may be in flight <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b24">24]</ref>, by exploiting the ability to address interleave LSQ banks.
levant to memory instructions. Additional details about the TRIPS microarchitecture can be found in <ref type="bibr" target="#b21">[21]</ref>.</p><p>The features of a distributed microarchitecture mos nted these flow control mechanisms on a simulator that closely models the TRIPS prototype processor <ref type="bibr" target="#b21">[21]</ref> which has been validated to be within 11% of the RTL for t
ads compare their speculative values to the cached values at commit time to detect mis-speculations <ref type="bibr" target="#b13">[13]</ref>. Roth proposed enhancements to Cain and Lipasti's scheme,
of area.</p><p>Garg, Rashid and Huang propose another mechanism for eliminating associative LQ/SQs <ref type="bibr" target="#b6">[6]</ref>. In the first phase of two-phase processing, loads and store
thms have been used to find k-way partitions, spectral clusterings, and separators in planar graphs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
="bibr" target="#b6">[7]</ref>, and further developed by Berkhin in his Bookmark Coloring Algorithm <ref type="bibr" target="#b0">[1]</ref>. The algorithms of Jeh-Widom and Berkhin compute many person gorithm is based on the algorithms of Jeh-Widom <ref type="bibr" target="#b6">[7]</ref> and Berkhin <ref type="bibr" target="#b0">[1]</ref>, both of which can be used to compute similar approximate Pa ">[6]</ref>, and have been used to provide personalized search ranking and context-sensitive search <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target
Vectors</head><p>PageRank was introduced by Brin and Page <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b2">3]</ref>. For convenience, we introduce a lazy variation of PageRank,
="bibr" target="#b6">[7]</ref>, and further developed by Berkhin in his Bookmark Coloring Algorithm <ref type="bibr" target="#b0">[1]</ref>. The algorithms of Jeh-Widom and Berkhin compute many person gorithm is based on the algorithms of Jeh-Widom <ref type="bibr" target="#b6">[7]</ref> and Berkhin <ref type="bibr" target="#b0">[1]</ref>, both of which can be used to compute similar approximate Pa ">[6]</ref>, and have been used to provide personalized search ranking and context-sensitive search <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target
5]</ref>. The analysis of the Nibble algorithm is based on a mixing result by Lov?sz and Simonovits <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, which shows that cut lity from Lemma 3 to relate apr(?, s, r) to itself. In contrast, the proof of Lov?sz and Simonovits <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> relates the walk dist
ed algorithm for computing approximate PageRank vectors. We use a technique introduced by Jeh-Widom <ref type="bibr" target="#b6">[7]</ref>, and further developed by Berkhin in his Bookmark Coloring A re some useful properties of PageRank vectors (also see <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b6">[7]</ref>). The proofs are given in the Appendix.</p><p>Proposition 1. uch that vol(Supp(p)) ? 1 ? . We remark that this algorithm is based on the algorithms of Jeh-Widom <ref type="bibr" target="#b6">[7]</ref> and Berkhin <ref type="bibr" target="#b0">[1]</ref>, both of ntext-sensitive search <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. The preference vectors used in our algorithms have all proba
tions, spectral clusterings, and separators in planar graphs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targe for j = 0, and j = n. The theorem will follow because f t+1 is concave, we have shown that equation <ref type="bibr" target="#b7">(8)</ref>  </p><formula xml:id="formula_48">p [k j ] ? ? + 1 2 f t (k
5]</ref>. The analysis of the Nibble algorithm is based on a mixing result by Lov?sz and Simonovits <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, which shows that cut lity from Lemma 3 to relate apr(?, s, r) to itself. In contrast, the proof of Lov?sz and Simonovits <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> relates the walk dist
target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. There is no known way to lower bound the size of the small ing of a graph, and is often used in spectral partitioning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. We will use the following degree-normalized version of a s
ed algorithm for computing approximate PageRank vectors. We use a technique introduced by Jeh-Widom <ref type="bibr" target="#b6">[7]</ref>, and further developed by Berkhin in his Bookmark Coloring A re some useful properties of PageRank vectors (also see <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b6">[7]</ref>). The proofs are given in the Appendix.</p><p>Proposition 1. uch that vol(Supp(p)) ? 1 ? . We remark that this algorithm is based on the algorithms of Jeh-Widom <ref type="bibr" target="#b6">[7]</ref> and Berkhin <ref type="bibr" target="#b0">[1]</ref>, both of ntext-sensitive search <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. The preference vectors used in our algorithms have all proba
tions, spectral clusterings, and separators in planar graphs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targe for j = 0, and j = n. The theorem will follow because f t+1 is concave, we have shown that equation <ref type="bibr" target="#b7">(8)</ref>  </p><formula xml:id="formula_48">p [k j ] ? ? + 1 2 f t (k
"bibr" target="#b13">(Liu et al., 2018;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., 2016)</ref>. However, most existing methods require la "bibr" target="#b13">(Liu et al., 2018;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., 2016)</ref> to define the score of the predicted seque "bibr" target="#b13">(Liu et al., 2018;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., 2016;</ref><ref type="bibr" target="#b3">Finkel et al. "bibr" target="#b13">(Liu et al., 2018;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., 2016;</ref><ref type="bibr" target="#b18">Ratinov and cannot deal with multi-label tokens. Therefore, we customize the conventional CRF layer in LSTM-CRF <ref type="bibr" target="#b10">(Lample et al., 2016)</ref> into a Fuzzy CRF layer, which allows each kens, such as "Thus" and "by", are labeled as O.</p><p>Fuzzy-LSTM-CRF. We revise the LSTM-CRF model <ref type="bibr" target="#b10">(Lample et al., 2016)</ref> to the Fuzzy-LSTM-CRF model to support th -Disease datasets, LM-LSTM-CRF <ref type="bibr" target="#b13">(Liu et al., 2018)</ref> and LSTM-CRF <ref type="bibr" target="#b10">(Lample et al., 2016)</ref> achieve the state-of-the-art F 1 scores w cent advances in neural models have freed do-main experts from handcrafting features for NER tasks. <ref type="bibr" target="#b10">(Lample et al., 2016;</ref><ref type="bibr" target="#b14">Ma and Hovy
n (NER) models without handcrafting features <ref type="bibr" target="#b13">(Liu et al., 2018;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., rvised models (e.g., neural sequence models) <ref type="bibr" target="#b13">(Liu et al., 2018;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., s (CRF) with the IOB or IOBES tagging scheme <ref type="bibr" target="#b13">(Liu et al., 2018;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., e label y j .</p><p>We follow previous works <ref type="bibr" target="#b13">(Liu et al., 2018;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., from handcrafting features for NER tasks. <ref type="bibr" target="#b10">(Lample et al., 2016;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b13">Liu et al., 20
" target="#b2">(Etzioni et al., 2005;</ref><ref type="bibr" target="#b6">Hanisch et al., 2005;</ref><ref type="bibr" target="#b12">Lin et al., 2012;</ref><ref type="bibr" target="#b7">He, 2017)</ref>.
traction <ref type="bibr" target="#b5">(Giannakopoulos et al., 2017)</ref>, and relation extraction <ref type="bibr" target="#b15">(Mintz et al., 2009)</ref>. Meanwhile, open knowledge bases (or dicti ses to supervise relation extraction tasks <ref type="bibr" target="#b0">(Craven et al., 1999;</ref><ref type="bibr" target="#b15">Mintz et al., 2009)</ref>. AutoPhrase has demonstrated powers in extr
" target="#b2">(Etzioni et al., 2005;</ref><ref type="bibr" target="#b6">Hanisch et al., 2005;</ref><ref type="bibr" target="#b12">Lin et al., 2012;</ref><ref type="bibr" target="#b7">He, 2017)</ref>.
rts. Originally, it was proposed to leverage knowledge bases to supervise relation extraction tasks <ref type="bibr" target="#b0">(Craven et al., 1999;</ref><ref type="bibr" target="#b15">Mintz et al.
ef type="foot" target="#foot_1">3</ref> , which is designed for the aspect-based sentiment analysis <ref type="bibr" target="#b25">(Wang et al., 2011)</ref>.</p><p>We treat out-of-dictionary phrases a
iu et al., 2018)</ref>. Such neural models are increasingly common in the domain-specific NER tasks <ref type="bibr" target="#b20">(Sahu and Anand, 2016;</ref><ref type="bibr" target="#b1">Dernoncourt
ef type="foot" target="#foot_1">3</ref> , which is designed for the aspect-based sentiment analysis <ref type="bibr" target="#b25">(Wang et al., 2011)</ref>.</p><p>We treat out-of-dictionary phrases a
traction <ref type="bibr" target="#b5">(Giannakopoulos et al., 2017)</ref>, and relation extraction <ref type="bibr" target="#b15">(Mintz et al., 2009)</ref>. Meanwhile, open knowledge bases (or dicti ses to supervise relation extraction tasks <ref type="bibr" target="#b0">(Craven et al., 1999;</ref><ref type="bibr" target="#b15">Mintz et al., 2009)</ref>. AutoPhrase has demonstrated powers in extr
ef type="foot" target="#foot_1">3</ref> , which is designed for the aspect-based sentiment analysis <ref type="bibr" target="#b25">(Wang et al., 2011)</ref>.</p><p>We treat out-of-dictionary phrases a
>[11]</ref> although it disregards many architectural details. Babka and T?ma present their work in <ref type="bibr" target="#b4">[4]</ref>, focusing mainly on translation lookaside buffers and cache
o circumvent perturbing hardware properties. Moreover, we use the performance analysis tool BenchIT <ref type="bibr" target="#b10">[10]</ref> to facilitate the benchmark implementation as well as the hmarks and supports the result evaluation. The Open Source tool BenchIT provides this functionality <ref type="bibr" target="#b10">[10]</ref>. It runs microbenchmarks on POSIX compliant systems and he
factor is the architecture of the memory subsystem in conjunction with the cache coherency protocol <ref type="bibr" target="#b12">[12]</ref>.</p><p>While the basic memory hierarchy structure is simil processors including a ping-pong implementation to analyze the latency of cache-to-cache transfers <ref type="bibr" target="#b12">[12]</ref>. However, they do not cover inter-core bandwidths. To the
factor is the architecture of the memory subsystem in conjunction with the cache coherency protocol <ref type="bibr" target="#b12">[12]</ref>.</p><p>While the basic memory hierarchy structure is simil processors including a ping-pong implementation to analyze the latency of cache-to-cache transfers <ref type="bibr" target="#b12">[12]</ref>. However, they do not cover inter-core bandwidths. To the
">[7]</ref> protocol to ensure cache coherency. AMD Opteron processors implement the MOESI protocol <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5]</ref>. The additional state literature that describes the AMD Opteron architecture including the MOESI protocol in much detail <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5]</ref>. However, details abou
">[7]</ref> protocol to ensure cache coherency. AMD Opteron processors implement the MOESI protocol <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5]</ref>. The additional state literature that describes the AMD Opteron architecture including the MOESI protocol in much detail <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5]</ref>. However, details abou
o circumvent perturbing hardware properties. Moreover, we use the performance analysis tool BenchIT <ref type="bibr" target="#b10">[10]</ref> to facilitate the benchmark implementation as well as the hmarks and supports the result evaluation. The Open Source tool BenchIT provides this functionality <ref type="bibr" target="#b10">[10]</ref>. It runs microbenchmarks on POSIX compliant systems and he
o circumvent perturbing hardware properties. Moreover, we use the performance analysis tool BenchIT <ref type="bibr" target="#b10">[10]</ref> to facilitate the benchmark implementation as well as the hmarks and supports the result evaluation. The Open Source tool BenchIT provides this functionality <ref type="bibr" target="#b10">[10]</ref>. It runs microbenchmarks on POSIX compliant systems and he
ent between processor cores that is preferably kept onchip. Common memory benchmarks such as STREAM <ref type="bibr" target="#b11">[11]</ref> are no longer sufficient to adequately assess the memory p ze implementation details of the memory hierarchy. A well-known and established benchmark is STREAM <ref type="bibr" target="#b11">[11]</ref> although it disregards many architectural details. Babka a
o circumvent perturbing hardware properties. Moreover, we use the performance analysis tool BenchIT <ref type="bibr" target="#b10">[10]</ref> to facilitate the benchmark implementation as well as the hmarks and supports the result evaluation. The Open Source tool BenchIT provides this functionality <ref type="bibr" target="#b10">[10]</ref>. It runs microbenchmarks on POSIX compliant systems and he
ent between processor cores that is preferably kept onchip. Common memory benchmarks such as STREAM <ref type="bibr" target="#b11">[11]</ref> are no longer sufficient to adequately assess the memory p ze implementation details of the memory hierarchy. A well-known and established benchmark is STREAM <ref type="bibr" target="#b11">[11]</ref> although it disregards many architectural details. Babka a
only with high numbers of client connections spreading requests out evenly over queues.</p><p>ZygOS <ref type="bibr" target="#b44">[46]</ref> improved on d-FCFS by implementing low-overhead task steal istributions.</p><p>We compare Shinjuku with IX <ref type="bibr" target="#b14">[16]</ref> and ZygOS <ref type="bibr" target="#b44">[46]</ref>, two state-of-the-art dataplane operating systems. Using s "bibr" target="#b14">[16]</ref>. Zy-gOS improves on IX by using work stealing to approximate c-FCFS <ref type="bibr" target="#b44">[46]</ref>. Linux applications built with libevent <ref type="bibr" t ficant departure from the common pattern in IX <ref type="bibr" target="#b14">[16]</ref> and Zy-gOS <ref type="bibr" target="#b44">[46]</ref>, which rely heavily on RSS to distribute incoming requests ">Evaluation</head><p>We compare Shinjuku to IX <ref type="bibr" target="#b14">[16]</ref> and ZygOS <ref type="bibr" target="#b44">[46]</ref>, two recent systems that use d-FCFS and approximate c-FCFS es its stealing overheads. A similar performance drop was also observed in the original ZygOS paper <ref type="bibr" target="#b44">[46]</ref>. Figure <ref type="figure">5c</ref> uses a Bimodal(99.5 -0 type="bibr" target="#b37">[39]</ref>, Chronos <ref type="bibr" target="#b30">[32]</ref>, and ZygOS <ref type="bibr" target="#b44">[46]</ref> fall in this category. Shinjuku improves on these systems duced in ELI for fast delivery of interrupts to VMs <ref type="bibr" target="#b8">[10]</ref>. ZygOS <ref type="bibr" target="#b44">[46]</ref> uses inter-processor interrupts for work stealing but does
optimize for different conditions (UDP, TCP, ROCE <ref type="bibr" target="#b50">[52]</ref>, TIMELY <ref type="bibr" target="#b39">[41]</ref>, etc.) and various optimized network stacks <ref type="bib
. Hence, it makes sense to adjust over time the number of workers a Shinjuku process uses. Shenango <ref type="bibr" target="#b4">[7]</ref> solves this problem by adjusting core allocation between app
<ref type="bibr" target="#b11">[13]</ref>; microservices and functionas-a-service (FaaS) frameworks <ref type="bibr" target="#b15">[17]</ref>; and in-memory stores or databases, such as RocksDB <ref t
s have developed network stacks, dataplanes, and full applications that bypass the operating system <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" ta orking stacks, dataplanes, RPC protocols, and applications <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" tar
ace thread libraries <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b0">1]</ref>. They all focus on co
atcher always takes the request at the head.</p><p>The queue selection algorithm is inspired by BVT <ref type="bibr" target="#b22">[24]</ref>, a process scheduling algorithm for latency sensitive task
nificantly improved without hardware changes, such as support for lightweight user-level interrupts <ref type="bibr" target="#b49">[51]</ref>.</p><p>Optimized interrupt sending. Finally, we remove the w-overhead message passing mechanism among different cores <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b49">51]</ref>. Ideally, such a mechanism would offer two variations, a pr
get="#b29">31,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" tar , and applications <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" tar nd those with fixed or low-dispersion service time distributions.</p><p>We compare Shinjuku with IX <ref type="bibr" target="#b14">[16]</ref> and ZygOS <ref type="bibr" target="#b44">[46]</ref>, two s duling policy <ref type="bibr" target="#b19">[21]</ref>, the IX dataplane being a canonical example <ref type="bibr" target="#b14">[16]</ref>. Zy-gOS improves on IX by using work stealing to approxima tem for low-latency applications. Shinjuku is a significant departure from the common pattern in IX <ref type="bibr" target="#b14">[16]</ref> and Zy-gOS <ref type="bibr" target="#b44">[46]</ref>, whic injuku dispatcher and worker code are 2535 SLOC. The network subsystem we used in ?4 is based on IX <ref type="bibr" target="#b14">[16]</ref>. All the aforementioned codebases are in C.</p></div> <div <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We compare Shinjuku to IX <ref type="bibr" target="#b14">[16]</ref> and ZygOS <ref type="bibr" target="#b44">[46]</ref>, two r he OS control plane, an idea originating in Exokernel <ref type="bibr" target="#b23">[25]</ref>. IX <ref type="bibr" target="#b14">[16]</ref>, Arrakis <ref type="bibr" target="#b43">[45]</ref>, MICA < ="formula">16</ref>) ZygOS( <ref type="formula">16</ref>) Shinjuku( <ref type="formula">16</ref> IX <ref type="bibr" target="#b14">(16)</ref> ZygOS( <ref type="formula">16</ref>) Shinjuku( <ref type=" 1.0"><head n="4.3">Shinjuku Analysis</head><p>How important is frequent preemption? Figure 7a ZygOS <ref type="bibr" target="#b14">(16)</ref> Shinjuku( <ref type="formula">16</ref>) IX( <ref type="for
We will also explore microsecond-scale scheduling policies that are localityand heterogeneity-aware <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b25">27]</ref>. For example, cons
s 3 orders of magnitude shorter than what this line of work can handle.</p><p>Flow scheduling: PIAS <ref type="bibr" target="#b10">[12]</ref> is a network flow scheduling mechanism that uses hardware
n. It achieved topic and keyword identification that the cross-language transfer learning method in <ref type="bibr" target="#b13">[14]</ref> used to learn the characteristics of low-resourced languag
eby improving the accuracy of classification or prediction <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Compared with the traditional shallow model, the deep lear
state of recession and dozens of which are endangered. This trend is continuing and even worsening <ref type="bibr" target="#b0">[1]</ref>. Therefore, it is an imperative duty to protect endangered l
ecognition technology <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" targe
edge between existing tasks and target tasks, which can help the model to learn target tasks better <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. According to <ref
state of recession and dozens of which are endangered. This trend is continuing and even worsening <ref type="bibr" target="#b0">[1]</ref>. Therefore, it is an imperative duty to protect endangered l
t research topics <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. In References <ref type="bibr" target="#b23">[24]</ref><r
in the processing of complex signals. Deep learning was first applied to speech recognition in 2009 <ref type="bibr" target="#b17">[18]</ref>. It provides a 20% improvement over the traditional Gaussi
tures, the CNN can obtain more robust features using local filtering and maximum pooling techniques <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. The CNN was origi
he BiLSTM and CTC networks has become a new standard combination in the field of speech recognition <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" t
f><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. In References <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" t
nese poetry generation have been mostly rulebased or template-based. Recurrent Neural Network (RNN) <ref type="bibr" target="#b10">[11]</ref> was recently introduced as it has been proved to be effect
f type="bibr" target="#b13">Yang et al. (2017)</ref>  <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b12">Wang et al. (2016)</ref>  <ref type="bibr" target="#b12">[13]</ref> e f type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b12">Wang et al. (2016)</ref>  <ref type="bibr" target="#b12">[13]</ref> employ a two-stage approach,</p><formula xml:id="formula_3
st salient characters from the so-far generated lines as a theme clue for generating the next line. <ref type="bibr" target="#b13">Yang et al. (2017)</ref>  <ref type="bibr" target="#b13">[14]</ref> a a theme clue for generating the next line. <ref type="bibr" target="#b13">Yang et al. (2017)</ref>  <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b12">Wang et al. (2016)</ref
rmula></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Early works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
-c.org/ns/1.0"><head n="4">Related Work</head><p>Early works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target
rmula></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Early works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
ls</head><p>We refer the readers to the blog<ref type="foot" target="#foot_0">3</ref> or the papers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target
ls</head><p>We refer the readers to the blog<ref type="foot" target="#foot_0">3</ref> or the papers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target
pairing of a generation are correct, various strategies have been adopted. For example, Yan (2016) <ref type="bibr" target="#b11">[12]</ref> proposes an iterative polishing schema, which refines the
st salient characters from the so-far generated lines as a theme clue for generating the next line. <ref type="bibr" target="#b13">Yang et al. (2017)</ref>  <ref type="bibr" target="#b13">[14]</ref> a a theme clue for generating the next line. <ref type="bibr" target="#b13">Yang et al. (2017)</ref>  <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b12">Wang et al. (2016)</ref
满江红), Shuidiaogetou(水调歌头), etc .</p><p>Various methods e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> have been proposed to generate classical Chinese poetry. Howe
rmula></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Early works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
e text generation, a classical Chinese poem should normally meet both form and content requirements <ref type="bibr" target="#b0">[1]</ref>. The form requirements includes the regulations on the numbe ce of content throughout a poem. For example, <ref type="bibr" target="#b0">Yi et al. (2018)</ref>  <ref type="bibr" target="#b0">[1]</ref> propose a salientclue mechanism which automatically selects cluding Xijiangyue(西江 月), Manjianghong(满江红), Shuidiaogetou(水调歌头), etc .</p><p>Various methods e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> have been proposed to , some other works have been investigating the coherence of content throughout a poem. For example, <ref type="bibr" target="#b0">Yi et al. (2018)</ref>  <ref type="bibr" target="#b0">[1]</ref> propos
pairing of a generation are correct, various strategies have been adopted. For example, Yan (2016) <ref type="bibr" target="#b11">[12]</ref> proposes an iterative polishing schema, which refines the
pairing of a generation are correct, various strategies have been adopted. For example, Yan (2016) <ref type="bibr" target="#b11">[12]</ref> proposes an iterative polishing schema, which refines the
</head><p>Early works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> on Chinese poetry gener
rmula></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Early works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
pairing of a generation are correct, various strategies have been adopted. For example, Yan (2016) <ref type="bibr" target="#b11">[12]</ref> proposes an iterative polishing schema, which refines the
-c.org/ns/1.0"><head n="4">Related Work</head><p>Early works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target
满江红), Shuidiaogetou(水调歌头), etc .</p><p>Various methods e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> have been proposed to generate classical Chinese poetry. Howe
e text generation, a classical Chinese poem should normally meet both form and content requirements <ref type="bibr" target="#b0">[1]</ref>. The form requirements includes the regulations on the numbe ce of content throughout a poem. For example, <ref type="bibr" target="#b0">Yi et al. (2018)</ref>  <ref type="bibr" target="#b0">[1]</ref> propose a salientclue mechanism which automatically selects cluding Xijiangyue(西江 月), Manjianghong(满江红), Shuidiaogetou(水调歌头), etc .</p><p>Various methods e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> have been proposed to , some other works have been investigating the coherence of content throughout a poem. For example, <ref type="bibr" target="#b0">Yi et al. (2018)</ref>  <ref type="bibr" target="#b0">[1]</ref> propos
st salient characters from the so-far generated lines as a theme clue for generating the next line. <ref type="bibr" target="#b13">Yang et al. (2017)</ref>  <ref type="bibr" target="#b13">[14]</ref> a a theme clue for generating the next line. <ref type="bibr" target="#b13">Yang et al. (2017)</ref>  <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b12">Wang et al. (2016)</ref
of localized spectral filters on graphs <ref type="bibr" target="#b11">(Hammond et al., 2011;</ref><ref type="bibr" target="#b5">Defferrard et al., 2016)</ref>.</p></div> <div xmlns="http://www.tei-c er neighborhood). The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. <ref type="bibr" target="#b5">Defferrard et al. (2016)</ref> use this K-localized convolution to def pectral graph convolutional neural networks, introduced in Bruna et al. (2014) and later extended by<ref type="bibr" target="#b5">Defferrard et al. (2016)</ref> with fast localized convolutions. In co introduced to the original frameworks of<ref type="bibr" target="#b3">Bruna et al. (2014)</ref> and<ref type="bibr" target="#b5">Defferrard et al. (2016)</ref> that improve scalability and classifica

i.e. linear in the number of edges. Stochasticity in the training process is introduced via dropout <ref type="bibr" target="#b23">(Srivastava et al., 2014)</ref>. We leave memory-efficient extensions
spired by the skip-gram model <ref type="bibr" target="#b18">(Mikolov et al., 2013)</ref>. DeepWalk <ref type="bibr" target="#b20">(Perozzi et al., 2014)</ref> learns embeddings via the prediction of ype="bibr" target="#b1">(Belkin et al., 2006)</ref> and skip-gram based graph embeddings (DeepWalk) <ref type="bibr" target="#b20">(Perozzi et al., 2014)</ref>. We omit TSVM <ref type="bibr" target="# plied to the karate club network. These results are comparable to embeddings obtained from DeepWalk <ref type="bibr" target="#b20">(Perozzi et al., 2014)</ref>, which uses a more expensive unsupervise
/www.tei-c.org/ns/1.0"><head n="3.2">IMPLEMENTATION</head><p>In practice, we make use of TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> for an efficient GPU-based implementation<r ts on a GPU and on a CPU-only implementation<ref type="foot" target="#foot_3">4</ref> in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>. Figure <ref type="figure" target="#fig_1">
t O(N 2 ) complexity, limiting the range of possible applications. In a different yet related model,<ref type="bibr" target="#b19">Niepert et al. (2016)</ref> convert graphs locally into sequences tha
dern practices for recurrent neural network training to the original graph neural network framework.<ref type="bibr" target="#b7">Duvenaud et al. (2015)</ref> introduced a convolution-like propagation
, X and Z are defined as in Section 3.1.</p><p>We apply this model on Zachary's karate club network <ref type="bibr" target="#b28">(Zachary, 1977)</ref>. This graph contains 34 nodes, connected by 154 s et al., 2008)</ref>. See Figure <ref type="figure" target="#fig_2">3a</ref> for an illustration.  <ref type="bibr" target="#b28">(Zachary, 1977)</ref>, colors denote communities obtained via modular "fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Left: Zachary's karate club network<ref type="bibr" target="#b28">(Zachary, 1977)</ref>, colors denote communities obtained via modular
tion has shifted to models that learn graph embeddings with methods inspired by the skip-gram model <ref type="bibr" target="#b18">(Mikolov et al., 2013)</ref>. DeepWalk <ref type="bibr" target="#b20"
tion has shifted to models that learn graph embeddings with methods inspired by the skip-gram model <ref type="bibr" target="#b18">(Mikolov et al., 2013)</ref>. DeepWalk <ref type="bibr" target="#b20"
all feature vectors.</p><p>NELL NELL is a dataset extracted from the knowledge graph introduced in <ref type="bibr" target="#b4">(Carlson et al., 2010)</ref>.</p><p>A knowledge graph is a set of enti labeled nodes that are used for training divided by the total number of nodes in each dataset. NELL<ref type="bibr" target="#b4">(Carlson et al., 2010;</ref><ref type="bibr" target="#b27">Yang et al.
/expressing emotions and emotionally interacting with the interlocutors. In literature, Zhou et al. <ref type="bibr" target="#b45">[46]</ref> successfully build an emotional chat machine (ECM) that is enerate plausible emotional sentence without sacrificing grammatical fluency and semantic coherence <ref type="bibr" target="#b45">[46]</ref>. Hence, the response generation problem faces a significan tional factors, which are most related to our proposed conversation generation problem. Zhou et al. <ref type="bibr" target="#b45">[46]</ref> develop an Emotional Chat Machine (ECM) model using three " target="#b32">[33]</ref>, to evaluate our experimental results. In particular, we follow the work <ref type="bibr" target="#b45">[46]</ref> to train an emotion classifier for assigning emotional lab ifferent datasets, i.e., NLPCC 2013 2 and NLPCC 2014 3 emotion classification datasets by following <ref type="bibr" target="#b45">[46]</ref>, which contain 29, 417 manually annotated data in total, a o any emotion information, and rare emotion categories like fear are removed. In particular, unlike <ref type="bibr" target="#b45">[46]</ref> using solely one label for classification, we consider bot rget="#b35">[36]</ref>, the traditional Seq2seq model is adopted as one of our baselines.</p><p>ECM <ref type="bibr" target="#b45">[46]</ref>, as mentioned, ECM model is improper to directly be as the onal matrix (if used). The parameters of imemory and ememory in ECM are the same as the settings in <ref type="bibr" target="#b45">[46]</ref>. We use stochastic gradient descent (SGD) with mini-batch <note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Here we follow the work<ref type="bibr" target="#b45">[46]</ref>, where the emotion categories are {Angry, Disgust, Happy, o the detected post's emotion over EIPs.</p><p>Seq2seq-emb <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46]</ref>, Seq2seq with emotion embedding (Seq2seqemb) is also adopte
n sentence due to semantic sparsity. Psychological studies <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> demonstrate that human emotion is quite complex and one sen
f type="bibr" target="#b36">[37]</ref>. The length of hidden layer is set at 20, and we use AdaGrad <ref type="bibr" target="#b6">[7]</ref> to update the trainable parameters and the learning rate is
</ref>, topic-aware model <ref type="bibr" target="#b43">[44]</ref>. Besides, several persona-based <ref type="bibr" target="#b14">[15]</ref> models and identity-coherent models <ref type="bibr" targe
th the ability of emotional communication with humans are essential for enhancing user satisfaction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" targ
on with humans are essential for enhancing user satisfaction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. To this end, it is
get="#b13">[14]</ref> and enhanced beam-search based model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>. Several approaches are also proposed for specific tasks, s
𝑇 ′ ,<label>(2)</label></formula><p>where 𝑓 is a non-linear transformation of RNN cells (e.g., LSTM <ref type="bibr" target="#b8">[9]</ref> or GRU <ref type="bibr" target="#b4">[5]</ref>).</p><p>Then,
responses according to a pre-defined emotion category, and several similar efforts are also made by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>, such as <ref type ng the most frequent response's emotion to the detected post's emotion over EIPs.</p><p>Seq2seq-emb <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46]</ref>, Seq2seq with emot tional variational auto-encoder framework to generate responses conditioned on the given emojis. In <ref type="bibr" target="#b10">[11]</ref>, Huang et al. propose three different models that are capa
get="#b13">[14]</ref> and enhanced beam-search based model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>. Several approaches are also proposed for specific tasks, s
motion injection procedure, which is used to model the diversity of emotions. By following the work <ref type="bibr" target="#b39">[40]</ref>, we use a new encoder to encode 𝒙 for obtaining a sequence
mple task which can be easily extracted from who is 'standing with whom' (as suggested by Yu et al. <ref type="bibr" target="#b19">[20]</ref>), but one quickly finds that in more crowded situations wh ours can be identified. Second, we make improvements over the baseline method proposed by Yu et al. <ref type="bibr" target="#b19">[20]</ref> by formulating the problem as one of identifying dominant of F-formations was not explicitly addressed.</p><p>The closest work to ours was done by Yu et al. <ref type="bibr" target="#b19">[20]</ref>, who proposed a system that could track and discover group test data set was rather artificial as 23 people were asked to "mingle in a 3-group configuration" <ref type="bibr" target="#b19">[20]</ref> (p.1468). Three groups were indeed identified but given th mlns="http://www.tei-c.org/ns/1.0"><head n="3.1">F-formations as High Modularity</head><p>Yu et al. <ref type="bibr" target="#b19">[20]</ref> defined the task of identifying groups of people in terms measuring the affinity between people is to use their relative proximity, as proposed by Yu et al. <ref type="bibr" target="#b19">[20]</ref>. The symmetric distance function between person i and j is ed dominant set S (GC), as described in Sec. 6. We compare our results with the method of Yu et al. <ref type="bibr" target="#b19">[20]</ref>, that used modularity cut and proximity to create the affi ity cut and Kernighan-Lin refinement (MC+KL), though the performance in all cases was improved over <ref type="bibr" target="#b19">[20]</ref>. In addition to the performance of different methods, we a ="table" target="#tab_1">2</ref> shows that all our proposed methods out-performs that the baseline <ref type="bibr" target="#b19">[20]</ref>. Both fully manual methods with our proposed dominant set ne to all other methods, we see that using the basic modularity cut algorithm proposed by Yu et al. <ref type="bibr" target="#b19">[20]</ref> only just out performs it, indicating how poor modularity as presented by formulating the problem as one of finding dominant sets. Compared to modularity cut <ref type="bibr" target="#b19">[20]</ref>, significant and more stable performance improvements were to everyone else in the network. For the task of identifying spatially separated groups, Yu et al. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b2">3]</ref> have shown the effec
d tested in restricted environments where the number of participants is relatively small (2 people: <ref type="bibr" target="#b18">[19]</ref>, 4 people: <ref type="bibr" target="#b1">[2,</ref><ref typ
that between the internal nodes and those that are external to it. This differs from modularity cut <ref type="bibr" target="#b15">[16]</ref> which is a global optimisation method where partitions of d a system that could track and discover groups of interacting people. The modularity cut algorithm <ref type="bibr" target="#b15">[16]</ref> was applied to identify these groups from automatically ex graph by maximising the remaining modularity of the uncut edges. Modularity was proposed by Newman <ref type="bibr" target="#b15">[16]</ref> as a metric for clustering social networks. The modularity we will provide a brief introduction here. Further details can be found in Newman's original paper <ref type="bibr" target="#b15">[16]</ref>. Modularity cut was originally developed for social networ can only provide an approximation for the division, a step inspired by the Kernighan-Lin algorithm <ref type="bibr" target="#b15">[16]</ref> can be applied to try to maximise Q further. This involves finement is kept. This step has been used widely for network analysis with significant improvements <ref type="bibr" target="#b15">[16]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> be much closer to those that they wish to talk to. The SMEFO is defined as ?i = arccos(?(pi, fi)), <ref type="bibr" target="#b15">(16)</ref> where pi is the location of person i and ?(pi, fi) is the
that between the internal nodes and those that are external to it. This differs from modularity cut <ref type="bibr" target="#b15">[16]</ref> which is a global optimisation method where partitions of d a system that could track and discover groups of interacting people. The modularity cut algorithm <ref type="bibr" target="#b15">[16]</ref> was applied to identify these groups from automatically ex graph by maximising the remaining modularity of the uncut edges. Modularity was proposed by Newman <ref type="bibr" target="#b15">[16]</ref> as a metric for clustering social networks. The modularity we will provide a brief introduction here. Further details can be found in Newman's original paper <ref type="bibr" target="#b15">[16]</ref>. Modularity cut was originally developed for social networ can only provide an approximation for the division, a step inspired by the Kernighan-Lin algorithm <ref type="bibr" target="#b15">[16]</ref> can be applied to try to maximise Q further. This involves finement is kept. This step has been used widely for network analysis with significant improvements <ref type="bibr" target="#b15">[16]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> be much closer to those that they wish to talk to. The SMEFO is defined as ?i = arccos(?(pi, fi)), <ref type="bibr" target="#b15">(16)</ref> where pi is the location of person i and ?(pi, fi) is the
of participants is relatively small (2 people: <ref type="bibr" target="#b18">[19]</ref>, 4 people: <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>), though some studie the affinity between detected people. Other work which is close to ours is that of Brdiczka et al. <ref type="bibr" target="#b1">[2]</ref> who analysed speech activity to identify who was interacting
past them, for example. An F-formation is considered as a specific instance of a focused encounter <ref type="bibr" target="#b4">[5]</ref>, which we will describe in more detail in the next section. be significantly affected by the layout of a room (e.g. position of furniture) or how crowded it is <ref type="bibr" target="#b4">[5]</ref>. Body orientation also plays a role as it provides a prior o opportunity for participants to monitor one another's mutual perceivings"(p. 95). Ciolek and Kendon <ref type="bibr" target="#b4">[5]</ref> took this idea further by defining a focused interaction mor
of participants is relatively small (2 people: <ref type="bibr" target="#b18">[19]</ref>, 4 people: <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>), though some studie the affinity between detected people. Other work which is close to ours is that of Brdiczka et al. <ref type="bibr" target="#b1">[2]</ref> who analysed speech activity to identify who was interacting
target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18]</ref> during walking, which are readily measurable from the extra
unacquainted groups <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b0">1]</ref>. Unacquainted groups will interact with each other through av direction of motion <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18]</ref> during walking, which
target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18]</ref> during walking, which are readily measurable from the extra
target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18]</ref> during walking, which are readily measurable from the extra
evidence to be retrieved from Wikipedia.</p><p>We constructed a purpose-built dataset for this task <ref type="bibr" target="#b15">(Thorne et al., 2018)</ref> that contains 185,445 human-generated cla nce when constructing the dataset was the trade-off between annotation velocity and evidence recall <ref type="bibr" target="#b15">(Thorne et al., 2018)</ref>. Evidence selected by annotators was ofte ata was released through the FEVER website. 1 We used the reserved portion of the data presented in <ref type="bibr" target="#b15">Thorne et al. (2018)</ref>   </p></div> <div xmlns="http://www.tei-c. www.tei-c.org/ns/1.0"><head n="2.2">Scoring Metric</head><p>We used the scoring metric described in <ref type="bibr" target="#b15">Thorne et al. (2018)</ref> to evaluate the submissions. The FEVER sha pe="table" target="#tab_2">2</ref>). 19 of these teams scored higher than the baseline presented in <ref type="bibr" target="#b15">Thorne et al. (2018)</ref>. All participating teams were invited to s her individually or as a group, can be used as evidence. We retained the annotation guidelines from <ref type="bibr" target="#b15">Thorne et al. (2018)</ref> (see Sections A.7.1, A.7.3 and A.8 from th rom 86 submissions from 23 teams. 19 of these teams exceeded the score of the baseline presented in <ref type="bibr" target="#b15">Thorne et al. (2018)</ref>. For the teams which provided a system des
erse frequency weightings <ref type="bibr" target="#b0">(Arora et al., 2017)</ref>, ELMo embeddings <ref type="bibr" target="#b11">(Peters et al., 2018)</ref> and TF-IDF <ref type="bibr" target="#b13" n module. Both Ohio State and UNC-NLP report alternative token encodings: UNC-NLP report using ELMo <ref type="bibr" target="#b11">(Peters et al., 2018)</ref> and WordNet <ref type="bibr" target="#b9" en further extracted the top 3 sentences using cosine similarity between vectors obtained from Elmo <ref type="bibr" target="#b11">(Peters et al., 2018)</ref> sentence embeddings of the claim and the
2017)</ref>, ELMo embeddings <ref type="bibr" target="#b11">(Peters et al., 2018)</ref> and TF-IDF <ref type="bibr" target="#b13">(Salton et al., 1983)</ref>. UCL Machine Reading Group and Directed A
key difference between this task and other textual entailment and natural language inference tasks <ref type="bibr" target="#b6">(Dagan et al., 2009;</ref><ref type="bibr" target="#b2">Bowman et al.,
2017)</ref>, ELMo embeddings <ref type="bibr" target="#b11">(Peters et al., 2018)</ref> and TF-IDF <ref type="bibr" target="#b13">(Salton et al., 1983)</ref>. UCL Machine Reading Group and Directed A
2017)</ref>, ELMo embeddings <ref type="bibr" target="#b11">(Peters et al., 2018)</ref> and TF-IDF <ref type="bibr" target="#b13">(Salton et al., 1983)</ref>. UCL Machine Reading Group and Directed A
against Wikipedia page titles or article bodies. BUPT-NLPer report using S-MART for entity linking <ref type="bibr" target="#b16">(Yang and Chang, 2015)</ref> and the highest scor-ing team, UNC-NLP,
key difference between this task and other textual entailment and natural language inference tasks <ref type="bibr" target="#b6">(Dagan et al., 2009;</ref><ref type="bibr" target="#b2">Bowman et al.,
key difference between this task and other textual entailment and natural language inference tasks <ref type="bibr" target="#b6">(Dagan et al., 2009;</ref><ref type="bibr" target="#b2">Bowman et al.,
nce from a large textual corpus. Furthermore, in comparison to large-scale question answering tasks <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>, systems must reason about information that
key difference between this task and other textual entailment and natural language inference tasks <ref type="bibr" target="#b6">(Dagan et al., 2009;</ref><ref type="bibr" target="#b2">Bowman et al.,
information stored per BTB entry, the better the performance, provided the content reduces aliasing <ref type="bibr" target="#b16">[16]</ref>.</p><p>A large BTB tends to be more accurate, but the incr cache) and large amount of data is accessed slowly (L2, L3, main memory etc.). It was suggested by <ref type="bibr" target="#b16">[16]</ref> that multilevel BTBs may be able to strike a balance betwe s in different threads <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>A balance must be made among the number of entries,
s latencies, silicon area, and power consumption limit designers from using overly large structures <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>This paper branch in the first level, it is looked for in the second level which can then provide a prediction <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>. Other approaches of accessing the BTB.</p><p>Slower BTB access increases instruction sequence redirection penalties <ref type="bibr" target="#b10">[10]</ref>. It is becoming increasingly difficult to complete a BTB s at has a fast access time is subject to aliasing with older entries being overwritten by newer ones <ref type="bibr" target="#b10">[10]</ref>. Since the BTB typically stores only a portion of the bran ef type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" tar
nt advantages may be obtained when the BTB is as large as possible, with all other factors constant <ref type="bibr" target="#b4">[4]</ref>. Theoretically, to maximize performance, BTBs would be large as a tag, aliasing can occur among branches within a thread and among branches in different threads <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target
a particular branch. They are the same size and similar configuration as in the IBM zEnterprise 196 <ref type="bibr" target="#b15">[15]</ref>. The PHT contains 4,096 entries and is indexed based on th
server. Trace 9 is a commercial database server.</p><p>Traces 10 and 11 are the DayTrader benchmark <ref type="bibr" target="#b23">[23]</ref>. Traces 12 and 13 are the Informix [24] and Trade6 workloa
nism for looking up addresses and redirecting the flow of instructions to prevent sequencing issues <ref type="bibr" target="#b7">[7]</ref>. Studies have shown that a branch prediction scheme with a B
f type="bibr" target="#b8">8,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12]</ref>. The solutions are or in the second level which can then provide a prediction <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>. Other approaches use a larger, slower second level to prel
umption limit designers from using overly large structures <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>This paper describes the two level hierarchical bran ing cycle time and increasing storage structure size <ref type="bibr">[9]</ref>. Jimenez et. al. in <ref type="bibr" target="#b14">[14]</ref> showed that latency constraints must be considered when de
from branch prediction, especially as lower cycle times have necessitated increased pipeline depths <ref type="bibr" target="#b3">[3]</ref>.</p><p>Significant advantages may be obtained when the BTB i
hat a traditional branch prediction implementation possesses <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b10">10,</r cond level to preload entries into the smaller, faster level <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>Virtualization
hat a traditional branch prediction implementation possesses <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b10">10,</r cond level to preload entries into the smaller, faster level <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>Virtualization
s based on ReRAMs have been proposed (e.g., RPBFS <ref type="bibr" target="#b17">[18]</ref>, GraphR <ref type="bibr" target="#b0">[1]</ref>, and HyVE <ref type="bibr" target="#b18">[19]</ref>), achiev on, it still suffers from two main problems. (1) Heavy writing overheads. Previous work like GraphR <ref type="bibr" target="#b0">[1]</ref> converted the edge list into the adjacency matrix by sequent red for unweighted graph algorithms, leading to 3.77x lower EDP.</p><p>We take the design of GraphR <ref type="bibr" target="#b0">[1]</ref> as the baseline and compare it with GraphSAR in Figure <ref , v j , e i, j ) 5:</formula><p>end for 6: end while 7: return V To ensure the scalability, GraphR <ref type="bibr" target="#b0">[1]</ref> and HyVE <ref type="bibr" target="#b18">[19]</ref> divide a s subgraphs in the memory where these subgraphs are stored. Thus, GraphSAR is different from GraphR <ref type="bibr" target="#b0">[1]</ref> and HyVE <ref type="bibr" target="#b18">[19]</ref>, by both d on the hybrid-centric model. The order of blocks and edges in lists is the same as that in GraphR <ref type="bibr" target="#b0">[1]</ref>, where data are stored in the columnoriented order, leading ter of ReRAM cell model is from the same source <ref type="bibr" target="#b19">[20]</ref> in GraphR <ref type="bibr" target="#b0">[1]</ref> (read/write energy consumption: 1.08pJ/7.4pJ, <ref type="foo ially when writing data to ReRAMs and only performing computation once over these data (e.g, GraphR <ref type="bibr" target="#b0">[1]</ref>). Compared with GraphR, GraphSAR actually exploits the proce (GAS) model is used to describe different graph algorithms. In GAS, processing one vertex includes: <ref type="bibr" target="#b0">(1)</ref> Gathering values from incoming neighbors; (2) Applying gathe on-</head><p>vert analog data to digital data. We share ADC among bitlines, based on Previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div> <div xmln
ing systems have been put forward, including single PC-based <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" targe ated to updating the destination vertex.</p><p>The edge-centric model is first proposed by X-stream <ref type="bibr" target="#b2">[3]</ref> to ensure the locality of graph processing. The interval-blo
ing systems have been put forward, including single PC-based <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" targe ated to updating the destination vertex.</p><p>The edge-centric model is first proposed by X-stream <ref type="bibr" target="#b2">[3]</ref> to ensure the locality of graph processing. The interval-blo
(ReRAM) and ReRAM crossbars show huge potential in energy efficient processing-in-memory operations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, especially for ma
target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, cluster-based <ref type="bibr" target="#b5">[6,</ref><ref t graph partitioning method is widely used in previous systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Vertices are divided into disjointed intervals, and edges ar
hs is a promising way to improve the performance of GraphSAR and GraphR. Inspired by previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, that vertices app
uding single PC-based <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, cluster-based <ref t f graph processing. The interval-block graph partitioning method is widely used in previous systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Vertices are divided
"bibr" target="#b28">[29]</ref> from many ReRAM simulators <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. We modify the model of ReRAM cell in NVSim.</p><p>The para
arget="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, and processing-inmemory designs <ref type="bibr" target="
ormance of GraphSAR and GraphR. Inspired by previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, that vertices appear together in the original edge list us
is always less than N ed?es . Moreover, because of heavy overheads of 'Write-and-verify' operations <ref type="bibr" target="#b19">[20]</ref>, T wr ite is usually larger than T r ead in ReRAM crossbar y the model of ReRAM cell in NVSim.</p><p>The parameter of ReRAM cell model is from the same source <ref type="bibr" target="#b19">[20]</ref> in GraphR <ref type="bibr" target="#b0">[1]</ref> (read/wr . For multi-level ReRAM cells, we modify NVSim according to the parallel sensing scheme proposed in <ref type="bibr" target="#b19">[20]</ref>, which enables the 'Write-and-verify' process of accessing
pogation on just single hidden-layer feedforward neural networks. Recent attempts in this direction <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref> propose efficient r the search space through tunable parameters, in contrast to rigid search procedures in prior work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>. Consequently, our of nodes. We contrast the performance of node2vec with state-of-the-art feature learning algorithms <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>. We experiment wit odel, recent research established an analogy for networks by representing a network as a "document" <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>. The same way as a r shortcoming of prior work which fail to offer any flexibility in sampling of nodes from a network <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>. Our algorithm nod roceed by extending the Skip-gram architecture to networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>. We seek to optimize the following objective function, whic normalized Laplacian matrix of graph G as the feature vector representations for nodes. • DeepWalk <ref type="bibr" target="#b23">[24]</ref>: This approach learns d-dimensional feature representation lude other matrix factorization approaches which have already been shown to be inferior to DeepWalk <ref type="bibr" target="#b23">[24]</ref>. We also exclude a recent approach, GraRep <ref type="bibr the sampling procedure computationally efficient. We showed how random walks, also used in DeepWalk <ref type="bibr" target="#b23">[24]</ref>, allow the sampled nodes to be reused as neighborhoods for SION</head><p>Both DeepWalk and LINE can be seen as rigid search strategies over networks. DeepWalk <ref type="bibr" target="#b23">[24]</ref> proposes search using uniform random walks. The obvious li
de efficient using negative sampling <ref type="bibr" target="#b21">[22]</ref> and asynchronous SGD <ref type="bibr" target="#b25">[26]</ref>. Many ideas from prior work serve as useful pointers in ma
and their extensions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> optimize an objecti s have been proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. These methods suff
scover novel interactions between genes, and in social networks, it can identify real-world friends <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Any supervise
ructural equivalence) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref>. For instance, in Figure <ref type="figure" target="#fig_0" br" target="#b11">[12]</ref>. Under the homophily hypothesis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref> nodes that are highly interconnected and belong to similar
or to DeepWalk <ref type="bibr" target="#b23">[24]</ref>. We also exclude a recent approach, GraRep <ref type="bibr" target="#b5">[6]</ref>, that generalizes LINE to incorporate information from netwo
r supervised feature learning based on existing and novel graph-specific deep network architectures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta
or to DeepWalk <ref type="bibr" target="#b23">[24]</ref>. We also exclude a recent approach, GraRep <ref type="bibr" target="#b5">[6]</ref>, that generalizes LINE to incorporate information from netwo
s in networks often shuttle between two kinds of similarities: homophily and structural equivalence <ref type="bibr" target="#b11">[12]</ref>. Under the homophily hypothesis <ref type="bibr" target="#
et="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39]</ref>. These architecture
and their extensions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> optimize an objecti s have been proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. These methods suff
[2]</ref>). All of the models mentioned above are shallow networks (less than 5 layers). Kim et al. <ref type="bibr" target="#b11">[12]</ref> first introduced the residual architecture for training mu ly concatenate together, which leads to the underutilization of local features. In 2016, Kim et al. <ref type="bibr" target="#b11">[12]</ref> proposed a residual learning framework (Fig. <ref type="fi r module, we design a set of comparative experiments to compare the performance with residual block <ref type="bibr" target="#b11">[12]</ref>, dense block <ref type="bibr" target="#b23">[24]</ref>   < figDesc>Fig. 5. Quantitative comparison of three different feature extraction blocks (residual block<ref type="bibr" target="#b11">[12]</ref>, dense block<ref type="bibr" target="#b23">[24]</ref>, and
target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> were proposed. EDSR <ref type="bibr" target="#b8">[9]</ref> ><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> were proposed. EDSR <ref type="bibr" target="#b8">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on e reconstructed some classic SR models, such as SRCNN <ref type="bibr" target="#b0">[1]</ref>, EDSR <ref type="bibr" target="#b8">[9]</ref> and SRResNet <ref type="bibr" target="#b7">[8]</ref>. During <ref type="bibr" target="#b5">[6]</ref>, SRResNet <ref type="bibr" target="#b7">[8]</ref>, and EDSR <ref type="bibr" target="#b8">[9]</ref>. Unfortunately, these models become more and more deeper and CN <ref type="bibr" target="#b4">[5]</ref>, LapSRN <ref type="bibr" target="#b5">[6]</ref> and EDSR <ref type="bibr" target="#b8">[9]</ref>. For fair, we retrain most of these models (except for EDSR <ref type="bibr" target="#b8">[9]</ref>. For fair, we retrain most of these models (except for EDSR <ref type="bibr" target="#b8">[9]</ref>, the results of EDSR provided by their original papers).</p> t upscaling factors and test-datasets. It can be seen that our results are slightly lower than EDSR <ref type="bibr" target="#b8">[9]</ref>. But it is worth noting that EDSR <ref type="bibr" target="# slightly lower than EDSR <ref type="bibr" target="#b8">[9]</ref>. But it is worth noting that EDSR <ref type="bibr" target="#b8">[9]</ref> use  RGB channels for training, meanwhile, the data augment nwhile, the data augment methods are different.</p><p>To better illustrate the difference with EDSR <ref type="bibr" target="#b8">[9]</ref>, we show a comparison of model specifications in Table <ref show a comparison of model specifications in Table <ref type="table" target="#tab_3">3</ref>. EDSR <ref type="bibr" target="#b8">[9]</ref> is an outstanding model gained amazing results. However, it memory, space and datasets. In contrast, the specifications of our model is much smaller than EDSR <ref type="bibr" target="#b8">[9]</ref>, which makes it easier to reproduce and promote.</p><p>In Fi nts the upscaling factor) mixed training method is used in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and geometric selfensemble method is proposed in <ref type= 4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and geometric selfensemble method is proposed in <ref type="bibr" target="#b8">[9]</ref>. We believe that these training tricks can also improve our
target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" targe at, many SR models have been proposed, including DRCN <ref type="bibr" target="#b4">[5]</ref>, DRNN <ref type="bibr" target="#b6">[7]</ref>, LapSRN <ref type="bibr" target="#b5">[6]</ref>, SRResNet <r
other 200 images are from <ref type="bibr" target="#b14">[15]</ref>. And some methods take ImageNet <ref type="bibr" target="#b15">[16]</ref> as training dataset, since it contains richer samples. In
p>Nowadays, many feature extraction blocks have been proposed. The main idea of the inception block <ref type="bibr" target="#b12">[13]</ref> (Fig. <ref type="figure" target="#fig_0">1</ref>.(c)) is t
e choose five widely used benchmark datasets: Set5 <ref type="bibr" target="#b16">[17]</ref>, Set14 <ref type="bibr" target="#b17">[18]</ref>, BSDS100 <ref type="bibr" target="#b18">[19]</ref>, Ur-ban
t widely used training dataset in previous studies includes 291 images, of which 91 images are from <ref type="bibr" target="#b13">[14]</ref> and the other 200 images are from <ref type="bibr" target=
target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> were proposed. EDSR < ="bibr" target="#b8">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on SRResNet <ref type="bibr" target="#b7">[8]</ref> while enhanced the network by removing the normalization lay <ref type="bibr" target="#b0">[1]</ref>, EDSR <ref type="bibr" target="#b8">[9]</ref> and SRResNet <ref type="bibr" target="#b7">[8]</ref>. During the reconstruction experiments, we find most existin N <ref type="bibr" target="#b6">[7]</ref>, LapSRN <ref type="bibr" target="#b5">[6]</ref>, SRResNet <ref type="bibr" target="#b7">[8]</ref>, and EDSR <ref type="bibr" target="#b8">[9]</ref>. Unfortuna
e choose five widely used benchmark datasets: Set5 <ref type="bibr" target="#b16">[17]</ref>, Set14 <ref type="bibr" target="#b17">[18]</ref>, BSDS100 <ref type="bibr" target="#b18">[19]</ref>, Ur-ban
compare the performance with residual block <ref type="bibr" target="#b11">[12]</ref>, dense block <ref type="bibr" target="#b23">[24]</ref>   </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><hea rent feature extraction blocks (residual block<ref type="bibr" target="#b11">[12]</ref>, dense block<ref type="bibr" target="#b23">[24]</ref>, and MSRB(our)) on SISR. The green line represents our mod
or image restoration challenge. During testing, we choose five widely used benchmark datasets: Set5 <ref type="bibr" target="#b16">[17]</ref>, Set14 <ref type="bibr" target="#b17">[18]</ref>, BSDS100
target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" targe yers) and achieved great performance. After that, many SR models have been proposed, including DRCN <ref type="bibr" target="#b4">[5]</ref>, DRNN <ref type="bibr" target="#b6">[7]</ref>, LapSRN <ref t FSRCNN <ref type="bibr" target="#b2">[3]</ref>, VDSR <ref type="bibr" target="#b3">[4]</ref>, DR-CN <ref type="bibr" target="#b4">[5]</ref>, LapSRN <ref type="bibr" target="#b5">[6]</ref> and EDSR <re
ent performance in terms of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM <ref type="bibr" target="#b9">[10]</ref>) in the SISR problem. Nevertheless, all of these models ten
nt network to learn the mapping between LR and HR images so that a series of CNNs-based SISR models <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" targe as upscaled to HR space via an upsampling operator as bicubic. However, this method has been proved <ref type="bibr" target="#b1">[2]</ref> that it will add computational complexity and produce visibl RCNN <ref type="bibr" target="#b2">[3]</ref>) and Efficient Sub-pixel Convolutional Networks (ESPCN <ref type="bibr" target="#b1">[2]</ref>). All of the models mentioned above are shallow networks (le actor (x4), with no specific instructions given to migrate to other upscaling factors. PixelShuffle <ref type="bibr" target="#b1">[2]</ref> and deconvolutional layer are widely used in SISR tasks. As fig_4">4</ref>(ours)), which is a simple, efficient, and flexible structure. Thanks to pixelshuffle <ref type="bibr" target="#b1">[2]</ref>, our modules can be migrated to any upscaling factor with mi xSR <ref type="bibr" target="#b19">[20]</ref>, SRCNN <ref type="bibr" target="#b0">[1]</ref>, ESPCN <ref type="bibr" target="#b1">[2]</ref>, FSRCNN <ref type="bibr" target="#b2">[3]</ref>, VDSR <ref t
target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> were proposed. EDSR < ="bibr" target="#b8">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on SRResNet <ref type="bibr" target="#b7">[8]</ref> while enhanced the network by removing the normalization lay <ref type="bibr" target="#b0">[1]</ref>, EDSR <ref type="bibr" target="#b8">[9]</ref> and SRResNet <ref type="bibr" target="#b7">[8]</ref>. During the reconstruction experiments, we find most existin N <ref type="bibr" target="#b6">[7]</ref>, LapSRN <ref type="bibr" target="#b5">[6]</ref>, SRResNet <ref type="bibr" target="#b7">[8]</ref>, and EDSR <ref type="bibr" target="#b8">[9]</ref>. Unfortuna
target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> were proposed. EDSR < ="bibr" target="#b8">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on SRResNet <ref type="bibr" target="#b7">[8]</ref> while enhanced the network by removing the normalization lay <ref type="bibr" target="#b0">[1]</ref>, EDSR <ref type="bibr" target="#b8">[9]</ref> and SRResNet <ref type="bibr" target="#b7">[8]</ref>. During the reconstruction experiments, we find most existin N <ref type="bibr" target="#b6">[7]</ref>, LapSRN <ref type="bibr" target="#b5">[6]</ref>, SRResNet <ref type="bibr" target="#b7">[8]</ref>, and EDSR <ref type="bibr" target="#b8">[9]</ref>. Unfortuna
target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" targe yers) and achieved great performance. After that, many SR models have been proposed, including DRCN <ref type="bibr" target="#b4">[5]</ref>, DRNN <ref type="bibr" target="#b6">[7]</ref>, LapSRN <ref t FSRCNN <ref type="bibr" target="#b2">[3]</ref>, VDSR <ref type="bibr" target="#b3">[4]</ref>, DR-CN <ref type="bibr" target="#b4">[5]</ref>, LapSRN <ref type="bibr" target="#b5">[6]</ref> and EDSR <re
hich 91 images are from <ref type="bibr" target="#b13">[14]</ref> and the other 200 images are from <ref type="bibr" target="#b14">[15]</ref>. And some methods take ImageNet <ref type="bibr" target="#
×64 and an epoch having 1000 iterations of back-propagation. We train our model with ADAM optimizer <ref type="bibr" target="#b21">[22]</ref> by setting the learning rate lr = 0.0001. In our final mod
NNs-based SISR models <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe roduce excessively smooth textures. Now, a variety of loss functions have been proposed such as VGG <ref type="bibr" target="#b3">[4]</ref> function and Charbonnier Penalty function <ref type="bibr" t ESPCN <ref type="bibr" target="#b1">[2]</ref>, FSRCNN <ref type="bibr" target="#b2">[3]</ref>, VDSR <ref type="bibr" target="#b3">[4]</ref>, DR-CN <ref type="bibr" target="#b4">[5]</ref>, LapSRN <ref mple, multi-scale (the scale here represents the upscaling factor) mixed training method is used in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and geometric self
efficient, and can easily migrate to any upscaling factors.</p><p>We train our models on the DIV2K <ref type="bibr" target="#b10">[11]</ref> dataset without special weight initialization method or ot b15">[16]</ref> as training dataset, since it contains richer samples. In our work, we choose DIV2K <ref type="bibr" target="#b10">[11]</ref> as our training dataset, a new highquality image dataset f
target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" targe yers) and achieved great performance. After that, many SR models have been proposed, including DRCN <ref type="bibr" target="#b4">[5]</ref>, DRNN <ref type="bibr" target="#b6">[7]</ref>, LapSRN <ref t FSRCNN <ref type="bibr" target="#b2">[3]</ref>, VDSR <ref type="bibr" target="#b3">[4]</ref>, DR-CN <ref type="bibr" target="#b4">[5]</ref>, LapSRN <ref type="bibr" target="#b5">[6]</ref> and EDSR <re
0"><head n="2.6.1">Implementation</head><p>We implement the neural network using the torch7 library <ref type="bibr" target="#b7">(Collobert et al., 2011a)</ref>. Training and inference are done on a

ngs<ref type="foot" target="#foot_1">3</ref> trained on 6 billion words from Wikipedia and Web text <ref type="bibr" target="#b26">(Pennington et al., 2014</ref>) and Google's word2vec embeddings<ref embeddings trained on in-domain text may perform better, we also used the publicly available GloVe <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref> program and an in-house re-implementa word embeddings. As word embedding quality depends on hyper-parameter choice during their training <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref>, and also, in our NER neural network,
ngs<ref type="foot" target="#foot_1">3</ref> trained on 6 billion words from Wikipedia and Web text <ref type="bibr" target="#b26">(Pennington et al., 2014</ref>) and Google's word2vec embeddings<ref embeddings trained on in-domain text may perform better, we also used the publicly available GloVe <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref> program and an in-house re-implementa word embeddings. As word embedding quality depends on hyper-parameter choice during their training <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref>, and also, in our NER neural network,
beddings on massive amounts of data <ref type="bibr" target="#b6">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013)</ref> and neural network training algorithms pe c embeddings<ref type="foot" target="#foot_2">4</ref> trained on 100 billion words from Google News <ref type="bibr" target="#b23">(Mikolov et al., 2013)</ref>.</p></div> <div xmlns="http://www.tei-c. program and an in-house re-implementation<ref type="foot" target="#foot_3">5</ref> of the word2vec <ref type="bibr" target="#b23">(Mikolov et al., 2013)</ref> program to train word embeddings on Wiki implementation of word2vec and its performance on the word analogy task was higher than reported by<ref type="bibr" target="#b23">Mikolov et al. (2013)</ref>.</note> 			<note xmlns="http://www.tei-c.
NER. While we considered using character-level bi-directional LSTMs, which was recently proposed by <ref type="bibr" target="#b20">Ling et al. (2015)</ref> for POStagging, preliminary evaluation shows ments, and that we employ word embeddings, which is much more important in NER than in POS tagging. <ref type="bibr" target="#b20">Ling et al. (2015)</ref> used both word-and character-level BLSTMs to
speech recognition <ref type="bibr" target="#b13">(Graves et al., 2013)</ref>, machine translation <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>, and language modeling <ref type="bibr" targe
rger but less-studied OntoNotes 5.0 dataset <ref type="bibr" target="#b16">(Hovy et al., 2006;</ref><ref type="bibr" target="#b29">Pradhan et al., 2013)</ref>. Table <ref type="table">2</ref> gives an


, machine translation <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>, and language modeling <ref type="bibr" target="#b22">(Mikolov et al., 2011)</ref>. The long-short term memory (LSTM) unit
echniques to mitigate this issue include multi-task learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and pre-trained components <ref type="bibr" target="#b10">[1 both fully supervised data and also weakly supervised data, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> use multi-task learning to train the ST model jointly with t g and multi-task learning as proposed in previous literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ
ype="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and pre-trained components <ref type="bibr" target="#b10">[11]</ref> in order to utilize weakly supervised data, i.e. speech-to ing so, both of them achieved better performance with the end-to-end model than the cascaded model. <ref type="bibr" target="#b10">[11]</ref> conducts experiments on a larger 236 hour English-to-Frenc n previous literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref> in order to improve
"#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and MT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target=
erformance of an end-to-end ST model. Synthetic data has also been used to improve ASR performance. <ref type="bibr" target="#b16">[17]</ref> builds a cycle chain between TTS and ASR models, in which
n typically used a cascade of an ASR model and an MT model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, giving the MT mode
vements in ASR <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and MT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target
r" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> tasks. These successes naturally led to attempts to construct
26]</ref>, except that we use a Griffin-Lim <ref type="bibr" target="#b27">[28]</ref> vocoder as in <ref type="bibr" target="#b28">[29]</ref> which has significantly lower cost, but results in reduced
n.</p><p>Incorporating recent advances in TTS to introduce more natural prosody and style variation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" ta
="#b26">[27]</ref> from <ref type="bibr" target="#b25">[26]</ref>, except that we use a Griffin-Lim <ref type="bibr" target="#b27">[28]</ref> vocoder as in <ref type="bibr" target="#b28">[29]</ref> wh
in which the output from one model is used to help the training of the other. Instead of using TTS, <ref type="bibr" target="#b17">[18]</ref>  The MT synthetic data in this work helps the system in a
th close to human naturalness <ref type="bibr" target="#b23">[24]</ref>, in varied speakers' voices <ref type="bibr" target="#b24">[25]</ref>, create novel voices by sampling from a continuous speaker
</head><p>Early work on speech translation typically used a cascade of an ASR model and an MT model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" ta
="#b1">2]</ref> and MT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target= ns/1.0"><head>Fine-tuning set</head><p>In-domain Out-of-domain Real + one-speaker TTS synthetic 59. <ref type="bibr" target="#b4">5</ref> 19.5 Only one-speaker TTS synthetic 38.5 13.8</p><p>Table <ref
in which the output from one model is used to help the training of the other. Instead of using TTS, <ref type="bibr" target="#b17">[18]</ref>  The MT synthetic data in this work helps the system in a
in which the output from one model is used to help the training of the other. Instead of using TTS, <ref type="bibr" target="#b17">[18]</ref>  The MT synthetic data in this work helps the system in a
o-sequence modeling have led to dramatic improvements in ASR <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and MT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr "#b21">[22]</ref> containing 16K tokens.</p><p>ASR model: Our ASR model follows the architecture of <ref type="bibr" target="#b1">[2]</ref>. We use a 5 layer bidirectional LSTM encoder, with cell size peech corpora by adding varying degrees of background noise and reverberation in the same manner as <ref type="bibr" target="#b1">[2]</ref>. The WPM shared among all models is trained with the 70M MT
TTS model</head><p>Recent TTS systems are able to synthesize speech with close to human naturalness <ref type="bibr" target="#b23">[24]</ref>, in varied speakers' voices <ref type="bibr" target="#b24" de speaker variation, we train models using data synthesized with the single speaker TTS model from <ref type="bibr" target="#b23">[24]</ref>. This model generates more natural speech than the multi-s
d to attempts to construct end-to-end speech-to-text translation systems as a single neural network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Such end-to-end syste and uncertainties from the ASR. Recent work has focused on training end-to-end ST in a single model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In order to uti speech translation was a requirement when no direct parallel training data is available, such as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. In contrast, we sho
The MT synthetic data in this work helps the system in a manner similar to knowledge distillation <ref type="bibr" target="#b19">[20]</ref>, since the network is trained to predict outputs from a pr Translate service to obtain such translations. This procedure is similar to knowledge distillation <ref type="bibr" target="#b19">[20]</ref>, except that it uses the final predictions as training tar
vements in ASR <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and MT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target
erformance of an end-to-end ST model. Synthetic data has also been used to improve ASR performance. <ref type="bibr" target="#b16">[17]</ref> builds a cycle chain between TTS and ASR models, in which
nformation retrieval <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b63">64]</ref>. A survey on the at
f type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b60">61]</ref>. Recently, Ai et al. <ref type="bibr" target="#b0">[1]</ref> have presented a personalized product search method, which f roduct Search 19:5 for products is also critical to persuade users to purchase. Recently, Ai et al. <ref type="bibr" target="#b0">[1]</ref> proposed a personalized product search model and took into c hood Model <ref type="bibr" target="#b68">[69]</ref> and Extended Query Likelihood with User Models <ref type="bibr" target="#b0">[1]</ref> and (2) representation learning approaches based on latent s nt Semantic Entity <ref type="bibr" target="#b58">[59]</ref> and Hierarchical Embedding Model (HEM) <ref type="bibr" target="#b0">[1]</ref>. It is worth noting that the recently proposed HEM is the st h User Models (UQL). This model is first introduced to the personalized product search by Ai et al. <ref type="bibr" target="#b0">[1]</ref>. Specifically, let U be the set of the most frequent words < 2 ? i ? 4}.</formula><p>Hierarchical Embedding Model (HEM). This model (HEM) proposed in Reference <ref type="bibr" target="#b0">[1]</ref> is the state-of-the-art approach for the personalized produc oduct as the query in retrieval. Based on this observation and following the strategy of References <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b58">59]</ref>, for each product a
mentioning that our model is applicable to many other scenarios, such as personalized movie search <ref type="bibr" target="#b44">[45]</ref> and academic article search <ref type="bibr" target="#b56"
ly model the relevance generation process and capture the important IR characteristics. Pang et al. <ref type="bibr" target="#b41">[42]</ref> extended the method in Reference <ref type="bibr" target="
et="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b67">68]</ref> focus on the click
that the long-term user preference is stable yet in fact, it changes over time slightly and slowly <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b60">61]</ref>. Recently, Ai et a fined ontology of semantic concepts to construct the short-term user profile; methods in References <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b60">61]</ref> determine the exte
own its efficiency in various tasks such as image captioning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b65">66]</ref>, visual question answering <ref type="bibr" target="#b64">[
well recognized that there are two types of user preferences <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b62">63]</ref>: long-term ones and short-term ones. The former refers to t ent events, such as new product release, season change and special personal occasions like birthday <ref type="bibr" target="#b62">[63]</ref>, which can be inferred from the user's recently purchased
ct, deep learning has been successfully applied in recommender systems to model the user preference <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" targ
r Settings. In the training procedure of ALSTP, the parameters are initialized by the xavier method <ref type="bibr" target="#b19">[20]</ref> and then optimized with the standard Stochastic Gradient D
ing, and Electronics. Following the strategy in References <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref>, we extracted the users' product purchasing behaviors based
t="#b40">[41,</ref><ref type="bibr" target="#b50">51]</ref> also share the similar idea. Guo et al. <ref type="bibr" target="#b21">[22]</ref> pointed out that the aforementioned methods only focus on acteristics. Pang et al. <ref type="bibr" target="#b41">[42]</ref> extended the method in Reference <ref type="bibr" target="#b21">[22]</ref> to better capture the intrinsic relevance and simulate the
del the long-term user preference based on the user's overall search logs. Approaches in References <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b55">56]</ref> apply a probabilis
dback equals to that of the query-product pairs. Query Extraction. As Rowley described in Reference <ref type="bibr" target="#b49">[50]</ref>, a typical scenario of user searching a product is to use
the first category <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b54">55]</ref> capture the short-term user preference from search sessions a session and how to learn the user preference in a session. To solve these problems, Sriram et al. <ref type="bibr" target="#b54">[55]</ref> used temporal closeness and probabilistic similarities bet
purchase, we adopt a Recurrent Neural Network (RNN) model equipped with Gated Recurrent Units (GRU) <ref type="bibr" target="#b10">[11]</ref> to model the short-term user preference, as it has been su rget="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>GRU was proposed in Reference <ref type="bibr" target="#b10">[11]</ref>. The activation of GRU is a linear interpolation between t
recent years, researchers have attempted to extract information from reviews to represent products <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
f type="bibr" target="#b26">[27]</ref>, ARC-2 <ref type="bibr" target="#b26">[27]</ref>, Match-SRNN <ref type="bibr" target="#b59">[60]</ref>, and MatchPyramid <ref type="bibr" target="#b40">[41,</ref
ntextual structures of entities (queries and documents). Other semantic matching methods like ARC-1 <ref type="bibr" target="#b26">[27]</ref>, ARC-2 <ref type="bibr" target="#b26">[27]</ref>, Match-SR ments). Other semantic matching methods like ARC-1 <ref type="bibr" target="#b26">[27]</ref>, ARC-2 <ref type="bibr" target="#b26">[27]</ref>, Match-SRNN <ref type="bibr" target="#b59">[60]</ref>, and
et="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b67">68]</ref> focus on the click
rt-term one changes more frequently and drastically.</p><p>Traditional approaches to product search <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" t can be hard for the search engine system. Some efforts have been dedicated to solving this problem <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" t tion from the entity space and the connection between the queries and product entities. Duan et al. <ref type="bibr" target="#b16">[17]</ref> noticed it and learned the query intent representation col
apture the intrinsic relevance and simulate the human judgment process. In addition, Borisov et al. <ref type="bibr" target="#b3">[4]</ref> introduced a neural click model to better understand the use
to represent products <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref> by using representation learning techniques (e.g., word2vec
eference Modeling for Personalized Product Search 19:3 search model. Methods in the second category <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b55">56]</ref> model the long-ter ization has been well recognized as an important way to improve the search experience in web search <ref type="bibr" target="#b35">[36]</ref>. Instead of giving a complete review on the personalized w [49]</ref> analyzed the long-term query logs to learn more about the user behavior. Matthijs et al. <ref type="bibr" target="#b35">[36]</ref> adopted NLP techniques, such as web page structure parsing
eference Modeling for Personalized Product Search 19:3 search model. Methods in the second category <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b55">56]</ref> model the long-ter ization has been well recognized as an important way to improve the search experience in web search <ref type="bibr" target="#b35">[36]</ref>. Instead of giving a complete review on the personalized w [49]</ref> analyzed the long-term query logs to learn more about the user behavior. Matthijs et al. <ref type="bibr" target="#b35">[36]</ref> adopted NLP techniques, such as web page structure parsing
type="bibr" target="#b1">[2]</ref> and information retrieval <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" tar ism on user preference modeling. To name just a few, in the field of recommender systems, Li et al. <ref type="bibr" target="#b33">[34]</ref> introduced a neural attentive model to the session-based r sfully applied in the session-based product recommendation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>GRU was proposed in Reference <ref type="bibr" targe
of clicks providing a strong signal of a user's interest in an item, the methods in the first line <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta
rt-term one changes more frequently and drastically.</p><p>Traditional approaches to product search <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" t can be hard for the search engine system. Some efforts have been dedicated to solving this problem <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" t tion from the entity space and the connection between the queries and product entities. Duan et al. <ref type="bibr" target="#b16">[17]</ref> noticed it and learned the query intent representation col
with different sizes: Phones, Toys, Clothing, and Electronics. Following the strategy in References <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref>, we extracted the
h the queries and documents, and then build the interaction between them. For example, Huang et al. <ref type="bibr" target="#b27">[28]</ref> leveraged a deep neural network (DNN) to project the queri
purchase, we adopt a Recurrent Neural Network (RNN) model equipped with Gated Recurrent Units (GRU) <ref type="bibr" target="#b10">[11]</ref> to model the short-term user preference, as it has been su rget="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>GRU was proposed in Reference <ref type="bibr" target="#b10">[11]</ref>. The activation of GRU is a linear interpolation between t
h the queries and documents, and then build the interaction between them. For example, Huang et al. <ref type="bibr" target="#b27">[28]</ref> leveraged a deep neural network (DNN) to project the queri
well recognized that there are two types of user preferences <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b62">63]</ref>: long-term ones and short-term ones. The former refers to t ent events, such as new product release, season change and special personal occasions like birthday <ref type="bibr" target="#b62">[63]</ref>, which can be inferred from the user's recently purchased
f type="bibr" target="#b2">[3]</ref>, it seems worthwhile to be unbiased, including negative deltas <ref type="bibr" target="#b3">[4]</ref> as well. 4) Matrices that are too sparse or empty (mcf_s1536
>[1]</ref>, that uses deltas instead of addresses to build more general models (originally for TLBs <ref type="bibr" target="#b1">[2]</ref>). In such cases, the acquired knowledge is applied to other s (such as a fully-associative or set-associative cache) usually employ a Least Recently Used (LRU) <ref type="bibr" target="#b1">[2]</ref> (or approximations <ref type="bibr" target="#b4">[5]</ref>), s 1, 1, 2, 1, 3, 1, 1, 2, 1, 3, ..., the transitions (1, 1), <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> and <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" t
ually employ a Least Recently Used (LRU) <ref type="bibr" target="#b1">[2]</ref> (or approximations <ref type="bibr" target="#b4">[5]</ref>), or a First-In First-Out (FIFO) replacement policy, which a y 'unexpected', sometimes temporary, page transitions. There are similar approaches in related work <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>. This does not modi 0 to 63).</p><p>? NRU bit: This bit is used for approximating the LRU replacement policy with 1 bit <ref type="bibr" target="#b4">[5]</ref>, by always evicting the Not-Recently Used (NRU) block.</p></ comparison is enough to point to the next layer. For a medium prefetch degree, the recursive lookup <ref type="bibr" target="#b4">[5]</ref> remains relatively efficient, although allowing a delay coul fetch degree, could have a performance overhead. This does not happen with multiple-delta histories <ref type="bibr" target="#b4">[5]</ref>. However, multiple-delta history matching could be negativel
>[1]</ref>, that uses deltas instead of addresses to build more general models (originally for TLBs <ref type="bibr" target="#b1">[2]</ref>). In such cases, the acquired knowledge is applied to other s (such as a fully-associative or set-associative cache) usually employ a Least Recently Used (LRU) <ref type="bibr" target="#b1">[2]</ref> (or approximations <ref type="bibr" target="#b4">[5]</ref>), s 1, 1, 2, 1, 3, 1, 1, 2, 1, 3, ..., the transitions (1, 1), <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> and <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" t
ubsequent accesses. Distance prefetching is a generalisation of the common Markov model prefetchers <ref type="bibr" target="#b0">[1]</ref>, that uses deltas instead of addresses to build more general short repeating delta patterns, such as 1, 1, 2, 1, 3, 1, 1, 2, 1, 3, ..., the transitions (1, 1), <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> and <ref type="bibr" t ansitions (1, 1), <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> and <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3)</ref> would yield an equal p
ually employ a Least Recently Used (LRU) <ref type="bibr" target="#b1">[2]</ref> (or approximations <ref type="bibr" target="#b4">[5]</ref>), or a First-In First-Out (FIFO) replacement policy, which a y 'unexpected', sometimes temporary, page transitions. There are similar approaches in related work <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>. This does not modi 0 to 63).</p><p>? NRU bit: This bit is used for approximating the LRU replacement policy with 1 bit <ref type="bibr" target="#b4">[5]</ref>, by always evicting the Not-Recently Used (NRU) block.</p></ comparison is enough to point to the next layer. For a medium prefetch degree, the recursive lookup <ref type="bibr" target="#b4">[5]</ref> remains relatively efficient, although allowing a delay coul fetch degree, could have a performance overhead. This does not happen with multiple-delta histories <ref type="bibr" target="#b4">[5]</ref>. However, multiple-delta history matching could be negativel
Proposal L1 &amp; L2' and KPCP over the single-core runs with non-prefetch (i.e. ?(IPCi/IPCalone_i) <ref type="bibr" target="#b7">[8]</ref>). It is clear that the multi-level prefetcher performs gener
so give the opportunity of phased-out prominent deltas to be evicted quickly (slight resemblance in <ref type="bibr" target="#b5">[6]</ref>). Each block in a set contains the next delta alongside a co by the page address. Page Tag (10), Delta Prev <ref type="bibr" target="#b6">(7)</ref>, Offset Prev <ref type="bibr" target="#b5">(6)</ref>, NRU bit (1) ? Page tag: to identify the page and distinguis
>[1]</ref>, that uses deltas instead of addresses to build more general models (originally for TLBs <ref type="bibr" target="#b1">[2]</ref>). In such cases, the acquired knowledge is applied to other s (such as a fully-associative or set-associative cache) usually employ a Least Recently Used (LRU) <ref type="bibr" target="#b1">[2]</ref> (or approximations <ref type="bibr" target="#b4">[5]</ref>), s 1, 1, 2, 1, 3, 1, 1, 2, 1, 3, ..., the transitions (1, 1), <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> and <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" t
next line/sequential prefetcher).</p><p>3) Instead of only supporting a limited coverage of deltas <ref type="bibr" target="#b2">[3]</ref>, it seems worthwhile to be unbiased, including negative delt e the performance of our prefetcher to two state-of-the art prefetchers, the Best-Offset Prefetcher <ref type="bibr" target="#b2">[3]</ref> (BOP) and the prefetcher from KPC <ref type="bibr" target="# remains relatively efficient, although allowing a delay could also prove beneficial for timeliness <ref type="bibr" target="#b2">[3]</ref>.</p><p>According to the use case, many parameters that impac et="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> and <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3)</ref> would yield an equal probability. This in combination with ot
next line/sequential prefetcher).</p><p>3) Instead of only supporting a limited coverage of deltas <ref type="bibr" target="#b2">[3]</ref>, it seems worthwhile to be unbiased, including negative delt e the performance of our prefetcher to two state-of-the art prefetchers, the Best-Offset Prefetcher <ref type="bibr" target="#b2">[3]</ref> (BOP) and the prefetcher from KPC <ref type="bibr" target="# remains relatively efficient, although allowing a delay could also prove beneficial for timeliness <ref type="bibr" target="#b2">[3]</ref>.</p><p>According to the use case, many parameters that impac et="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> and <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3)</ref> would yield an equal probability. This in combination with ot
CAREER Award CCR -0133777.</p><p>technology, the larger the cache, the slower the cache will become <ref type="bibr" target="#b1">[2]</ref>, and larger caches increase the cost of manufacturing. Anoth he. An inclusive cache system implies that the contents of the L1 cache be a subset of the L2 cache <ref type="bibr" target="#b1">[2]</ref>. This decreases the effective cache capacity available for u
are small, high-speed buffer memories, which contain the most recently used portions of main memory <ref type="bibr" target="#b0">[1]</ref>. For on-chip cache, the cycle time of a small cache can matc
el verification was performed through deterministic benchmarks, the procedure of which is available <ref type="bibr" target="#b13">[15]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
el verification was performed through deterministic benchmarks, the procedure of which is available <ref type="bibr" target="#b13">[15]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
are small, high-speed buffer memories, which contain the most recently used portions of main memory <ref type="bibr" target="#b0">[1]</ref>. For on-chip cache, the cycle time of a small cache can matc
are small, high-speed buffer memories, which contain the most recently used portions of main memory <ref type="bibr" target="#b0">[1]</ref>. For on-chip cache, the cycle time of a small cache can matc
www.tei-c.org/ns/1.0"><head n="3.3">Methodology and Benchmarks</head><p>10 benchmarks from SPEC2000 <ref type="bibr" target="#b6">[7]</ref> were used in the experiments presented. These benchmarks wer
www.tei-c.org/ns/1.0"><head n="3.3">Methodology and Benchmarks</head><p>10 benchmarks from SPEC2000 <ref type="bibr" target="#b6">[7]</ref> were used in the experiments presented. These benchmarks wer


are small, high-speed buffer memories, which contain the most recently used portions of main memory <ref type="bibr" target="#b0">[1]</ref>. For on-chip cache, the cycle time of a small cache can matc
arge in order to express the huge number of interest profiles at Tmall. Deep Interest Network (DIN) <ref type="bibr" target="#b29">[30]</ref> makes the user representation vary over different items wi ">[3]</ref>. Besides the industrial applications proposed by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>, various types of deep models have gained significant atten
ant attention. Neural Collaborative Filtering (NCF) <ref type="bibr" target="#b8">[9]</ref>, DeepFM <ref type="bibr" target="#b6">[7]</ref> and Deep Matrix Factorization Models (DMF) <ref type="bibr"
f distributed representation learning, user embeddings obtained by neural networks are widely used. <ref type="bibr" target="#b3">[4]</ref> employs RNN-GRU to learn user embeddings from the temporal o
ways. Collaborative filtering-based methods represent user interests by historical interacted items <ref type="bibr" target="#b20">[21]</ref> or hidden factors <ref type="bibr" target="#b14">[15]</ref ble user preference as vectors composed of interested items <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>, keywords <ref type="bibr" target="#b5">[6]</ref> and topic
the most successful deep learning method used for industrial recommendation systems.</p><p>• MaxMF <ref type="bibr" target="#b24">[25]</ref> The method introduces a highly scalable method for learnin
ctors. For example, the deep neural network proposed for YouTube video recommendation (YouTube DNN) <ref type="bibr" target="#b4">[5]</ref> represents each user by one fixed-length vector transformed on is made based on compatibilities between hidden factors of users and target items. • YouTube DNN <ref type="bibr" target="#b4">[5]</ref> As mentioned above, YouTube DNN is one of the most successfu algorithms <ref type="bibr" target="#b2">[3]</ref>. Besides the industrial applications proposed by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>, various types of de
nterests by historical interacted items <ref type="bibr" target="#b20">[21]</ref> or hidden factors <ref type="bibr" target="#b14">[15]</ref>, which suffer from sparsity problem or computationally dem
ef type="bibr" target="#b20">21]</ref>, keywords <ref type="bibr" target="#b5">[6]</ref> and topics <ref type="bibr" target="#b27">[28]</ref>. As the emergence of distributed representation learning,
ly used in RS. Traditional methods assemble user preference as vectors composed of interested items <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>, keywords <ref type
ant attention. Neural Collaborative Filtering (NCF) <ref type="bibr" target="#b8">[9]</ref>, DeepFM <ref type="bibr" target="#b6">[7]</ref> and Deep Matrix Factorization Models (DMF) <ref type="bibr"
ly used in RS. Traditional methods assemble user preference as vectors composed of interested items <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>, keywords <ref type
ype="bibr" target="#b9">Daumé III, 2009;</ref><ref type="bibr" target="#b5">Coke et al., 2016;</ref><ref type="bibr" target="#b22">Littell et al., 2017)</ref>.</p><p>In this study, we examine whether h feature vectors from previous work based on the genetic and geographic distance between languages <ref type="bibr" target="#b22">(Littell et al., 2017)</ref>. Results show that the extracted represe up</head><p>Typology Database: To perform our analysis, we use the URIEL language typology database <ref type="bibr" target="#b22">(Littell et al., 2017)</ref>, which is a collection of binary feature not necessarily require pre-existing knowledge of the typological features in the language at hand, <ref type="bibr" target="#b22">Littell et al. (2017)</ref> have proposed a method for inferring typo
fically engineered alignment-based models <ref type="bibr" target="#b21">(Lewis and Xia, 2008;</ref><ref type="bibr" target="#b30">Östling, 2015;</ref><ref type="bibr" target="#b5">Coke et al., 2016)< ("S "), phonology ("P "), and inventory ("I ") classes.  <ref type="bibr" target="#b21">2008;</ref><ref type="bibr" target="#b30">Östling, 2015;</ref><ref type="bibr" target="#b5">Coke et al., 2016)<
common in Southeast Asia (Anderson, 2013), and lateral consonants are uncommon in the Amazon Basin <ref type="bibr" target="#b23">(Maddieson, 2013a)</ref>. Since these are also regions with a particu
br" target="#b34">(Shi et al., 2016;</ref><ref type="bibr" target="#b19">Kuncoro et al., 2017;</ref><ref type="bibr" target="#b2">Belinkov et al., 2017)</ref> showing that syntactic knowledge can be e
ng and loanword prediction <ref type="bibr" target="#b37">(Tsvetkov et al., 2016)</ref>, POStagging <ref type="bibr" target="#b38">(Zhang et al., 2012)</ref>, and machine translation <ref type="bibr"
er databases, from known entries <ref type="bibr" target="#b10">(Daumé III and Campbell, 2007;</ref><ref type="bibr" target="#b9">Daumé III, 2009;</ref><ref type="bibr" target="#b5">Coke et al., 2016;
f type="bibr" target="#b11">(Dryer and Haspelmath, 2013)</ref>, has proven useful in many NLP tasks <ref type="bibr" target="#b29">(O'Horan et al., 2016)</ref>, such as multilingual dependency parsing
ng and loanword prediction <ref type="bibr" target="#b37">(Tsvetkov et al., 2016)</ref>, POStagging <ref type="bibr" target="#b38">(Zhang et al., 2012)</ref>, and machine translation <ref type="bibr"
nd Language texts <ref type="bibr" target="#b3">(Berzak et al., 2014)</ref>, as well as work in NLP <ref type="bibr" target="#b34">(Shi et al., 2016;</ref><ref type="bibr" target="#b19">Kuncoro et al. nterpretable information such as syntax <ref type="bibr" target="#b18">(Karpathy et al., 2015;</ref><ref type="bibr" target="#b34">Shi et al., 2016)</ref> or  <ref type="table">1</ref>: Accuracy of sy
nerative parsing in low-resource settings <ref type="bibr" target="#b26">(Naseem et al., 2012;</ref><ref type="bibr" target="#b35">Täckström et al., 2013)</ref>, phonological language modeling and loa
languages. Typological information from sources like the World Atlas of Language Structures (WALS) <ref type="bibr" target="#b11">(Dryer and Haspelmath, 2013)</ref>, has proven useful in many NLP tas
e most widely used one, <ref type="bibr">BERT [Devlin et al., 2018]</ref> builds on the Transformer <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> architecture and improves the pre-trainin -c.org/ns/1.0"><head n="4.1">Background of BERT</head><p>Based on a multi-layer Transformer encoder <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> (The transformer architecture has been ub /1.0"><head>Visit Embedding</head><p>Similar to BERT, we use a multi-layer Transformer architecture <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> as our visit encoder. The model takes the
ethod based on BERT for Instance-based methods focus on current health conditions. Among them, Leap <ref type="bibr" target="#b7">[Zhang et al., 2017]</ref> formulates a multi-instance multi-label lea
bibr" target="#b5">[Xiao et al., 2018a;</ref><ref type="bibr" target="#b3">Shang et al., 2019;</ref><ref type="bibr" target="#b0">Baytas et al., 2017;</ref><ref type="bibr">Choi et al., 2018;</ref><re n previous works for improving medical code representations <ref type="bibr">[Ma et al., 2018;</ref><ref type="bibr" target="#b0">Baytas et al., 2017;</ref><ref type="bibr">Choi et al., 2018]</ref>, t
diagnoses. Longitudinal-based methods leverage the temporal dependencies among clinical events, see <ref type="bibr" target="#b1">[Choi et al., 2016;</ref><ref type="bibr" target="#b6">Xiao et al., 20 ref type="bibr" target="#b2">[Peters et al., 2018;</ref><ref type="bibr">Radford et al., 2018;</ref><ref type="bibr" target="#b1">Devlin et al., 2018]</ref> have shown to largely improve the performan (GCN) <ref type="bibr" target="#b2">[Kipf and Welling, 2017]</ref>, message passing networks (MPNN) <ref type="bibr" target="#b1">[Gilmer et al., 2017]</ref>, graph attention networks (GAT) <ref type= ="bibr">[Velickovic et al., 2017]</ref>. GNNs have already been demonstrated useful on EHR modeling <ref type="bibr" target="#b1">[Choi et al., 2017;</ref><ref type="bibr" target="#b3">Shang et al., 2 whether one sentence is the next sentence of the other.</p><p>A typical input to BERT is as follows <ref type="bibr" target="#b1">( [Devlin et al., 2018]</ref>):</p><formula xml:id="formula_4">Input = p>It is worth mentioning that our graph embedding method on medical ontology is different from GRAM <ref type="bibr" target="#b1">[Choi et al., 2017]</ref> from the following two aspects: 1. Initializ with weight as 1.1. For deep learning models, we implemented RNN using a gated recurrent unit (GRU) <ref type="bibr" target="#b1">[Cho et al., 2014]</ref> and utilize dropout with a probability of 0.4
diagnoses. Longitudinal-based methods leverage the temporal dependencies among clinical events, see <ref type="bibr" target="#b1">[Choi et al., 2016;</ref><ref type="bibr" target="#b6">Xiao et al., 20 ref type="bibr" target="#b2">[Peters et al., 2018;</ref><ref type="bibr">Radford et al., 2018;</ref><ref type="bibr" target="#b1">Devlin et al., 2018]</ref> have shown to largely improve the performan (GCN) <ref type="bibr" target="#b2">[Kipf and Welling, 2017]</ref>, message passing networks (MPNN) <ref type="bibr" target="#b1">[Gilmer et al., 2017]</ref>, graph attention networks (GAT) <ref type= ="bibr">[Velickovic et al., 2017]</ref>. GNNs have already been demonstrated useful on EHR modeling <ref type="bibr" target="#b1">[Choi et al., 2017;</ref><ref type="bibr" target="#b3">Shang et al., 2 whether one sentence is the next sentence of the other.</p><p>A typical input to BERT is as follows <ref type="bibr" target="#b1">( [Devlin et al., 2018]</ref>):</p><formula xml:id="formula_4">Input = p>It is worth mentioning that our graph embedding method on medical ontology is different from GRAM <ref type="bibr" target="#b1">[Choi et al., 2017]</ref> from the following two aspects: 1. Initializ with weight as 1.1. For deep learning models, we implemented RNN using a gated recurrent unit (GRU) <ref type="bibr" target="#b1">[Cho et al., 2014]</ref> and utilize dropout with a probability of 0.4
bibr" target="#b5">[Xiao et al., 2018a;</ref><ref type="bibr" target="#b3">Shang et al., 2019;</ref><ref type="bibr" target="#b0">Baytas et al., 2017;</ref><ref type="bibr">Choi et al., 2018;</ref><re n previous works for improving medical code representations <ref type="bibr">[Ma et al., 2018;</ref><ref type="bibr" target="#b0">Baytas et al., 2017;</ref><ref type="bibr">Choi et al., 2018]</ref>, t
oral dependencies among clinical events, see <ref type="bibr" target="#b1">[Choi et al., 2016;</ref><ref type="bibr" target="#b6">Xiao et al., 2018b;</ref><ref type="bibr" target="#b2">Lipton et al.,
diagnoses. Longitudinal-based methods leverage the temporal dependencies among clinical events, see <ref type="bibr" target="#b1">[Choi et al., 2016;</ref><ref type="bibr" target="#b6">Xiao et al., 20 ref type="bibr" target="#b2">[Peters et al., 2018;</ref><ref type="bibr">Radford et al., 2018;</ref><ref type="bibr" target="#b1">Devlin et al., 2018]</ref> have shown to largely improve the performan (GCN) <ref type="bibr" target="#b2">[Kipf and Welling, 2017]</ref>, message passing networks (MPNN) <ref type="bibr" target="#b1">[Gilmer et al., 2017]</ref>, graph attention networks (GAT) <ref type= ="bibr">[Velickovic et al., 2017]</ref>. GNNs have already been demonstrated useful on EHR modeling <ref type="bibr" target="#b1">[Choi et al., 2017;</ref><ref type="bibr" target="#b3">Shang et al., 2 whether one sentence is the next sentence of the other.</p><p>A typical input to BERT is as follows <ref type="bibr" target="#b1">( [Devlin et al., 2018]</ref>):</p><formula xml:id="formula_4">Input = p>It is worth mentioning that our graph embedding method on medical ontology is different from GRAM <ref type="bibr" target="#b1">[Choi et al., 2017]</ref> from the following two aspects: 1. Initializ with weight as 1.1. For deep learning models, we implemented RNN using a gated recurrent unit (GRU) <ref type="bibr" target="#b1">[Cho et al., 2014]</ref> and utilize dropout with a probability of 0.4
oral dependencies among clinical events, see <ref type="bibr" target="#b1">[Choi et al., 2016;</ref><ref type="bibr" target="#b6">Xiao et al., 2018b;</ref><ref type="bibr" target="#b2">Lipton et al.,
oral dependencies among clinical events, see <ref type="bibr" target="#b1">[Choi et al., 2016;</ref><ref type="bibr" target="#b6">Xiao et al., 2018b;</ref><ref type="bibr" target="#b2">Lipton et al.,
l contributions:</p><p>1. Pre-training to leverage more data: Pre-training techniques, such as ELMo <ref type="bibr" target="#b2">[Peters et al., 2018]</ref>, OpenAI <ref type="bibr">GPT [Radford et a "bibr" target="#b1">[Choi et al., 2016;</ref><ref type="bibr" target="#b6">Xiao et al., 2018b;</ref><ref type="bibr" target="#b2">Lipton et al., 2015]</ref>. Among them, <ref type="bibr">RETAIN [Choi ions. Pre-training has been shown extremely effective in various areas such as image classification <ref type="bibr" target="#b2">[Hinton et al., 2006]</ref> and machine translation <ref type="bibr" t ef type="bibr">[Erhan et al., 2010]</ref>. Recently, language model pre-training techniques such as <ref type="bibr" target="#b2">[Peters et al., 2018;</ref><ref type="bibr">Radford et al., 2018;</ref osed to encode the graph-structure information, including graph convolutional neural networks (GCN) <ref type="bibr" target="#b2">[Kipf and Welling, 2017]</ref>, message passing networks (MPNN) <ref t .tei-c.org/ns/1.0"><head n="5.1">Experimental Setting Data</head><p>We used EHR data from MIMIC-III <ref type="bibr" target="#b2">[Johnson et al., 2016]</ref> and conducted all our experiments on a co 300 and thershold for final prediction as 0.3 for better performance. Training is done through Adam <ref type="bibr" target="#b2">[Kingma and Ba, 2014]</ref> at learning rate 5e-4. We fix the best mod
graph).</p><p>Our method also connects to PinSage <ref type="bibr" target="#b20">[21]</ref> and GAT <ref type="bibr" target="#b14">[15]</ref>. But note that both PinSage and GAT are designed for homog ula><p>(5) 2 The knowledge graph G is treated undirected. 3 Technically, S(v) • Neighbor aggregator <ref type="bibr" target="#b14">[15]</ref> directly takes the neighborhood representation of entity v
hod for a special type of graphs (i.e., knowledge graph).</p><p>Our method also connects to PinSage <ref type="bibr" target="#b20">[21]</ref> and GAT <ref type="bibr" target="#b14">[15]</ref>. But not
ling rigorous semantic relatedness (e.g., TransE <ref type="bibr" target="#b0">[1]</ref> and TransR <ref type="bibr" target="#b11">[12]</ref> assume head + relation = tail), which are more suitable fo
ttp://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>• LibFM<ref type="bibr" target="#b13">[14]</ref> is a feature-based factorization model in CTR scenarios. W
ling rigorous semantic relatedness (e.g., TransE <ref type="bibr" target="#b0">[1]</ref> and TransR <ref type="bibr" target="#b11">[12]</ref> assume head + relation = tail), which are more suitable fo
de degree <ref type="bibr" target="#b5">[6]</ref>, extracting locally connected regions from graphs <ref type="bibr" target="#b12">[13]</ref>, or sampling a fixed-size set of neighbors as the support
e="bibr" target="#b16">17]</ref>.</p><p>A few recent studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar pared with KG-free methods, incorporating KG into recommendation benefits the results in three ways <ref type="bibr" target="#b17">[18]</ref>: <ref type="bibr" target="#b0">(1)</ref> The rich semantic n manually designed meta-paths or meta-graphs, which are hardly to be optimal in reality. RippleNet <ref type="bibr" target="#b17">[18]</ref> is a memory-network-like model that propagates users' pote ecommendation. We implement CKE as CF plus a structural knowledge module in this paper. • RippleNet <ref type="bibr" target="#b17">[18]</ref> is a memory-network-like approach that propagates users' p more natural and intuitive way is to design a graph algorithm directly to exploit the KG structure <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta
methods. Hyper-parameter settings for baselines are introduced in the next subsection.</p><p>• SVD <ref type="bibr" target="#b10">[11]</ref> is a classic CF-based model using inner product to model u
r the sparsity and improve the performance of recommendation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>A few recent studies <ref type="bibr" target="#b8">[
heterogeneity. One feasible way is to preprocess the KG by knowledge graph embedding (KGE) methods <ref type="bibr" target="#b19">[20]</ref>, which map entities and relations to low-dimensional repre
ers and items are used to compensate for the sparsity and improve the performance of recommendation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>A few recent
e weight sharing property of CNN, researchers propose learning a weight matrix for each node degree <ref type="bibr" target="#b5">[6]</ref>, extracting locally connected regions from graphs <ref type=
e="bibr" target="#b16">17]</ref>.</p><p>A few recent studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar pared with KG-free methods, incorporating KG into recommendation benefits the results in three ways <ref type="bibr" target="#b17">[18]</ref>: <ref type="bibr" target="#b0">(1)</ref> The rich semantic n manually designed meta-paths or meta-graphs, which are hardly to be optimal in reality. RippleNet <ref type="bibr" target="#b17">[18]</ref> is a memory-network-like model that propagates users' pote ecommendation. We implement CKE as CF plus a structural knowledge module in this paper. • RippleNet <ref type="bibr" target="#b17">[18]</ref> is a memory-network-like approach that propagates users' p more natural and intuitive way is to design a graph algorithm directly to exploit the KG structure <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta
methods. Hyper-parameter settings for baselines are introduced in the next subsection.</p><p>• SVD <ref type="bibr" target="#b10">[11]</ref> is a classic CF-based model using inner product to model u
ess a vast amount of online content, such as news <ref type="bibr" target="#b24">[25]</ref>, movies <ref type="bibr" target="#b4">[5]</ref>, and commodities <ref type="bibr" target="#b25">[26]</ref>.
heterogeneity. One feasible way is to preprocess the KG by knowledge graph embedding (KGE) methods <ref type="bibr" target="#b19">[20]</ref>, which map entities and relations to low-dimensional repre
rget="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" t ><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. For example, PER <ref type="bibr" target="#b21">[22]</ref> and FMG <ref type="bibr" target="#b23">[24]</ref> treat KG ation learned by TransE <ref type="bibr" target="#b0">[1]</ref> to each user-item pair.</p><p>• PER <ref type="bibr" target="#b21">[22]</ref> treats the KG as heterogeneous information networks and ex ign a graph algorithm directly to exploit the KG structure <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. For example, PER <
et="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> have gone a step and extracts meta-path features to represent the connectivity between users and items.</p><p>• CKE <ref type="bibr" target="#b22">[23]</ref> combines CF with structural, textual, and visual knowledge presentation vectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>. However, commonly-used KGE methods focus on modeling rigor
e="bibr" target="#b16">17]</ref>.</p><p>A few recent studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar pared with KG-free methods, incorporating KG into recommendation benefits the results in three ways <ref type="bibr" target="#b17">[18]</ref>: <ref type="bibr" target="#b0">(1)</ref> The rich semantic n manually designed meta-paths or meta-graphs, which are hardly to be optimal in reality. RippleNet <ref type="bibr" target="#b17">[18]</ref> is a memory-network-like model that propagates users' pote ecommendation. We implement CKE as CF plus a structural knowledge module in this paper. • RippleNet <ref type="bibr" target="#b17">[18]</ref> is a memory-network-like approach that propagates users' p more natural and intuitive way is to design a graph algorithm directly to exploit the KG structure <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta
heterogeneity. One feasible way is to preprocess the KG by knowledge graph embedding (KGE) methods <ref type="bibr" target="#b19">[20]</ref>, which map entities and relations to low-dimensional repre
methods. Hyper-parameter settings for baselines are introduced in the next subsection.</p><p>• SVD <ref type="bibr" target="#b10">[11]</ref> is a classic CF-based model using inner product to model u
t="#b18">19,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> have gone a step further than simply using attributes: The ="bibr" target="#b23">24]</ref>. For example, PER <ref type="bibr" target="#b21">[22]</ref> and FMG <ref type="bibr" target="#b23">[24]</ref> treat KG as a heterogeneous information network, and extra t the KG structure <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. For example, PER <ref type="bibr" target="#b21">[22]</ref>
y, "feature-wise linear modulations" (FiLM) were introduced in the visual question answering domain <ref type="bibr" target="#b10">(Perez et al., 2017)</ref>. Here, the hypernetwork is fed with an enc the extreme case in which the dimension of each chunk is 1, this method coincides with the ideas of <ref type="bibr" target="#b10">Perez et al. (2017)</ref>, who propose to use layers of element-wise
USAGE IN PROGRAMS (VARMISUSE)</head><p>Finally, the models were evaluated on the VarMisuse task of <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>. This task requires to process a graph r PPI and QM9, with R-GCN performing best. All re-implemented baselines beat the results reported by <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>, who also reported that R-GCN and GGNN s t R-GCNs are outperforming GGNNs substantially on the VarMisuse task, contradicting the findings of <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>. ? The GNN-MLP models are obvious extens 3 :</head><label>3</label><figDesc>Accuracy on VarMisuse task. GGNN * result taken from appendix of<ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>.</figDesc><table><row><cell>Model</cell>
t taken into consideration. A (partial) exception to this is the family of Graph Attention Networks <ref type="bibr" target="#b16">(Veli?kovi? et al., 2018)</ref>, where the agreement between source a v 0 ? v, allowing state associated with a node to be kept.</p><p>In Graph Attention Networks (GAT) <ref type="bibr" target="#b16">(Veli?kovi? et al., 2018)</ref>, new node representations are compute r" target="#b12">(Schlichtkrull et al., 2018</ref>) (see Eq. ( <ref type="formula">2</ref>)), R-GAT <ref type="bibr" target="#b16">(Veli?kovi? et al., 2018)</ref> (see Eq. (3)), and R-GIN (Hamilton et ver the ten runs. The results for all re-implemented models are better than the results reported by <ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref> for the GAT model (without edge types) ficantly improved the results. Third, the larger node representation sizes (compared to 256 used by <ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref>) improved the results again. However, t GATs have no advantage over GGNNs or R-GCNs on the PPI task, which does not match the findings by <ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref>. ? The results in Tab. 3 indicate that b_1"><head>Table 1 :</head><label>1</label><figDesc>GNN results on PPI task. GAT * result taken from<ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref>.</figDesc><table><row><cell>Model</cel
USAGE IN PROGRAMS (VARMISUSE)</head><p>Finally, the models were evaluated on the VarMisuse task of <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>. This task requires to process a graph r PPI and QM9, with R-GCN performing best. All re-implemented baselines beat the results reported by <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>, who also reported that R-GCN and GGNN s t R-GCNs are outperforming GGNNs substantially on the VarMisuse task, contradicting the findings of <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>. ? The GNN-MLP models are obvious extens 3 :</head><label>3</label><figDesc>Accuracy on VarMisuse task. GGNN * result taken from appendix of<ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>.</figDesc><table><row><cell>Model</cell>

USAGE IN PROGRAMS (VARMISUSE)</head><p>Finally, the models were evaluated on the VarMisuse task of <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>. This task requires to process a graph r PPI and QM9, with R-GCN performing best. All re-implemented baselines beat the results reported by <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>, who also reported that R-GCN and GGNN s t R-GCNs are outperforming GGNNs substantially on the VarMisuse task, contradicting the findings of <ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>. ? The GNN-MLP models are obvious extens 3 :</head><label>3</label><figDesc>Accuracy on VarMisuse task. GGNN * result taken from appendix of<ref type="bibr" target="#b0">Allamanis et al. (2018)</ref>.</figDesc><table><row><cell>Model</cell>
the following summary of tasks from the literature should illustrate.</p><p>? Cora/Citeseer/Pubmed <ref type="bibr" target="#b14">(Sen et al., 2008)</ref>: Each task consists of a single graph of ? 1
an edge (in the R-GAT case Eq. (3)). Sometimes unnamed GNN variants of the above are used (e.g., by <ref type="bibr" target="#b13">Selsam et al. (2019)</ref>; <ref type="bibr" target="#b9">Paliwal et
t taken into consideration. A (partial) exception to this is the family of Graph Attention Networks <ref type="bibr" target="#b16">(Veli?kovi? et al., 2018)</ref>, where the agreement between source a v 0 ? v, allowing state associated with a node to be kept.</p><p>In Graph Attention Networks (GAT) <ref type="bibr" target="#b16">(Veli?kovi? et al., 2018)</ref>, new node representations are compute r" target="#b12">(Schlichtkrull et al., 2018</ref>) (see Eq. ( <ref type="formula">2</ref>)), R-GAT <ref type="bibr" target="#b16">(Veli?kovi? et al., 2018)</ref> (see Eq. (3)), and R-GIN (Hamilton et ver the ten runs. The results for all re-implemented models are better than the results reported by <ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref> for the GAT model (without edge types) ficantly improved the results. Third, the larger node representation sizes (compared to 256 used by <ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref>) improved the results again. However, t GATs have no advantage over GGNNs or R-GCNs on the PPI task, which does not match the findings by <ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref>. ? The results in Tab. 3 indicate that b_1"><head>Table 1 :</head><label>1</label><figDesc>GNN results on PPI task. GAT * result taken from<ref type="bibr" target="#b16">Veli?kovi? et al. (2018)</ref>.</figDesc><table><row><cell>Model</cel
ormation provided in the dataset. However, the re-implementation of the task uses the insights from <ref type="bibr" target="#b4">Cvitkovic et al. (2019)</ref>, who use character CNNs to encode node l
ormation provided in the dataset. However, the re-implementation of the task uses the insights from <ref type="bibr" target="#b4">Cvitkovic et al. (2019)</ref>, who use character CNNs to encode node l
Release Consistency (RC) are often mixed with the concept of "SC for data-race-free (DRF) programs" <ref type="bibr" target="#b27">[28]</ref>. It should be noted that "SC for DRF" is inadequate for an

#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b38">[39]</ref>, PSO <ref type="bibr" target="#b3">[4]</ref>, RMO <ref typ
f>, <ref type="bibr" target="#b60">[61]</ref> on an 8-core multiprocessor using the ESESC simulator <ref type="bibr" target="#b61">[62]</ref>. We ran all benchmarks except ocean ncp, which allocates t

taneous Instruction Execution (I 2 E), which is the style used in the operational definitions of SC <ref type="bibr" target="#b0">[1]</ref> and TSO <ref type="bibr" target="#b1">[2]</ref>, <ref type=" e conclusion.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>SC <ref type="bibr" target="#b0">[1]</ref> is the simplest model, but naive implementations of SC suffe a processor is appended to the tail of the list of the segment connected to the processor (e.g., s <ref type="bibr" target="#b0">[1]</ref> for P1). OOO no longer contains a store buffer; after a stor he request at the head of the list of a segment can flow into the parent segment (e.g., flow from s <ref type="bibr" target="#b0">[1]</ref> into s <ref type="bibr" target="#b4">[5]</ref>) or the atomi

e for an ISA memory model, which must specify behaviors of all programs. The original RC definition <ref type="bibr" target="#b5">[6]</ref> attempts to specify all program behaviors, and are more comp ype="bibr" target="#b39">[40]</ref>, Weak Consistency <ref type="bibr" target="#b40">[41]</ref>, RC <ref type="bibr" target="#b5">[6]</ref>, CRF <ref type="bibr" target="#b41">[42]</ref>, Instruction ments (s[1 . . . 6]). Each segment is a list of memory requests, (e.g., the list of blue nodes in s <ref type="bibr" target="#b5">[6]</ref>, whose head is at the bottom and the tail is at the top).</p ype="bibr" target="#b4">[5]</ref>) or the atomic memory (in case the parent of the segment, e.g., s <ref type="bibr" target="#b5">[6]</ref>, is m). Details of these operations are shown in Figure <ref igure" target="#fig_7">19</ref>, stores in segments s <ref type="bibr" target="#b2">[3]</ref> and s <ref type="bibr" target="#b5">[6]</ref> are on the path from P3 to m; a store in s <ref type="bibr" >[3]</ref> and s <ref type="bibr" target="#b5">[6]</ref> are on the path from P3 to m; a store in s <ref type="bibr" target="#b5">[6]</ref> is older than any store (for the same address) in s <ref typ RC sc and RC pc ) and RMO, which have been pointed out in Section I. RC: Although the RC definition <ref type="bibr" target="#b5">[6]</ref> allows the behaviors of WRC and IRIW (Figures <ref type="fig


]</ref>, and researchers are even seeking operational definitions for high-level languages like C++ <ref type="bibr" target="#b25">[26]</ref>. This is perhaps because all possible program results can ount of research has also been devoted to specifying the memory models of high-level languages: C++ <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b46">[47]</ref>- <ref type="bib
cantly improved image classification <ref type="bibr" target="#b13">[14]</ref> and object detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref> accuracy. Compared t task that requires more complex methods to solve. Due to this complexity, current approaches (e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ n this paper, we streamline the training process for stateof-the-art ConvNet-based object detectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. We propose a single egressors, rather than training a softmax classifier, SVMs, and regressors in three separate stages <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. The components of t very deep detection network (VGG16 <ref type="bibr" target="#b19">[20]</ref>) 9× faster than R-CNN <ref type="bibr" target="#b8">[9]</ref> and 3× faster than SPPnet <ref type="bibr" target="#b10">[11 1.0"><head n="1.1.">R-CNN and SPPnet</head><p>The Region-based Convolutional Network method (R-CNN) <ref type="bibr" target="#b8">[9]</ref> achieves excellent object detection accuracy by using a deep k h , for each of the K object classes, indexed by k. We use the parameterization for t k given in <ref type="bibr" target="#b8">[9]</ref>, in which t k specifies a scale-invariant translation and lo eparates localization and classification. OverFeat <ref type="bibr" target="#b18">[19]</ref>, R-CNN <ref type="bibr" target="#b8">[9]</ref>, and SPPnet <ref type="bibr" target="#b10">[11]</ref> also t tions of the dataset). We use mini-batches of size R = 128, sampling 64 RoIs from each image. As in <ref type="bibr" target="#b8">[9]</ref>, we take 25% of the RoIs from object proposals that have int rm non-maximum suppression independently for each class using the algorithm and settings from R-CNN <ref type="bibr" target="#b8">[9]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n= he first is the CaffeNet (essentially AlexNet <ref type="bibr" target="#b13">[14]</ref>) from R-CNN <ref type="bibr" target="#b8">[9]</ref>. We alternatively refer to this CaffeNet as model S, for "sm
t information on the ConvNet architectures used; they are variants of the Network-in-Network design <ref type="bibr" target="#b16">[17]</ref>. All other methods are initialized from the same pre-train </label><figDesc>VOC 2010 test detection average precision (%). BabyLearning uses a network based on<ref type="bibr" target="#b16">[17]</ref>. All other methods use VGG16.</figDesc><table><row><cell>B
their spatial locations.</p><p>The resulting method can train a very deep detection network (VGG16 <ref type="bibr" target="#b19">[20]</ref>) 9× faster than R-CNN <ref type="bibr" target="#b8">[9]</r er. We call this network model M, for "medium." The final network is the very deep VGG16 model from <ref type="bibr" target="#b19">[20]</ref>. Since this model is the largest, we call it model L. In t
sion targets v i to have zero mean and unit variance. All experiments use λ = 1.</p><p>We note that <ref type="bibr" target="#b5">[6]</ref> uses a related loss to train a classagnostic object proposal uses a related loss to train a classagnostic object proposal network. Different from our approach, <ref type="bibr" target="#b5">[6]</ref> advocates for a two-network system that separates localizati
2.2.">Initializing from pre-trained networks</head><p>We experiment with three pre-trained ImageNet <ref type="bibr" target="#b3">[4]</ref> networks, each with five max pooling layers and between five
mprove results because the tasks influence each other through a shared representation (the ConvNet) <ref type="bibr" target="#b1">[2]</ref>. Does multi-task training improve object detection accuracy
</ref>). Large fully connected layers are easily accelerated by compressing them with truncated SVD <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>In this techn
rget="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>) train models in multi-stage pipelines that are slow and in faster than the other methods, which are all based on the "slow" R-CNN pipeline. On VOC10, SegDeepM <ref type="bibr" target="#b24">[25]</ref> achieves a higher mAP than Fast R-CNN (67.2% vs. 66.1%). S
2.2.">Initializing from pre-trained networks</head><p>We experiment with three pre-trained ImageNet <ref type="bibr" target="#b3">[4]</ref> networks, each with five max pooling layers and between five
rnatively refer to this CaffeNet as model S, for "small." The second network is VGG CNN M 1024 from <ref type="bibr" target="#b2">[3]</ref>, which has the same depth as S, but is wider. We call this n
details. Yet it achieves a mAP of 66.9%, which is slightly higher than the 66.0% reported for R-CNN <ref type="bibr" target="#b9">[10]</ref>, even though R-CNN uses "infinite" scales in the sense that

otable works on graph neural networks include <ref type="bibr" target="#b25">(Li et al., 2016;</ref><ref type="bibr" target="#b33">Schütt et al., 2017;</ref><ref type="bibr" target="#b0">Battaglia et
he proposed graph learning architectures <ref type="bibr" target="#b7">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b17">Kearnes et al., 2016;</ref><ref type="bibr" target="#b28">Niepert et
gression, random forests, gradient boosted trees, optimal assignment Wesifeiler-Lehman graph kernel <ref type="bibr" target="#b22">(Kriege et al., 2016)</ref> (WL), neural graph fingerprints <ref type nitialized the base features of each vertex with computed histogram alignment features, inspired by <ref type="bibr" target="#b22">(Kriege et al., 2016)</ref>, of depth up to 10. Each vertex receives

"bibr" target="#b30">Perona, 1995;</ref><ref type="bibr" target="#b37">Teo &amp; Hel-Or, 1998;</ref><ref type="bibr" target="#b26">Manduchi et al., 1998)</ref> and the neural networks field <ref type=

type="bibr" target="#b40">(Vishwanathan et al., 2010)</ref>, label propagation schemes with hashing <ref type="bibr" target="#b35">(Shervashidze et al., 2011;</ref><ref type="bibr">Neumann et al., 201 ated Weisfeiler-Lehman kernel, arguably the most successful kernel-based approach to graph learning <ref type="bibr" target="#b35">(Shervashidze et al., 2011)</ref>. Note also that in label propagatio
both classical signal processing <ref type="bibr" target="#b12">(Freeman &amp; Adelson, 1991;</ref><ref type="bibr" target="#b36">Simoncelli et al., 1992;</ref><ref type="bibr" target="#b30">Perona,
12">(Freeman &amp; Adelson, 1991;</ref><ref type="bibr" target="#b36">Simoncelli et al., 1992;</ref><ref type="bibr" target="#b30">Perona, 1995;</ref><ref type="bibr" target="#b37">Teo &amp; Hel-Or, 1

f a premise and hypothesis through tree-LSTM <ref type="bibr" target="#b35">(Zhu et al., 2015;</ref><ref type="bibr" target="#b30">Tai et al., 2015;</ref><ref type="bibr" target="#b15">Le and Zuidema,
o this end, we will also encode syntactic parse trees of a premise and hypothesis through tree-LSTM <ref type="bibr" target="#b35">(Zhu et al., 2015;</ref><ref type="bibr" target="#b30">Tai et al., 20 "#b20">(Munkhdalai and Yu, 2016b)</ref>.</p><p>We ensemble our ESIM model with syntactic tree-LSTMs <ref type="bibr" target="#b35">(Zhu et al., 2015)</ref> based on syntactic parse trees and achieve s here are no enough leaves to form a full tree. Each tree node is implemented with a tree-LSTM block <ref type="bibr" target="#b35">(Zhu et al., 2015)</ref> same as in model ( <ref type="formula" targe
="bibr" target="#b35">(Zhu et al., 2015;</ref><ref type="bibr" target="#b30">Tai et al., 2015;</ref><ref type="bibr" target="#b15">Le and Zuidema, 2015)</ref>, which extends the chain LSTM to a recurs
in the context of recognizing textual entailment (RTE) <ref type="bibr">(Mehdad et al., 2010;</ref><ref type="bibr" target="#b10">Ferrone and Zanzotto, 2014)</ref>. In this paper, we are interested i
ed networks have been developed since then <ref type="bibr" target="#b2">(Bowman et al., 2016;</ref><ref type="bibr" target="#b31">Vendrov et al., 2015;</ref><ref type="bibr" target="#b19">Mou et al., owman et al. (2016)</ref> encodes the premise and hypothesis with two different LSTMs. The model in <ref type="bibr" target="#b31">Vendrov et al. (2015)</ref> uses unsupervised "skip-thoughts" pre-tra pe="bibr" target="#b2">(Bowman et al., 2016)</ref> 3.0M 83.9 80.6 (3) 1024D pretrained GRU encoders <ref type="bibr" target="#b31">(Vendrov et al., 2015)</ref> 15M 98.8 81.4 (4) 300D tree-based CNN en
machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref>, speech recognition <ref type="bibr" target="#b8">(Chorowski et al., 2015;</ref><ref type="bibr" target="#b3">Chan et al
e of wordlevel inference relations. Modeling negation more closely within neural network frameworks <ref type="bibr" target="#b29">(Socher et al., 2013;</ref><ref type="bibr" target="#b34">Zhu et al.,
vious work.</p><p>The parse trees used in this paper are produced by the Stanford PCFG Parser 3.5.3 <ref type="bibr" target="#b14">(Klein and Manning, 2003)</ref> and they are delivered as part of the
locks in our models. We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) <ref type="bibr" target="#b7">(Cho et al., 2014)</ref> and they are inferior to LSTMs on the heldout
get="#b25">Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b32">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b6">Cheng et al., 2016;</ref><ref type="bibr" target="#b22">Parikh et al., ypothesis and the premise.</p><p>Long short-term memory-networks (LSTMN) with deep attention fusion <ref type="bibr" target="#b6">(Cheng et al., 2016)</ref> link the current word to previous words sto
literature survey), which includes a large bulk of work on recognizing textual entailment, such as <ref type="bibr" target="#b9">(Dagan et al., 2005;</ref><ref type="bibr" target="#b12">Iftene and Ba
e of wordlevel inference relations. Modeling negation more closely within neural network frameworks <ref type="bibr" target="#b29">(Socher et al., 2013;</ref><ref type="bibr" target="#b34">Zhu et al.,
f 0.5, which is applied to all feedforward connections. We use pre-trained 300-D Glove 840B vectors <ref type="bibr" target="#b24">(Pennington et al., 2014)</ref> to initialize our word embeddings. Ou
e of wordlevel inference relations. Modeling negation more closely within neural network frameworks <ref type="bibr" target="#b29">(Socher et al., 2013;</ref><ref type="bibr" target="#b34">Zhu et al.,
ling inference in human language is very challenging. With the availability of large annotated data <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>, it has recently become feasible to train creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>. The corpus has 570,000 human-written Engl ><head n="4">Experimental Setup</head><p>Data The Stanford Natural Language Inference (SNLI) corpus <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref> focuses on three basic relationships betwe rge annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI <ref type="bibr" target="#b1">(Bowman et al., 2015</ref><ref type="bibr" target="#b2">(Bowman et al. ef type="bibr" target="#b12">Iftene and Balahur-Dobrescu, 2007)</ref>, among others. More recently, <ref type="bibr" target="#b1">Bowman et al. (2015)</ref> made available the SNLI dataset with 570,00 ple human annotators. As in the related work, we remove this category. We used the same split as in <ref type="bibr" target="#b1">Bowman et al. (2015)</ref> and other previous work.</p><p>The parse tr >1</ref> shows the results of different models. The first row is a baseline classifier presented by <ref type="bibr" target="#b1">Bowman et al. (2015)</ref> that considers handcrafted features such as
rget="#b20">Munkhdalai and Yu, 2016b;</ref><ref type="bibr" target="#b22">Parikh et al., 2016;</ref><ref type="bibr" target="#b27">Sha et al., 2016;</ref><ref type="bibr" target="#b21">Paria et al., 2 rget="#b22">Parikh et al., 2016;</ref><ref type="bibr" target="#b20">Munkhdalai and Yu, 2016b;</ref><ref type="bibr" target="#b27">Sha et al., 2016;</ref><ref type="bibr" target="#b21">Paria et al., 2 ef type="bibr" target="#b20">(Munkhdalai and Yu, 2016b)</ref> 3.2M 88.5 87.3 (14) 300D re-read LSTM <ref type="bibr" target="#b27">(Sha et al., 2016)</ref> 2.0M 90.7 87.5 (15) 300D btree-LSTM encoders get="#b32">Wang and Jiang (2016)</ref> to a full n-ary tree model and achieves further improvement. <ref type="bibr" target="#b27">Sha et al. (2016)</ref> proposes a special LSTM variant which conside
ling inference in human language is very challenging. With the availability of large annotated data <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>, it has recently become feasible to train creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>. The corpus has 570,000 human-written Engl ><head n="4">Experimental Setup</head><p>Data The Stanford Natural Language Inference (SNLI) corpus <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref> focuses on three basic relationships betwe rge annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI <ref type="bibr" target="#b1">(Bowman et al., 2015</ref><ref type="bibr" target="#b2">(Bowman et al. ef type="bibr" target="#b12">Iftene and Balahur-Dobrescu, 2007)</ref>, among others. More recently, <ref type="bibr" target="#b1">Bowman et al. (2015)</ref> made available the SNLI dataset with 570,00 ple human annotators. As in the related work, we remove this category. We used the same split as in <ref type="bibr" target="#b1">Bowman et al. (2015)</ref> and other previous work.</p><p>The parse tr >1</ref> shows the results of different models. The first row is a baseline classifier presented by <ref type="bibr" target="#b1">Bowman et al. (2015)</ref> that considers handcrafted features such as
own to achieve the state of the art on SNLI <ref type="bibr" target="#b1">(Bowman et al., 2015</ref><ref type="bibr" target="#b2">(Bowman et al., , 2016;;</ref><ref type="bibr" target="#b20">Munkhdala ic areas of the sentences.</p><p>A variety of more advanced networks have been developed since then <ref type="bibr" target="#b2">(Bowman et al., 2016;</ref><ref type="bibr" target="#b31">Vendrov et a f>)-( <ref type="formula" target="#formula_7">7</ref>) are based on sentence encoding. The model of <ref type="bibr" target="#b2">Bowman et al. (2016)</ref> encodes the premise and hypothesis with two t al. (2016)</ref> considers tree-based CNN to capture sentence-level semantics, while the model of <ref type="bibr" target="#b2">Bowman et al. (2016)</ref> introduces a stack-augmented parser-interpr proposed by <ref type="bibr" target="#b20">Munkhdalai and Yu (2016a)</ref>  (2) 300D LSTM encoders <ref type="bibr" target="#b2">(Bowman et al., 2016)</ref> 3.0M 83.9 80.6 (3) 1024D pretrained GRU en s <ref type="bibr" target="#b19">(Mou et al., 2016)</ref> 3.5M 83.3 82.1 (5) 300D SPINN-PI encoders <ref type="bibr" target="#b2">(Bowman et al., 2016)</ref> 3.7M 89.2 83.2 (6) 600D BiLSTM intra-atten
s exploring the usefulness of external resources such as Word-Net and contrasting-meaning embedding <ref type="bibr" target="#b5">(Chen et al., 2015)</ref> to help increase the coverage of wordlevel i
ed networks have been developed since then <ref type="bibr" target="#b2">(Bowman et al., 2016;</ref><ref type="bibr" target="#b31">Vendrov et al., 2015;</ref><ref type="bibr" target="#b19">Mou et al., owman et al. (2016)</ref> encodes the premise and hypothesis with two different LSTMs. The model in <ref type="bibr" target="#b31">Vendrov et al. (2015)</ref> uses unsupervised "skip-thoughts" pre-tra pe="bibr" target="#b2">(Bowman et al., 2016)</ref> 3.0M 83.9 80.6 (3) 1024D pretrained GRU encoders <ref type="bibr" target="#b31">(Vendrov et al., 2015)</ref> 15M 98.8 81.4 (4) 300D tree-based CNN en
ed networks have been developed since then <ref type="bibr" target="#b2">(Bowman et al., 2016;</ref><ref type="bibr" target="#b31">Vendrov et al., 2015;</ref><ref type="bibr" target="#b19">Mou et al., owman et al. (2016)</ref> encodes the premise and hypothesis with two different LSTMs. The model in <ref type="bibr" target="#b31">Vendrov et al. (2015)</ref> uses unsupervised "skip-thoughts" pre-tra pe="bibr" target="#b2">(Bowman et al., 2016)</ref> 3.0M 83.9 80.6 (3) 1024D pretrained GRU encoders <ref type="bibr" target="#b31">(Vendrov et al., 2015)</ref> 15M 98.8 81.4 (4) 300D tree-based CNN en
e of wordlevel inference relations. Modeling negation more closely within neural network frameworks <ref type="bibr" target="#b29">(Socher et al., 2013;</ref><ref type="bibr" target="#b34">Zhu et al.,
f 0.5, which is applied to all feedforward connections. We use pre-trained 300-D Glove 840B vectors <ref type="bibr" target="#b24">(Pennington et al., 2014)</ref> to initialize our word embeddings. Ou
ibr" target="#b15">Le and Zuidema, 2015)</ref>, which extends the chain LSTM to a recursive network <ref type="bibr" target="#b28">(Socher et al., 2011)</ref>.</p><p>Specifically, given the parse of a
e of wordlevel inference relations. Modeling negation more closely within neural network frameworks <ref type="bibr" target="#b29">(Socher et al., 2013;</ref><ref type="bibr" target="#b34">Zhu et al.,
et al., 2014)</ref>, speech recognition <ref type="bibr" target="#b8">(Chorowski et al., 2015;</ref><ref type="bibr" target="#b3">Chan et al., 2016)</ref>, image caption <ref type="bibr" target="#b33"
ibr" target="#b15">Le and Zuidema, 2015)</ref>, which extends the chain LSTM to a recursive network <ref type="bibr" target="#b28">(Socher et al., 2011)</ref>.</p><p>Specifically, given the parse of a
t="#b2">(Bowman et al., , 2016;;</ref><ref type="bibr" target="#b20">Munkhdalai and Yu, 2016b;</ref><ref type="bibr" target="#b22">Parikh et al., 2016;</ref><ref type="bibr" target="#b27">Sha et al., br" target="#b32">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b6">Cheng et al., 2016;</ref><ref type="bibr" target="#b22">Parikh et al., 2016;</ref><ref type="bibr" target="#b20">Munkhdalai a ="#b21">Paria et al., 2016)</ref>. Among them, more relevant to ours are the approaches proposed by <ref type="bibr" target="#b22">Parikh et al. (2016)</ref> and <ref type="bibr" target="#b20">Munkhda e="bibr" target="#b20">Munkhdalai and Yu (2016b)</ref>, which are among the best performing models. <ref type="bibr" target="#b22">Parikh et al. (2016)</ref> propose a relatively simple but very effec icated combinations of attention models, which provide about 0.5% gain over the results reported by <ref type="bibr" target="#b22">Parikh et al. (2016)</ref>.</p><p>It is, however, not very clear if t r">(Mac-Cartney, 2009)</ref>. In neural network models, this is often achieved with soft attention. <ref type="bibr" target="#b22">Parikh et al. (2016)</ref> decomposed this process: the word sequence -trained word embedding by itself does not automatically consider the context around a word in NLI. <ref type="bibr" target="#b22">Parikh et al. (2016)</ref> did take into account the word order and c his plays an important role in achieving our best results, and the intra-sentence attention used by <ref type="bibr" target="#b22">Parikh et al. (2016)</ref> actually does not further improve over our TM to encode the premise and hypothesis, respectively. In our sequential inference model, unlike in <ref type="bibr" target="#b22">Parikh et al. (2016)</ref> which proposed to use a function F (ā i ), " target="#b6">(Cheng et al., 2016)</ref> link the current word to previous words stored in memory. <ref type="bibr" target="#b22">Parikh et al. (2016)</ref> proposed a decomposable attention model wi layers. If we remove the pooling layer in inference composition and replace it with summation as in <ref type="bibr" target="#b22">Parikh et al. (2016)</ref>, the accuracy drops to 87.1%. If we remove inference enhancement layer, the accuracy drops to 87.0%. To provide some detailed comparison with <ref type="bibr" target="#b22">Parikh et al. (2016)</ref>, replacing bidirectional LSTMs in inferenc to the final classifier to determine the overall inference relationship. We consider that summation <ref type="bibr" target="#b22">(Parikh et al., 2016</ref>) could be sensitive to the sequence length n <ref type="bibr">(Cheng et al., 2016) 3.4M 88.5 86.3 (11)</ref> 200D decomposable attention model <ref type="bibr" target="#b22">(Parikh et al., 2016)</ref> 380K 89.5 86.3 (12) Intra-sentence attent bibr" target="#b22">(Parikh et al., 2016)</ref> 380K 89.5 86.3 (12) Intra-sentence attention + (11) <ref type="bibr" target="#b22">(Parikh et al., 2016)</ref> 580K 90.5 86.8 (13) 300D NTI-SLSTM-LSTM <
rget="#b20">Munkhdalai and Yu, 2016b;</ref><ref type="bibr" target="#b22">Parikh et al., 2016;</ref><ref type="bibr" target="#b27">Sha et al., 2016;</ref><ref type="bibr" target="#b21">Paria et al., 2 rget="#b22">Parikh et al., 2016;</ref><ref type="bibr" target="#b20">Munkhdalai and Yu, 2016b;</ref><ref type="bibr" target="#b27">Sha et al., 2016;</ref><ref type="bibr" target="#b21">Paria et al., 2 ef type="bibr" target="#b20">(Munkhdalai and Yu, 2016b)</ref> 3.2M 88.5 87.3 (14) 300D re-read LSTM <ref type="bibr" target="#b27">(Sha et al., 2016)</ref> 2.0M 90.7 87.5 (15) 300D btree-LSTM encoders get="#b32">Wang and Jiang (2016)</ref> to a full n-ary tree model and achieves further improvement. <ref type="bibr" target="#b27">Sha et al. (2016)</ref> proposes a special LSTM variant which conside
ibr" target="#b15">Le and Zuidema, 2015)</ref>, which extends the chain LSTM to a recursive network <ref type="bibr" target="#b28">(Socher et al., 2011)</ref>.</p><p>Specifically, given the parse of a
" target="#b2">(Bowman et al., 2016;</ref><ref type="bibr" target="#b31">Vendrov et al., 2015;</ref><ref type="bibr" target="#b19">Mou et al., 2016;</ref><ref type="bibr" target="#b16">Liu et al., 201 ent-wise product are then concatenated with the original vectors, ā and ã, or b and b, respectively <ref type="bibr" target="#b19">(Mou et al., 2016;</ref><ref type="bibr">Zhang et al., 2017)</ref>. T 015)</ref> uses unsupervised "skip-thoughts" pre-training in GRU encoders. The approach proposed by <ref type="bibr" target="#b19">Mou et al. (2016)</ref> considers tree-based CNN to capture sentence- pe="bibr" target="#b31">(Vendrov et al., 2015)</ref> 15M 98.8 81.4 (4) 300D tree-based CNN encoders <ref type="bibr" target="#b19">(Mou et al., 2016)</ref> 3.5M 83.3 82.1 (5) 300D SPINN-PI encoders <r
e to the space limit, we will skip the description of the basic chain LSTM and readers can refer to <ref type="bibr" target="#b11">Hochreiter and Schmidhuber (1997)</ref> for details. Briefly, when mo
y challenging but is a basic problem towards true natural language understanding, as pointed out by <ref type="bibr" target="#b18">MacCartney and Manning (2008)</ref>, "a necessary (if not sufficient)
ng, most notably the group of NLP models known as word2vec <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. A number of recent research publications have proposed wor learn the distributed representations of words in a corpus <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Inspired by it, DeepWalk <ref type="bibr" target="#b21">[2 lelized by using the same mechanism as word2vec and node2vec <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. All codes are implemented in C and C++ and our experiments e distributed representations of words in natural language <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Building on word2vec, Perozzi et al. suggested that the "c ghborhoods with network semantics for various types of nodes. Second, we extend the skip-gram model <ref type="bibr" target="#b17">[18]</ref> to facilitate the modeling of geographically and semantica p 2 &amp; p 3 ).</p><p>To achieve e cient optimization, Mikolov et al. introduced negative sampling <ref type="bibr" target="#b17">[18]</ref>, in which a relatively small set of words (nodes) are samp aximize the network probability in terms of local structures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>, that is:</p><formu d as a so max function <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>, that is:</p><formu
e <ref type="table">1</ref>: Case study of similarity search in the heterogeneous DBIS data used in <ref type="bibr" target="#b25">[26]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> "#b25">[26]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>PathSim <ref type="bibr" target="#b25">[26]</ref> DeepWalk / node2vec <ref type="bibr" target="#b7">[8,</ref all in NIPS and the other has 10 publications all in ICML; their "APCPA"-based Path-Sim similarity <ref type="bibr" target="#b25">[26]</ref> would be zero-this will be naturally overcome by network r s well as the top-ve similarity search results in the DBIS network for the same two queries used in <ref type="bibr" target="#b25">[26]</ref> (see Section 4 for details). By modeling the heterogeneous -and concentrated nodes-those with a governing percentage of paths pointing to a small set of nodes <ref type="bibr" target="#b25">[26]</ref>.</p><p>In light of these issues, we design meta-path-based t <ref type="bibr" target="#b31">[31]</ref> and the Database and Information Systems (DBIS) dataset <ref type="bibr" target="#b25">[26]</ref>. Both datasets and code are publicly available 1 . is AMin ch as collaboration relationships on a paper. e DBIS dataset was constructed and used by Sun et al. <ref type="bibr" target="#b25">[26]</ref>. It covers 464 venues, their top-5000 authors, and corresp "APA" and "APVPA" <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. Notice that "APA "#b12">[13]</ref>, node clustering <ref type="bibr" target="#b26">[27]</ref>, and similarity search <ref type="bibr" target="#b25">[26]</ref>. In addition, we also use the embedding projector in Tenso luster the data and evaluate the clustering results in terms of normalized mutual information (NMI) <ref type="bibr" target="#b25">[26]</ref>. In addition, we also report metapath2vec++'s sensitivity type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, and similarity search <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">35]</ref>. In contrast to co s, its rst node type V 1 is the same with the last one V l <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>, facilitating its r
t="#b15">16]</ref>, node classi cation <ref type="bibr" target="#b32">[32]</ref>, relational mining <ref type="bibr" target="#b18">[19]</ref>, and role discovery <ref type="bibr" target="#b8">[9]</ref
ication of factorization models for recommendation systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, node classi cation <ref type="bibr" target="#b32">[32]</re
ef type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, node classi cation <ref type="bibr" target="#b32">[32]</ref>, relational mining <ref type="bibr" target="#b18">[19]</re

target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>. In particular, to
="#b32">[32]</ref>, relational mining <ref type="bibr" target="#b18">[19]</ref>, and role discovery <ref type="bibr" target="#b8">[9]</ref>. is rich line of research focuses on factorizing the matrix/
ef type="bibr" target="#b21">22]</ref> LINE (1st+2nd) <ref type="bibr" target="#b29">[30]</ref> PTE <ref type="bibr" target="#b28">[29]</ref> metapath2vec metapath2vec++</p><p>Input meta-paths heterog he metapath2vec and metapath2vec++ models also di er from the Predictive Text Embedding (PTE) model <ref type="bibr" target="#b28">[29]</ref> in several ways. First, PTE is a semi-supervised learning to one type of neighbors-venues V , authors A, organizations O, and papers P.</p><p>Inspired by PTE <ref type="bibr" target="#b28">[29]</ref>, the sampling distribution is also speci ed by the node ty e the advanced version of LINE by considering both the 1st-and 2nd-order of node proximity; (3) PTE <ref type="bibr" target="#b28">[29]</ref>: We construct three bipartite heterogeneous networks (auth 9">[30]</ref>, which was further developed into a semi-supervised model PTE for embedding text data <ref type="bibr" target="#b28">[29]</ref>.</p><p>Our work furthers this direction of investigation b
t factor models for network analysis and graph mining tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">34]</ref>, such as the application of factorization models for recomm
lications have proposed word2vec-based network representation learning frameworks, such as DeepWalk <ref type="bibr" target="#b21">[22]</ref>, LINE <ref type="bibr" target="#b29">[30]</ref>, and node2 e="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Inspired by it, DeepWalk <ref type="bibr" target="#b21">[22]</ref> and node2vec <ref type="bibr" target="#b7">[8]</ref> aim t ed Random Walks. How to e ectively transform the structure of a network into skip-gram? In DeepWalk <ref type="bibr" target="#b21">[22]</ref> and node2vec <ref type="bibr" target="#b7">[8]</ref>, this and metapath2vec++ with several recent network representation learning methods:</p><p>(1) DeepWalk <ref type="bibr" target="#b21">[22]</ref> / node2vec <ref type="bibr" target="#b7">[8]</ref>: With t suggested that the "context" of a node can be denoted by their co-occurrence in a random walk path <ref type="bibr" target="#b21">[22]</ref>. Formally, they put random walkers over networks to record ref type="bibr" target="#b25">[26]</ref> DeepWalk / node2vec <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref> LINE (1st+2nd) <ref type="bibr" target="#b29">[30]</ref> PT of local structures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>, that is:</p><formula xml:id="formula_2">arg max θ ∈V c ∈N etwork embedding models, which focus on homogeneous networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>. Speci cally, conve the proximity between a node and its neighborhood (context) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>. In a heterogeneous
f an LIP cache. We classify instruction cache misses into three categories as originally defined in <ref type="bibr" target="#b48">[51]</ref>. Non-repetitive misses do not belong to any recurring patt onding patterns.</p><p>We propose to use the Temporal Instruction Fetch Streaming (TIFS) prefetcher <ref type="bibr" target="#b48">[51]</ref> to prefetch recurring missing instructions. TIFS predicts nal hardware cost, we choose LIP instead of BIP. TIFS is implemented as described by Ferdman et al. <ref type="bibr" target="#b48">[51]</ref>. We find that it is sufficient for the IML to keep track o
ny SPEC CPU 2006 applications whose code footprints are attributed to a few hot static instructions <ref type="bibr" target="#b41">[44]</ref> and yet do not suffer from poor I-TLB performance.</p><p>T
rmine the number of repetitive patterns in the instruction cache miss sequence. We use the SEQUITUR <ref type="bibr" target="#b47">[50]</ref> tool, which is widely used to detect patterns in a given s
negligible energy costs, and therefore scaling the TLB sizes will likely increase energy per access <ref type="bibr" target="#b42">[45,</ref><ref type="bibr" target="#b43">46]</ref>. In fact, TLB size
unique microarchitecture bottlenecks of Node.js. Other serverside applications, such as CloudSuite <ref type="bibr" target="#b29">[32]</ref>, MapReduce <ref type="bibr" target="#b30">[33]</ref>, BigD e unique microarchitecture bottlenecks of Node.js. Other serverside applications, such as CloudSuite<ref type="bibr" target="#b29">[32]</ref>, MapReduce<ref type="bibr" target="#b30">[33]</ref>, BigDa umerous research efforts have been devoted to characterizing warehouse-scale and big data workloads <ref type="bibr" target="#b29">[32,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" ta
code reuse. Recent studies on client-side event-driven applications also derive similar conclusions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. We take this rese cution Analysis Prior work on event-driven applications primarily focus on client-side applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> whereas we study s
t="#b32">35,</ref><ref type="bibr" target="#b52">[56]</ref><ref type="bibr" target="#b53">[57]</ref><ref type="bibr" target="#b54">[58]</ref>.</p><p>We address a new and emerging computing paradigm, i
get="#b1">[2]</ref>, type checking <ref type="bibr" target="#b2">[3]</ref>, exploiting parallelisms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, and leveraging hardwa
ny SPEC CPU 2006 applications whose code footprints are attributed to a few hot static instructions <ref type="bibr" target="#b41">[44]</ref> and yet do not suffer from poor I-TLB performance.</p><p>T
et="#b29">[32,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b32">35,</ref><ref type="bibr" target="#b52">[56]</ref><ref type="bibr" target="#b53">[57]</ref><ref type="bibr" t
event-driven applications also derive similar conclusions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. We take this research a step further to make the key obser n applications primarily focus on client-side applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> whereas we study server-side applications. While prior art
close attention to map matching techniques on low-samplingrate trajectories. ST-Matching algorithm <ref type="bibr" target="#b3">[4]</ref> is the first one solving the problem about low-sampling-rate ter information, such as the data from mobile devices <ref type="bibr" target="#b12">[13]</ref>. In <ref type="bibr" target="#b3">[4]</ref>, the author combines the spatial structures of the road netw
and interactive voting. HRIS <ref type="bibr" target="#b5">[6]</ref> and feature-based-map-matching <ref type="bibr" target="#b6">[7]</ref> concerned on the history features and action features of tra nt factors. History information is referred to in <ref type="bibr" target="#b5">[6]</ref> and paper <ref type="bibr" target="#b6">[7]</ref> takes human factors into consideration to estimate the missi
ref>, hot route discover <ref type="bibr" target="#b1">[2]</ref> and even social relationship infer <ref type="bibr" target="#b2">[3]</ref>. However, due to the limitations caused by the equipment acc
the patterns connected by whole or part of trajectories. E.g., matching a point to the nearest road <ref type="bibr" target="#b7">[8]</ref>. Topological map matching methods concentrate on the connect
rom company to home. This phenomenon can be illustrated by the conception of stay point proposed in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Usually, a stay point s is determined by two parame ><p>Usually, we don't have to pay more attention to the stay points which are easy for map matching <ref type="bibr" target="#b16">[17]</ref>. Therefore, we partition the trajectories into several eff
re algorithms <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> try to solve the problem of low-sampling-rate trajectories
ms: one uses the Fr?chet distance to measure the similarity between trajectories and road sequences <ref type="bibr" target="#b8">[9]</ref>, another considers incremental model which means measuring a
weight of different points. After that, more algorithms <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> try to solve th
incremental model which means measuring a point and judging the consecutive points at the same time <ref type="bibr" target="#b9">[10]</ref>.</p><p>Map matching algorithms for low-sampling-rate trajec
weight of different points. After that, more algorithms <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> try to solve th
get="#b3">[4]</ref> is the first one solving the problem about low-sampling-rate map matching. IVMM <ref type="bibr" target="#b4">[5]</ref> improved ST-Matching algorithm by adding weighted influence he accuracy of map matching for trajectories with time interval no less than 2 minutes. Then, paper <ref type="bibr" target="#b4">[5]</ref> improves ST-Matching algorithm by considering the weight of
it.</p><p>This paper presents the results of our study of the nonuniform cache architecture (NUCA) <ref type="bibr" target="#b36">[35]</ref> characteristics of LLC in Intel processors where the LLC i target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b66">65,</ref><ref type="bibr" tar
t="#b65">64,</ref><ref type="bibr" target="#b83">[81]</ref><ref type="bibr" target="#b84">[82]</ref><ref type="bibr" target="#b85">[83]</ref> hardware-based cache partitioning to better use the LLC in
ation profiling without considering Intel's LLC Complex Addressing. In contrast, other works (e.g., <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b87">85]</ref>) extended traditio
e to the explosion of data and the advent of hundred gigabit per second networks (100/200/400 Gbps) <ref type="bibr" target="#b8">[9]</ref>. Introducing faster links exposes processing elements to pac
ave also been efforts to optimize the kernel network stack <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b60">59]</ref>. Additionally, seve
get="#b37">36,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b67">66]</ref>) proposed software techniques for cache-aware memory alloca
Measurement Method. For measuring end-to-end latency, we follow the black box approach explained in <ref type="bibr" target="#b19">[19]</ref>, where data is collected on the egress/ingress port of the
modifications to the CPU architecture -such as data migration, data placement, and data replication <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target
</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" target="#b77">76]</ref> or exploited <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b64">63,</ref><ref type="bibr" ta
rget="#b7">[8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b63">62,</ref><ref type="bibr" tar
/ref><ref type="bibr" target="#b77">76]</ref> or exploited <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b64">63,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" tar
rget="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar algorithm design, our work is most closely related to Hamilton et al. (2017a)'s GraphSAGE algorithm <ref type="bibr" target="#b17">[18]</ref> and the closely related follow-up work of <ref type="bibr" nodes to aggregate from allows us to control the memory footprint of the algorithm during training <ref type="bibr" target="#b17">[18]</ref>. Second, it allows Algorithm 1 to take into account the im ean); • mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in <ref type="bibr" target="#b17">[18]</ref>. • mean-pooling-hard is the same as mean-pooling, except t ing and cross-entropy settings are extensions of the best-performing GCN model from Hamilton et al. <ref type="bibr" target="#b17">[18]</ref>-other variants (e.g., based on Kipf et al. <ref type="bibr
for graph-structured data.</p><p>The notion of neural networks for graph data was first outlined in <ref type="bibr" target="#b14">Gori et al. (2005)</ref>  <ref type="bibr" target="#b14">[15]</ref> a works for graph data was first outlined in <ref type="bibr" target="#b14">Gori et al. (2005)</ref>  <ref type="bibr" target="#b14">[15]</ref> and further elaborated on in <ref type="bibr" target="#b26
ang="en"> 		<body> <div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref><p>. The representations learned using deep models can be us e of deep learning approaches for generating such embeddings <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>. We also conduct ab
spectral convolutions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar
get="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Most prominent among these recent advancements is th
it user-to-item interaction graphs as well as social graphs) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar ncements is the success of deep learning architectures known as Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta tructure. GCN-based methods have set a new standard on countless recommender system benchmarks (see <ref type="bibr" target="#b18">[19]</ref> for a survey). However, these gains on benchmark tasks hav [20,</ref><ref type="bibr" target="#b30">31]</ref>. <ref type="bibr">Hamilton et al. (2017b)</ref>  <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b5">Bronstein et al. (2017)< ng and Inference Runtime Analysis</head><p>One advantage of GCNs is that they can be made inductive <ref type="bibr" target="#b18">[19]</ref>: at the inference (i.e., embedding generation) step, we ar
actorization or random walks (e.g., node2vec <ref type="bibr" target="#b16">[17]</ref> and DeepWalk <ref type="bibr" target="#b25">[26]</ref>), and their success has led to a surge of interest in appl e that graph embedding methods like node2vec <ref type="bibr" target="#b16">[17]</ref> and DeepWalk <ref type="bibr" target="#b25">[26]</ref> cannot be applied here. First, these are unsupervised meth
hSAGE algorithm <ref type="bibr" target="#b17">[18]</ref> and the closely related follow-up work of <ref type="bibr" target="#b7">Chen et al. (2018)</ref>  <ref type="bibr" target="#b7">[8]</ref>. Gra > and the closely related follow-up work of <ref type="bibr" target="#b7">Chen et al. (2018)</ref>  <ref type="bibr" target="#b7">[8]</ref>. GraphSAGE is an inductive variant of GCNs that we modify to
xtensions, and approximations of these spectral convolutions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ
4">Gori et al. (2005)</ref>  <ref type="bibr" target="#b14">[15]</ref> and further elaborated on in <ref type="bibr" target="#b26">Scarselli et al. (2009)</ref>  <ref type="bibr" target="#b26">[27]</r 5]</ref> and further elaborated on in <ref type="bibr" target="#b26">Scarselli et al. (2009)</ref>  <ref type="bibr" target="#b26">[27]</ref>. However, these initial approaches to deep learning on gra
get="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>, leading to new state-of-the-art results on benchmarks such s <ref type="bibr" target="#b23">[24]</ref> to drug design <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref>. <ref type="bibr">Hamilton et al. (2017b)</ref>  <ref type=
="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref>, and fine-grain reconfigurable processors <ref type="bibr" target="#b64">[64]</ref> are all examples.</p><p>Highly configurable processors hav ile maintaining a homogeneous fabric. The CASH architecture is inspired by the Sharing Architecture <ref type="bibr" target="#b64">[64]</ref>, but improves on it with fast reconfiguration, a well defi type="bibr" target="#b4">[5]</ref> on the CASH Architecture (which extends the Sharing Architecture <ref type="bibr" target="#b64">[64]</ref>). The video encoder is an excellent example of our target 1.0"><head>A. Architecture Overview</head><p>The CASH architecture extends the Sharing Architecture <ref type="bibr" target="#b64">[64]</ref> -a prior configurable core architecture -by <ref type="bib et="#b14">[15,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b64">64]</ref> allow fine grain control over resource scheduling. As fine- able architectures <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b64">64]</ref>. The one drawback of these approaches is that fine-grain co
Cloud. Microservers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b59">59]</ref>, data center optimized accelerators <ref type="bibr" target ">[55]</ref>, Tilera <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60]</ref>, TRIPS <ref type="b
ax k {s k /c k |s k &lt; s(t)} t over = ? ?</formula><p>s(t)s under s overs under t under = ?t over <ref type="bibr" target="#b5">(6)</ref> Thus, if we know s k and c k for all k then the optimization ce and Slice-to-memory latency. SSim is driven by the full system version of the the Alpha ISA GEM5 <ref type="bibr" target="#b5">[6]</ref> simulator. Table <ref type="table" target="#tab_3">I</ref> s
et="#b48">49]</ref> have been proposed to perform online adaptation using offline training. Flicker <ref type="bibr" target="#b41">[42]</ref> also assembles "virtual cores" using dynamic management, b
ources, which is complementary to those techniques. Runtime systems for datacenter resource sharing <ref type="bibr" target="#b61">[61]</ref> improve server utilization while guaranteeing QoS. Bubble-
e architectural optimization approaches have been proposed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b62">62]</ref>, but they have limi
e. Scheduling algorithms for heterogeneous processors have been proposed for MS Bing's index search <ref type="bibr" target="#b44">[45]</ref>. While heterogeneous processors improve interactive data c
get="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref> and configurable pipeline architectures (modifiable issue w
ty of warehouse scale computer systems. Hardware/Software co-design for datacenter power management <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" ta
k has provided more general implementations by implementing control systems at the middleware layer <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" ta
2">23,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60]</ref>, TRIPS <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, and Wavescalar <r ansport of operands, and dynamic instruction ordering. TRIPS <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref> has an array of ALUs connected by a Scalar Operand Network
p>In the simplest case, for unlabeled points x, prior work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref> adds the loss term</p><formula xml:id="formula_0">p model ( ered in <ref type="bibr" target="#b34">[35]</ref> (Π-Model <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>, Mean Teacher <ref type="bibr" target="#b43">[44]</ref>, Vi ed example is common in consistency regularization methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Algorithm 1
is loss term falls into one of three classes (discussed further in Section 2): entropy minimization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>-which encourages t re that the classifier output low-entropy predictions on unlabeled data. This is done explicitly in <ref type="bibr" target="#b17">[18]</ref> with a loss term which minimizes the entropy of p model (y
gment(x), an augmentation of itself.</p><p>In the simplest case, for unlabeled points x, prior work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref> adds the loss term btain an artificial target for an unlabeled example is common in consistency regularization methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" ta itive to incorrect predictions. For this reason, it is often used as the unlabeled data loss in SSL <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44]</ref> as well as a measu b25">[26]</ref>. We do not propagate gradients through computing the guessed labels, as is standard <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" ta ines, we consider the four methods considered in <ref type="bibr" target="#b34">[35]</ref> (Π-Model <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>, Mean Teacher <ref
rder to memorize the training data and therefore hopefully make it generalize better to unseen data <ref type="bibr" target="#b18">[19]</ref>. We use weight decay which penalizes the L 2 norm of the m
ses (discussed further in Section 2): entropy minimization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>-which encourages the model to output confident predictions ed with VAT in <ref type="bibr" target="#b30">[31]</ref> to obtain stronger results. "Pseudo-Label" <ref type="bibr" target="#b27">[28]</ref> does entropy minimization implicitly by constructing hard 44]</ref>, Virtual Adversarial Training <ref type="bibr" target="#b30">[31]</ref>, and Pseudo-Label <ref type="bibr" target="#b27">[28]</ref>) which are described in section 2. We also use MixUp <ref
input and its label. Hence, approaches for deep learning from private training data, such as DP-SGD <ref type="bibr" target="#b0">[1]</ref> and PATE <ref type="bibr" target="#b35">[36]</ref>, benefit
ansductive" models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref>, graph-based methods <ref type="bibr" target="#b48">[49,</r
arget="#b40">41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" tar
ef type="bibr" target="#b23">[24]</ref>, SVHN <ref type="bibr" target="#b31">[32]</ref>, and STL-10 <ref type="bibr" target="#b7">[8]</ref>. Standard practice for evaluating semi-supervised learning o
ef type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref>, graph-based methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targ </ref><ref type="bibr" target="#b41">42]</ref>, etc.). More comprehensive overviews are provided in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6]</ref>. In the following, w
rget="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" targe
ere is a wide literature on SSL techniques that we do not discuss here (e.g., "transductive" models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta
e cross-validation is difficult with small validation sets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref>. However, we find i
arget="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar
<ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29]</ref>, generative modeling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" targ
et="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref> </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n ll validation sets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref>. However, we find in practice that most of MixMatch's hyper hyperparameters can be problematic because cross-validation is difficult with small validation sets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta details</head><p>Unless otherwise noted, in all experiments we use the "Wide ResNet-28" model from <ref type="bibr" target="#b34">[35]</ref>. Our implementation of the model and training procedure cl t="#b34">[35]</ref>. Our implementation of the model and training procedure closely matches that of <ref type="bibr" target="#b34">[35]</ref> (including using 5000 examples to select the hyperparamete ><head n="4.2.1">Baseline Methods</head><p>As baselines, we consider the four methods considered in <ref type="bibr" target="#b34">[35]</ref> (Π-Model <ref type="bibr" target="#b24">[25,</ref><ref typ ss-entropy loss between the MixUp-generated guess label and the model's prediction. As advocated by <ref type="bibr" target="#b34">[35]</ref>, we reimplemented each of these methods in the same codeba h baseline method, which generally resulted in a marginal accuracy improvement compared to those in <ref type="bibr" target="#b34">[35]</ref>, thereby providing a more competitive experimental setting ef> has also considered the use of a larger, 26 million-parameter model. Our base model, as used in <ref type="bibr" target="#b34">[35]</ref>, has only 1.5 million parameters which confounds compariso
aches is that they use domain-specific data augmentation strategies. "Virtual Adversarial Training" <ref type="bibr" target="#b30">[31]</ref> (VAT) addresses this by instead computing an additive pert p model (y | x; θ) for unlabeled data x. This form of entropy minimization was combined with VAT in <ref type="bibr" target="#b30">[31]</ref> to obtain stronger results. "Pseudo-Label" <ref type="bibr 39">40]</ref>, Mean Teacher <ref type="bibr" target="#b43">[44]</ref>, Virtual Adversarial Training <ref type="bibr" target="#b30">[31]</ref>, and Pseudo-Label <ref type="bibr" target="#b27">[28]</ref of 11.08% with only 250 labels. For comparison, at 250 labels the next-best-performing method (VAT <ref type="bibr" target="#b30">[31]</ref>) achieves an error rate of 36.03, over 4.5× higher than Mi mplars of each class.</p><p>We compare the accuracy-privacy trade-off achieved by MixMatch to a VAT <ref type="bibr" target="#b30">[31]</ref> baseline on SVHN.</p><p>VAT achieved the previous state-of ls, as is standard <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref> </p></div> <div xml
ef type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref>, graph-based methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targ </ref><ref type="bibr" target="#b41">42]</ref>, etc.). More comprehensive overviews are provided in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6]</ref>. In the following, w
">[25,</ref><ref type="bibr" target="#b43">44]</ref> as well as a measure of predictive uncertainty <ref type="bibr" target="#b25">[26]</ref>. We do not propagate gradients through computing the guess
s that we do not discuss here (e.g., "transductive" models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref>, graph-based method
mance with all labels in the corresponding training set. sophisticated "shake-shake" regularization <ref type="bibr" target="#b14">[15]</ref>. For this model, we used a weight decay of 0.0008. We used
ses (discussed further in Section 2): entropy minimization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>-which encourages the model to output confident predictions ed with VAT in <ref type="bibr" target="#b30">[31]</ref> to obtain stronger results. "Pseudo-Label" <ref type="bibr" target="#b27">[28]</ref> does entropy minimization implicitly by constructing hard 44]</ref>, Virtual Adversarial Training <ref type="bibr" target="#b30">[31]</ref>, and Pseudo-Label <ref type="bibr" target="#b27">[28]</ref>) which are described in section 2. We also use MixUp <ref
ansductive" models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref>, graph-based methods <ref type="bibr" target="#b48">[49,</r
o terms in eq. ( <ref type="formula" target="#formula_0">1</ref>) are not identical. "Mean Teacher" <ref type="bibr" target="#b43">[44]</ref> replaces one of the terms in eq. ( <ref type="formula" tar arly ramp up λ U to its maximum value over the first 16,000 steps of training as is common practice <ref type="bibr" target="#b43">[44]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head del <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>, Mean Teacher <ref type="bibr" target="#b43">[44]</ref>, Virtual Adversarial Training <ref type="bibr" target="#b3 ully supervised learning. In addition, at 4000 labels the next-best-performing method (Mean Teacher <ref type="bibr" target="#b43">[44]</ref>) obtains an error rate of 10.36%, which suggests that MixM singly, after additional tuning we were able to obtain extremely good performance from Mean Teacher <ref type="bibr" target="#b43">[44]</ref>, though its error rate was consistently slightly higher th moving average (EMA) of model parameters when producing guessed labels, as is done by Mean Teacher <ref type="bibr" target="#b43">[44]</ref> • performing MixUp between labeled examples only, unlabele larization methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Algorithm 1 MixMatch takes a batch of labeled data X reason, it is often used as the unlabeled data loss in SSL <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44]</ref> as well as a measure of predictive uncertainty <ref type="b ients through computing the guessed labels, as is standard <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar mple efficiency which is central to SSL. CIFAR-10 and CIFAR-100 with a larger model Some prior work <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b1">2]</ref> has also considered >, though we note that the comparison still remains problematic due to the fact that the model from <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b1">2]</ref>   <ref type="table">
aches is that they use domain-specific data augmentation strategies. "Virtual Adversarial Training" <ref type="bibr" target="#b30">[31]</ref> (VAT) addresses this by instead computing an additive pert p model (y | x; θ) for unlabeled data x. This form of entropy minimization was combined with VAT in <ref type="bibr" target="#b30">[31]</ref> to obtain stronger results. "Pseudo-Label" <ref type="bibr 39">40]</ref>, Mean Teacher <ref type="bibr" target="#b43">[44]</ref>, Virtual Adversarial Training <ref type="bibr" target="#b30">[31]</ref>, and Pseudo-Label <ref type="bibr" target="#b27">[28]</ref of 11.08% with only 250 labels. For comparison, at 250 labels the next-best-performing method (VAT <ref type="bibr" target="#b30">[31]</ref>) achieves an error rate of 36.03, over 4.5× higher than Mi mplars of each class.</p><p>We compare the accuracy-privacy trade-off achieved by MixMatch to a VAT <ref type="bibr" target="#b30">[31]</ref> baseline on SVHN.</p><p>VAT achieved the previous state-of ls, as is standard <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref> </p></div> <div xml
o terms in eq. ( <ref type="formula" target="#formula_0">1</ref>) are not identical. "Mean Teacher" <ref type="bibr" target="#b43">[44]</ref> replaces one of the terms in eq. ( <ref type="formula" tar arly ramp up λ U to its maximum value over the first 16,000 steps of training as is common practice <ref type="bibr" target="#b43">[44]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head del <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>, Mean Teacher <ref type="bibr" target="#b43">[44]</ref>, Virtual Adversarial Training <ref type="bibr" target="#b3 ully supervised learning. In addition, at 4000 labels the next-best-performing method (Mean Teacher <ref type="bibr" target="#b43">[44]</ref>) obtains an error rate of 10.36%, which suggests that MixM singly, after additional tuning we were able to obtain extremely good performance from Mean Teacher <ref type="bibr" target="#b43">[44]</ref>, though its error rate was consistently slightly higher th moving average (EMA) of model parameters when producing guessed labels, as is done by Mean Teacher <ref type="bibr" target="#b43">[44]</ref> • performing MixUp between labeled examples only, unlabele larization methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Algorithm 1 MixMatch takes a batch of labeled data X reason, it is often used as the unlabeled data loss in SSL <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44]</ref> as well as a measure of predictive uncertainty <ref type="b ients through computing the guessed labels, as is standard <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar mple efficiency which is central to SSL. CIFAR-10 and CIFAR-100 with a larger model Some prior work <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b1">2]</ref> has also considered >, though we note that the comparison still remains problematic due to the fact that the model from <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b1">2]</ref>   <ref type="table">
ses (discussed further in Section 2): entropy minimization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>-which encourages the model to output confident predictions ed with VAT in <ref type="bibr" target="#b30">[31]</ref> to obtain stronger results. "Pseudo-Label" <ref type="bibr" target="#b27">[28]</ref> does entropy minimization implicitly by constructing hard 44]</ref>, Virtual Adversarial Training <ref type="bibr" target="#b30">[31]</ref>, and Pseudo-Label <ref type="bibr" target="#b27">[28]</ref>) which are described in section 2. We also use MixUp <ref
">[25,</ref><ref type="bibr" target="#b43">44]</ref> as well as a measure of predictive uncertainty <ref type="bibr" target="#b25">[26]</ref>. We do not propagate gradients through computing the guess
rder to memorize the training data and therefore hopefully make it generalize better to unseen data <ref type="bibr" target="#b18">[19]</ref>. We use weight decay which penalizes the L 2 norm of the m
e cross-validation is difficult with small validation sets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref>. However, we find i
rder to memorize the training data and therefore hopefully make it generalize better to unseen data <ref type="bibr" target="#b18">[19]</ref>. We use weight decay which penalizes the L 2 norm of the m
ansductive" models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref>, graph-based methods <ref type="bibr" target="#b48">[49,</r
standard benchmark datasets: CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b23">[24]</ref>, SVHN <ref type="bibr" target="#b31">[32]</ref>, and STL-10 <ref type="bibr" target="#b7">[8]</ref>. Stand
ue of memory instructions. Chrysos and Emer proposed using Store Sets to predict memory dependences <ref type="bibr" target="#b2">[3]</ref>. They show that an out-of-order processor using store sets a rresponding store is added to the load's store set.</p><p>The store set implementation presented in <ref type="bibr" target="#b2">[3]</ref> uses a pair of tables to predict memory dependences. The fir ssive processor model that is comparable to the configuration used in the original store sets study <ref type="bibr" target="#b2">[3]</ref> and the 8-wide configuration from the Stack Value File study e sets based memory bypassing. In both cases, we also use the set merging optimization described in <ref type="bibr" target="#b2">[3]</ref>, which we found to improve performance slightly. We simulate . Chrysos and Emer proposed store sets for memory dependence prediction which we used in this study <ref type="bibr" target="#b2">[3]</ref>. The Alpha 21264 speculatively issues loads, but uses a simp
ta to the dependents of a load by combining memory dependence prediction with load value prediction <ref type="bibr" target="#b7">[8]</ref>. Jourdan et al explore several renaming-based techniques for
le 1. We use a McFarling style hybrid branch predictor (gshare/PAs) to predict conditional branches <ref type="bibr" target="#b8">[9]</ref>.</p><p>The simulated benchmarks come from the SPEC2000 in-te
e used memory dependence to speed up loads from memory. Tyson and Austin introduced memory renaming <ref type="bibr" target="#b15">[17]</ref>, which has similarities to memory bypassing. Memory renami
nput sets from the test data set and the reduced run-length inputs from the University of Minnesota <ref type="bibr" target="#b5">[6]</ref>. We skipped the initial start-up sections for each benchmark
squashes and reissues those instructions that are actually data-dependent on the misspeculated load <ref type="bibr" target="#b11">[12]</ref>. This results in higher amounts of instruction-level paral
ta to the dependents of a load by combining memory dependence prediction with load value prediction <ref type="bibr" target="#b7">[8]</ref>. Jourdan et al explore several renaming-based techniques for
ta to the dependents of a load by combining memory dependence prediction with load value prediction <ref type="bibr" target="#b7">[8]</ref>. Jourdan et al explore several renaming-based techniques for
squashes and reissues those instructions that are actually data-dependent on the misspeculated load <ref type="bibr" target="#b11">[12]</ref>. This results in higher amounts of instruction-level paral
squashes and reissues those instructions that are actually data-dependent on the misspeculated load <ref type="bibr" target="#b11">[12]</ref>. This results in higher amounts of instruction-level paral
e used memory dependence to speed up loads from memory. Tyson and Austin introduced memory renaming <ref type="bibr" target="#b15">[17]</ref>, which has similarities to memory bypassing. Memory renami
ge R-CNN framework <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23]</ref>, where detection is et="#b12">[13]</ref> introduced the idea of region-wise feature extraction. Later, the Faster R-CNN <ref type="bibr" target="#b29">[30]</ref> achieved further speeds-up by introducing a Region Proposa network. SSD <ref type="bibr" target="#b24">[25]</ref> detects objects in a way similar to the RPN <ref type="bibr" target="#b29">[30]</ref>, but uses multiple feature maps at different resolutions t We focus on modeling a multistage detection sub-network, and adopt, but are not limited to, the RPN <ref type="bibr" target="#b29">[30]</ref> for proposal detection.</p></div> <div xmlns="http://www.t evels. At inference, since the majority of the hypotheses produced by a proposal detector, e.g. RPN <ref type="bibr" target="#b29">[30]</ref> or selective search <ref type="bibr" target="#b32">[33]</r ">Object Detection</head><p>In this paper, we extend the two-stage architecture of the Faster R-CNN <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23]</ref>, shown in Figure < zed by its mean and variance, i.e. is replaced by ′ =( − )/ . This is widely used in the literature <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" targ PN+.</p><p>Detection Performance: Again, our implementations are better than the original detectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targ was further experimented on PAS-CAL VOC dataset <ref type="bibr" target="#b7">[8]</ref>. Following <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b24">25]</ref>, the models were t e noted. The sampling of the first detection stage follows <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref>. In the following stages, resampling is implemented by simp
ime object detection, by forwarding the input image once through an efficient backbone network. SSD <ref type="bibr" target="#b24">[25]</ref> detects objects in a way similar to the RPN <ref type="bib dataset <ref type="bibr" target="#b7">[8]</ref>. Following <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b24">25]</ref>, the models were trained on VOC2007 and VOC2012 trainval an
bject detectors are based on the two-stage R-CNN framework <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar ns in the R-CNN for speeds-up, the SPP-Net <ref type="bibr" target="#b16">[17]</ref> and Fast R-CNN <ref type="bibr" target="#b12">[13]</ref> introduced the idea of region-wise feature extraction. Lat b ), so as to minimize the bounding box 1 loss function, ( ( , b ), g ),a s suggested in Fast R-CNN <ref type="bibr" target="#b12">[13]</ref>. To encourage a regression invariant to scale and location n with = {0.5, 0.6, 0.7}, unless otherwise noted. The sampling of the first detection stage follows <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref>. In the following
bounding box regression, where a R-CNN is applied several times, to produce better bounding boxes. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta
et="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23]</ref>, where detection is framed as a multi-task learning problem , we extend the two-stage architecture of the Faster R-CNN <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23]</ref>, shown in Figure <ref type="figure">3 (a)</ref>. The first e original detectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23]</ref>. Still, the Cascade R-CNN improves on these baselines consi computations of the Faster R-CNN; while the MS-CNN <ref type="bibr" target="#b0">[1]</ref> and FPN <ref type="bibr" target="#b22">[23]</ref> detect high-recall proposals at multiple output layers, so et <ref type="bibr" target="#b31">[32]</ref>, R-FCN <ref type="bibr" target="#b3">[4]</ref> and FPN <ref type="bibr" target="#b22">[23]</ref> with ResNet backbone <ref type="bibr" target="#b17">[18]</ s were trained with =0 .5.I t is noted that our FPN+ implementation is better than the original FPN <ref type="bibr" target="#b22">[23]</ref>, providing a very strong baseline. In addition, the extens target="#b29">[30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Some works <
uced by a proposal detector, e.g. RPN <ref type="bibr" target="#b29">[30]</ref> or selective search <ref type="bibr" target="#b32">[33]</ref>, have low quality, the detector must be more discriminant
et="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23]</ref>, where detection is framed as a multi-task learning problem , we extend the two-stage architecture of the Faster R-CNN <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23]</ref>, shown in Figure <ref type="figure">3 (a)</ref>. The first e original detectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23]</ref>. Still, the Cascade R-CNN improves on these baselines consi computations of the Faster R-CNN; while the MS-CNN <ref type="bibr" target="#b0">[1]</ref> and FPN <ref type="bibr" target="#b22">[23]</ref> detect high-recall proposals at multiple output layers, so et <ref type="bibr" target="#b31">[32]</ref>, R-FCN <ref type="bibr" target="#b3">[4]</ref> and FPN <ref type="bibr" target="#b22">[23]</ref> with ResNet backbone <ref type="bibr" target="#b17">[18]</ s were trained with =0 .5.I t is noted that our FPN+ implementation is better than the original FPN <ref type="bibr" target="#b22">[23]</ref>, providing a very strong baseline. In addition, the extens target="#b29">[30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Some works <
target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Some works <ref type="bibr" target="#b9">[10,</ref>< source code was publicly available for FPN, our implementation details could be different. RoIAlign <ref type="bibr" target="#b15">[16]</ref> was used for a stronger baseline. This is denoted as FPN+ e="bibr" target="#b4">[5]</ref>, RetinaNet <ref type="bibr" target="#b23">[24]</ref> and Mask R-CNN <ref type="bibr" target="#b15">[16]</ref>. Compared to the best multi-stage detector on COCO, Attrac
age scale, with no further bells and whistles. All baseline detectors were reimplemented with Caffe <ref type="bibr" target="#b19">[20]</ref>, on the same codebase for fair comparison.</p></div> <div
rocedure to generate accurate proposals, and forwarded them to an accurate model (e.g. Fast R-CNN). <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b26">27]</ref> also attempted to
er bounding boxes. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref> used a multi-stage procedure to generate accurate proposals ><ref type="bibr" target="#b15">16]</ref>.</p><p>Some works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref> have argued that a ing, in the form of proposal accumulation, box voting, etc. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>, and has somewhat u pe="bibr" target="#b15">[16]</ref>. Compared to the best multi-stage detector on COCO, AttractioNet <ref type="bibr" target="#b10">[11]</ref>, although it used many enhancements, the vanilla Cascade R
cture used for NLP <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar on heads (as well as # attention channels) in multi-head attention layers similar to Shazeer et al. <ref type="bibr" target="#b33">[34]</ref>. Please refer to the supplementary material for a detailed line parallelism have been proposed as solutions to counter these challenges.</p><p>Mesh-Tensorflow <ref type="bibr" target="#b33">[34]</ref> follows the SPMD paradigm, which extends the Single Instru

" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target
allows researchers to easily scale neural networks for a large variety of machine learning tasks.  <ref type="bibr" target="#b4">[5]</ref> and model size for representative state-of-the-art image cla

tational units, which are then placed on different devices <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar


state-of-the-art image classification models in recent years <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target=
allows researchers to easily scale neural networks for a large variety of machine learning tasks.  <ref type="bibr" target="#b4">[5]</ref> and model size for representative state-of-the-art image cla
models in recent years <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target=
compared to DNNs <ref type="bibr" target="#b12">[13]</ref>. Recently, very deep CNNs architectures <ref type="bibr" target="#b13">[14]</ref> have also been shown to be successful in ASR <ref type="bi R, recently there have been several advancements in the computer vision community on very deep CNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref> that have not been TMs.</p><p>We are driven by same motivation that led to the success of very deep networks in vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta
oped for classical ASR <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target= nd decoder models using recurrent models with LSTMs <ref type="bibr" target="#b5">[6]</ref> or GRUs <ref type="bibr" target="#b3">[4]</ref>. However, their use of hierarchy in the encoders demonstrate eep CNN techniques to significantly improve over previous shallow seq2seq speech recognition models <ref type="bibr" target="#b3">[4]</ref>. Our best model achieves a WER of 10.53% where our baseline oder depth of the baseline model without using any convolutional layers. Our baseline model follows <ref type="bibr" target="#b3">[4]</ref> using the skip connection technique in its time reduction. T btained 10.5% WER without a language model, an 8.5% absolute improvement over published best result <ref type="bibr" target="#b3">[4]</ref>. While we demonstrated our results only on the seq2seq task,
cements in the computer vision community on very deep CNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref> that have not been * Work done as Google Brain interns. exp build such deeper models. NiN has seen great success in computer vision, building very deep models <ref type="bibr" target="#b17">[18]</ref>. We show how to apply NiN principles in hierarchical Recur on that led to the success of very deep networks in vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar
show how BN can be applied to seq2seq acoustic model encoders.</p><p>3. Residual Networks (ResNets) <ref type="bibr" target="#b22">[23]</ref> learns a residual function of the input through the usage ient problem. In this study, we use a residual CNN/LSTM, to train deeper networks. Residual network <ref type="bibr" target="#b22">[23]</ref> contains direct links between the lower layer outputs and et="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> -add depth of processing using more non-linearities and exp proposed recently to enable training of very deep networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar

new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target
rchitectures <ref type="bibr" target="#b13">[14]</ref> have also been shown to be successful in ASR <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta
new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target

Neural Networks (RNNs) <ref type="bibr" target="#b19">[20]</ref>.</p><p>2. Batch Normalization (BN) <ref type="bibr" target="#b20">[21]</ref> normalizes each layer's inputs to reduce internal covariat ="http://www.tei-c.org/ns/1.0"><head n="2.4.">Batch Normalization</head><p>Batch normalization (BN) <ref type="bibr" target="#b20">[21]</ref> is a technique to accelerate training and improve generali networks in vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> -add depth of proce
ry deep. Several architectures have been proposed recently to enable training of very deep networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta
dels, using recent developments in the vision community.</p><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b7">[8]</ref> have been successfully applied to many ASR tasks <ref type="
">[8]</ref> have been successfully applied to many ASR tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Unlike Deep Neural
lipping to 1 was applied, together with Gaussian weight noise N (0, 0.075) and L2 weight decay 1e−5 <ref type="bibr" target="#b29">[30]</ref>. We used ADAM with the default hyperparameters described i
ry deep. Several architectures have been proposed recently to enable training of very deep networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta
successful in ASR <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, using more non-linearities, but fewer parameters. Such a s
new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target
successful in ASR <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, using more non-linearities, but fewer parameters. Such a s
ariance properties, thus typically yield better generalized and more robust models compared to DNNs <ref type="bibr" target="#b12">[13]</ref>. Recently, very deep CNNs architectures <ref type="bibr" t CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref>, we investigate th
dels, using recent developments in the vision community.</p><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b7">[8]</ref> have been successfully applied to many ASR tasks <ref type="

target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Unlike Deep Neural Networks (DNNs) <ref type="bibr" target="#b11">[12]</ref>, CNNs explicitly exploit structural locality in the spectr
dels, using recent developments in the vision community.</p><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b7">[8]</ref> have been successfully applied to many ASR tasks <ref type="
">[8]</ref> have been successfully applied to many ASR tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Unlike Deep Neural
et="#b26">[27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. The idea behind these approaches is similar to the LSTM in
lipping to 1 was applied, together with Gaussian weight noise N (0, 0.075) and L2 weight decay 1e−5 <ref type="bibr" target="#b29">[30]</ref>. We used ADAM with the default hyperparameters described i
target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Unlike Deep Neural Networks (DNNs) <ref type="bibr" target="#b11">[12]</ref>, CNNs explicitly exploit structural locality in the spectr
dependence assumptions of Hidden Markov Model (HMM) and Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b6">[7]</ref> models. As a result, a single end-to-end model can jointly a
">[8]</ref> have been successfully applied to many ASR tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Unlike Deep Neural
lipping to 1 was applied, together with Gaussian weight noise N (0, 0.075) and L2 weight decay 1e−5 <ref type="bibr" target="#b29">[30]</ref>. We used ADAM with the default hyperparameters described i
dels, using recent developments in the vision community.</p><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b7">[8]</ref> have been successfully applied to many ASR tasks <ref type="
very deep networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. The idea behind th
" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. It is able to do this ted WER on WSJ without an LM was the seq2seq model with Task Loss Estimation achieving 18.0% WER in <ref type="bibr" target="#b4">[5]</ref>. Our baseline, also a seq2seq model, achieved 14.76% WER. Ou /ref>. Our baseline, also a seq2seq model, achieved 14.76% WER. Our model is different from that of <ref type="bibr" target="#b4">[5]</ref> in that we did not use location-based priors on the attentio
xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>Previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38]</ref> describes support me ware support for thread activation and deactivation, as found in prior studies of thread scheduling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38]</ref>. While those works u
ate impedes performance as execution migrates across cores <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. This is a well-doc
e four cores communicate over a shared bus. Caches are kept coherent with a MESI coherence protocol <ref type="bibr" target="#b24">[25]</ref> and snooping; our techniques readily apply to systems with
ours; we also find that caches often hold data irrelevant to future accesses.</p><p>Data Marshaling <ref type="bibr" target="#b35">[36]</ref> mitigates inter-core data misses in Staged Execution model
able model, but even the OS overhead for migration can be significantly reduced from current levels <ref type="bibr" target="#b34">[35]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
ess space as the main thread, inheriting its memory state.</p><p>Heterogeneous multi-core proposals <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> move threads betwe
e Multithreading (SpMT), loss of cache state impedes performance as execution migrates across cores <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" ta
type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5]</ref>. Software data spreading <ref type="bibr" target="#b16">[17]</ref> frequently migrates threads at compiler-determined points
hen thread-level parallelism changes <ref type="bibr" target="#b0">[1]</ref> or at each system call <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5]</ref>. Software data sprea
1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Summary generatorbased<ref type="bibr" target="#b27">[28]</ref> prefetching. Pointer tracks loaded values used as addresse
rallel threads, each thread inheriting the execution context of the previous thread. Helper threads <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ
arget="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37]</ref>, is that repetitive control flow graph traversals lead to r temporal streaming <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> from prefetching approaches that only retrieve a constant n y counting successful prefetches. Hence, like past designs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref>, TIFS uses the next-best option, the Recent heuristic, as i target data accesses <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref>. TIFS adds three logical structures to the chip: a set of S cts the anatomy of the SVB. Our SVB design is adapted from <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>. The SVB contains a small fully-associative buffer for temp arget="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37]</ref>. These prefetchers target primarily off-chip data reference prefetcher to retrieve instruction-cache blocks ahead of the fetch unit for the rest of the stream <ref type="bibr" target="#b36">[37]</ref>.</p><p>Figure <ref type="figure">5</ref> shows the cumulat the L2 cache (see <ref type="bibr">Section 5)</ref>.</p><p>The term temporal stream, introduced in <ref type="bibr" target="#b36">[37]</ref>, refers to extended sequences of data references that recu
get="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar truction working sets that overwhelm L1 instruction caches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. Although total on- te its simplicity, substantially reduces L1 instruction-cache misses in commercial server workloads <ref type="bibr" target="#b16">[17]</ref>, but is only effective for straight-line code. More advanc e instruction prefetching remains critical to the performance of modern commercial server workloads <ref type="bibr" target="#b16">[17]</ref>. More recent work on instruction-stream prefetching genera
ruction streams in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>, which comprise a sequence of contiguous basic blocks, temp r to arbitrary-length sequences of contiguous basic blocks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>A greater challenge lies in prefetching at fetch dis nstruction streams <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> in two key respects: (1) temporal instruction streams are d
d a variety of hardware prefetching schemes. The widely-implemented nextline instruction prefetcher <ref type="bibr" target="#b28">[29]</ref>, despite its simplicity, substantially reduces L1 instruct "bibr" target="#b1">[2]</ref>, and prefetching into caches was analyzed by Smith in the late 1970's <ref type="bibr" target="#b28">[29]</ref>. Although computer architectures and workloads have evolve
ails of our workload suite. We collect traces with the FLEXUS full-system simulation infrastructure <ref type="bibr" target="#b37">[38]</ref>. We trace the DSS queries in their entirety, and four bill
is on a low hardware-cost implementation. Where possible, we virtualize the TIFS storage structures <ref type="bibr" target="#b3">[4]</ref>-that is, predictor storage is allocated within the L2 cache re overhead by virtualizing IMLs and storing their contents inside the L2 data array as proposed in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Finally, rather than implementing the Index Table as ). To further reduce the hardware overhead of TIFS, we employ a variant of predictor virtualization <ref type="bibr" target="#b3">[4]</ref>, a technique for storing prefetcher meta-data in the L2 cach
ruction streams in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>, which comprise a sequence of contiguous basic blocks, temp r to arbitrary-length sequences of contiguous basic blocks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>A greater challenge lies in prefetching at fetch dis nstruction streams <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> in two key respects: (1) temporal instruction streams are d
type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> and Runahead Execution <ref type="bibr" target="#b19">[20]</ref> similarly use branch prediction to enable speculative cont
has been reported by Larus <ref type="bibr" target="#b15">[16]</ref> and underlies trace scheduling <ref type="bibr" target="#b8">[9]</ref> and trace caches <ref type="bibr" target="#b25">[26]</ref>.
es of data prefetching <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar eve a constant number of blocks in response to a miss (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>). Without this abil s-correlated prefetching proposals that target data accesses <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref>. TIFS adds three lo temporal data streams <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar discover stream length after the fact, by counting successful prefetches. Hence, like past designs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref>, TIFS uses the nex
reading mechanisms <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> and Runahead Execution <ref type="bibr" target="#b19">[20]<
nject high-level variations with latent variables. Variational Hierarchical Conversation RNN (VHCR) <ref type="bibr" target="#b30">(Park et al., 2018)</ref> is a most similar model to ours, which also

y et al., 2003;</ref><ref type="bibr" target="#b6">van Deemter et al., 2005)</ref> or grammar-based <ref type="bibr" target="#b1">(Bateman, 1997;</ref><ref type="bibr" target="#b8">Espinosa et al., 20
e responsible for what to say and how to say respectively; they are typically based on hand-crafted <ref type="bibr" target="#b18">(Kukich, 1983;</ref><ref type="bibr" target="#b5">Dalianis and Hovy,
he entire training set).</p><p>To further verify the planning diversity, we also computed self-BLEU <ref type="bibr" target="#b46">(Zhu et al., 2018)</ref> to evaluate how different planning results (
or improvement. For instance, in <ref type="bibr" target="#b31">(Puduppully et al., 2019)</ref> and <ref type="bibr" target="#b34">(Sha et al., 2018)</ref>, planning is merely designed for ordering in o-Sequence (Seq2Seq) <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. Link-S2S Link-S2S <ref type="bibr" target="#b34">(Sha et al., 2018)</ref>  where a link matrix parameterizes the proba explicitly modeled to better capture inter-sentence coherence.</figDesc><table /><note>et al., 2018;<ref type="bibr" target="#b34">Sha et al., 2018;</ref><ref type="bibr" target="#b27">Nema et al., 20
ref>. Some text-to-text generation models <ref type="bibr" target="#b33">(Serban et al., 2017;</ref><ref type="bibr" target="#b45">Zhao et al., 2017)</ref> inject high-level variations with latent var ELBO) of log P (y|x) (L 1 ), the loss of predicting the stop signal (L 2 ) and the bag-of-word loss <ref type="bibr" target="#b45">(Zhao et al., 2017</ref>) (L 3 ). We first derive the ELBO: L 2 is gi l>18</label></formula><formula xml:id="formula_14">)</formula><p>L 3 is the sum of bag-of-word loss <ref type="bibr" target="#b45">(Zhao et al., 2017)</ref> applied to each sentence, which is another ration <ref type="bibr" target="#b14">(Kiddon et al., 2016)</ref>. CVAE: The CVAE model proposed by <ref type="bibr" target="#b45">Zhao et al. (2017)</ref> uses a latent variable to capture the divers
y et al., 2003;</ref><ref type="bibr" target="#b6">van Deemter et al., 2005)</ref> or grammar-based <ref type="bibr" target="#b1">(Bateman, 1997;</ref><ref type="bibr" target="#b8">Espinosa et al., 20
et="#b31">(Puduppully et al., 2019)</ref> that decides the order of input data with Pointer Network <ref type="bibr" target="#b41">(Vinyals et al., 2015)</ref> before generation with Sequence-to-Seque
ef type="bibr" target="#b43">(Xing et al., 2017)</ref>, post-processing with beam search and rerank <ref type="bibr" target="#b20">(Li et al., 2016)</ref>, and designing effective models <ref type="bi e also considered. (3) Length: The average length of the generated texts. (4) Distinct-4: Distinctn <ref type="bibr" target="#b20">(Li et al., 2016</ref>) is a common metric for diversity which measur
tic Evaluation Metrics</head><p>We adopted the following automatic metrics. (1) Corpus BLEU: BLEU-4 <ref type="bibr" target="#b29">(Papineni et al., 2002)</ref>.</p><p>(2) Coverage: This metric measur
dversarial samples <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>; adversaries subtly alter legitimate inputs (call input per g on previous work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> describing how adversaries can efficiently select perturbat f previous attacks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> for knowledge of the target architecture and parameters. We ike neural networks<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>. In addition, we introduce new techniques to craft adversar ep neural network (DNN) using crafted inputs and output labels generated by the target "victim" DNN <ref type="bibr" target="#b18">[19]</ref>. Thereafter, the local network was used to generate advers d in <ref type="bibr" target="#b11">[12]</ref> or the Jacobian-based iterative approach proposed in <ref type="bibr" target="#b18">[19]</ref>. We only provide here a brief description of the fast grad
s to verify this. One could also try to deploy other defense mechanisms like defensive distillation <ref type="bibr" target="#b20">[21]</ref>. Unfortunately, as we do not have any control on the train them because of our lack of access to the machine learning model targeted-for instance distillation <ref type="bibr" target="#b20">[21]</ref>. This failure is most likely due to the shallowness of mod
rvice platforms. The existence of such a threat vector calls for the design of defensive mechanisms <ref type="bibr" target="#b16">[17]</ref>. Unfortunately, we found that defenses proposed in the lit
used to learn models was only considered in the context of binary SVMs whose training data is known <ref type="bibr" target="#b6">[7]</ref> or anomaly detection systems whose underlying model is known
s to verify this. One could also try to deploy other defense mechanisms like defensive distillation <ref type="bibr" target="#b20">[21]</ref>. Unfortunately, as we do not have any control on the train them because of our lack of access to the machine learning model targeted-for instance distillation <ref type="bibr" target="#b20">[21]</ref>. This failure is most likely due to the shallowness of mod
ms have been shown to be vulnerable to adversarial samples <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>; adversaries subtly other models f -even if their architectures greatly differ <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>. A practical impact of the models involved in the transfer is a neural network <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>, while we aim to mo learned using these techniques. Building on previous work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> describing how adve rgeted classifiers alleviates the need of previous attacks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> for knowledge of th ading non-linear and non-convex models like neural networks<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>. In addition, we in extend vulnerable to cross-technique transferability. In fact, as pointed out by Goodfellow et al. <ref type="bibr" target="#b11">[12]</ref>, shallow models like logistic regression are unable to cop h knowledge of the model f and its parameters θ can use the fast gradient sign method introduced in <ref type="bibr" target="#b11">[12]</ref> or the Jacobian-based iterative approach proposed in <ref sample x * approximatively solving the optimization problem stated in Equation 1, Goodfellow et al. <ref type="bibr" target="#b11">[12]</ref> proposed to compute the following perturbation:</p><formul samples misclassified by multi-class logistic regression models using the fast gradient sign method <ref type="bibr" target="#b11">[12]</ref>. In the case of logistic regression, the method finds the nately, we found that defenses proposed in the literature-such as training with adversarial samples <ref type="bibr" target="#b11">[12]</ref>-were noneffective, or we were unable to deploy them becaus
rvice platforms. The existence of such a threat vector calls for the design of defensive mechanisms <ref type="bibr" target="#b16">[17]</ref>. Unfortunately, we found that defenses proposed in the lit
r SVM f . To the best of our knowledge, this method is more computationally efficient than previous <ref type="bibr" target="#b3">[4]</ref>: it does not require any optimization. To craft adversarial
substantial improvements.</p><p>Reservoir Sampling -We also introduce the use of reservoir sampling <ref type="bibr" target="#b22">[23]</ref> as a mean to reduce the number of queries made to the orac
documentation. This work is part of a series of security evaluations of machine learning algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. Unlike us, previous w
neralization knowledge learned by a model into another model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>This paper demonstrates that adversaries can reliably
)</ref> proposed FloWorM system that includes tracker, analyzer and reporter based on NetFlow data. <ref type="bibr" target="#b5">Abdulla et al. (2011)</ref> presented a support vector machine (SVM) m nti and Rossi, 2011)</ref>, or data fusion with other log files such as Snort, DNS related requests <ref type="bibr" target="#b5">(Abdulla et al., 2011)</ref> (number of DNS requests, response, normal

ort on NetFlow P2P analysis. These include methods based on: (a) default P2P port for heavy-hitters <ref type="bibr" target="#b134">(Wagner et al., 2006)</ref>, (b) port usage pattern of specific P2P
roduction to <ref type="bibr">Cisco IOS NetFlow (2012)</ref> for IP-Flow based intrusion detection, <ref type="bibr" target="#b156">Zhu et al. (2008)</ref> for botnet detection, <ref type="bibr" targe
roduction to <ref type="bibr">Cisco IOS NetFlow (2012)</ref> for IP-Flow based intrusion detection, <ref type="bibr" target="#b156">Zhu et al. (2008)</ref> for botnet detection, <ref type="bibr" targe
intervals to address the estimation errors in a multistage combination of sampling and aggregation. <ref type="bibr" target="#b130">Trammell et al. (2011)</ref> characterized, quantified, and correcte
ection based on sketch-based schemes that use a hash table for storing aggregated flow measurement. <ref type="bibr" target="#b63">Kim et al. (2004)</ref> described different DoS attacks based on traf <ref type="bibr" target="#b147">(Yin et al., 2004)</ref> Links between machines or domains IDS 2004 <ref type="bibr" target="#b63">(Kim et al., 2004)</ref> Statistic patterns DoS and DDoS 2005 <ref ty
t aggregates small volume flows with a fixed number of nodes in an IP tree for spatial measurement. <ref type="bibr" target="#b56">Jiang et al. (2010)</ref> characterized network prefix-level traffic
tworks have been studied using NetFlow data, including network performance based on round-trip time <ref type="bibr" target="#b126">(Strohmeier et al., 2011</ref><ref type="bibr">(Strohmeier et al., )
, misuse-based (knowledge-based or signature-based), or combination of both anomaly and misusebased <ref type="bibr" target="#b124">(Sperotto et al., 2010)</ref>. Alternatively, IDSes can be categoriz earch has shown that machine learning approaches are better than statistical and streaming methods. <ref type="bibr" target="#b124">Sperotto et al. (2010)</ref> conducted an overview of IP flow-based
intervals to address the estimation errors in a multistage combination of sampling and aggregation. <ref type="bibr" target="#b130">Trammell et al. (2011)</ref> characterized, quantified, and correcte
m recent works in image and audio generation that discretize the space, namely PixelRNN and Wavenet <ref type="bibr" target="#b37">(Oord et al., 2016a;</ref><ref type="bibr">b)</ref>. Discretization m
come the model-class of choice for many sequential prediction problems. Notably, speech recognition <ref type="bibr" target="#b13">(Hinton et al., 2012)</ref> and natural language processing <ref type
rocessors and lock onto stable, repeatable deltas (differences between subsequent memory addresses) <ref type="bibr" target="#b7">(Gindele, 1977;</ref><ref type="bibr" target="#b19">Jouppi, 1990;</ref
uling algorithms <ref type="bibr" target="#b15">(Ipek et al., 2008)</ref>, tuning performance knobs <ref type="bibr" target="#b2">(Blanton et al., 2015)</ref>, and using bandits to identify patterns i

modern applications can spend over 50% of all compute cycles waiting for data to arrive from memory <ref type="bibr" target="#b23">(Kozyrakis et al., 2010;</ref><ref type="bibr" target="#b6">Ferdman e
waiting for data to arrive from memory <ref type="bibr" target="#b23">(Kozyrakis et al., 2010;</ref><ref type="bibr" target="#b6">Ferdman et al., 2012;</ref><ref type="bibr" target="#b20">Kanev et al. s such as SPEC CPU2006 and continue to grow <ref type="bibr" target="#b0">(Ayers et al., 2018;</ref><ref type="bibr" target="#b6">Ferdman et al., 2012;</ref><ref type="bibr" target="#b10">Gutierrez et

t al. use neural networks to mine online code repositories to automatically synthesize applications <ref type="bibr" target="#b4">(Cummins et al., 2017)</ref>.</p></div> <div xmlns="http://www.tei-c.o
, 2014)</ref> creates a generative model of source code using a probabilistic context-free grammar. <ref type="bibr" target="#b12">(Hindle et al., 2012</ref>) models source code as if it were natural

aints that help generate more content rich responses that are based on a model of syntax and topics <ref type="bibr" target="#b12">(Griffiths et al., 2005)</ref> and semantic similarity <ref type="bib o estimate these distributions, we leverage the unsupervised model of topics and syntax proposed by <ref type="bibr" target="#b12">Griffiths and Steyvers (2005)</ref>. The second constraint encourages
me recent work has explored the possibility of adversarial evaluation of neural conversation models <ref type="bibr" target="#b23">(Lowe et al., 2017;</ref><ref type="bibr" target="#b21">Li et al., 20
mizing likelihood of the response. This line of work was further extended with adversarial learning <ref type="bibr" target="#b21">(Li et al., 2017)</ref> that rewards generated conversations that are al evaluation of neural conversation models <ref type="bibr" target="#b23">(Lowe et al., 2017;</ref><ref type="bibr" target="#b21">Li et al., 2017)</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns
evelop models which converse while assuming a persona defined by a short description of attributes. <ref type="bibr" target="#b37">Wang et. al. (2017)</ref> suggested decoding methods that influence t
BLEU-1 is not generally accepted to correlate with human judgments in conversation generation tasks <ref type="bibr" target="#b22">(Liu et al., 2016)</ref> as there are many acceptable ways to reply t
type="bibr" target="#b43">Yu et al., 2017)</ref>. Some of the earliest work on data-driven chatbots <ref type="bibr" target="#b28">(Ritter et al., 2011)</ref> explored the use of phrase-based Statisti
ddition to an approach that conditions on topic models as additional context in neural conversation <ref type="bibr" target="#b42">(Xing et al., 2017)</ref>. While encouraging the model to generate le t="#b17">(Li et al., 2016a)</ref>. TA-Seq2Seq: Another relevant baseline is the TA-Seq2Seq model of <ref type="bibr" target="#b42">Xing et. al. (2017)</ref> that integrates information from a pre-trai
hat rewards generated conversations that are indistinguishable from real conversations in the data. <ref type="bibr" target="#b16">Lewis et. al. (2017)</ref> applied reinforcement learning with dialog
orpus of 100K information-seeking QA dialogues that are answerable using text spans from Wikipedia. <ref type="bibr" target="#b26">Niu and Bansal (2018)</ref> designed a number of weakly-supervised mo

ef><ref type="bibr" target="#b17">Li et al., 2016a;</ref><ref type="bibr">Serban et al., 2016;</ref><ref type="bibr" target="#b31">Shao et al., 2017)</ref>. This model consists of two parts, an encode lection.</p><p>There are numerous examples of related work on improving neural conversation models. <ref type="bibr" target="#b31">Shao et. al. (2017)</ref> introduced a stochastic approach to beam se
over the past 20 years as new drug approvals lag behind, ballooning research and development costs <ref type="bibr" target="#b0">(1)</ref>. An increasing proportion of experimental drugs are failing
s that as many as nine drugs may be required to achieve the desired efficacies for a single disease <ref type="bibr" target="#b31">(32)</ref>. Computational algorithms that leverage genetic interactio
both a systems-level understanding of the targeted biological functions and a quantifiable outcome <ref type="bibr" target="#b14">(15)</ref>. As such, until recently, combination therapies have been
increasingly apparent that the traditional approach of "one drug-one target" is no longer effective <ref type="bibr" target="#b2">(3)</ref>. This is especially true as more emphasis is placed on treat
already led to advances in our understanding of some notoriously complex diseases, including autism <ref type="bibr" target="#b15">(16)</ref>, schizophrenia <ref type="bibr" target="#b16">(17)</ref>, genetic network where the edges represented the likelihood of two genes sharing a genetic phenotype <ref type="bibr" target="#b15">(16)</ref>. When combined with copy number variant (CNV) data generat
e in drug development <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref>.</p><p>Unfortunately, even if progress is made to increase e
tal model systems, such as animal models, have had notoriously poor performance in drug development <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" targe
increasingly apparent that the traditional approach of "one drug-one target" is no longer effective <ref type="bibr" target="#b2">(3)</ref>. This is especially true as more emphasis is placed on treat
action potential being the most famous. Perhaps inspired by these pioneers, Bagheri and colleagues <ref type="bibr" target="#b42">(43)</ref> used an ordinary differential equation (ODE) model of mito re able to confirm that MEK inhibitors functioned independently of an established oncolytic pathway <ref type="bibr" target="#b42">(43)</ref>. Unfortunately, when applied to large systems with many pr
to monitor drug safety in the context of drug interactions <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b45">46)</ref>. Complicating the matter, drugs often cause their side effe
to monitor drug safety in the context of drug interactions <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b45">46)</ref>. Complicating the matter, drugs often cause their side effe
genera-tive model based on a Laplacian pyramid framework (LAP-GAN) to generate realistic images in <ref type="bibr" target="#b8">[6]</ref>, which is the most related to our work. However, the propose
rse representation <ref type="bibr" target="#b39">[37,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b41">39]</ref>. Instead of directly modeling the complex patch space over rry out extensive experiments using 5 datasets: SET5 <ref type="bibr" target="#b4">[2]</ref>, SET14 <ref type="bibr" target="#b41">[39]</ref>, BSDS100 <ref type="bibr" target="#b3">[1]</ref>, UR-BAN10
approaches. SR based on internal databases. Several methods <ref type="bibr" target="#b11">[9,</ref><ref type="bibr" target="#b14">12]</ref> exploit the self-similarity property in natural images and
of applications, such as image blending <ref type="bibr" target="#b6">[4]</ref>, texture synthesis <ref type="bibr" target="#b16">[14]</ref>, edge-aware filtering <ref type="bibr" target="#b26">[24]<
d a sparse coding-based network <ref type="bibr" target="#b35">[33]</ref> or use a deeper structure <ref type="bibr" target="#b19">[17]</ref>. While these models demonstrate promising results, there a SRCNN <ref type="bibr" target="#b9">[7]</ref>, SCN <ref type="bibr" target="#b35">[33]</ref>, VDSR <ref type="bibr" target="#b19">[17]</ref>, and DRCN <ref type="bibr" target="#b20">[18]</ref>. Simil jointly optimize all the steps and learn the nonlinear mapping in the image space. The VDSR network <ref type="bibr" target="#b19">[17]</ref> demonstrates significant improvement over SRCNN <ref type= CN <ref type="bibr" target="#b35">[33]</ref>, ESPCN <ref type="bibr" target="#b30">[28]</ref>, VDSR <ref type="bibr" target="#b19">[17]</ref>, and the proposed LapSRN. The number of layers includes bo RFL <ref type="bibr" target="#b28">[26]</ref>, SCN <ref type="bibr" target="#b35">[33]</ref>, VDSR <ref type="bibr" target="#b19">[17]</ref> and DRCN <ref type="bibr" target="#b20">[18]</ref>. We car entary material.</p><p>Ground-truth HR Bicubic FSRCNN <ref type="bibr" target="#b10">[8]</ref> VDSR <ref type="bibr" target="#b19">[17]</ref> LapSRN (ours) Ground-truth HR Bicubic FSRCNN <ref type="bi 7]</ref> LapSRN (ours) Ground-truth HR Bicubic FSRCNN <ref type="bibr" target="#b10">[8]</ref> VDSR <ref type="bibr" target="#b19">[17]</ref> LapSRN (ours)</p><p>Figure <ref type="figure">7</ref>: Com the ringing artifacts.</p><p>Ground-truth HR HR SRCNN <ref type="bibr" target="#b9">[7]</ref> VDSR <ref type="bibr" target="#b19">[17]</ref> LapSRN (ours)</p><p>Figure <ref type="figure">8</ref>: Vis g/ns/1.0"><head>Ground-truth HR HR</head><p>SelfExSR <ref type="bibr" target="#b17">[15]</ref> VDSR <ref type="bibr" target="#b19">[17]</ref> LapSRN (ours)</p><p>Figure <ref type="figure">9</ref>: A f f SRCNN<ref type="bibr" target="#b9">[7]</ref>, FSRCNN<ref type="bibr" target="#b10">[8]</ref>, VDSR<ref type="bibr" target="#b19">[17]</ref>, DRCN<ref type="bibr" target="#b20">[18]</ref> and the pro <ref type="bibr" target="#b3">[1]</ref> as our training data. The same training dataset is used in <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26]</ref>    the protocol of target="#b28">26]</ref>    the protocol of existing methods <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b19">17]</ref>, we generate the LR training patches using the bicubic down or pre-processing generate results with noticeable artifacts <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" tar hallenging to predict HR images from bicubicupsampled images <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b32">30]</ref> or using one-step u parametric SR methods <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b20">18]</ref>. Another limitation
lters with the size of 3 × 3. We initialize the convolutional filters using the method of He et al. <ref type="bibr" target="#b15">[13]</ref>. The size of the transposed convolutional filters is 4 × 4
R-HR patch pairs may not be sufficient to cover large textural variations in an image. Singh et al. <ref type="bibr" target="#b31">[29]</ref> decompose patches into directional frequency sub-bands and
R-HR patch pairs may not be sufficient to cover large textural variations in an image. Singh et al. <ref type="bibr" target="#b31">[29]</ref> decompose patches into directional frequency sub-bands and
ovide comparisons with LAPGAN in the supplementary material. Adversarial training. The SRGAN method <ref type="bibr" target="#b22">[20]</ref> optimizes the network using the perceptual loss <ref type=
ovide comparisons with LAPGAN in the supplementary material. Adversarial training. The SRGAN method <ref type="bibr" target="#b22">[20]</ref> optimizes the network using the perceptual loss <ref type=
ator with sub-pixel convolution <ref type="bibr" target="#b30">[28]</ref> or transposed convolution <ref type="bibr" target="#b10">[8]</ref> (also named as deconvolution in some of the literature). Th r" target="#b19">[17]</ref>, and DRCN <ref type="bibr" target="#b20">[18]</ref>. Similar to FSR-CNN <ref type="bibr" target="#b10">[8]</ref>, our LapSRN achieves real-time speed on most of the evaluat ref>: Comparisons of CNN based SR algorithms: SRCNN <ref type="bibr" target="#b9">[7]</ref>, FSRCNN <ref type="bibr" target="#b10">[8]</ref>, SCN <ref type="bibr" target="#b35">[33]</ref>, ESPCN <ref places the bicubic upsampling operation with an efficient sub-pixel convolution. The FSRCNN network <ref type="bibr" target="#b10">[8]</ref> adopts a similar idea and uses a hourglass-shaped CNN with A+ <ref type="bibr" target="#b32">[30]</ref>, SRCNN <ref type="bibr" target="#b9">[7]</ref>, FSRCNN <ref type="bibr" target="#b10">[8]</ref>, SelfExSR <ref type="bibr" target="#b17">[15]</ref>, RFL <r "bibr" target="#b19">17,</ref><ref type="bibr" target="#b32">30]</ref> or using one-step upsampling <ref type="bibr" target="#b10">[8]</ref>. The state-of-the-art methods do not super-resolve the fine time of all evaluated datasets in the supplementary material.</p><p>Ground-truth HR Bicubic FSRCNN <ref type="bibr" target="#b10">[8]</ref> VDSR <ref type="bibr" target="#b19">[17]</ref> LapSRN (ours ]</ref> VDSR <ref type="bibr" target="#b19">[17]</ref> LapSRN (ours) Ground-truth HR Bicubic FSRCNN <ref type="bibr" target="#b10">[8]</ref> VDSR <ref type="bibr" target="#b19">[17]</ref> LapSRN (ours abel><figDesc>Figure1: Network architectures of SRCNN<ref type="bibr" target="#b9">[7]</ref>, FSRCNN<ref type="bibr" target="#b10">[8]</ref>, VDSR<ref type="bibr" target="#b19">[17]</ref>, DRCN<ref ty This is a common limitation shared by parametric SR methods <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" targ

on to FGRA, similar to existing CC-NUMA optimizations (e.g., Linux's memory placement optimizations <ref type="bibr" target="#b42">[42]</ref>). To study the potential impact of this enhancement, we mo
results in a trap to the VMM, and VMM software must initiate the page transfer. Based on prior work <ref type="bibr" target="#b40">[40]</ref>, we assume a total of 330 ns (roughly 1,000 cycles on a 3

tical limitations to functionality, generality, software transparency, total costs, and performance <ref type="bibr" target="#b27">[28]</ref>. A recent commercial design in this space, Versatile SMP <
is transfer is released for reassignment.</p><p>Alternatively, many VMMs provide a "balloon driver" <ref type="bibr" target="#b37">[37]</ref> within the guest OS to allocate and pin memory pages, whic opportunities to reduce memory requirements via copy-on-write content-based page sharing across VMs <ref type="bibr" target="#b37">[37]</ref>. Disaggregated memory offers an even larger scope for shar
o an interesting opportunity to address these challenges-namely that of optimizing for the ensemble <ref type="bibr" target="#b9">[9]</ref>. For example, several studies have shown that there is signi
, thus delaying but failing to fundamentally address the memory capacity wall.</p><p>A recent study <ref type="bibr" target="#b36">[36]</ref> demonstrates the viability of a two-level memory organizat
de controller might be extended to implement wear-leveling and other lifetime management strategies <ref type="bibr" target="#b43">[43]</ref>. Furthermore, disaggregated memory offers the potential fo
tical limitations to functionality, generality, software transparency, total costs, and performance <ref type="bibr" target="#b27">[28]</ref>. A recent commercial design in this space, Versatile SMP <
results in a trap to the VMM, and VMM software must initiate the page transfer. Based on prior work <ref type="bibr" target="#b40">[40]</ref>, we assume a total of 330 ns (roughly 1,000 cycles on a 3


mance. This is important for many algorithms and for any application with serialized code sections <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">15,</r
are based on a 28nm FDSOI (fully depleted silicon on insulator) process technology as described in <ref type="bibr" target="#b18">[19]</ref>. Depending on operating conditions and the choice of low tal power for a processor implemented with fast, low Vi devices operating at nominal voltage (0.9V) <ref type="bibr" target="#b18">[19]</ref>. The use of regular leakage devices will reduce leakage po
mption is a primary limiter of system performance and scalability, both in large scale data centers <ref type="bibr" target="#b16">[17]</ref> and in consumer devices <ref type="bibr" target="#b21">[22



micity and uses it as a mechanism to exploit fine-grained heterogeneity.</p><p>Other Related Works: <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">40]</ref> propose pre-scheduling instruc tio

mance. This is important for many algorithms and for any application with serialized code sections <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">15,</r
s by studying the two-month history made available by Amazon <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">15]</ref>. Though these statistics alone are sufficient for the user are limited to statistical studies of historical spot prices <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">15]</ref>.</p><p>Game theoretic pricing. Spot pricing is a distribute tradeoff is the fact that user jobs can have long runtimes spanning many changes in the spot price <ref type="bibr" target="#b13">[15]</ref>. Users then face two key challenges: 1) Users must predict int ensures that the job is sufficiently interruptible. We use p to denote the optimal bid price to <ref type="bibr" target="#b13">(15)</ref>.</p><p>We now observe that the expected running time in <r of the spot price monotonically decreases, i.e., F π (p) is concave, the optimal bid price solving <ref type="bibr" target="#b13">(15)</ref> </p><formula xml:id="formula_29">is p = ψ −1 t k t r − 1 , . Comparing <ref type="bibr" target="#b17">(19)</ref> to bidding for a single persistent request in <ref type="bibr" target="#b13">(15)</ref>, we see that <ref type="bibr" target="#b17">(19)</ref> can "#b13">(15)</ref>, we see that <ref type="bibr" target="#b17">(19)</ref> can be solved similarly to <ref type="bibr" target="#b13">(15)</ref> in Proposition 5.</p><p>By comparing the costs for multipl t k ts .</p><p>Proof of Proposition 5.</p><p>Proof. By taking the first-order derivative of Φ(p) in <ref type="bibr" target="#b13">(15)</ref>  <ref type="figure" target="#fig_3">3</ref>), F π (p) is c
Usage-based pricing can affect overall demand levels, but does not even out short-term fluctuations <ref type="bibr" target="#b11">[13]</ref>. To manage these fluctuations in demand for a fixed amount re have a shorter expected running time. Job interruptibility. We can use the expected running time <ref type="bibr" target="#b11">(13)</ref> to observe the effect of the recovery time parameter, t r s feasible at any price.</p><p>The optimal bid price. We can now multiply the expected running time <ref type="bibr" target="#b11">(13)</ref> with the expected spot price <ref type="bibr" target="#b7" o <ref type="bibr" target="#b13">(15)</ref>.</p><p>We now observe that the expected running time in <ref type="bibr" target="#b11">(13)</ref> decreases with the bid price, while the expected spot pric very, execution, and overhead times. Hence, we can extend the result for a single persistent bid in <ref type="bibr" target="#b11">(13)</ref> as</p><formula xml:id="formula_32">M i=1 T i F π (p) = t s
me works have shown that users can reduce their costs by using spot rather than on-demand instances <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b35">37]</ref>, they only conside
, are also possible <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39]</ref>; in fact, some stud-ies have suggested that Amazon does not
nding of Amazon's prevailing spot prices by studying the two-month history made available by Amazon <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">15]</ref>. Though these statis orks on both provider and user actions are limited to statistical studies of historical spot prices <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">15]</ref>.</p><p>Game theoreti /ref>; in fact, some stud-ies have suggested that Amazon does not use revenuemaximizing spot prices <ref type="bibr" target="#b0">[1]</ref>. Thus, we also include a capacity utilization term β log (1 across different instance types is consistent, though the spot prices are different, agreeing with <ref type="bibr" target="#b0">[1]</ref>'s findings.</p><p>We next estimate the spot price PDF with ( far in advance. Since the spot prices' autocorrelation drops off rapidly with a longer    lag time <ref type="bibr" target="#b0">[1]</ref>, such predictions are likely to be difficult. We discuss thi of the spot prices in 2010 shows the presence of limited autocorrelation for consecutive time slots <ref type="bibr" target="#b0">[1]</ref>. Incorporating these correlations into users' spot price pre </ref>. Thus, we can expect the cumulative distribution functions to have a knee, as was observed in<ref type="bibr" target="#b0">[1]</ref>.</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="
c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">This observation is consistent with the findings in<ref type="bibr" target="#b16">[18]</ref>. Thus, we can expect the cumulative distribution functions
ng, in which users pay a static per-unit price (per workload or per hour) to access cloud resources <ref type="bibr" target="#b19">[21]</ref>. Usage-based pricing can affect overall demand levels, but
re truthful user bids <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b27">29]</ref>, including improvem
tions in this work. Unlike most works on spot pricing, which consider only the provider's viewpoint <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" tar siderations, e.g., dynamically allocating cloud resources, so as to maximize the provider's revenue <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" tar pe="foot" target="#foot_2">3</ref> Other objectives, such as clearing the market, are also possible <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" tar
me works have shown that users can reduce their costs by using spot rather than on-demand instances <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b35">37]</ref>, they only conside
rovider's viewpoint <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38]</ref>, we aim to develop provider's revenue <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b32">34]</ref> or social welfare <ref type="bibr" target="#b23">[25,</ref> identically distributed (i.i.d.), following a distribution f Λ with expected value λ and variance σ <ref type="bibr" target="#b32">[34]</ref>. Writing N (t) in terms of the spot price π (t), i.e., N (
c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">This observation is consistent with the findings in<ref type="bibr" target="#b16">[18]</ref>. Thus, we can expect the cumulative distribution functions
me works have shown that users can reduce their costs by using spot rather than on-demand instances <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b35">37]</ref>, they only conside
tions for smart grid electricity <ref type="bibr" target="#b7">[9]</ref>, secondary spectrum access <ref type="bibr" target="#b14">[16]</ref>, grid computing <ref type="bibr" target="#b17">[19]</ref>, is minimized when ∂Φ sp (p)/∂p = 0, i.e., g(p) = 0. Letting g(p) = 0, we thus deduce which leads to <ref type="bibr" target="#b14">(16)</ref>. In addition, Φ(p ) &lt; Φ(π) = (t s − t r )E(π | π ≤ π) ≤
e in cloud scenarios <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32]</ref>, although they do not consider optimal bids in a resource a
tions for smart grid electricity <ref type="bibr" target="#b7">[9]</ref>, secondary spectrum access <ref type="bibr" target="#b14">[16]</ref>, grid computing <ref type="bibr" target="#b17">[19]</ref>, is minimized when ∂Φ sp (p)/∂p = 0, i.e., g(p) = 0. Letting g(p) = 0, we thus deduce which leads to <ref type="bibr" target="#b14">(16)</ref>. In addition, Φ(p ) &lt; Φ(π) = (t s − t r )E(π | π ≤ π) ≤
tions in this work. Unlike most works on spot pricing, which consider only the provider's viewpoint <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" tar siderations, e.g., dynamically allocating cloud resources, so as to maximize the provider's revenue <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" tar pe="foot" target="#foot_2">3</ref> Other objectives, such as clearing the market, are also possible <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" tar
deed, many works have studied the problem of designing online auctions to ensure truthful user bids <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" targe
ibution for f p , as is often used to model distributions of user valuations for computing services <ref type="bibr" target="#b28">[30]</ref>.</p><p>We solve (1) to find that the optimal spot price π
ng, in which users pay a static per-unit price (per workload or per hour) to access cloud resources <ref type="bibr" target="#b19">[21]</ref>. Usage-based pricing can affect overall demand levels, but
their costs by using spot rather than on-demand instances <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b35">37]</ref>, they only consider heuristic bidding strategies for single
s have been proposed as a solution to generic distributed allocation games over multiple time slots <ref type="bibr" target="#b18">[20]</ref>. Users' optimal bidding strategies in such auctions can be lative distribution functions for the master and slave node instance types. The first constraint in <ref type="bibr" target="#b18">(20)</ref> ensures that the master node runs longer than any of the s
is is called prediction table interference, and is the main cause for decreased prediction accuracy <ref type="bibr" target="#b25">[28]</ref>.</p><p>Dynamic prediction tables can be organized in a cle ppens more often than positive interference, and is the main cause of decreased prediction accuracy <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b17">20]</ref>.</p><p>Dealiased b
ce, and is the main cause of decreased prediction accuracy <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b17">20]</ref>.</p><p>Dealiased branch predictors reduce negative PHT inte
routine <ref type="bibr">[8,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b16">191</ref> to minimize the number of conflict misses. Reducing the num
sually implying code replication <ref type="bibr" target="#b26">[14,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">13,</ref><ref type="bibr">161</ref>. These co
ode transformations, which usually imply code replication [14, 27, 9, 13, 161, and branch alignment <ref type="bibr" target="#b2">[3]</ref>. This branch alignment is nothing but a code reordering opti predict the branch direction, and then align the branch so that it follows a more simple heuristic <ref type="bibr" target="#b2">[3]</ref>, like making all branches usually taken (or usually not take



conflict with each other, we can reduce the number of cache misses by almost an order of magnitude <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">6]</ n of the instruction cache, and use profile data or heuristics to lay out the routines in a program <ref type="bibr" target="#b14">[ 17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">61< #b6">7,</ref><ref type="bibr">61</ref>, and the basic blocks in a routine <ref type="bibr">[8,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b16">191
conflict with each other, we can reduce the number of cache misses by almost an order of magnitude <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">6]</ n of the instruction cache, and use profile data or heuristics to lay out the routines in a program <ref type="bibr" target="#b14">[ 17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">61< #b6">7,</ref><ref type="bibr">61</ref>, and the basic blocks in a routine <ref type="bibr">[8,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b16">191
e same outcome for a given branch. This prediction was obtained either using very simple heuristics <ref type="bibr" target="#b20">[23]</ref>, static analysis [l], or profile information <ref type="bi ken (or usually not taken), or aligning branches so that only a forward branch is usually not taken <ref type="bibr" target="#b20">[23]</ref>. In this work we examine how code layout optimizations tar adaptive predictors</head><p>The more simple dynamic branch predictor (the bimodal branch predictor <ref type="bibr" target="#b20">[23]</ref>) simply keeps a saturating two-bit counter for each branch ranch behavior, and lookup the data each time the branch executes to produce a direction prediction <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b23">26]</ref>.</p><p>But the siz

tables is due to conflict aliasing, not capacity problems. Derived from the skew-associative caches <ref type="bibr" target="#b18">[21]</ref>, the gskew predictor stores r; branches in three separate
time the branch executes to produce a direction prediction <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b23">26]</ref>.</p><p>But the size of these dynamic tables is limited, and As shown in Figure <ref type="figure" target="#fig_8">1</ref>, two-level adaptive branch predictors <ref type="bibr" target="#b23">[26]</ref>  By storing data this way, any given entry in the PHT corr
re which keeps track of its sub-stream.</p><p>The gskew branch predictor <ref type="bibr">[12,</ref><ref type="bibr" target="#b19">22]</ref> (Figure <ref type="figure" target="#fig_0">2</ref>.c) is ba


conflict with each other, we can reduce the number of cache misses by almost an order of magnitude <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">6]</ n of the instruction cache, and use profile data or heuristics to lay out the routines in a program <ref type="bibr" target="#b14">[ 17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">61< #b6">7,</ref><ref type="bibr">61</ref>, and the basic blocks in a routine <ref type="bibr">[8,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b16">191





tables is due to conflict aliasing, not capacity problems. Derived from the skew-associative caches <ref type="bibr" target="#b18">[21]</ref>, the gskew predictor stores r; branches in three separate





the number of cache misses by almost an order of magnitude <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">6]</ref>. By aligning basic blocks so that th e data or heuristics to lay out the routines in a program <ref type="bibr" target="#b14">[ 17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">61</ref>, and the basic blocks in a routine <
conflict with each other, we can reduce the number of cache misses by almost an order of magnitude <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">6]</ n of the instruction cache, and use profile data or heuristics to lay out the routines in a program <ref type="bibr" target="#b14">[ 17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">61< #b6">7,</ref><ref type="bibr">61</ref>, and the basic blocks in a routine <ref type="bibr">[8,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b16">191


ristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM) <ref type="bibr" target="#b29">[30]</ref>, are performed to maintain a manageable balance between fo ves, focusing all attention on the hard negative examples.</p><p>Online Hard Example Mining (OHEM): <ref type="bibr" target="#b29">[30]</ref> proposed to improve training of two-stage detectors by con hard example mining <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In this paper, we propose a new loss function that a rous extensions to this framework have been proposed, e.g. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar rget="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21]</ref> that samples hard e nt performance saturates. (d) FL outperforms the best variants of online hard example mining (OHEM) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21]</ref> by over 3 points A
ref>, this two-stage framework consistently achieves top accuracy on the challenging COCO benchmark <ref type="bibr" target="#b19">[20]</ref>.</p><p>Despite the success of two-stage detectors, a natur present experimental results on the bounding box detection track of the challenging COCO benchmark <ref type="bibr" target="#b19">[20]</ref>.</p><p>For training, we follow common practice <ref type="
at a different scale. FPN improves multi-scale predictions from fully convolutional networks (FCN) <ref type="bibr" target="#b21">[22]</ref>, as shown by its gains for RPN <ref type="bibr" target="#b
olutional neural networks to handwritten digit recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref>. Viola and Jones <ref type="bibr" target="#b35">[36]</ref>
etectors are based on a two-stage, proposal-driven mechanism. As popularized in the R-CNN framework <ref type="bibr" target="#b10">[11]</ref>, the first stage generates a sparse set of candidate objec ocations, and the second stage classifies the proposals into foreground classes / background. R-CNN <ref type="bibr" target="#b10">[11]</ref> upgraded the second-stage classifier to a convolutional ne set between the anchor and the groundtruth box (we use the standard box parameterization from R-CNN <ref type="bibr" target="#b10">[11]</ref>). We note that unlike most recent work, we use a class-agn
type="bibr" target="#b33">[34]</ref>, EdgeBoxes <ref type="bibr" target="#b36">[37]</ref>, DeepMask <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, RPN <ref type="bi target="#b9">10]</ref> and by using learned object proposals <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. Region Proposal Ne g. The first cascade stage is an object proposal mechanism <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> that reduces the ne s shown by its gains for RPN <ref type="bibr" target="#b26">[27]</ref> and DeepMask-style proposals <ref type="bibr" target="#b22">[23]</ref>, as well at two-stage detectors such as Fast R-CNN <ref ty
et="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>One-stage De Following <ref type="bibr" target="#b18">[19]</ref>, we build FPN on top of the ResNet architecture <ref type="bibr" target="#b14">[15]</ref>. We construct a pyramid with levels P 3 through P 7 , wher e base ResNet-50 and ResNet-101 models are pre-trained on ImageNet1k; we use the models released by <ref type="bibr" target="#b14">[15]</ref>. New layers added for FPN are initialized as in <ref type= tion along with various optimization strategies. For all experiments we use depth 50 or 101 ResNets <ref type="bibr" target="#b14">[15]</ref> with a Feature Pyramid Network (FPN) <ref type="bibr" targ (FPN)<ref type="bibr" target="#b18">[19]</ref> backbone on top of a feedforward ResNet architecture<ref type="bibr" target="#b14">[15]</ref> (a) to generate a rich, multi-scale convolutional feature
type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b27">28]</ref> or hard example mining <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" targ oss.</p><p>Class Imbalance: Both classic one-stage object detection methods, like boosted detectors <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5]</ref> and DPMs <ref type=" <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref>. Viola and Jones <ref type="bibr" target="#b35">[36]</ref> used boosted object detectors for face detection, leading n solution is to perform some form of hard negative mining <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" targe
eBoxes <ref type="bibr" target="#b36">[37]</ref>, DeepMask <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, RPN <ref type="bibr" target="#b26">[27]</ref>) rapidly nar
utperforms all previous one-stage and two-stage detectors, including the best reported Faster R-CNN <ref type="bibr" target="#b26">[27]</ref> system from <ref type="bibr" target="#b18">[19]</ref>. We ="#b18">[19]</ref> or Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> variants of Faster R-CNN <ref type="bibr" target="#b26">[27]</ref>. To achieve this result, we identify class imbalance durin f>, DeepMask <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, RPN <ref type="bibr" target="#b26">[27]</ref>) rapidly narrows down the number of candidate object locat h the second-stage classifier into a single convolution network, forming the Faster R-CNN framework <ref type="bibr" target="#b26">[27]</ref>. Numerous extensions to this framework have been proposed, imilarities with previous dense detectors, in particular the concept of 'anchors' introduced by RPN <ref type="bibr" target="#b26">[27]</ref> and use of features pyramids as in SSD <ref type="bibr" ta nvolutional networks (FCN) <ref type="bibr" target="#b21">[22]</ref>, as shown by its gains for RPN <ref type="bibr" target="#b26">[27]</ref> and DeepMask-style proposals <ref type="bibr" target="#b22 er of object classes, and a 4-vector of box regression targets. We use the assignment rule from RPN <ref type="bibr" target="#b26">[27]</ref> but modified for multiclass detection and with adjusted th target="#fig_4">3</ref> (c). We use C = 256 and A = 9 in most experiments.</p><p>In contrast to RPN <ref type="bibr" target="#b26">[27]</ref>, our object classification subnet is deeper, uses only 3×3 ular approach for achieving high coverage of boxes in these approaches is to use multiple 'anchors' <ref type="bibr" target="#b26">[27]</ref> at each spatial position to cover boxes of various scales outperforms all previous one-stage and two-stage detectors, including the best reported Faster R-CNN<ref type="bibr" target="#b26">[27]</ref> system from<ref type="bibr" target="#b18">[19]</ref>. We s onvolutional neural network. Through a sequence of advances <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar of recent ideas from <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19]</ref>. RetinaNet is effic ned object proposals <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. Region Proposal Networks (RPN) integrated proposal generat proposal mechanism <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> that reduces the nearly infinite set of possible object loc
ref>, this two-stage framework consistently achieves top accuracy on the challenging COCO benchmark <ref type="bibr" target="#b19">[20]</ref>.</p><p>Despite the success of two-stage detectors, a natur present experimental results on the bounding box detection track of the challenging COCO benchmark <ref type="bibr" target="#b19">[20]</ref>.</p><p>For training, we follow common practice <ref type="
ehring et al., 2017;</ref><ref type="bibr" target="#b4">Kalchbrenner et al., 2016)</ref>, attention <ref type="bibr" target="#b10">(Vaswani et al., 2017)</ref>, or a combination of recurrence and atte ></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer</head><p>The Transformer <ref type="bibr" target="#b10">(Vaswani et al., 2017)</ref> employs an encoder-decoder structure, co for all sequences, heads, and positions in a batch using parallel matrix multiplication operations <ref type="bibr" target="#b10">(Vaswani et al., 2017)</ref>. Without relative position representatio d><p>We compared our model using only relative position representations to the baseline Transformer <ref type="bibr" target="#b10">(Vaswani et al., 2017)</ref> with sinusoidal position encodings. We g e a deterministic function of position <ref type="bibr" target="#b7">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b10">Vaswani et al., 2017)</ref> or learned representations. Convolutional se in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as <ref type="bibr" target="#b10">Vaswani et al. (2017)</ref>.</p></div> <div xmlns="http://www.tei-c.o 1 = 0.9, β 2 = 0.98, and = 10 −9 . We used the same warmup and decay strategy for learning rate as <ref type="bibr" target="#b10">Vaswani et al. (2017)</ref>, with 4,000 warmup steps. During training
tion information to the model. These position encodings can be a deterministic function of position <ref type="bibr" target="#b7">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b10">Vaswani
tion information to the model. These position encodings can be a deterministic function of position <ref type="bibr" target="#b7">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b10">Vaswani
017)</ref>, with 4,000 warmup steps. During training, we employed label smoothing of value ls = 0.1 <ref type="bibr" target="#b9">(Szegedy et al., 2016)</ref>. For evaluation, we used beam search with
roduction</head><p>Recent approaches to sequence to sequence learning typically leverage recurrence <ref type="bibr" target="#b8">(Sutskever et al., 2014)</ref>, convolution <ref type="bibr" target="#
n-based models have therefore used position encodings or biased attention weights based on distance <ref type="bibr" target="#b6">(Parikh et al., 2016)</ref>.</p><p>In this work we present an efficien
017)</ref>, with 4,000 warmup steps. During training, we employed label smoothing of value ls = 0.1 <ref type="bibr" target="#b9">(Szegedy et al., 2016)</ref>. For evaluation, we used beam search with
017)</ref>, with 4,000 warmup steps. During training, we employed label smoothing of value ls = 0.1 <ref type="bibr" target="#b9">(Szegedy et al., 2016)</ref>. For evaluation, we used beam search with
017)</ref>, with 4,000 warmup steps. During training, we employed label smoothing of value ls = 0.1 <ref type="bibr" target="#b9">(Szegedy et al., 2016)</ref>. For evaluation, we used beam search with
tion information to the model. These position encodings can be a deterministic function of position <ref type="bibr" target="#b7">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b10">Vaswani
n-based models have therefore used position encodings or biased attention weights based on distance <ref type="bibr" target="#b6">(Parikh et al., 2016)</ref>.</p><p>In this work we present an efficien
been applied in the transductive setting with fixed graphs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. In this work we both extend GCNs to the task of inductive aph convolutional network (GCN), introduced by Kipf et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. The original GCN algorithm <ref type="bibr" target="#b16"> n O(|V|), so this requirement is not entirely unreasonable <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Following Theorem 1, we let x v ∈ U, ∀v ∈ V denote t trained on a single, fixed graph. (That said, Kipf et al <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b17">[18]</ref> found that GCN-based approach consistently outperformed De
ng over graph structures using convolution operators that offer promise as an embedding methodology <ref type="bibr" target="#b16">[17]</ref>. So far, graph convolutional networks (GCNs) have only bee nificant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks <ref type="bibr" target="#b16">[17]</ref>. Lastly, we probe the expressive capability of our approac "bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. The original GCN algorithm <ref type="bibr" target="#b16">[17]</ref> is designed for semi-supervised learning in a transductive r is nearly equivalent to the convolutional propagation rule used in the transductive GCN framework <ref type="bibr" target="#b16">[17]</ref>. In particular, we can derive an inductive variant of the regator convolutional since it is a rough, linear approximation of a localized spectral convolution <ref type="bibr" target="#b16">[17]</ref>. An important distinction between this convolutional aggre utional" variant of GraphSAGE is an extended, inductive version of Kipf et al's semi-supervised GCN <ref type="bibr" target="#b16">[17]</ref>, we term this variant GraphSAGE-GCN. We test unsupervised ctive setting, where it can be extensively trained on a single, fixed graph. (That said, Kipf et al <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b17">[18]</ref> found that GCN-b d="foot_2">Note that this differs from Kipf et al's exact equation by a minor normalization constant<ref type="bibr" target="#b16">[17]</ref>.</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. In this work we b our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. The original GCN less" GCN approach has parameter dimension O(|V|), so this requirement is not entirely unreasonable <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Following T " target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>). The majority of t
e an alignment procedure to share information between the graphs, such as the procedure proposed by <ref type="bibr" target="#b11">[12]</ref> for aligning the output of word embedding algorithms. Inve
work approaches to supervised learning over graph structures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" targ
itectures for learning over graphs have been proposed (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target= phs or are designed for whole-graph classification (or both) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target=
borhood vector h k N (v) . This concatenation can be viewed as a simple form of a "skip connection" <ref type="bibr" target="#b12">[13]</ref> between the different "search depths", or "layers" of the
proved extremely useful as feature inputs for a wide variety of prediction and graph analysis tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ naturally generalize to unseen data, since they make predictions on nodes in a single, fixed graph <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ ensional embeddings using random walk statistics and matrix factorization-based learning objectives <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ
dels were implemented in TensorFlow <ref type="bibr" target="#b0">[1]</ref> with the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> (except DeepWalk, which performed better with the vanilla
are also a number of recent neural network approaches to supervised learning over graph structures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
arget="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>. Our approach is conceptually inspired by a number of these
over graph structures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>. Our approach is co
single, fixed graph <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar ref>. These methods also bear close relationships to more classic approaches to spectral clustering <ref type="bibr" target="#b22">[23]</ref>, multi-dimensional scaling <ref type="bibr" target="#b18">
t of a node, i.e., the proportion of triangles that are closed within the node's 1-hop neighborhood <ref type="bibr" target="#b37">[38]</ref>.</p><p>The clustering coefficient is a popular measure of inal output values generated by Algorithm 1 and c v are node clustering coefficients, as defined in <ref type="bibr" target="#b37">[38]</ref>.</p><p>Proof. Without loss of generality, we describe how
arget="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>. Our approach is conceptually inspired by a number of these
are also a number of recent neural network approaches to supervised learning over graph structures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
work approaches to supervised learning over graph structures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" targ
for validation). For features, we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors <ref type="bibr" target="#b26">[27]</ref>; for each post, we concatenated (i) the average embedding
e an alignment procedure to share information between the graphs, such as the procedure proposed by <ref type="bibr" target="#b11">[12]</ref> for aligning the output of word embedding algorithms. Inve
e been proposed (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" targe assification (or both) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref>. However, our approac
ed by recent advancements in applying neural network architectures to learn over general point sets <ref type="bibr" target="#b28">[29]</ref>.</p><p>Intuitively, the multi-layer perceptron can be thou is capable of approximating any Hausdorff continuous, symmetric function to an arbitrary precision <ref type="bibr" target="#b28">[29]</ref>.</p><p>We note that all of the following are essentially i the pooling aggregator can approximate any Hausdorff continuous function to an arbitrary precision <ref type="bibr" target="#b28">[29]</ref>. Note that we can always account for normalization constan y the pooling aggregator (or a variant of it), but the symmetric universal approximation theorem of <ref type="bibr" target="#b28">[29]</ref> along with Lipschitz continuity arguments suffice for the ue to our reliance on two universal approximation theorems <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>, the required dimensionality is in principle O(|V|). We can
itectures for learning over graphs have been proposed (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target= phs or are designed for whole-graph classification (or both) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target=
proved extremely useful as feature inputs for a wide variety of prediction and graph analysis tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ naturally generalize to unseen data, since they make predictions on nodes in a single, fixed graph <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ ensional embeddings using random walk statistics and matrix factorization-based learning objectives <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ
ead n="3.1">Back-boost learning</head><p>Back-boost learning borrows the idea from back translation <ref type="bibr" target="#b49">(Sennrich et al., 2016)</ref> in NMT, referring to training a backwar
vskaya et al., 2014)</ref> uses a classifier-based approach, which is improved by the latter system <ref type="bibr" target="#b44">(Rozovskaya and Roth, 2016)</ref> through combining with an SMT-based "bibr" target="#b50">(Susanto et al., 2014;</ref><ref type="bibr">Chollampatt et al., 2016b,a;</ref><ref type="bibr" target="#b44">Rozovskaya and Roth, 2016;</ref><ref type="bibr" target="#b26">Junczy
arget="#b46">Sakaguchi et al., 2016;</ref><ref type="bibr" target="#b34">Napoles et al., 2016;</ref><ref type="bibr" target="#b3">Bryant et al., 2017;</ref><ref type="bibr" target="#b0">Asano et al.,
="table" target="#tab_8">6</ref> shows the JFLEG leaderboard. Instead of M 2 score, JFLEG uses GLEU <ref type="bibr" target="#b33">(Napoles et al., 2015)</ref> as its evaluation metric, which is a flu type="bibr">Madnani et al., 2011;</ref><ref type="bibr" target="#b12">Dahlmeier and Ng, 2012c;</ref><ref type="bibr" target="#b33">Napoles et al., 2015;</ref><ref type="bibr" target="#b46">Sakaguchi e
yer bidirectional GRU RNN and the decoder is a 2-layer GRU RNN with the general attention mechanism <ref type="bibr" target="#b29">(Luong et al., 2015)</ref>. Both the dimensionality of word embedding
towards GLEU on JFLEG Dev set, reports a higher result (GLEU=61.50 on JFLEG test set). or MT-based <ref type="bibr" target="#b2">(Brockett et al., 2006;</ref><ref type="bibr">Dahlmeier and</ref><ref th training for GEC, despite some previous studies that explore artificial error generation for GEC <ref type="bibr" target="#b2">(Brockett et al., 2006;</ref><ref type="bibr" target="#b18">Foster and


5)</ref> as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU <ref type="bibr" target="#b38">(Papineni et al., 2002)</ref> and has several advantages over M 2 for

arget="#b33">Napoles et al., 2015;</ref><ref type="bibr" target="#b46">Sakaguchi et al., 2016;</ref><ref type="bibr" target="#b34">Napoles et al., 2016;</ref><ref type="bibr" target="#b3">Bryant et al
="#b4">(Chodorow et al., 2007;</ref><ref type="bibr" target="#b15">De Felice and Pulman, 2008;</ref><ref type="bibr" target="#b21">Han et al., 2010;</ref><ref type="bibr" target="#b28">Leacock et al.,
yer bidirectional GRU RNN and the decoder is a 2-layer GRU RNN with the general attention mechanism <ref type="bibr" target="#b29">(Luong et al., 2015)</ref>. Both the dimensionality of word embedding
rget="#b28">Leacock et al., 2010;</ref><ref type="bibr" target="#b53">Tetreault et al., 2010a;</ref><ref type="bibr" target="#b14">Dale and Kilgarriff, 2011)</ref> 7 The state-of-the-art result on CoN
ype="bibr" target="#b54">(Tetreault et al., 2010b;</ref><ref type="bibr">Madnani et al., 2011;</ref><ref type="bibr" target="#b12">Dahlmeier and Ng, 2012c;</ref><ref type="bibr" target="#b33">Napoles
et="#b50">(Susanto et al., 2014;</ref><ref type="bibr" target="#b5">Chollampatt et al., 2016a;</ref><ref type="bibr" target="#b6">Chollampatt and Ng, 2017</ref>) that combine SMT with other techniques
16)</ref> through combining with an SMT-based approach.</p><p>• NUS14, NUS16 and NUS17: GEC systems <ref type="bibr" target="#b50">(Susanto et al., 2014;</ref><ref type="bibr" target="#b5">Chollampatt ibr">shared task (Ng et al., 2014)</ref> use either of the methods. Recently, many novel approaches <ref type="bibr" target="#b50">(Susanto et al., 2014;</ref><ref type="bibr">Chollampatt et al., 2016
"#b28">(Leacock et al., 2010;</ref><ref type="bibr" target="#b40">Rei and Yannakoudakis, 2016;</ref><ref type="bibr" target="#b27">Kaneko et al., 2017)</ref> and GEC evaluation <ref type="bibr" target
rget="#b28">Leacock et al., 2010;</ref><ref type="bibr" target="#b53">Tetreault et al., 2010a;</ref><ref type="bibr" target="#b14">Dale and Kilgarriff, 2011)</ref> 7 The state-of-the-art result on CoN
sfluency candidates generated, the more helpful for training an error correction model. Inspired by <ref type="bibr" target="#b22">He et al. (2016)</ref> and <ref type="bibr" target="#b61">Zhang et al
r" target="#b34">Napoles et al., 2016;</ref><ref type="bibr" target="#b3">Bryant et al., 2017;</ref><ref type="bibr" target="#b0">Asano et al., 2017)</ref>. We do not introduce them in detail because
16)</ref> through combining with an SMT-based approach.</p><p>• NUS14, NUS16 and NUS17: GEC systems <ref type="bibr" target="#b50">(Susanto et al., 2014;</ref><ref type="bibr" target="#b5">Chollampatt ibr">shared task (Ng et al., 2014)</ref> use either of the methods. Recently, many novel approaches <ref type="bibr" target="#b50">(Susanto et al., 2014;</ref><ref type="bibr">Chollampatt et al., 2016
by changing the parameters of the optimization algorithm to depend on the network activation values <ref type="bibr" target="#b22">(Wiesler et al., 2014;</ref><ref type="bibr" target="#b14">Raiko et a
When the input distribution to a learning system changes, it is said to experience covariate shift <ref type="bibr" target="#b17">(Shimodaira, 2000)</ref>. This is typically handled via domain adapta
tion in the rest of the text. The training was performed on a large-scale, distributed architecture <ref type="bibr" target="#b1">(Dean et al., 2012)</ref>, using 5 concurrent steps on each of 10 mode
divergence. Furthermore, batch normalization regularizes the model and reduces the need for Dropout <ref type="bibr" target="#b18">(Srivastava et al., 2014)</ref>. Finally, Batch Normalization makes i g rate 6 times faster.</p><p>Remove Local Response Normalization While Inception and other networks <ref type="bibr" target="#b18">(Srivastava et al., 2014)</ref> benefit from it, we found that with B

variants such as momentum <ref type="bibr" target="#b19">(Sutskever et al., 2013)</ref> and Adagrad <ref type="bibr" target="#b3">(Duchi et al., 2011)</ref> have been used to achieve state of the art astic Gradient Descent with a mini-batch size m &gt; 1, or with any of its variants such as Adagrad <ref type="bibr" target="#b3">(Duchi et al., 2011)</ref>. The normalization of activations that depe


depend on the network activation values <ref type="bibr" target="#b22">(Wiesler et al., 2014;</ref><ref type="bibr" target="#b14">Raiko et al., 2012;</ref><ref type="bibr" target="#b13">Povey et al.,
SGD) has proved to be an effective way of training deep networks, and SGD variants such as momentum <ref type="bibr" target="#b19">(Sutskever et al., 2013)</ref> and Adagrad <ref type="bibr" target="# </ref>, using 5 concurrent steps on each of 10 model replicas, using asynchronous SGD with momentum <ref type="bibr" target="#b19">(Sutskever et al., 2013)</ref>, with the mini-batch size of 32. All n
tion in the rest of the text. The training was performed on a large-scale, distributed architecture <ref type="bibr" target="#b1">(Dean et al., 2012)</ref>, using 5 concurrent steps on each of 10 mode
depend on the network activation values <ref type="bibr" target="#b22">(Wiesler et al., 2014;</ref><ref type="bibr" target="#b14">Raiko et al., 2012;</ref><ref type="bibr" target="#b13">Povey et al.,
tion in the rest of the text. The training was performed on a large-scale, distributed architecture <ref type="bibr" target="#b1">(Dean et al., 2012)</ref>, using 5 concurrent steps on each of 10 mode


depend on the network activation values <ref type="bibr" target="#b22">(Wiesler et al., 2014;</ref><ref type="bibr" target="#b14">Raiko et al., 2012;</ref><ref type="bibr" target="#b13">Povey et al.,


SGD) has proved to be an effective way of training deep networks, and SGD variants such as momentum <ref type="bibr" target="#b19">(Sutskever et al., 2013)</ref> and Adagrad <ref type="bibr" target="# </ref>, using 5 concurrent steps on each of 10 model replicas, using asynchronous SGD with momentum <ref type="bibr" target="#b19">(Sutskever et al., 2013)</ref>, with the mini-batch size of 32. All n

depend on the network activation values <ref type="bibr" target="#b22">(Wiesler et al., 2014;</ref><ref type="bibr" target="#b14">Raiko et al., 2012;</ref><ref type="bibr" target="#b13">Povey et al.,

nteraction-focused <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref> or representation-focused <ref type="bibr" target="#b14">[1 h positions. It is also similar to the indicator matching matrix proposed previously by Pang et al. <ref type="bibr" target="#b30">[31]</ref>. While the interaction matrix X perfectly captures every q <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Pang et al. <ref type="bibr" target="#b30">[31]</ref> propose the use of matching matrices to represent the simi
get="#b40">41]</ref> and modelling short-text similarities <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" tar e="bibr" target="#b30">31]</ref> or representation-focused <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" ta the distributed model improves drastically in the presence of more data. Unlike some previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" ta nt. Our n-graph based input encoding is motivated by the trigraph encoding proposed by Huang et al. <ref type="bibr" target="#b15">[16]</ref>, but unlike their approach we don't limit our input repres on the retrieval task, as a baseline in this paper. Both the deep structured semantic model (DSSM) <ref type="bibr" target="#b15">[16]</ref> and its convolutional variant CDSSM <ref type="bibr" targe lated papers that use short text such as title, for document ranking or related tasks. Huang et al. <ref type="bibr" target="#b15">[16]</ref> learn a distributed representation of query and title, for
>[21]</ref>, query auto-completion <ref type="bibr" target="#b25">[26]</ref>, next query prediction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38]</ref>, and entity extrac
rget="#b13">[14]</ref> and latent Dirichlet allocation (LDA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref> learn lowdimensional vector representations of terms, and m r" target="#b22">[23]</ref>. LDA-based document models combine exact matching with inexact matching <ref type="bibr" target="#b38">[39]</ref>. Query hypergraphs capture all three <ref type="bibr" targ
alysis (LSA) <ref type="bibr" target="#b4">[5]</ref>, probabilistic latent semantic analysis (PLSA) <ref type="bibr" target="#b13">[14]</ref> and latent Dirichlet allocation (LDA) <ref type="bibr" tar
>[21]</ref>, query auto-completion <ref type="bibr" target="#b25">[26]</ref>, next query prediction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38]</ref>, and entity extrac
rget="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref> and modelling short-text similarities <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta ="#b11">[12]</ref> classify recent DNN models for short-text matching as either interaction-focused <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta pe="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref> or representation-focused <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta elines. Other convolutional models that match short texts using distributed representations include <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref>, also showing good
get="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref> and modelling short etrieval tasks has been rather limited. Some recent papers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref> report worse performance of neural embedding models when co NNs on the document ranking task. Unlike other recent work <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>, our model significantly outperforms classic IR approaches embeddings include <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Pang et al. <ref type="bibr" target="#b30">[31]</ref
pace, then it is their distributed representations that are compared. Along these lines, Guo et al. <ref type="bibr" target="#b11">[12]</ref> classify recent DNN models for short-text matching as eith T was constrained by memory requirements, and we pick 499 for our experiments.</p><p>The DRMM model <ref type="bibr" target="#b11">[12]</ref> uses a DNN to perform term matching, with few hundred para y using the match matrix to generate summary statistics it is possible to make the method work well <ref type="bibr" target="#b11">[12]</ref>, which is our DRMM baseline.</p><p>These term embeddings a ve results have been reported for title-based DSSM and CDSSM on the ad hoc document retrieval tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>, we included docum
pace, then it is their distributed representations that are compared. Along these lines, Guo et al. <ref type="bibr" target="#b11">[12]</ref> classify recent DNN models for short-text matching as eith T was constrained by memory requirements, and we pick 499 for our experiments.</p><p>The DRMM model <ref type="bibr" target="#b11">[12]</ref> uses a DNN to perform term matching, with few hundred para y using the match matrix to generate summary statistics it is possible to make the method work well <ref type="bibr" target="#b11">[12]</ref>, which is our DRMM baseline.</p><p>These term embeddings a ve results have been reported for title-based DSSM and CDSSM on the ad hoc document retrieval tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>, we included docum
target task has been explored in the context of other IR scenarios, including query classification <ref type="bibr" target="#b20">[21]</ref>, query auto-completion <ref type="bibr" target="#b25">[26]
sed on word embeddings <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ ance when training a specialized term embedding. Other papers incorporating word embeddings include <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" tar
-text similarities <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" ta
et="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. In traditional W t="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. They claim that cked titles over randomly chosen titles, and the test metric is NDCG with human labels. Shen et al. <ref type="bibr" target="#b35">[36]</ref> developed a convolutional version of the model. These are ly in the presence of more data. Unlike some previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> that train on click
target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar bedding. Other papers incorporating word embeddings include <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Pang et al.
rget="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref> and modelling short-text similarities <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta ="#b11">[12]</ref> classify recent DNN models for short-text matching as either interaction-focused <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta pe="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref> or representation-focused <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta elines. Other convolutional models that match short texts using distributed representations include <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref>, also showing good
ations of queries and documents for use in Web page ranking. Our measure of ranking quality is NDCG <ref type="bibr" target="#b16">[17]</ref>, which rewards a ranker for returning documents with highe
ith inexact matching <ref type="bibr" target="#b38">[39]</ref>. Query hypergraphs capture all three <ref type="bibr" target="#b0">[1]</ref>. Our method also combines these techniques but, unlike prior
m the corpus. 1 While surprising, this last property is important for detecting quality web content <ref type="bibr" target="#b41">[42]</ref>.  Query terms are laid out along the vertical axis, and th
et="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" t esentation-focused <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" t atch short texts using distributed representations include <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref>, also showing good performance on short text ranking tasks.
in areas where large scale training corpora are available <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Some of the lack of positive results from neural models in ts on a variety of tasks such as speech recognition, visual object recognition and object detection <ref type="bibr" target="#b19">[20]</ref>.</p><p>This paper learns a text representation end-to-end
ith inexact matching <ref type="bibr" target="#b38">[39]</ref>. Query hypergraphs capture all three <ref type="bibr" target="#b0">[1]</ref>. Our method also combines these techniques but, unlike prior
pe="bibr" target="#b2">(Caruana, 1995;</ref><ref type="bibr" target="#b1">Bengio et al., 2011;</ref><ref type="bibr" target="#b0">Bengio, 2011)</ref>. In transfer learning, we first train a base netwo
ImageNet dataset, we find lower performance than has been previously reported for smaller datasets <ref type="bibr" target="#b7">(Jarrett et al., 2009)</ref> when using features computed from random /ns/1.0"><head n="4.3">Random Weights</head><p>We also compare to random, untrained weights because <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref> showed -quite strikingly -that the combina ks may not be as straightforward as it was for the smaller network size and smaller dataset used by <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref>. However, the comparison is not straightfo htforward. Whereas our networks have max pooling and local normalization on layers 1 and 2, just as <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref> did, we use a different nonlinearity (relu nt task is better than using random filters. One possible reason this latter result may differ from <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref> is because their fully-trained (non-random
ImageNet dataset, we find lower performance than has been previously reported for smaller datasets <ref type="bibr" target="#b7">(Jarrett et al., 2009)</ref> when using features computed from random /ns/1.0"><head n="4.3">Random Weights</head><p>We also compare to random, untrained weights because <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref> showed -quite strikingly -that the combina ks may not be as straightforward as it was for the smaller network size and smaller dataset used by <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref>. However, the comparison is not straightfo htforward. Whereas our networks have max pooling and local normalization on layers 1 and 2, just as <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref> did, we use a different nonlinearity (relu nt task is better than using random filters. One possible reason this latter result may differ from <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref> is because their fully-trained (non-random
een reported elsewhere in the literature <ref type="bibr" target="#b6">(Girshick et al., 2013;</ref><ref type="bibr" target="#b5">Donahue et al., 2013b)</ref>, to our knowledge these results have been on relatively small networks of two or three learned layers and on the smaller Caltech-101 dataset <ref type="bibr" target="#b5">(Fei-Fei et al., 2004)</ref>.</p><p>It is natural to ask whether or no
ImageNet dataset, we find lower performance than has been previously reported for smaller datasets <ref type="bibr" target="#b7">(Jarrett et al., 2009)</ref> when using features computed from random /ns/1.0"><head n="4.3">Random Weights</head><p>We also compare to random, untrained weights because <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref> showed -quite strikingly -that the combina ks may not be as straightforward as it was for the smaller network size and smaller dataset used by <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref>. However, the comparison is not straightfo htforward. Whereas our networks have max pooling and local normalization on layers 1 and 2, just as <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref> did, we use a different nonlinearity (relu nt task is better than using random filters. One possible reason this latter result may differ from <ref type="bibr" target="#b7">Jarrett et al. (2009)</ref> is because their fully-trained (non-random
get="#b4">(Donahue et al., 2013a;</ref><ref type="bibr" target="#b13">Zeiler and Fergus, 2013;</ref><ref type="bibr" target="#b12">Sermanet et al., 2014)</ref>, collectively suggesting that these laye
ote> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2012" xml:id="foot_1">(ILSVRC2012)<ref type="bibr" target="#b3">(Deng et al., 2009)</ref> contains 1,281,167 labeled training images a
="bibr" target="#b11">(Lee et al., 2009)</ref>, and unsupervised learning of sparse representations <ref type="bibr" target="#b10">(Le et al., 2011)</ref>.</p><p>Because finding these standard feature
tasets, but even with very different training objectives, including supervised image classification <ref type="bibr" target="#b9">(Krizhevsky et al., 2012)</ref>, unsupervised density learning <ref ty rmula></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>Since <ref type="bibr" target="#b9">Krizhevsky et al. (2012)</ref> won the ImageNet 2012 competition, ther
tasets, but even with very different training objectives, including supervised image classification <ref type="bibr" target="#b9">(Krizhevsky et al., 2012)</ref>, unsupervised density learning <ref ty rmula></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>Since <ref type="bibr" target="#b9">Krizhevsky et al. (2012)</ref> won the ImageNet 2012 competition, ther
tasets, but even with very different training objectives, including supervised image classification <ref type="bibr" target="#b9">(Krizhevsky et al., 2012)</ref>, unsupervised density learning <ref ty rmula></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>Since <ref type="bibr" target="#b9">Krizhevsky et al. (2012)</ref> won the ImageNet 2012 competition, ther
ng works has paid special attention to embed bipartite networks. While a recent work by Dong et al. <ref type="bibr" target="#b13">[14]</ref> proposed metapath2vec++ for embedding heterogeneous networ ght be suboptimal for learning vertex representations for a bipartite network.</p><p>Metapath2vec++ <ref type="bibr" target="#b13">[14]</ref>, HNE <ref type="bibr" target="#b26">[27]</ref> and EOE <re We assign a probability to stop a random walk in each step. In contrast to DeepWalk and other work <ref type="bibr" target="#b13">[14]</ref> that apply a fixed length on the random walk, we allow the hyper-parameters p and q are set to 0.5 which has empirically shown good results. • Metapath2vec++ <ref type="bibr" target="#b13">[14]</ref>: This is the state-of-the-art method for embedding heterog
-order and 2nd-order proximities between vertices to embed homogeneous networks. Specifically, LINE <ref type="bibr" target="#b19">[20]</ref> learns two separated embeddings for 1st-order and 2nd-orde ignal on constructing the bipartite network. Similar to the modeling of 1st-order proximity in LINE <ref type="bibr" target="#b19">[20]</ref>, we model explicit relations by considering the local prox of vertex sequences. Then the word2vec is applied on the corpus to learn vertex embeddings. • LINE <ref type="bibr" target="#b19">[20]</ref>: This approach optimizes both the 1st-order and 2nd-order vec inspire many works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref> to use inner product to model the interaction between two e ural embedding methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>, we parameterize the conditional probability P(u c |u i ) a
target="#b3">[4,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Following the pioneering work of DeepWalk <ref type="bibr" ral proposals to incorporate side information into vertex embedding learning, such as vertex labels <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>, community informat
arget="#b19">[20]</ref> learns two separated embeddings for 1st-order and 2nd-order relations; SDNE <ref type="bibr" target="#b20">[21]</ref> incorporates both 1st-order and 2nd-order proximities to p
arget="#b19">[20]</ref> learns two separated embeddings for 1st-order and 2nd-order relations; SDNE <ref type="bibr" target="#b20">[21]</ref> incorporates both 1st-order and 2nd-order proximities to p
, we plan to extend our BiNE method to model auxiliary side information, such as numerical features <ref type="bibr" target="#b41">[42]</ref>, textual descriptions <ref type="bibr" target="#b42">[43]<
, we plan to extend our BiNE method to model auxiliary side information, such as numerical features <ref type="bibr" target="#b41">[42]</ref>, textual descriptions <ref type="bibr" target="#b42">[43]<
cal implementation of LFM is based on matrix factorization <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Recent advances several competitive methods 7 that are designed for the top-K item recommendation task.</p><p>• BPR <ref type="bibr" target="#b30">[31]</ref>:This method optimizes the matrix factorization (MF) model
dvances in data mining and information retrieval have focused on learning representations from data <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe learning techniques. The pioneer work DeepWalk <ref type="bibr" target="#b7">[8]</ref> and Node2vec <ref type="bibr" target="#b3">[4]</ref> extend the idea of Skip-gram <ref type="bibr" target="#b10"> /head><p>(1) To evaluate the link prediction task, we apply the same protocol as the node2vec paper <ref type="bibr" target="#b3">[4]</ref>. Specifically, for the Wikepedia dataset, the observed links etwork. We use the LINE(1st+2nd) method which has shown the best results in their paper. • Node2vec <ref type="bibr" target="#b3">[4]</ref>: This method extends DeepWalk by performing biased random wa works have primarily focused on embedding homogeneous networks where vertices are of the same type <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" targe wo vertices in the embedding space. The effectiveness and prevalence of word2vec inspire many works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target ming random walks on the network, which has been used in some homogeneous network embedding methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. However, directly per C S (v j ) P (v c |v j ).<label>(6)</label></formula><p>Following existing neural embedding methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
han 5 minutes, otherwise, it is treated as an negative instance. For both datasets,</p><p>• RankALS <ref type="bibr" target="#b38">[39]</ref>: This method also optimizes the MF model for the ranking t
, we plan to extend our BiNE method to model auxiliary side information, such as numerical features <ref type="bibr" target="#b41">[42]</ref>, textual descriptions <ref type="bibr" target="#b42">[43]<
, we plan to extend our BiNE method to model auxiliary side information, such as numerical features <ref type="bibr" target="#b41">[42]</ref>, textual descriptions <ref type="bibr" target="#b42">[43]<
the most representative model. And a typical implementation of LFM is based on matrix factorization <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" t
esentations from data <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. In particular, they
incorporates both 1st-order and 2nd-order proximities to preserve the network structure; and GraRep <ref type="bibr" target="#b21">[22]</ref> further extends the method to capture higher-order proximi
arget="#b19">[20]</ref> learns two separated embeddings for 1st-order and 2nd-order relations; SDNE <ref type="bibr" target="#b20">[21]</ref> incorporates both 1st-order and 2nd-order proximities to p
sers' click behaviors that provide valuable relevance signal <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>; in another application of recommender systems, users and ite
network to obtain a "corpus" of vertices, and then applying word embedding methods such as word2vec <ref type="bibr" target="#b10">[11]</ref> to obtain the embeddings for vertices. Despite effectivene t="#b7">[8]</ref> and Node2vec <ref type="bibr" target="#b3">[4]</ref> extend the idea of Skip-gram <ref type="bibr" target="#b10">[11]</ref> to model homogeneous network, which is convert to a corpus networks respectively, we obtain two corpora of vertex sequences. Next we employ the Skipgram model <ref type="bibr" target="#b10">[11]</ref> on the two corpora to learn vertex embeddings. The aim is ch is very time-costing. To reduce the learning complexity, we employ the idea of negative sampling <ref type="bibr" target="#b10">[11]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> some heuristics have been applied, such as sampling from popularity-biased non-uniform distribution <ref type="bibr" target="#b10">[11]</ref>. Here we propose a more grounded sampling method that cate
arget="#b19">[20]</ref> learns two separated embeddings for 1st-order and 2nd-order relations; SDNE <ref type="bibr" target="#b20">[21]</ref> incorporates both 1st-order and 2nd-order proximities to p
re is no stationary distribution of random walks on bipartite networks due to the periodicity issue <ref type="bibr" target="#b33">[34]</ref>. To address this issue, we consider performing random walk
cal implementation of LFM is based on matrix factorization <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Recent advances several competitive methods 7 that are designed for the top-K item recommendation task.</p><p>• BPR <ref type="bibr" target="#b30">[31]</ref>:This method optimizes the matrix factorization (MF) model
dvances in data mining and information retrieval have focused on learning representations from data <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe learning techniques. The pioneer work DeepWalk <ref type="bibr" target="#b7">[8]</ref> and Node2vec <ref type="bibr" target="#b3">[4]</ref> extend the idea of Skip-gram <ref type="bibr" target="#b10"> /head><p>(1) To evaluate the link prediction task, we apply the same protocol as the node2vec paper <ref type="bibr" target="#b3">[4]</ref>. Specifically, for the Wikepedia dataset, the observed links etwork. We use the LINE(1st+2nd) method which has shown the best results in their paper. • Node2vec <ref type="bibr" target="#b3">[4]</ref>: This method extends DeepWalk by performing biased random wa works have primarily focused on embedding homogeneous networks where vertices are of the same type <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" targe wo vertices in the embedding space. The effectiveness and prevalence of word2vec inspire many works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target ming random walks on the network, which has been used in some homogeneous network embedding methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. However, directly per C S (v j ) P (v c |v j ).<label>(6)</label></formula><p>Following existing neural embedding methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
ardware mechanism that predicts "when" a block in a data cache becomes evictable. In a recent paper <ref type="bibr" target="#b6">[7]</ref>, we proposed trace-based predictors that record a trace of s placement and a subsequent address to prefetch for the corresponding block frame. In a recent paper <ref type="bibr" target="#b6">[7]</ref>, we proposed Last-Touch Predictors (LTPs) to predict memory g array and stores a trace encoding associated with every tag. We use truncated addition (as before <ref type="bibr" target="#b6">[7]</ref>) to maintain a fixed-size encoding for every instruction tra ave dead-block signatures that are proper subsequences of each other resulting in subtrace aliasing <ref type="bibr" target="#b6">[7]</ref>. To prevent aliasing, DBP maintains deadblock signatures per yond just predicting memory invalidation and sharing for scientific applications in multiprocessors <ref type="bibr" target="#b6">[7]</ref>. The results corroborate the intuition that because memory i is that while data structures are often referenced in multiple distinct program contexts or phases <ref type="bibr" target="#b6">[7]</ref> (e.g., a given sequence procedure invocations), they are not
ity Pittsburgh, PA 15213 babak@ece.cmu.edu http://www.ece.cmu.edu/-impetus with non-blocking caches <ref type="bibr" target="#b18">[19]</ref> allow overlapping the miss latency among the higher cache
">6]</ref> or software <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>, many researchers a target="#b8">[9]</ref>, Lipasti, et al. <ref type="bibr" target="#b7">[8]</ref>, and Ozawa, et al. <ref type="bibr" target="#b13">[14]</ref>, also evaluate the effectiveness of heuristicsbased techni
" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targe roposals for hardware prefetchers target specific memory access patterns --such as strided accesses <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targ and correlated them with a "starting" address to prefetch them sequentially. Palacharla and Kessler <ref type="bibr" target="#b14">[15]</ref> extended stream buffers to non-unit stride stream. Mehrort

target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>, many researchers and vendors opt for hardware implementati y addresses are subsequently referenced and when the data can be placed in the cache. Mowry, et al. <ref type="bibr" target="#b11">[12]</ref>, show that for numerical and scientific applications, soft
target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> or software <ref type=" specific memory access patterns --such as strided accesses <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> and accesses to linked stomize hardware for specific memory reference patterns. Chert and Baer proposed stride prefetchers <ref type="bibr" target="#b3">[4]</ref> that correlate non-unit data address strides with a memory i
" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targe roposals for hardware prefetchers target specific memory access patterns --such as strided accesses <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targ and correlated them with a "starting" address to prefetch them sequentially. Palacharla and Kessler <ref type="bibr" target="#b14">[15]</ref> extended stream buffers to non-unit stride stream. Mehrort
target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>, many researchers and vendors opt for hardware implementati y addresses are subsequently referenced and when the data can be placed in the cache. Mowry, et al. <ref type="bibr" target="#b11">[12]</ref>, show that for numerical and scientific applications, soft

ity Pittsburgh, PA 15213 babak@ece.cmu.edu http://www.ece.cmu.edu/-impetus with non-blocking caches <ref type="bibr" target="#b18">[19]</ref> allow overlapping the miss latency among the higher cache

bilinear models, e.g. DistMult <ref type="bibr" target="#b22">(Yang et al. 2015)</ref> and ComplEx <ref type="bibr" target="#b20">(Trouillon et al. 2016)</ref>, have achieved promising performance in es et al. 2013)</ref>, DistMult <ref type="bibr" target="#b22">(Yang et al. 2015)</ref> and ComplEx <ref type="bibr" target="#b20">(Trouillon et al. 2016)</ref>, (ii) an uncertain graph embedding mode mance in many tasks, such as link prediction <ref type="bibr" target="#b22">(Yang et al. 2015;</ref><ref type="bibr" target="#b20">Trouillon et al. 2016)</ref>, relation extraction (Weston, <ref type= behind a term that has critically supported short text understanding tasks involving disambiguation <ref type="bibr" target="#b20">(Wang et al. 2015;</ref><ref type="bibr">Wang and Wang 2016)</ref>. F ark dataset for deterministic KG embeddings <ref type="bibr" target="#b1">(Bordes et al. 2013;</ref><ref type="bibr" target="#b20">Wang et al. 2014;</ref><ref type="bibr" target="#b22">Yang et al. 201 is based on mean reciprocal rank (MRR) on the validation set. We adopt the implementation given by <ref type="bibr" target="#b20">(Trouillon et al. 2016</ref>) and choose the best hyper-parameters fo n facts from lowconfidence ones.</p><p>Evaluation protocol We follow a procedure that is similar to <ref type="bibr" target="#b20">(Wang et al. 2014)</ref>. Our test set consists of relation facts fro

cial feature models that benefit numerous knowledge-driven tasks (Bordes, Weston, and Usunier 2014; <ref type="bibr" target="#b8">He et al. 2017;</ref><ref type="bibr" target="#b5">Das et al. 2018)</r
amount of negative links as existing relation facts into the test sets.</p><p>We use Adam optimizer <ref type="bibr" target="#b16">(Kingma and Ba 2014)</ref> for training, for which we set the exponen idates and evaluate the ranking performance using the normalized Discounted Cumulative G ain (nDCG) <ref type="bibr" target="#b16">(Li, Liu, and Zhai 2009)</ref>. We define the gain in retrieving a re

bilinear models, e.g. DistMult <ref type="bibr" target="#b22">(Yang et al. 2015)</ref> and ComplEx <ref type="bibr" target="#b20">(Trouillon et al. 2016)</ref>, have achieved promising performance in es et al. 2013)</ref>, DistMult <ref type="bibr" target="#b22">(Yang et al. 2015)</ref> and ComplEx <ref type="bibr" target="#b20">(Trouillon et al. 2016)</ref>, (ii) an uncertain graph embedding mode mance in many tasks, such as link prediction <ref type="bibr" target="#b22">(Yang et al. 2015;</ref><ref type="bibr" target="#b20">Trouillon et al. 2016)</ref>, relation extraction (Weston, <ref type= behind a term that has critically supported short text understanding tasks involving disambiguation <ref type="bibr" target="#b20">(Wang et al. 2015;</ref><ref type="bibr">Wang and Wang 2016)</ref>. F ark dataset for deterministic KG embeddings <ref type="bibr" target="#b1">(Bordes et al. 2013;</ref><ref type="bibr" target="#b20">Wang et al. 2014;</ref><ref type="bibr" target="#b22">Yang et al. 201 is based on mean reciprocal rank (MRR) on the validation set. We adopt the implementation given by <ref type="bibr" target="#b20">(Trouillon et al. 2016</ref>) and choose the best hyper-parameters fo n facts from lowconfidence ones.</p><p>Evaluation protocol We follow a procedure that is similar to <ref type="bibr" target="#b20">(Wang et al. 2014)</ref>. Our test set consists of relation facts fro
ational learning <ref type="bibr">(Nickel, Rosasco, and Poggio 2016)</ref>, and ontology population <ref type="bibr" target="#b4">(Chen et al. 2018)</ref>.</p><p>While current embedding models focus o
/p><p>One recent work has proposed a matrix-factorizationbased approach to embed uncertain networks <ref type="bibr" target="#b9">(Hu et al. 2017</ref>). However, it cannot be generalized to embed unc pe="bibr" target="#b20">(Trouillon et al. 2016)</ref>, (ii) an uncertain graph embedding model URGE <ref type="bibr" target="#b9">(Hu et al. 2017)</ref> Here linear stands for linear gain, and exp. st
-driven tasks (Bordes, Weston, and Usunier 2014; <ref type="bibr" target="#b8">He et al. 2017;</ref><ref type="bibr" target="#b5">Das et al. 2018)</ref>. Recently, extensive efforts have been devoted
ike Neural Tensor Network (NTN) <ref type="bibr" target="#b19">(Socher et al. 2013)</ref> and ConvE <ref type="bibr" target="#b6">(Dettmers et al. 2018)</ref>, and the circular-correlation-based model
nalysis to prove that our transductive model is a more general form than existing models (e.g., MNE <ref type="bibr" target="#b42">[43]</ref>). • Efficient and scalable learning algorithms for GATNE h beds networks with multiple views in a single collaborated embedding using attention mechanism. MNE <ref type="bibr" target="#b42">[43]</ref> uses one common embedding and several additional embedding r ∈ R s×d is a trainable transformation matrix.</p><p>Connection with Previous Work. We choose MNE <ref type="bibr" target="#b42">[43]</ref>, a recent representative work for MHEN, as the base model PMNE <ref type="bibr" target="#b21">[22]</ref>, MVE <ref type="bibr" target="#b29">[30]</ref>, MNE <ref type="bibr" target="#b42">[43]</ref>. We denote the three methods of PMNE as PMNE(n), PMNE(r) a
"#b0">[1]</ref>, link prediction <ref type="bibr" target="#b38">[39]</ref>, and community detection <ref type="bibr" target="#b7">[8]</ref>. DeepWalk <ref type="bibr" target="#b26">[27]</ref>, LINE <r
learning tasks such as node classification <ref type="bibr" target="#b0">[1]</ref>, link prediction <ref type="bibr" target="#b38">[39]</ref>, and community detection <ref type="bibr" target="#b7">[8]
To deal with the partial observation problem, we further extend the model to the inductive context <ref type="bibr" target="#b41">[42]</ref> and present a new inductive model named as GATNE-I. For bo rved data. However, in many real-world applications, the networked data is often partially observed <ref type="bibr" target="#b41">[42]</ref>. We then extend our model to the inductive context and pre
tly, we can add dynamic information into node attributes. For example, we can use methods like LSTM <ref type="bibr" target="#b13">[14]</ref> to capture the dynamic activities of users. Secondly, the
type="bibr" target="#b26">[27]</ref>, LINE <ref type="bibr" target="#b34">[35]</ref>, and node2vec <ref type="bibr" target="#b9">[10]</ref> are pioneering works that introduce deep learning technique .org/ns/1.0"><head>Single</head><p>Single / LINE <ref type="bibr" target="#b34">[35]</ref> node2vec <ref type="bibr" target="#b9">[10]</ref> NetMF <ref type="bibr" target="#b28">[29]</ref> NetSMF <ref ons on large-scale networks while preserving both firstorder and second-order proximities. node2vec <ref type="bibr" target="#b9">[10]</ref> designs a biased random walk procedure to efficiently explo type="bibr" target="#b26">[27]</ref>, LINE <ref type="bibr" target="#b34">[35]</ref>, and node2vec <ref type="bibr" target="#b9">[10]</ref>. As these methods can only deal with HON, we feed separate or firstorder and second-order embeddings. The number of samples is set to 1000 million. • node2vec <ref type="bibr" target="#b9">[10]</ref>. The codes of node2vec are from the corresponding author's mization</head><p>We discuss how to learn the proposed transductive and inductive models. Following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar
cently due to significant progress in downstream network learning tasks such as node classification <ref type="bibr" target="#b0">[1]</ref>, link prediction <ref type="bibr" target="#b38">[39]</ref>, stributed cloud platform.</p><p>Function Selection. Many different aggregator functions in Equation <ref type="bibr" target="#b0">(1)</ref>, such as the mean aggregator (Cf. Equation ( <ref type="form
walk to generate node sequences and then perform skip-gram <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> over the node sequences to learn embeddings. Since each vie
"#b0">[1]</ref>, link prediction <ref type="bibr" target="#b38">[39]</ref>, and community detection <ref type="bibr" target="#b7">[8]</ref>. DeepWalk <ref type="bibr" target="#b26">[27]</ref>, LINE <r
a under the ROC curve (ROC-AUC) <ref type="bibr" target="#b11">[12]</ref> and the PR curve (PR-AUC) <ref type="bibr" target="#b4">[5]</ref> in our experiments. We also use F1 score as the other metric
efinition 1 (Heterogeneous Network). A heterogeneous network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref> is a network G = (V, E) associated with a node type mapping
h as the element-wise mean E p [z] = [π 1 , ..., π k ] of these vectors.</p><p>The Gumbel-Max trick <ref type="bibr" target="#b8">(Gumbel, 1954;</ref><ref type="bibr" target="#b12">Maddison et al., 20

et al., 2015)</ref>, and memory locations <ref type="bibr" target="#b5">(Graves et al., 2014;</ref><ref type="bibr" target="#b4">Graves et al., 2016)</ref>. Discrete representations are often more in

k ] of these vectors.</p><p>The Gumbel-Max trick <ref type="bibr" target="#b8">(Gumbel, 1954;</ref><ref type="bibr" target="#b12">Maddison et al., 2014)</ref> provides a simple and efficient way to d
ent variables using backpropagation <ref type="bibr" target="#b9">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b21">Rezende et al., 2014b)</ref>. As shown in Figure <ref type="figure" t
k ] of these vectors.</p><p>The Gumbel-Max trick <ref type="bibr" target="#b8">(Gumbel, 1954;</ref><ref type="bibr" target="#b12">Maddison et al., 2014)</ref> provides a simple and efficient way to d
-BASED GRADIENT ESTIMATORS</head><p>The score function estimator (SF, also referred to as REINFORCE <ref type="bibr" target="#b26">(Williams, 1992)</ref> and likelihood ratio estimator <ref type="bibr
k ] of these vectors.</p><p>The Gumbel-Max trick <ref type="bibr" target="#b8">(Gumbel, 1954;</ref><ref type="bibr" target="#b12">Maddison et al., 2014)</ref> provides a simple and efficient way to d
distinct semantic classes <ref type="bibr" target="#b10">(Kingma et al., 2014)</ref>, image regions <ref type="bibr" target="#b27">(Xu et al., 2015)</ref>, and memory locations <ref type="bibr" target
distinct semantic classes <ref type="bibr" target="#b10">(Kingma et al., 2014)</ref>, image regions <ref type="bibr" target="#b27">(Xu et al., 2015)</ref>, and memory locations <ref type="bibr" target
complete KGs, extensive research efforts <ref type="bibr" target="#b21">(Nickel et al., 2011;</ref><ref type="bibr" target="#b2">Bordes et al., 2013</ref>  et <ref type="bibr">al., 2014;</ref><ref ty t al., 2011)</ref> is one of the earlier work that models the relationship using tensor operations. <ref type="bibr" target="#b2">Bordes et al. (2013)</ref> proposed to model relationships in the 1-D local connections in knowledge graph.</p><p>Although the entity embeddings from KG embedding models <ref type="bibr" target="#b2">(Bordes et al., 2013;</ref><ref type="bibr" target="#b38">Yang et al., embedding-based methods: RESCAL <ref type="bibr" target="#b21">(Nickel et al., 2011)</ref>, TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, Dist-Mult <ref type="bibr" target="#b38">
o two main categories: (1) metric based approaches <ref type="bibr" target="#b10">(Koch, 2015;</ref><ref type="bibr" target="#b33">Vinyals et al., 2016;</ref><ref type="bibr" target="#b26">Snell et al ese network <ref type="bibr" target="#b10">(Koch, 2015)</ref>. One example is the Matching Networks <ref type="bibr" target="#b33">(Vinyals et al., 2016)</ref>, which make predictions by comparing the used to predict new facts with oneshot examples. Following the standard one-shot learning settings <ref type="bibr" target="#b33">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b23">Ravi and L huber, 1997) recurrent "processing" block <ref type="bibr" target="#b32">(Vinyals et al., 2015</ref><ref type="bibr" target="#b33">(Vinyals et al., , 2016) )</ref> to perform multi-step matching. Ever

br" target="#b1">Bollacker et al., 2008;</ref><ref type="bibr" target="#b0">Auer et al., 2007;</ref><ref type="bibr" target="#b3">Carlson et al., 2010)</ref> represent every piece of information as bi
br" target="#b1">Bollacker et al., 2008;</ref><ref type="bibr" target="#b0">Auer et al., 2007;</ref><ref type="bibr" target="#b3">Carlson et al., 2010)</ref> represent every piece of information as bi
riterion.</p><p>Previous few-shot learning research mainly focuses on vision and imitation learning <ref type="bibr" target="#b6">(Duan et al., 2017)</ref> domains. In the language domain, <ref type="
mparing the input example with a small labeled support set;</p><p>(2) meta-learner based approaches <ref type="bibr" target="#b23">(Ravi and Larochelle, 2017;</ref><ref type="bibr" target="#b19">Munkh del parameters) given the gradients on few-shot examples. One example is the LSTMbased meta-learner <ref type="bibr" target="#b23">(Ravi and Larochelle, 2017)</ref>, which learns the step size for eac the standard one-shot learning settings <ref type="bibr" target="#b33">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b23">Ravi and Larochelle, 2017)</ref>, we assume access to a set of traini

portant background knowledge for us to match entity pairs.</p><p>Our first dataset is based on NELL <ref type="bibr" target="#b18">(Mitchell et al., 2018)</ref>, a system that continuously collects st
example triple (h 0 , r, t 0 ). The candidates set is constructed using the entity type constraint <ref type="bibr" target="#b30">(Toutanova et al., 2015)</ref>. It is also worth noting that when we hyperparameter to be tuned.  Existing benchmarks for knowledge graph completion, such as FB15k-237 <ref type="bibr" target="#b30">(Toutanova et al., 2015)</ref> and <ref type="bibr">YAGO3-10 (Mahdiso
symbols. More recently, several models <ref type="bibr" target="#b25">(Shi and Weninger, 2017;</ref><ref type="bibr" target="#b35">Xie et al., 2016)</ref> have been proposed to handle unseen entities
lso are sensitive to small, worst-case perturbations of the input, so-called "adversarial examples" <ref type="bibr" target="#b32">(Szegedy et al., 2014)</ref>. This latter phenomenon has struck many et al., 2004;</ref><ref type="bibr" target="#b3">Biggio &amp; Roli, 2018)</ref>. Since the work of <ref type="bibr" target="#b32">Szegedy et al. (2014)</ref>, a subfield has focused specifically on t ly on the phenomenon of small adversarial perturbations of the input, or "adversarial examples." In <ref type="bibr" target="#b32">Szegedy et al. (2014)</ref> it was proposed these adversarial example mproved robustness to small perturbations.</p><p>In the introduction we referred to a question from <ref type="bibr" target="#b32">Szegedy et al. (2014)</ref> about why we find errors so close to our lem for every point in the test set <ref type="bibr" target="#b23">(Katz et al., 2017)</ref>. Since <ref type="bibr" target="#b32">Szegedy et al. (2014)</ref>, hundreds of adversarial defense papers h
imization <ref type="bibr" target="#b19">(Guo et al., 2017)</ref>, respresentation-guided denoising <ref type="bibr" target="#b24">(Liao et al., 2018)</ref>, and random resizing and random padding of inimization<ref type="bibr" target="#b19">(Guo et al., 2017)</ref>, respresentation-guided denoising<ref type="bibr" target="#b24">(Liao et al., 2018)</ref>, and random resizing and random padding of

possible to deduce a bound relating these two quantities from the Gaussian isoperimetric inequality <ref type="bibr" target="#b4">(Borell, 1975)</ref>. The form we will use is:</p><p>Theorem (Gaussian
ed by a factor of 0.2 at epochs 60, 120, 160.</p><p>Models trained on ImageNet. The ResNet-50 model <ref type="bibr" target="#b20">(He et al., 2016)</ref> was trained with a learning rate of 1.6, batc
or the error set of a neural network. In fact, finding the distance to the nearest error is NP-hard <ref type="bibr" target="#b23">(Katz et al., 2017)</ref>. Instead, the best we can do is to search f dversarial robustness perfectly requires solving an NP-hard problem for every point in the test set <ref type="bibr" target="#b23">(Katz et al., 2017)</ref>. Since <ref type="bibr" target="#b32">Szege
g studies general ways in which an adversary may interact with an ML system, and dates back to 2004 <ref type="bibr" target="#b8">(Dalvi et al., 2004;</ref><ref type="bibr" target="#b3">Biggio &amp; R
ref type="bibr" target="#b10">Das et al., 2018;</ref><ref type="bibr">2017)</ref>, Pixel Deflection <ref type="bibr" target="#b28">(Prakash et al., 2018)</ref>, total variance minimization <ref type=" ir et al., 2018;</ref><ref type="bibr" target="#b10">Das et al., 2018;</ref> 2017), Pixel Deflection<ref type="bibr" target="#b28">(Prakash et al., 2018)</ref>, total variance minimization<ref type="b
of image space. However, more recent work has provided evidence that this is not true. For example, <ref type="bibr" target="#b14">Fawzi et al. (2016)</ref>; <ref type="bibr" target="#b16">Franceschi more consistent with the second of these two possibilities. This relationship was also explored in <ref type="bibr" target="#b14">Fawzi et al. (2016;</ref><ref type="bibr">2018)</ref>; here we additi
, 2017b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b34">Vasiljevic et al., 2016;</ref><ref type="bibr" target="#b39">Zheng et al., 2016)</ref>. Our work suggests that adversarial defense
possible to deduce a bound relating these two quantities from the Gaussian isoperimetric inequality <ref type="bibr" target="#b4">(Borell, 1975)</ref>. The form we will use is:</p><p>Theorem (Gaussian
d><p>Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes <ref type="bibr" target="#b14">[15]</ref>. The network predicts 4 coordinates for each bounding box, ocation of filter application using a sigmoid function. This figure blatantly self-plagiarized from <ref type="bibr" target="#b14">[15]</ref>.</p><p>is not the best but does overlap a ground truth obj location of filter application using a sigmoid function. This figure blatantly self-plagiarized from<ref type="bibr" target="#b14">[15]</ref>.</figDesc></figure> <figure xmlns="http://www.tei-c.org/ns
Our system extracts features from those scales using a similar concept to feature pyramid networks <ref type="bibr" target="#b7">[8]</ref>. From our base feature extractor we add several convolutiona

d was "set deliberately low to account for inaccuracies in bounding boxes in the ground truth data" <ref type="bibr" target="#b1">[2]</ref>. Does COCO have better labelling than VOC? This is definitel
d was "set deliberately low to account for inaccuracies in bounding boxes in the ground truth data" <ref type="bibr" target="#b1">[2]</ref>. Does COCO have better labelling than VOC? This is definitel



d was "set deliberately low to account for inaccuracies in bounding boxes in the ground truth data" <ref type="bibr" target="#b1">[2]</ref>. Does COCO have better labelling than VOC? This is definitel
d was "set deliberately low to account for inaccuracies in bounding boxes in the ground truth data" <ref type="bibr" target="#b1">[2]</ref>. Does COCO have better labelling than VOC? This is definitel
d was "set deliberately low to account for inaccuracies in bounding boxes in the ground truth data" <ref type="bibr" target="#b1">[2]</ref>. Does COCO have better labelling than VOC? This is definitel
d was "set deliberately low to account for inaccuracies in bounding boxes in the ground truth data" <ref type="bibr" target="#b1">[2]</ref>. Does COCO have better labelling than VOC? This is definitel
bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is sur-prisingly difficult." <ref type="bibr" target="#b18">[18]</ref> If humans have a hard time telling the difference, how muc
d was "set deliberately low to account for inaccuracies in bounding boxes in the ground truth data" <ref type="bibr" target="#b1">[2]</ref>. Does COCO have better labelling than VOC? This is definitel
a 3-d tensor encoding bounding box, objectness, and class predictions. In our experiments with COCO <ref type="bibr" target="#b9">[10]</ref> we predict 3 boxes at each scale so the tensor is N × N × [
bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is sur-prisingly difficult." <ref type="bibr" target="#b18">[18]</ref> If humans have a hard time telling the difference, how muc

d was "set deliberately low to account for inaccuracies in bounding boxes in the ground truth data" <ref type="bibr" target="#b1">[2]</ref>. Does COCO have better labelling than VOC? This is definitel
bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is sur-prisingly difficult." <ref type="bibr" target="#b18">[18]</ref> If humans have a hard time telling the difference, how muc
a 3-d tensor encoding bounding box, objectness, and class predictions. In our experiments with COCO <ref type="bibr" target="#b9">[10]</ref> we predict 3 boxes at each scale so the tensor is N × N × [


d new items. State-of-art reinforcement learning methods usually apply the simple ϵ-greedy strategy <ref type="bibr" target="#b29">[31]</ref> or Upper Confidence Bound (UCB) <ref type="bibr" target="# dynamic nature of news characteristics and user preference, we propose to use Deep Q-Learning (DQN) <ref type="bibr" target="#b29">[31]</ref> framework. This framework can consider current reward and avoid the harm to recommendation accuracy induced by classical exploration strategies like ϵ-greedy <ref type="bibr" target="#b29">[31]</ref> and Upper Confidence Bound <ref type="bibr" target="#b21"> ss stored in the memory to update the network Q.</p><p>Here, we use the experience replay technique <ref type="bibr" target="#b29">[31]</ref> to update the network. Specifically, agent G maintains a m ture of news recommendation and the need to estimate future reward, we apply a Deep Q-Network (DQN) <ref type="bibr" target="#b29">[31]</ref> to model the probability that one user may click on one sp ead><p>The most straightforward strategies to do exploration in reinforcement learning are ϵ-greedy <ref type="bibr" target="#b29">[31]</ref> and UCB <ref type="bibr" target="#b21">[23]</ref>. ϵ-greed
n, but also the potential reward in the future iterations. <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" tar ">50]</ref> due to its explicit modeling of future rewards, and different from previous MDP methods <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" ta
rget="#b4">[6,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" tar n our dataset.(An improved version of the original LinUCB-HLinUCB will also be compared.) • HLinUCB <ref type="bibr" target="#b40">[42]</ref> is another state-of-art bandit-based approach in recommend state-of-art bandit-based approach in recommendation problem. Hidden Linear Upper Confidence Bound <ref type="bibr" target="#b40">[42]</ref> further allows learned hidden feature to model the reward.
tent based methods <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b31">33]</ref>, collaborative filtering based methods <ref type="bibr" tar tent-based methods <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b31">33]</ref> will maintain news term frequency features (e.g., TF-IDF) a tent based methods <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b31">33]</ref>, collaborative filtering based methods <ref type="bibr" tar
the long run.</p><p>Second, current recommendation methods <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" tar future iterations. <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" tar of future rewards, and different from previous MDP methods <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" tar discrete user log to represent state and hence can not be scaled to large systems (MDP-based works <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b34">36]</ref>). In contrast, our
ent learning in recommendation</head><p>2.2.1 Contextual Multi-Armed Bandit models. A group of work <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target target="#b21">[23]</ref>.</p><p>Our method is significantly different from the MAB group of methods <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target
survival models here to estimate the user activeness, other alternatives like Poisson point process <ref type="bibr" target="#b11">[13]</ref> can also be applied and should serve similar function.</p>
b23">25]</ref>. Recently, as an extension and integration of previous methods, deep learning models <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" targ ofile modeling. Recently, as an extension and integration of previous methods, deep learning models <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" targ arget="#b22">24,</ref><ref type="bibr" target="#b23">25]</ref>. Recently, many deep learning models <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" targ use the combination of features and their interactions to do the click prediction.</p><p>• W&amp;D <ref type="bibr" target="#b6">[8]</ref>. Wide &amp; Deep is a widely used state-of-art deep learning
also implicitly considered).</p><p>We use survival models <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref> to model user return and user activeness. Survival analysis o model user return and user activeness. Survival analysis <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref> has been applied in the field of estimating user return tim
g based collaborative filtering <ref type="bibr" target="#b12">[14]</ref>, and matrix factorization <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" targ
pe="bibr" target="#b12">[14]</ref>, and matrix factorization <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" tar
GCN, an algorithm to design the batches based on efficient graph clustering algorithms (e.g., METIS <ref type="bibr" target="#b8">[8]</ref>). We take this idea further by proposing a stochastic multi- p>We use graph clustering algorithms to partition the graph. Graph clustering methods such as Metis <ref type="bibr" target="#b8">[8]</ref> and Graclus <ref type="bibr" target="#b4">[4]</ref> aim to c
SAGE <ref type="bibr" target="#b5">[5]</ref> FastGCN <ref type="bibr" target="#b1">[1]</ref> VR-GCN <ref type="bibr" target="#b2">[2]</ref> Cluster-GCN Time complexity</p><formula xml:id="formula_3">O ="#b1">[1]</ref> proposed an important sampling strategy to improve the gradient estimation. VR-GCN <ref type="bibr" target="#b2">[2]</ref> proposed a strategy to store the previous computed embedding lgorithm): the proposed fast GCN training method. • VRGCN<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b2">[2]</ref>: It maintains the historical embedding of all the nodes in t a few neighbors to speedup training. The number of sampled neighbors is set to be 2 as suggested in <ref type="bibr" target="#b2">[2]</ref> <ref type="foot" target="#foot_3">4</ref> . • GraphSAGE<ref [9]</ref> has difficulty to scale to large graphs, we do not compare with it here. Also as shown in <ref type="bibr" target="#b2">[2]</ref> that VRGCN is faster than FastGCN, so we do not compare with ing speed and memory requirement of GCN in some recent works <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b5">5]</ref>. Instead of computing t <head n="6.2">Implementation details</head><p>Previous works <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref> propose to pre-compute the multiplication of AX in the first
tate-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by <ref type="bibr" target="#b16">[16]</ref>.</p></div> 			</abstract> 		</profileDesc> 	</teiHeader>
sampled sizes for each layer (S 1 = 25, S 2 = 10) in GraphSAGE. We implement our method in PyTorch <ref type="bibr" target="#b13">[13]</ref>. For the other methods, we use all the original papers' co
e graph. Graph clustering methods such as Metis <ref type="bibr" target="#b8">[8]</ref> and Graclus <ref type="bibr" target="#b4">[4]</ref> aim to construct the partitions over the vertices in the gra
tate-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by <ref type="bibr" target="#b16">[16]</ref>.</p></div> 			</abstract> 		</profileDesc> 	</teiHeader>
tate-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by <ref type="bibr" target="#b16">[16]</ref>.</p></div> 			</abstract> 		</profileDesc> 	</teiHeader>
tate-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by <ref type="bibr" target="#b16">[16]</ref>.</p></div> 			</abstract> 		</profileDesc> 	</teiHeader>

In <ref type="bibr" target="#b9">[9]</ref>, they adopt a technique similar to residual connections <ref type="bibr" target="#b6">[6]</ref> to enable the model to carry the information from a previous

for word embedding has been shown to be an implicit factorization of a certain word-context matrix <ref type="bibr" target="#b25">[24]</ref>, and there is recent effort to theoretically explaining th ol(G) log д −x ⊤ i y j .</formula><p>Let us define z i, j = x ⊤ i y j . Following Levy and Goldberg <ref type="bibr" target="#b25">[24]</ref>, where the authors suggested that for a sufficient large e w i−T , • • • , w i−1 , w i+1 , • • • , w i+T .</formula><p>Following the work by Levy and Goldberg <ref type="bibr" target="#b25">[24]</ref>, SGNS is implicitly factorizing</p><formula xml:id="formul x is not only ill-defined (since log 0 = −∞), but also dense. Inspired by the Shifted PPMI approach <ref type="bibr" target="#b25">[24]</ref>, we define M ′ such that M ′ i, j = max(M i, j , 1) (Line </ref>. Recently, there has been effort in understanding this model. For example, Levy and Goldberg <ref type="bibr" target="#b25">[24]</ref> prove that SGNS is actually conducting an implicit matrix target="#b19">[18]</ref> frame word embedding as a metric learning problem. Built upon the work in <ref type="bibr" target="#b25">[24]</ref>, we theoretically analyze popular skip-gram based network

vertex attributes <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b50">49]</ref>, network embedding with high order structure <ref type="bib
ng. It would be necessary to investigate whether and how the development in random-walk polynomials <ref type="bibr" target="#b10">[9]</ref> can support fast approximations of the closed-form matrices
of the Wikipedia dump. The labels are the Part-of-Speech (POS) tags inferred by Stanford POS-Tagger <ref type="bibr" target="#b41">[40]</ref>.</p><p>Flickr <ref type="bibr" target="#b39">[38]</ref> is
some recent progresses <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b16">15]</ref> that try to understand or approximate the 2ndorder random w
tent vector representation for each word in the corpus. Notably, inspired by this setting, DeepWalk <ref type="bibr" target="#b32">[31]</ref> pioneers network embedding by considering the vertex paths ="http://www.tei-c.org/ns/1.0"><head n="2.2">DeepWalk</head><p>In this section, we analyze DeepWalk <ref type="bibr" target="#b32">[31]</ref> and illustrate the essence of DeepWalk is actually perform random walks with other distributions (e.g., the uniform distribution in the original DeepWalk work <ref type="bibr" target="#b32">[31]</ref>)? It turns out that, for a connected, undirected, and non- F (T = 1) and NetMF (T = 10) with LINE (2nd) <ref type="bibr" target="#b38">[37]</ref> and DeepWalk <ref type="bibr" target="#b32">[31]</ref>, which we have introduced in previous sections. For NetMF 128 for all methods.</p><p>Prediction Setting Following the same experimental procedure in DeepWalk <ref type="bibr" target="#b32">[31]</ref>, we randomly sample a portion of labeled vertices for trai Following a few pioneer works such as SocDim <ref type="bibr" target="#b39">[38]</ref> and DeepWalk <ref type="bibr" target="#b32">[31]</ref>, a growing number of literature has tried to address the p h vertex, and then to train a predictive model to perform context prediction. For example, DeepWalk <ref type="bibr" target="#b32">[31]</ref>, node2vec <ref type="bibr" target="#b17">[16]</ref>, and m ef type="bibr" target="#b40">[39]</ref>, we assume that the number of labels for test data is given <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b40">39]</ref>. We repeat the pre
tried to address the problem from various of perspectives, such as heterogeneous network embedding <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" targ
rs. The vertex labels represent interests of the bloggers.</p><p>Protein-Protein Interactions (PPI) <ref type="bibr" target="#b36">[35]</ref> is a subgraph of the PPI network for Homo Sapiens. The lab
case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE <ref type="bibr" target="#b37">[36]</ref> can be viewed as the joint factorization of multiple netwo k embedding models have been developed, such as LINE <ref type="bibr" target="#b38">[37]</ref>, PTE <ref type="bibr" target="#b37">[36]</ref>, and node2vec <ref type="bibr" target="#b17">[16]</ref>.</ rix logarithm, and</p><formula xml:id="formula_12">D = diag(d 1 , • • • , d |V | ).</formula><p>PTE <ref type="bibr" target="#b37">[36]</ref> PTE is an extension of LINE (2nd) in heterogeneous text ne raining wherein edges are sampled from each of three sub-networks alternatively (see Section 4.2 in <ref type="bibr" target="#b37">[36]</ref>).</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head rget="#b9">[8,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b37">36]</ref>, semi-supervised network embedding <ref type="bibr" target=
b48">47,</ref><ref type="bibr" target="#b50">49]</ref>, network embedding with high order structure <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b17">16]</ref>, signed network embe
e proposed method naturally extends our previous work of unsupervised information network embedding <ref type="bibr" target="#b26">[27]</ref> and first learns a low dimensional embedding for words thr e network is embedded into a low dimensional vector space that preserves the second-order proximity <ref type="bibr" target="#b26">[27]</ref> between the vertices in the network. The representation of is suitable for arbitrary types of information networks: undirected or directed, binary or weighted <ref type="bibr" target="#b26">[27]</ref>. The LINE model optimizes an objective function which aims vious work, we introduced the LINE model to learn the embedding of large-scale information networks <ref type="bibr" target="#b26">[27]</ref>. LINE is mainly designed for homogeneous networks, i.e., n l for embedding bipartite networks. The essential idea is to make use of the second-order proximity <ref type="bibr" target="#b26">[27]</ref> between vertices, which assumes vertices with similar neig jective (3) can be optimized with stochastic gradient descent using the techniques of edge sampling <ref type="bibr" target="#b26">[27]</ref> and negative sampling <ref type="bibr" target="#b17">[18]< descent in learning network embeddings. For the detailed optimization process, readers can refer to <ref type="bibr" target="#b26">[27]</ref>.</p><p>The embeddings of the word-word, word-document, and (4) is to merge the all the edges in the three sets Eww, E wd , E wl and then deploy edge sampling <ref type="bibr" target="#b26">[27]</ref>, which samples an edge for model updating in each step, wi 0]</ref>.</p><p>• LINE: the large-scale information network embedding model proposed by Tang et al. <ref type="bibr" target="#b26">[27]</ref>. We use the LINE model to learn unsupervised embeddings wi
fer from the sparsity in short documents, with similar results observed in statistical topic models <ref type="bibr" target="#b25">[26]</ref>. The performance of PVDM is still inferior to that of PVDB
and large-scale unlabeled data.</p><p>Another piece of work similar to predictive word embedding is <ref type="bibr" target="#b14">[15]</ref>, which learns word vectors that are particularly tuned for articles as labeled documents for training; (3) Imdb, a data set for sentiment classification from <ref type="bibr" target="#b14">[15]</ref> <ref type="foot" target="#foot_2">3</ref> . To avoid the d
br" target="#b8">[9]</ref>, IsoMap <ref type="bibr" target="#b27">[28]</ref> and Laplacian eigenmap <ref type="bibr" target="#b0">[1]</ref> are not applicable for embedding large-scale networks that c , we can define the conditional probabilities p(vi|vj), p(vi|dj) and p(vi|lj) according to equation <ref type="bibr" target="#b0">(1)</ref>, and then learn the embeddings by optimizing objective funct

shuffle the training and test data sets; (4) RCV1, a large benchmark corpus for text classification <ref type="bibr" target="#b11">[12]</ref> <ref type="foot" target="#foot_3">4</ref> . Four subsets i
>. Classical graph embedding algorithms such as MDS <ref type="bibr" target="#b8">[9]</ref>, IsoMap <ref type="bibr" target="#b27">[28]</ref> and Laplacian eigenmap <ref type="bibr" target="#b0">[1]</
applications, e.g., node classification <ref type="bibr" target="#b2">[3]</ref> and link prediction <ref type="bibr" target="#b12">[13]</ref>. Classical graph embedding algorithms such as MDS <ref typ
diction <ref type="bibr" target="#b12">[13]</ref>. Classical graph embedding algorithms such as MDS <ref type="bibr" target="#b8">[9]</ref>, IsoMap <ref type="bibr" target="#b27">[28]</ref> and Laplac
ssical text representations such as statistical topic models, e.g., the latent Dirichlet allocation <ref type="bibr" target="#b3">[4]</ref>. To capture the document-level word cooccurrences, we introd
edges. There are some recent work attempting to embed very large realworld networks. Perozzi et al. <ref type="bibr" target="#b19">[20]</ref> proposed a network embedding model called the "DeepWalk,"

ng boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene <ref type="bibr" target="#b12">[13]</ref>. These complex pipelines are slow and hard to optimize bec Then, classifiers <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref> or localizers <ref t
liding window approach where the classifier is run at evenly spaced locations over the entire image <ref type="bibr" target="#b9">[10]</ref>.</p><p>More recent approaches like R-CNN use region proposa mable parts models. Deformable parts models (DPM) use a sliding window approach to object detection <ref type="bibr" target="#b9">[10]</ref>. DPM uses a disjoint pipeline to extract static features, c et="#b34">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref> or localizers <ref type="bibr" target="#b0">[1,</ref><ref ty
asses like faces or people can be highly optimized since they have to deal with much less variation <ref type="bibr" target="#b35">[36]</ref>. YOLO is a general purpose detector that learns to detect
pe="bibr" target="#b2">[3]</ref>. We compare YOLO to other detection systems on the Picasso Dataset <ref type="bibr" target="#b11">[12]</ref> and the People-Art Dataset <ref type="bibr" target="#b2">[
the whole image or on some subset of regions in the image <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref>. We compare the YOL



l short of real-time performance.</p><p>Many research efforts focus on speeding up the DPM pipeline <ref type="bibr" target="#b29">[30]</ref> [37] <ref type="bibr" target="#b4">[5]</ref>. They speed u . They speed up HOG computation, use cascades, and push computation to GPUs. However, only 30Hz DPM <ref type="bibr" target="#b29">[30]</ref> actually runs in real-time.</p><p>Instead of trying to opt ion focus on making standard detection pipelines fast. <ref type="bibr" target="#b4">[5]</ref> [37] <ref type="bibr" target="#b29">[30]</ref> [14] <ref type="bibr" target="#b16">[17]</ref> [27] Howeve et al. actually produce a detection system that runs in real-time (30 frames per second or better) <ref type="bibr" target="#b29">[30]</ref>. We compare YOLO to their GPU implementation of DPM which <div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-Time Detectors</head><p>Train mAP FPS 100Hz DPM <ref type="bibr" target="#b29">[30]</ref> 2007 16.0 100 30Hz DPM <ref type="bibr" target="#b29">[30] s</head><p>Train mAP FPS 100Hz DPM <ref type="bibr" target="#b29">[30]</ref> 2007 16.0 100 30Hz DPM <ref type="bibr" target="#b29">[30]</ref> 2007 R-CNN, it still falls short of real-time and takes a
"#b3">[4]</ref>, convolutional features <ref type="bibr" target="#b5">[6]</ref>). Then, classifiers <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta
eeds up DPM without sacrificing much mAP but it still misses real-time performance by a factor of 2 <ref type="bibr" target="#b36">[37]</ref>. It also is limited by DPM's relatively low accuracy on de
gions in the image <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref>. We compare the YOLO detection system to several top detect
pe="bibr" target="#b2">[3]</ref>. We compare YOLO to other detection systems on the Picasso Dataset <ref type="bibr" target="#b11">[12]</ref> and the People-Art Dataset <ref type="bibr" target="#b2">[
and it's detections are publicly available.</p><p>We use the methodology and tools of Hoiem et al. <ref type="bibr" target="#b18">[19]</ref> For each category at test time we look at the top N predic
search efforts focus on speeding up the DPM pipeline <ref type="bibr" target="#b29">[30]</ref> [37] <ref type="bibr" target="#b4">[5]</ref>. They speed up HOG computation, use cascades, and push compu ead><p>Many research efforts in object detection focus on making standard detection pipelines fast. <ref type="bibr" target="#b4">[5]</ref> [37] <ref type="bibr" target="#b29">[30]</ref> [14] <ref typ
aining</head><p>We pretrain our convolutional layers on the ImageNet 1000-class competition dataset <ref type="bibr" target="#b28">[29]</ref>. For pretraining we use the first 20 convolutional layers
Haar <ref type="bibr" target="#b24">[25]</ref>, SIFT <ref type="bibr" target="#b22">[23]</ref>, HOG <ref type="bibr" target="#b3">[4]</ref>, convolutional features <ref type="bibr" target="#b5">[6]</r
ref type="bibr" target="#b5">[6]</ref>). Then, classifiers <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar
Haar <ref type="bibr" target="#b24">[25]</ref>, SIFT <ref type="bibr" target="#b22">[23]</ref>, HOG <ref type="bibr" target="#b3">[4]</ref>, convolutional features <ref type="bibr" target="#b5">[6]</r
Detection pipelines generally start by extracting a set of robust features from input images (Haar <ref type="bibr" target="#b24">[25]</ref>, SIFT <ref type="bibr" target="#b22">[23]</ref>, HOG <ref
Haar <ref type="bibr" target="#b24">[25]</ref>, SIFT <ref type="bibr" target="#b22">[23]</ref>, HOG <ref type="bibr" target="#b3">[4]</ref>, convolutional features <ref type="bibr" target="#b5">[6]</r
">Rosas et al., 2014)</ref>, which was then used to develop a multimodal deception detection system <ref type="bibr" target="#b1">(Abouelenien et al., 2014)</ref>. An extensive review of approaches fo
isplays and hand movements, as they have been previously found to correlate with deceptive behavior <ref type="bibr" target="#b8">(Depaulo et al., 2003)</ref>. The gesture annotation is performed usin es is motivated by previous research that has suggested that deceivers' speech has lower complexity <ref type="bibr" target="#b8">(Depaulo et al., 2003)</ref>. We use the tool described in <ref type=" or gaze) and nod (Side-Turn-R) more frequently than truth-tellers. This agrees with the findings in <ref type="bibr" target="#b8">(Depaulo et al., 2003)</ref> that liars who are more motivated to get
</ref> considered features such as face orientation and facial expression intensity. Owayjan et al. <ref type="bibr" target="#b31">(Owayjan et al., 2012)</ref> extracted geometricbased features from f
ere analyzed using smoothness and asymmetry measurements to further relate them to an act of deceit <ref type="bibr" target="#b11">(Ekman, 2003)</ref>. <ref type="bibr" target="#b36">Tian et al. (Tian
y tracking the hand movements of the subjects <ref type="bibr" target="#b21">(Lu et al., 2005;</ref><ref type="bibr" target="#b38">Tsechpenakis et al., 2005)</ref>, or using geometric features related
o the hand and head motion <ref type="bibr" target="#b24">(Meservy et al., 2005)</ref>. Caso et al. <ref type="bibr" target="#b5">(Caso et al., 2006)</ref> identified particular hand gestures that can
et al. (2010)</ref> found that fewer iconic hand gestures were a sign of a deceptive narration, and <ref type="bibr" target="#b15">Hillman et al. (2012)</ref> determined that increased speech promptin
12)</ref> indicated that relying solely on physiological measurements can be biased and misleading. <ref type="bibr" target="#b6">Chittaranjan et al. (Chittaranjan and Hung, 2010</ref>) created an aud
12)</ref> indicated that relying solely on physiological measurements can be biased and misleading. <ref type="bibr" target="#b6">Chittaranjan et al. (Chittaranjan and Hung, 2010</ref>) created an aud
12)</ref> indicated that relying solely on physiological measurements can be biased and misleading. <ref type="bibr" target="#b6">Chittaranjan et al. (Chittaranjan and Hung, 2010</ref>) created an aud
ies were integrated in order to find a combination of multimodal features with superior performance <ref type="bibr" target="#b4">(Burgoon et al., 2009;</ref><ref type="bibr" target="#b18">Jensen et a
nd gestures, blob analysis was used to detect deceit by tracking the hand movements of the subjects <ref type="bibr" target="#b21">(Lu et al., 2005;</ref><ref type="bibr" target="#b38">Tsechpenakis et
ng machine learning approaches <ref type="bibr" target="#b27">(Mihalcea and Strapparava, 2009;</ref><ref type="bibr" target="#b3">Ángela Almela et al., 2012)</ref> and showed that the use of psycholin
">Tsechpenakis et al., 2005)</ref>, or using geometric features related to the hand and head motion <ref type="bibr" target="#b24">(Meservy et al., 2005)</ref>. Caso et al. <ref type="bibr" target="#b
e manually transcribed. The transcription was performed by two transcribers using the Elan software <ref type="bibr" target="#b41">(Wittenburg et al., 2006)</ref>. We asked transcribers to include wor movements.</p><p>The multimodal annotation was performed by two annotators using the Elan software <ref type="bibr" target="#b41">(Wittenburg et al., 2006)</ref>. We decided to perform the gesture an
et al. (2010)</ref> found that fewer iconic hand gestures were a sign of a deceptive narration, and <ref type="bibr" target="#b15">Hillman et al. (2012)</ref> determined that increased speech promptin
ents to further relate them to an act of deceit <ref type="bibr" target="#b11">(Ekman, 2003)</ref>. <ref type="bibr" target="#b36">Tian et al. (Tian et al., 2005)</ref> considered features such as fac
y tracking the hand movements of the subjects <ref type="bibr" target="#b21">(Lu et al., 2005;</ref><ref type="bibr" target="#b38">Tsechpenakis et al., 2005)</ref>, or using geometric features related
</ref> considered features such as face orientation and facial expression intensity. Owayjan et al. <ref type="bibr" target="#b31">(Owayjan et al., 2012)</ref> extracted geometricbased features from f
nd gestures, blob analysis was used to detect deceit by tracking the hand movements of the subjects <ref type="bibr" target="#b21">(Lu et al., 2005;</ref><ref type="bibr" target="#b38">Tsechpenakis et
ccurs on a daily basis in different areas of life <ref type="bibr" target="#b25">(Meyer, 2010;</ref><ref type="bibr" target="#b35">Smith et al., 2014)</ref>, the need arises for automated methodologie
been proposed to address the deception detection task using a number of modalities, including text <ref type="bibr" target="#b12">(Feng et al., 2012)</ref> and speech <ref type="bibr" target="#b16">( and also more complex linguistic features derived from syntactic CFG trees and part of speech tags <ref type="bibr" target="#b12">(Feng et al., 2012;</ref><ref type="bibr" target="#b42">Xu and Zhao,
as the training progresses <ref type="bibr" target="#b7">[8]</ref>. Madry, Makelov, Schmidt, et al. <ref type="bibr" target="#b24">[25]</ref> used adversarial training on the cifar dataset, which stil gest known attack for this metric. PGD has been conjectured to be a near-optimal first-order attack <ref type="bibr" target="#b24">[25]</ref>. We use the Fo olBox library for the implementation of the we also wish to address ac o n c e r nr a i s e db yM a d r y ,M a k e l o v ,S c h m i d t ,et al. <ref type="bibr" target="#b24">[25]</ref>, which is the computational cost of a threat model. They a
gradients were accounted for <ref type="bibr" target="#b16">[17]</ref>. Xie, Zhang, Yuille, et al. <ref type="bibr" target="#b17">[18]</ref> claimed 86% accuracy and Guo, Rana, Cissé, et al. <ref typ
sification (e.g., <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>). As far as we are aware, these types of defenses have all
kind of image pre-processing before classification (e.g., <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>). As far as we ar et al. <ref type="bibr" target="#b17">[18]</ref> claimed 86% accuracy and Guo, Rana, Cissé, et al. <ref type="bibr" target="#b12">[13]</ref> 75%, but these were later also reduced to 0% accuracy <ref group at each step. A simple zoom into a random portion of the image is done, similar to prior work <ref type="bibr" target="#b12">[13]</ref>, as well as a content-aware zoom based on seam carving <re
se difficulties could be circumvented making it possible for adversarial examples to be constructed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Still, the i around a randomly selected point in the image. The radius of intensity is randomly selected from U <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">200]</ref></p><formula xml:id="formula_1">, ing lossy JPEG compression to introduce artifacts was introduced by Kurakin, Goodfellow, and Bengio <ref type="bibr" target="#b9">[10]</ref>. Their work looked at how different values of the JPEG comp
sts a rich literature of computer vision transformations to pull from. Athalye, Carlini, and Wagner <ref type="bibr" target="#b3">[4]</ref> has shown that the many attempts to find such a defensive tr account all of the methods by which obfuscated gradients can mislead us into using a broken defense <ref type="bibr" target="#b3">[4]</ref>.</p><p>Overall we provide the following contributions:</p><p ocedure <ref type="bibr" target="#b1">[2]</ref>, or by properly accounting for obfuscated gradients <ref type="bibr" target="#b3">[4]</ref>. Obfuscated gradients occur when the defense has, intentiona which have proposed workarounds to obtain a suitable approximate gradient for the adversary to use <ref type="bibr" target="#b3">[4]</ref>. In this work, we use only techniques which have already bee al. <ref type="bibr" target="#b12">[13]</ref> 75%, but these were later also reduced to 0% accuracy <ref type="bibr" target="#b3">[4]</ref>. Even different approaches with more modest claims were late e and capable of fully defending against the best known adversaries under the whitebox threat model <ref type="bibr" target="#b3">[4]</ref>. Kurakin, Goodfellow, and Bengio <ref type="bibr" target="#b y Xu, Evans, and Qi <ref type="bibr" target="#b26">[27]</ref> and later reduced to 0% effectiveness <ref type="bibr" target="#b3">[4]</ref>. It works by simply reducing the number of bits used to repr t values of ✏  16.H o w e v e r ,i tw a ss u b s e q u e n t l y defeated, having 0% effectiveness <ref type="bibr" target="#b3">[4]</ref>. When using this approach, we randomize it by selecting the tions are differentiable. The solution to this problem was proposed by Athalye, Carlini, and Wagner <ref type="bibr" target="#b3">[4]</ref>, and is called Backward Pass Differentiable Approximation (B


odel is familiar with the transformations we apply at test time. Following Biggio, Fumera, and Roli <ref type="bibr" target="#b33">[34]</ref>, we will now fully state the threat model that we will ope
eduction Reducing bit-resolution of color was originally proposed as a defense by Xu, Evans, and Qi <ref type="bibr" target="#b26">[27]</ref> and later reduced to 0% effectiveness <ref type="bibr" tar ef type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, and non-local mean <ref type="bibr" target="#b26">[27]</ref> as defenses, but all have since been defeated.</p></div> <
odel is familiar with the transformations we apply at test time. Following Biggio, Fumera, and Roli <ref type="bibr" target="#b33">[34]</ref>, we will now fully state the threat model that we will ope
Zeiler and Fergus, 2014)</ref> of the input. The best networks are using more than 150 layers as in <ref type="bibr" target="#b6">(He et al., 2016a;</ref><ref type="bibr" target="#b7">He et al., 2016b n computer vision, in particular <ref type="bibr" target="#b17">(Simonyan and Zisserman, 2015;</ref><ref type="bibr" target="#b6">He et al., 2016a)</ref>.</p><p>This paper is structured as follows. Th ayers <ref type="bibr" target="#b17">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref type="bibr" target="#b6">(He et al., 2016a)</ref>. In the remainder of this paper, we describe serman, 2015)</ref>. We have also investigated the same kind of "ResNet shortcut" connections as in <ref type="bibr" target="#b6">(He et al., 2016a)</ref>, namely identity and 1 × 1 convolutions (see ng even deeper degrades accuracy. Shortcut connections help reduce the degradation. As described in <ref type="bibr" target="#b6">(He et al., 2016a)</ref>, the gain in accuracy due to the the increase onnections between convolutional blocks that allow the gradients to flow more easily in the network <ref type="bibr" target="#b6">(He et al., 2016a)</ref>.</p><p>We evaluate the impact of shortcut con works to temporal convolutions as we think this a milestone for going deeper in NLP. Residual units <ref type="bibr" target="#b6">(He et al., 2016a)</ref>  work and ImageNet is that the latter deals w
rs. The design of our architecture is inspired by recent progress in computer vision, in particular <ref type="bibr" target="#b17">(Simonyan and Zisserman, 2015;</ref><ref type="bibr" target="#b6">He ch deeper networks <ref type="bibr" target="#b12">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type="bibr" target="#b17">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref t erarchical manner. Our architecture can be in fact seen as a temporal adaptation of the VGG network <ref type="bibr" target="#b17">(Simonyan and Zisserman, 2015)</ref>. We have also investigated the s
as in <ref type="bibr">(Zhang et al., 2015)</ref>. We initialize our convolutional layers following <ref type="bibr" target="#b5">(He et al., 2015)</ref>. One epoch took from 24 minutes to 2h45 for de

words as basic units. An important step was the introduction of continuous representations of words <ref type="bibr" target="#b1">(Bengio et al., 2003)</ref>. These word embeddings are now the state-o
words as basic units. An important step was the introduction of continuous representations of words <ref type="bibr" target="#b1">(Bengio et al., 2003)</ref>. These word embeddings are now the state-o
words as basic units. An important step was the introduction of continuous representations of words <ref type="bibr" target="#b1">(Bengio et al., 2003)</ref>. These word embeddings are now the state-o
using many layers of convolutions and pooling to sequentially extract a hierarchical representation <ref type="bibr" target="#b23">(Zeiler and Fergus, 2014)</ref> of the input. The best networks are u

end in computer vision where significant improvements have been reported using much deeper networks <ref type="bibr" target="#b12">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type="bibr" ta

words as basic units. An important step was the introduction of continuous representations of words <ref type="bibr" target="#b1">(Bengio et al., 2003)</ref>. These word embeddings are now the state-o
ig_1">2</ref>) is a sequence of two convolutional layers, each one followed by a temporal BatchNorm <ref type="bibr" target="#b9">(Ioffe and Szegedy, 2015)</ref> layer and an ReLU activation. The kern
certainly LSTMs <ref type="bibr">(Hochreiter and</ref>   <ref type="bibr">meyer et al., 2012;</ref><ref type="bibr" target="#b20">Sutskever et al., 2014)</ref> to name just a few. However, we argue t
At each node, the left and right context are combined using weights which are shared for all nodes <ref type="bibr" target="#b18">(Socher et al., 2011)</ref>. The state of the top node is fed to the
ter vision, handcrafted features were used, for instance "scale-invariant feature transform (SIFT)" <ref type="bibr" target="#b14">(Lowe, 2004)</ref>, followed by some classifier. The fundamental idea
"#b3">Collobert et al., 2011)</ref>. They have been subsequently applied to sentence classification <ref type="bibr" target="#b11">(Kim, 2014;</ref><ref type="bibr" target="#b10">Kalchbrenner et al., hich are projected into a high-dimensional space.</p><p>A rather shallow neural net was proposed in <ref type="bibr" target="#b11">(Kim, 2014)</ref>: one convolutional layer (using multiple widths and


016)</ref> or words up to whole sentences or even paragraphs.</p><p>After a couple of pioneer works <ref type="bibr" target="#b0">(Bengio et al. (2001)</ref>, <ref type="bibr" target="#b2">Collobert a
n one architecture has also been investigated, with the goal to "get the best of both worlds", e.g. <ref type="bibr" target="#b15">(Pinheiro and Collobert, 2014)</ref>. The same idea was recently appl
end in computer vision where significant improvements have been reported using much deeper networks <ref type="bibr" target="#b12">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type="bibr" ta
>1</ref> depicts an example of 3-shot link prediction in KGs.</p><p>To do few-shot link prediction, <ref type="bibr" target="#b22">Xiong et al. (2018)</ref> made the first trial and proposed GMatching ed parameters, it's like "a gradient through a gradient".</p><p>As far as we know, work proposed by <ref type="bibr" target="#b22">Xiong et al. (2018)</ref> is the first research on few-shot learning and Evaluation Metrics</head><p>We use two datasets, NELL-One and Wiki-One which are constructed by <ref type="bibr" target="#b22">Xiong et al. (2018)</ref>. NELL-One and Wiki-One are derived from NEL simple TransE embedding model, denoted as -g -r. The result under the third setting is copied from <ref type="bibr" target="#b22">Xiong et al. (2018)</ref>. It uses the triples from background graph, e transferring relation meta to incomplete triples during prediction.</p><p>Compared with GMatching <ref type="bibr" target="#b22">(Xiong et al., 2018</ref>) which relies on a background knowledge gra e heavily rely on rich training instances <ref type="bibr" target="#b26">(Zhang et al., 2019b;</ref><ref type="bibr" target="#b22">Xiong et al., 2018)</ref>, thus are limited to do few-shot link predi r" target="#b17">Vinyals et al., 2016;</ref><ref type="bibr" target="#b15">Snell et al., 2017;</ref><ref type="bibr" target="#b22">Xiong et al., 2018)</ref>, which tries to learn a matching metric bet f> is a typical method using symmetric twin networks to compute the metric of two inputs. GMatching <ref type="bibr" target="#b22">(Xiong et al., 2018)</ref>, the first trial on one-shot link predicti own in Table <ref type="table" target="#tab_6">4</ref>. The baseline in our experiment is GMatching <ref type="bibr" target="#b22">(Xiong et al., 2018)</ref>, which made the first trial on few-shot li
</ref> and applied to various applications <ref type="bibr" target="#b2">(Bordes et al., 2014;</ref><ref type="bibr" target="#b24">Zhang et al., 2016</ref><ref type="bibr" target="#b25">Zhang et al.,
h embedding methods.</p><p>Traditional embedding models are heavily rely on rich training instances <ref type="bibr" target="#b26">(Zhang et al., 2019b;</ref><ref type="bibr" target="#b22">Xiong et al
rediction, predicting new triples based on existing ones. For link prediction, KG embedding methods <ref type="bibr" target="#b1">(Bordes et al., 2013;</ref><ref type="bibr" target="#b13">Nickel et al mbedding of r as in normal knowledge graph embedding methods. One line of work is started by TransE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> with distance score function. TransH <ref p>where x represents the L2 norm of vector x.</p><p>We design the score function inspired by TransE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> which assumes the head entity embedding h,
thods so far: (1) Metric-based meta-learning <ref type="bibr" target="#b7">(Koch et al., 2015;</ref><ref type="bibr" target="#b17">Vinyals et al., 2016;</ref><ref type="bibr" target="#b15">Snell et al
11)</ref>, trying to mine latent semantics in different ways. There are also some others like ConvE <ref type="bibr" target="#b4">(Dettmers et al., 2018)</ref> using convolutional structure to score t
nvolutional structure to score triples and models using additional information such as entity types <ref type="bibr" target="#b21">(Xie et al., 2016)</ref> and relation paths <ref type="bibr" target="
</ref><ref type="bibr" target="#b3">Carlson et al., 2010)</ref> and applied to various applications <ref type="bibr" target="#b2">(Bordes et al., 2014;</ref><ref type="bibr" target="#b24">Zhang et al.
distance score function. TransH <ref type="bibr" target="#b20">(Wang et al., 2014)</ref> and TransR <ref type="bibr" target="#b10">(Lin et al., 2015b)</ref> are two typical models using different meth
</ref> and applied to various applications <ref type="bibr" target="#b2">(Bordes et al., 2014;</ref><ref type="bibr" target="#b24">Zhang et al., 2016</ref><ref type="bibr" target="#b25">Zhang et al.,
local graph structures which also can be regarded as a metric-based method. (2) Model-based method <ref type="bibr" target="#b14">(Santoro et al., 2016;</ref><ref type="bibr" target="#b12">Munkhdalai
gment every physical register with a Superseded bit and a Pending count. This support is similar to <ref type="bibr" target="#b16">[17]</ref>. The Superseded bit marks whether the instruction that sup techniques.</p><p>The second category includes work related to register recycling. Moudgill et al. <ref type="bibr" target="#b16">[17]</ref> discuss performing early register recycling in out-of-orde er processors that support precise exceptions. However, the implementation of precise exceptions in <ref type="bibr" target="#b16">[17]</ref> relies on either checkpoint/rollback for every replay even
four categories.</p><p>The first category includes work on precise exception handling. Hwu and Patt <ref type="bibr" target="#b6">[7]</ref> use checkpointing to support precise exceptions in out-of-or
ve Multithreading (SM) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar culatively in parallel <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar
f all the in-flight instructions is achieved through the use of two-level register files similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, and a large load/st
arget="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. These two checkpoint-based techniques complement each othe arget="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Value updates by speculative threads are buffered, typical
etter resource utilization. Lozano and Gao <ref type="bibr" target="#b11">[12]</ref>, Martin et al. <ref type="bibr" target="#b14">[15]</ref>, and Lo et al. <ref type="bibr" target="#b10">[11]</ref> u bibr" target="#b11">[12]</ref>, register kill instructions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, or cloned versions of opcodes that implicitly kill registe
y events using the ROB, and precise exception handling using checkpointing. Wallace and Bagherzadeh <ref type="bibr" target="#b21">[22]</ref>, and later Monreal et al. <ref type="bibr" target="#b15">[
esources are load/store queue entries and physical registers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" targe
d>Applications</head><p>We evaluate Cherry using most of the applications of the SPEC CPU2000 suite <ref type="bibr" target="#b4">[5]</ref>. The first column of Table <ref type="table" target="#tab_4"
d>Applications</head><p>We evaluate Cherry using most of the applications of the SPEC CPU2000 suite <ref type="bibr" target="#b4">[5]</ref>. The first column of Table <ref type="table" target="#tab_4"
esources are load/store queue entries and physical registers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" targe
">[28]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b59">[60]</ref>, have made sign o existing dimensions of depth <ref type="bibr" target="#b46">[47]</ref>, width 2 , and cardinality <ref type="bibr" target="#b55">[56]</ref>. We state in Sec. 4.4 that increasing scale is more effect rformance of state-of-the-art CNNs, e.g., ResNet <ref type="bibr" target="#b22">[23]</ref>, ResNeXt <ref type="bibr" target="#b55">[56]</ref>, and DLA <ref type="bibr" target="#b59">[60]</ref>.</p><p> ">[28]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b59">[60]</ref>, achieving stat modern backbone CNNs architectures, e.g., ResNet <ref type="bibr" target="#b22">[23]</ref>, ResNeXt <ref type="bibr" target="#b55">[56]</ref>, and DLA <ref type="bibr" target="#b59">[60]</ref>. Instea odules have been proposed in recent years, including cardinality dimension introduced by Xie et al. <ref type="bibr" target="#b55">[56]</ref>, as well as squeeze and excitation (SE) block presented by nts. As shown in Fig. <ref type="figure">3</ref>, we can easily integrate the cardinality dimension <ref type="bibr" target="#b55">[56]</ref> and the SE block <ref type="bibr" target="#b24">[25]</ref> sion cardinality.</head><p>The dimension cardinality indicates the number of groups within a filter <ref type="bibr" target="#b55">[56]</ref>. This dimension changes filters from single-branch to mult ig. <ref type="figure">3</ref>: The Res2Net module can be integrated with the dimension cardinality <ref type="bibr" target="#b55">[56]</ref> (replace conv with group conv) and SE <ref type="bibr" tar into the state-ofthe-art models, such as ResNet <ref type="bibr" target="#b22">[23]</ref>, ResNeXt <ref type="bibr" target="#b55">[56]</ref>, DLA <ref type="bibr" target="#b59">[60]</ref> and Big-Lit and bLRes2Net-50, respectively.</p><p>The proposed scale dimension is orthogonal to the cardinality <ref type="bibr" target="#b55">[56]</ref> dimension and width <ref type="bibr" target="#b22">[23]</r 4]</ref> dataset, we mainly use the ResNet-50 <ref type="bibr" target="#b22">[23]</ref>, ResNeXt-50 <ref type="bibr" target="#b55">[56]</ref>, DLA-60 <ref type="bibr" target="#b59">[60]</ref>, and bLR ments on the CIFAR <ref type="bibr" target="#b26">[27]</ref> dataset, we use the ResNeXt-29, 8c×64w <ref type="bibr" target="#b55">[56]</ref> as our baseline model. Empirical evaluations and discussio ons, we use the Pytorch implementation of ResNet <ref type="bibr" target="#b22">[23]</ref>, ResNeXt <ref type="bibr" target="#b55">[56]</ref>, DLA <ref type="bibr" target="#b59">[60]</ref> as well as type="bibr" target="#b22">[23]</ref>. On the CIFAR dataset, we use the implementation of ResNeXt-29 <ref type="bibr" target="#b55">[56]</ref>. For all tasks, we use the original implementations of bas ement of 0.73% in terms of top-1 error over the  <ref type="bibr" target="#b22">[23]</ref>, ResNeXt <ref type="bibr" target="#b55">[56]</ref>, SE-Net <ref type="bibr" target="#b24">[25]</ref>, bLResNe ve been shown to have stronger representation capability <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b55">[56]</ref> for vision tasks. To validate our model with greater depth which contains 50k training images and 10k testing images from 100 classes. The ResNeXt-29, 8c×64w <ref type="bibr" target="#b55">[56]</ref> is used as the baseline model. We only replace the origina iv xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Scale Variation</head><p>Similar to Xie et al. <ref type="bibr" target="#b55">[56]</ref>, we evaluate the test performance of the baseline model by ng different CNN dimensions, including scale (Equation ( <ref type="formula">1</ref>)), cardinality <ref type="bibr" target="#b55">[56]</ref>, and depth <ref type="bibr" target="#b46">[47]</ref>. Whil fix all other dimensions. A series of networks are trained and evaluated under these changes. Since <ref type="bibr" target="#b55">[56]</ref> has already shown that increasing cardinality is more effe
black blob). <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bib bibr" target="#b55">[56]</ref>, as well as squeeze and excitation (SE) block presented by Hu et al. <ref type="bibr" target="#b24">[25]</ref>. The proposed Res2Net module introduces the scale dimensio sily integrate the cardinality dimension <ref type="bibr" target="#b55">[56]</ref> and the SE block <ref type="bibr" target="#b24">[25]</ref> with the proposed Res2Net module.</p></div> <div xmlns="ht mension cardinality <ref type="bibr" target="#b55">[56]</ref> (replace conv with group conv) and SE <ref type="bibr" target="#b24">[25]</ref> blocks.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0" calibrates channel-wise feature responses by explicitly modelling inter-dependencies among channels <ref type="bibr" target="#b24">[25]</ref>. Similar to <ref type="bibr" target="#b24">[25]</ref>, we y modelling inter-dependencies among channels <ref type="bibr" target="#b24">[25]</ref>. Similar to <ref type="bibr" target="#b24">[25]</ref>, we add the SE block right before the residual connections ref type="bibr" target="#b22">[23]</ref>, ResNeXt <ref type="bibr" target="#b55">[56]</ref>, SE-Net <ref type="bibr" target="#b24">[25]</ref>, bLResNet <ref type="bibr" target="#b4">[5]</ref>, and DLA
#b12">[13]</ref> or multiscale region features <ref type="bibr" target="#b52">[53]</ref>. Li et al. <ref type="bibr" target="#b28">[29]</ref> propose one of the earliest methods that enables multi-sca ef type="bibr" target="#b57">[58]</ref>, PASCAL-S <ref type="bibr" target="#b29">[30]</ref>, HKU-IS <ref type="bibr" target="#b28">[29]</ref>, and DUT-OMRON <ref type="bibr" target="#b58">[59]</ref> d
2Net potentially valuable for object region mining in weakly supervised semantic segmentation tasks <ref type="bibr" target="#b53">[54]</ref>. </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
gn good features for multi-scale stimuli for visual cognition tasks, including image classification <ref type="bibr" target="#b27">[28]</ref>, object detection <ref type="bibr" target="#b42">[43]</ref ">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bib es in numerous vision tasks with state-of-theart performance. Earlier architectures such as AlexNet <ref type="bibr" target="#b27">[28]</ref> and VGGNet <ref type="bibr" target="#b46">[47]</ref> stack ">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bib re representation ability since the input information follows a fine-to-coarse fashion. The AlexNet <ref type="bibr" target="#b27">[28]</ref> stacks filters sequentially and achieves significant perfo
2Net potentially valuable for object region mining in weakly supervised semantic segmentation tasks <ref type="bibr" target="#b53">[54]</ref>. </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
InceptionNets <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>), residual modules (e.g., ResNet <ref type="bibr" target=" to its limited parameter efficiency. The Inception Nets <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b51">[52]</ref> stack more filters in each path of the parallel paths in t ed image. We use the same data argumentation strategy as <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Similar to <ref type="bibr" target="#b22">[23]</ref>, we
lity was subsequently improved by using conv layers with different kernel size (e.g., InceptionNets <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bib imited by the computational constraints due to its limited parameter efficiency. The Inception Nets <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b51">[52]</ref> stack more filt
ef type="bibr" target="#b38">[39]</ref> and deep learning <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Obtaining multi-scale representations in vision tasks req ">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bib v layers with different kernel size (e.g., InceptionNets <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>), residual modu ">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bib . The 1 × 1 convolution introduced in NIN has been a popular module to fuse features. The GoogLeNet <ref type="bibr" target="#b50">[51]</ref> utilizes parallel filters with different kernel sizes to e
">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bib ules (e.g., ResNet <ref type="bibr" target="#b22">[23]</ref>), shortcut connections (e.g., DenseNet <ref type="bibr" target="#b25">[26]</ref>), and hierarchical layer aggregation (e.g., DLA <ref type= bone networks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bib in a large number of equivalent feature scales. Similarly, densely connected layers in the DenseNet <ref type="bibr" target="#b25">[26]</ref> enable the network to process objects in a very wide range ated with deeper models to achieve better performance. We also compare our method with the DenseNet <ref type="bibr" target="#b25">[26]</ref>. Compared with the DenseNet-161, the best performing model
[46]</ref>, semantic segmentation <ref type="bibr" target="#b5">[6]</ref>, salient object detection <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b23">[24]</ref>, object proposal
s CNN models to process features at various scales for effective semantic segmentation. Long et al. <ref type="bibr" target="#b37">[38]</ref> propose one of the earliest methods that enables multi-sca
="bibr" target="#b5">[6]</ref>, salient object detection <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b64">[65]</ref>, and skeleton detection <ref type="bibr" target="#b66">[67
size in this work since it requires more meticulous designs such as depthwise separable convolution <ref type="bibr" target="#b39">[40]</ref>, model pruning <ref type="bibr" target="#b18">[19]</ref>,
">Object Detection</head><p>For object detection task, we validate the Res2Net on the PAS-CAL VOC07 <ref type="bibr" target="#b16">[17]</ref> and MS COCO <ref type="bibr" target="#b32">[33]</ref> data
concurrent works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref> that improve the multi-scale ability by utilizing features ti-scale features <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Big-Little Net s the standard convolution into two resolutions to process features at different frequencies. MSNet <ref type="bibr" target="#b10">[11]</ref> utilizes a high-resolution network to learn high-frequency mmon operation in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bib
pe="bibr" target="#b29">[30]</ref>, HKU-IS <ref type="bibr" target="#b28">[29]</ref>, and DUT-OMRON <ref type="bibr" target="#b58">[59]</ref> datasets. The F-measure and Mean Absolute Error (MAE) are
="#b19">[20]</ref> which contains 10582 training images and 1449 val images. We use the Deeplab v3+ <ref type="bibr" target="#b7">[8]</ref> as our segmentation method. All implementations remain the s et="#b7">[8]</ref> as our segmentation method. All implementations remain the same with Deeplab v3+ <ref type="bibr" target="#b7">[8]</ref> except that the backbone network is replaced with ResNet and
[46]</ref>, semantic segmentation <ref type="bibr" target="#b5">[6]</ref>, salient object detection <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b23">[24]</ref>, object proposal
="#b42">[43]</ref>, attention prediction <ref type="bibr" target="#b44">[45]</ref>, target tracking <ref type="bibr" target="#b62">[63]</ref>, action recognition <ref type="bibr" target="#b45">[46]</r
concurrent works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref> that improve the multi-scale ability by utilizing features ti-scale features <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Big-Little Net s the standard convolution into two resolutions to process features at different frequencies. MSNet <ref type="bibr" target="#b10">[11]</ref> utilizes a high-resolution network to learn high-frequency mmon operation in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bib
">[47]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b59">[60]</ref>, have made significant advances in numerous vision tasks w DenseNet <ref type="bibr" target="#b25">[26]</ref>), and hierarchical layer aggregation (e.g., DLA <ref type="bibr" target="#b59">[60]</ref>). The advances in backbone CNN architectures have demonstr ef type="bibr" target="#b22">[23]</ref>, ResNeXt <ref type="bibr" target="#b55">[56]</ref>, and DLA <ref type="bibr" target="#b59">[60]</ref>.</p><p>1. Convolutional operators and filters are used int ">[47]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b59">[60]</ref>, achieving state-of-theart performance in various vision t -usage ability of ResNet and the feature exploration ability of DenseNet. The recently proposed DLA <ref type="bibr" target="#b59">[60]</ref> method combines layers in a tree structure. The hierarchic ef type="bibr" target="#b22">[23]</ref>, ResNeXt <ref type="bibr" target="#b55">[56]</ref>, and DLA <ref type="bibr" target="#b59">[60]</ref>. Instead of extracting features using a group of 3×3 filte t <ref type="bibr" target="#b22">[23]</ref>, ResNeXt <ref type="bibr" target="#b55">[56]</ref>, DLA <ref type="bibr" target="#b59">[60]</ref> and Big-Little Net <ref type="bibr" target="#b4">[5]</ref> type="bibr" target="#b22">[23]</ref>, ResNeXt-50 <ref type="bibr" target="#b55">[56]</ref>, DLA-60 <ref type="bibr" target="#b59">[60]</ref>, and bLResNet-50 <ref type="bibr" target="#b4">[5]</ref> a t <ref type="bibr" target="#b22">[23]</ref>, ResNeXt <ref type="bibr" target="#b55">[56]</ref>, DLA <ref type="bibr" target="#b59">[60]</ref> as well as bLResNet-50 <ref type="bibr" target="#b4">[5]</ ref type="bibr" target="#b24">[25]</ref>, bLResNet <ref type="bibr" target="#b4">[5]</ref>, and DLA <ref type="bibr" target="#b59">[60]</ref> are the state-of-the-art CNN models. Compared with these s
s have been achieved dramatic improvement in SR. Dong et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> first exploit a three-layer convolutional neural network, nam posed method with other SR methods, including bicubic, SRCNN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, VDSR <ref type="bibr" target="#b11">[12]</ref>, DRC-N <ref t
struction performance. <ref type="bibr">Kim et al.</ref> propose a 20-layer CNN model known as VDSR <ref type="bibr" target="#b11">[12]</ref>, which adopts residual learning and adaptive gradient clip eover, the traditional convolutional networks usually adopt cascaded network topologies, e.g., VDSR <ref type="bibr" target="#b11">[12]</ref> and DRC-N <ref type="bibr" target="#b12">[13]</ref>. In th cise structure of the proposed IDN, it is much faster than several CNN-based SR methods, e.g., VDSR <ref type="bibr" target="#b11">[12]</ref>, DRCN <ref type="bibr" target="#b12">[13]</ref>, LapSRN <r o accelerate SRCNN in combination with smaller filter sizes and more convolution layers. Kim et al. <ref type="bibr" target="#b11">[12]</ref> propose a very deep CNN model with global residual archite nerate the residual image. The bias term of this transposed convolution can auto-Dataset Scale VDSR <ref type="bibr" target="#b11">[12]</ref> DRCN <ref type="bibr" target="#b12">[13]</ref> LapSRN <ref bicubic, SRCNN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, VDSR <ref type="bibr" target="#b11">[12]</ref>, DRC-N <ref type="bibr" target="#b12">[13]</ref>, LapSRN < v> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training datasets</head><p>By following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" ta
ided into ml sub × ml sub size sub-images. As the proposed model is trained using the Caffe package <ref type="bibr" target="#b10">[11]</ref>, its transposed convolution filters will generate the outp
culty, <ref type="bibr">Mao et al.</ref> propose a very deep residual encoder-decoder network (RED) <ref type="bibr" target="#b16">[17]</ref>, which consists of a series of convolutional and subsequen skip connection to avoid introducing additional parameters when the depth is increasing. Mao et al. <ref type="bibr" target="#b16">[17]</ref> tackle the general image restoration problem with encoderd
e ground truths. <ref type="bibr">Tai et al.</ref> propose a deep recursive residual network (DRRN) <ref type="bibr" target="#b21">[22]</ref>, which employs parameters sharing strategy to alleviate th N <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet <ref type="bibr" target="#b22">[23]</ref> as il l LR images as input and progressively reconstructs the sub-band residuals of HR images. Tai et al. <ref type="bibr" target="#b21">[22]</ref> propose the deep recursive residual network to effectively ey Segmentation Dataset (BSD) <ref type="bibr" target="#b17">[18]</ref> as the training data. As in <ref type="bibr" target="#b21">[22]</ref>, to make full use of the training data, we apply data augm RCN <ref type="bibr" target="#b12">[13]</ref> LapSRN <ref type="bibr" target="#b14">[15]</ref> DRRN <ref type="bibr" target="#b21">[22]</ref> MemNet <ref type="bibr" target="#b22">[23]</ref> IDN  mati N <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet <ref type="bibr" target="#b22">[23]</ref>. Tabl d IFC. Meanwhile the inference time substantially exceeds the state-of-the-art methods such as DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet <ref type="bibr" target="#b22">[23]</ref>. This ad><p>By following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, we use 91 images f
parameters of network, we use the grouped convolution layer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref> in the second and fourth layers in each enhancement unit wi
="#b1">[2]</ref>, random forest <ref type="bibr" target="#b19">[20]</ref> and sparse representation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. While these appro
R <ref type="bibr" target="#b11">[12]</ref>, DRCN <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet l image restoration problem with encoderdecoder networks and symmetric skip connections. Lai et al. <ref type="bibr" target="#b14">[15]</ref> propose the laplacian pyramid super-resolution network (La DSR <ref type="bibr" target="#b11">[12]</ref> DRCN <ref type="bibr" target="#b12">[13]</ref> LapSRN <ref type="bibr" target="#b14">[15]</ref> DRRN <ref type="bibr" target="#b21">[22]</ref> MemNet <ref <ref type="bibr" target="#b11">[12]</ref>, DRC-N <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet 0"><head n="4.1.1">Training datasets</head><p>By following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar
image and thus fails to generate satisfactory prediction for images of other classes. Huang et al. <ref type="bibr" target="#b9">[10]</ref> extend self-similarity based SR to handle the affine and pe ef type="bibr" target="#b26">[27]</ref>, BSD100 <ref type="bibr" target="#b17">[18]</ref>, Urban100 <ref type="bibr" target="#b9">[10]</ref>. Among these datasets, Set5, Set14 and BSD100 consist of na
LR/HR patches, such as nearest neighbor <ref type="bibr" target="#b6">[7]</ref>, manifold embedding <ref type="bibr" target="#b1">[2]</ref>, random forest <ref type="bibr" target="#b19">[20]</ref> and
4, 16 and 4 respectively. To reduce the parameters of network, we use the grouped convolution layer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref> in the second and fo
ref type="bibr" target="#b14">[15]</ref>, DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet <ref type="bibr" target="#b22">[23]</ref> as illustrated in Figure <ref type="figure" target="#fig_0 st the accuracy. The authors also present a very deep end-to-end persistent memory network (MemNet) <ref type="bibr" target="#b22">[23]</ref> for image restoration task, which tackles the long-term de SRN <ref type="bibr" target="#b14">[15]</ref> DRRN <ref type="bibr" target="#b21">[22]</ref> MemNet <ref type="bibr" target="#b22">[23]</ref> IDN  matically adjust the central value of the residual im ref type="bibr" target="#b14">[15]</ref>, DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet <ref type="bibr" target="#b22">[23]</ref>. Table <ref type="table" target="#tab_1">2</ref> shows the get="#tab_2">3</ref> shows the proposed method achieves the best performance and outperforms MemNet <ref type="bibr" target="#b22">[23]</ref> by a considerable margin. Figure <ref type="figure" target mark datasets. It is noteworthy that the proposed IDN is approximately 500 times faster than MemNet <ref type="bibr" target="#b22">[23]</ref> with 2× magnification on the Urban100 dataset.</p></div> < eeds the state-of-the-art methods such as DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet <ref type="bibr" target="#b22">[23]</ref>. This compact network will be more widely applicable in pr et="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, we use 91 images from Yang et al. <ref type="bibr" target=
kled with deep learning, neural networks have been achieved dramatic improvement in SR. Dong et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> first exploit a three- of-the-arts</head><p>We compare the proposed method with other SR methods, including bicubic, SRCNN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, VDSR <ref type="bibr"
4, 16 and 4 respectively. To reduce the parameters of network, we use the grouped convolution layer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref> in the second and fo
aluated on four widely used benchmark datasets: Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b26">[27]</ref>, BSD100 <ref type="bibr" target="#b17">[18]</ref>, Urban10
control the model parameters, the authors construct a deeply-recursive convolutional network (DRCN) <ref type="bibr" target="#b12">[13]</ref> by adopting recursive layer. To mitigate training difficul y adopt cascaded network topologies, e.g., VDSR <ref type="bibr" target="#b11">[12]</ref> and DRC-N <ref type="bibr" target="#b12">[13]</ref>. In this way, the feature maps of each layer are sent to t aster than several CNN-based SR methods, e.g., VDSR <ref type="bibr" target="#b11">[12]</ref>, DRCN <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, DRRN <r ch utilizes contextual information over large image regions. Another network designed by Kim et al. <ref type="bibr" target="#b12">[13]</ref>, which has recursive convolution with skip connection to a s transposed convolution can auto-Dataset Scale VDSR <ref type="bibr" target="#b11">[12]</ref> DRCN <ref type="bibr" target="#b12">[13]</ref> LapSRN <ref type="bibr" target="#b14">[15]</ref> DRRN <ref </ref><ref type="bibr" target="#b3">4]</ref>, VDSR <ref type="bibr" target="#b11">[12]</ref>, DRC-N <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, DRRN <r
image and thus fails to generate satisfactory prediction for images of other classes. Huang et al. <ref type="bibr" target="#b9">[10]</ref> extend self-similarity based SR to handle the affine and pe ef type="bibr" target="#b26">[27]</ref>, BSD100 <ref type="bibr" target="#b17">[18]</ref>, Urban100 <ref type="bibr" target="#b9">[10]</ref>. Among these datasets, Set5, Set14 and BSD100 consist of na
R <ref type="bibr" target="#b11">[12]</ref>, DRCN <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet l image restoration problem with encoderdecoder networks and symmetric skip connections. Lai et al. <ref type="bibr" target="#b14">[15]</ref> propose the laplacian pyramid super-resolution network (La DSR <ref type="bibr" target="#b11">[12]</ref> DRCN <ref type="bibr" target="#b12">[13]</ref> LapSRN <ref type="bibr" target="#b14">[15]</ref> DRRN <ref type="bibr" target="#b21">[22]</ref> MemNet <ref <ref type="bibr" target="#b11">[12]</ref>, DRC-N <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet 0"><head n="4.1.1">Training datasets</head><p>By following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar
target="#b7">[8]</ref> and the biases are set to zero. The proposed network is optimized using Adam <ref type="bibr" target="#b13">[14]</ref>. We set the parameters of mini-batch size and weight decay
w to learn a compact dictionary or manifold space to relate LR/HR patches, such as nearest neighbor <ref type="bibr" target="#b6">[7]</ref>, manifold embedding <ref type="bibr" target="#b1">[2]</ref>,
LR/HR patches, such as nearest neighbor <ref type="bibr" target="#b6">[7]</ref>, manifold embedding <ref type="bibr" target="#b1">[2]</ref>, random forest <ref type="bibr" target="#b19">[20]</ref> and
e negative scope of LReLU is set as 0.05. We initialize the weights by using the method proposed in <ref type="bibr" target="#b7">[8]</ref> and the biases are set to zero. The proposed network is opti
in that represents the DUT. The Markov Chain is then used to generate test-cases for the design. In <ref type="bibr" target="#b10">[11]</ref>, the coverage analysis results trigger a set of generation
the coverage analysis to automatically modify the directives to the test generator. For example, in <ref type="bibr" target="#b1">[2]</ref>, a genetic algorithm is used to select and modify test-cases
b1">[2]</ref>, a genetic algorithm is used to select and modify test-cases to increase coverage. In <ref type="bibr" target="#b12">[13]</ref>, coverage analysis data is used to modify the parameters o
o encode the uncertain knowledge of an expert and can be dated back to the geneticist Sewall Wright <ref type="bibr" target="#b14">[15]</ref>. Their initial development in the late 1970s was motivated
b1">[2]</ref>, a genetic algorithm is used to select and modify test-cases to increase coverage. In <ref type="bibr" target="#b12">[13]</ref>, coverage analysis data is used to modify the parameters o
ed via different directions, the chances to discover hidden bugs related to this task are increased <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the past, two general approaches for CDG have been
to take on a role from the complexity stand point, by breaking the barges cliques in the graph (see <ref type="bibr" target="#b3">[4]</ref>).</p><p>After the Bayesian network structure is specified, i
ed via different directions, the chances to discover hidden bugs related to this task are increased <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the past, two general approaches for CDG have been
ed via different directions, the chances to discover hidden bugs related to this task are increased <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the past, two general approaches for CDG have been
based schemes. Bayesian networks also play a crucial role in diagnosis and decision support systems <ref type="bibr" target="#b9">[10]</ref>.</p><p>Obviously, there's a computational problem in dealin
amily of coverage events that share common properties are grouped together to form a coverage model <ref type="bibr" target="#b6">[7]</ref>. Members of the coverage model are called coverage tasks and l are called coverage tasks and are considered part of the test plan. Cross-product coverage models <ref type="bibr" target="#b6">[7]</ref> are of special interest. These models are defined by a basic
ns of Bayesian networks: Dynamic Bayesian networks and influence diagrams. The first extension (see <ref type="bibr" target="#b5">[6]</ref>) enables the incorporation of time, thus modeling temporal d ions and coverage tasks and among the instructions using a two-slice Dynamic Bayesian Network (DBN) <ref type="bibr" target="#b5">[6]</ref>. Rather than an accurate mapping of the specific state machi
e test generators and to better hit areas or specific tasks in the design that are not covered well <ref type="bibr" target="#b4">[5]</ref>.</p><p>The analysis of coverage reports, and their translati
ns of Bayesian networks: Dynamic Bayesian networks and influence diagrams. The first extension (see <ref type="bibr" target="#b5">[6]</ref>) enables the incorporation of time, thus modeling temporal d ions and coverage tasks and among the instructions using a two-slice Dynamic Bayesian Network (DBN) <ref type="bibr" target="#b5">[6]</ref>. Rather than an accurate mapping of the specific state machi
is used to generate test directives designed to accurately hit the coverage tasks. For example, in <ref type="bibr" target="#b13">[14]</ref> an FSM model of pipelines is used to generate tests that c
amily of coverage events that share common properties are grouped together to form a coverage model <ref type="bibr" target="#b6">[7]</ref>. Members of the coverage model are called coverage tasks and l are called coverage tasks and are considered part of the test plan. Cross-product coverage models <ref type="bibr" target="#b6">[7]</ref> are of special interest. These models are defined by a basic
to take on a role from the complexity stand point, by breaking the barges cliques in the graph (see <ref type="bibr" target="#b3">[4]</ref>).</p><p>After the Bayesian network structure is specified, i
is used to generate test directives designed to accurately hit the coverage tasks. For example, in <ref type="bibr" target="#b13">[14]</ref> an FSM model of pipelines is used to generate tests that c
b1">[2]</ref>, a genetic algorithm is used to select and modify test-cases to increase coverage. In <ref type="bibr" target="#b12">[13]</ref>, coverage analysis data is used to modify the parameters o
amily of coverage events that share common properties are grouped together to form a coverage model <ref type="bibr" target="#b6">[7]</ref>. Members of the coverage model are called coverage tasks and l are called coverage tasks and are considered part of the test plan. Cross-product coverage models <ref type="bibr" target="#b6">[7]</ref> are of special interest. These models are defined by a basic
is used to generate test directives designed to accurately hit the coverage tasks. For example, in <ref type="bibr" target="#b13">[14]</ref> an FSM model of pipelines is used to generate tests that c
e test generators and to better hit areas or specific tasks in the design that are not covered well <ref type="bibr" target="#b4">[5]</ref>.</p><p>The analysis of coverage reports, and their translati
parameters using 600⇥600 images for training. In this paper, we focus on the ResNet-50 architecture <ref type="bibr" target="#b10">[11]</ref> due to its good accuracy/cost tradeoff (25.6M parameters) -of-the-art neural network architectures with no modifications, We consider in particular ResNet-50 <ref type="bibr" target="#b10">[11]</ref>. For larger experiments, we use PNASNet-5-Large <ref type= although this means that sev- eral forward passes are required to classify one image. For example, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" ta Another performanceboosting strategy is to classify an image by feeding it at multiple resolutions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" ta
image understanding. Models pre-trained for image classification, usually on the ImageNet database <ref type="bibr" target="#b7">[8]</ref>, transfer to a variety of other applications <ref type="bibr
ibr" target="#b26">[27]</ref>, inpainting <ref type="bibr" target="#b36">[37]</ref>, style transfer <ref type="bibr" target="#b8">[9]</ref> and even image compression <ref type="bibr" target="#b27">[2
f>. Furthermore, advances in image classification translate to improved results on many other tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Recent resea
r" target="#b16">[17]</ref>, object detection <ref type="bibr" target="#b26">[27]</ref>, inpainting <ref type="bibr" target="#b36">[37]</ref>, style transfer <ref type="bibr" target="#b8">[9]</ref> an
#b36">[37]</ref>, style transfer <ref type="bibr" target="#b8">[9]</ref> and even image compression <ref type="bibr" target="#b27">[28]</ref>. In order to obtain the best possible performance from the
xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks <ref type="bibr" target="#b17">[18]</ref> (CNNs) are used extensively in computer vision tasks such
f (25.6M parameters) and its popularity. We also conduct some experiments using the PNASNet-5-Large <ref type="bibr" target="#b20">[21]</ref> architecture (86.1M parameters), which exhibits good perfo ResNet-50 <ref type="bibr" target="#b10">[11]</ref>. For larger experiments, we use PNASNet-5-Large <ref type="bibr" target="#b20">[21]</ref>, learned using neural architecture search as a succession
ypical transformations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref> include: random-size crop, horizontal flip and color jitter mage. For example, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref> used ten crops (one central, and one for each corner of the ltiple resolutions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>, again averaging the predictions. More recently, multi-scal
#b36">[37]</ref>, style transfer <ref type="bibr" target="#b8">[9]</ref> and even image compression <ref type="bibr" target="#b27">[28]</ref>. In order to obtain the best possible performance from the
f>. Furthermore, advances in image classification translate to improved results on many other tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Recent resea
tly. On one hand, prior non-uniform cache access (NUCA) work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target= NUCA by adaptively placing data close to the requesting core <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target= istance. However, these best-effort techniques often result in hotspots and additional interference <ref type="bibr" target="#b2">[3]</ref>. On the other hand, prior work has proposed a variety of par at D-NUCA often causes significant bank contention and uneven distribution of accesses across banks <ref type="bibr" target="#b2">[3]</ref>. We also see this effect in Sec. VI -R-NUCA has the highest
only provide few partitions and often degrade performance, D-NUCA schemes seldom use them. ASP-NUCA <ref type="bibr" target="#b11">[12]</ref>, ESP-NUCA <ref type="bibr" target="#b30">[31]</ref>, and E tency of private caches. These schemes often size partitions using hill-climbing (e.g., shadow tags <ref type="bibr" target="#b11">[12]</ref> or LRU way hit counters <ref type="bibr" target="#b15">[16
get="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref> has proposed a variety of placement, migration, and replica get="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref>. They involve a combination of placement, migration, and re
istical properties of skew-associative caches <ref type="bibr" target="#b38">[39]</ref> and zcaches <ref type="bibr" target="#b35">[36]</ref> to implement partitioning efficiently. Vantage supports hu d-private and per-process shares. In all configurations, banks use 4-way 52-candidate zcache arrays <ref type="bibr" target="#b35">[36]</ref> with H 3 hash functions, though results are similar with m
Virtual Hierarchies rely on a logical two-level directory to partition a cache at bank granularity <ref type="bibr" target="#b28">[29]</ref>, but this comes at the cost of doubling directory overhead
caches. On the hardware side, we leverage recent prior work on efficient fine-grained partitioning <ref type="bibr" target="#b36">[37]</ref> to structure the last-level cache as a collection of distr by up to 2.2× (18.4% gmean) over a shared LRU LLC, up to 35% (9.4% gmean) over Vantage partitioning <ref type="bibr" target="#b36">[37]</ref>, up to 2.05× (11.4% gmean) over R-NUCA <ref type="bibr" ta e no guarantees and often require many more ways than partitions to work well. In contrast, Vantage <ref type="bibr" target="#b36">[37]</ref> leverages the statistical properties of skew-associative c erns the bank partitioning technique used (e.g., in Vantage, this requires changing a few registers <ref type="bibr" target="#b36">[37]</ref>), and is transparent to Jigsaw. Second, the share descript small but statistically significant number of sets. UMONs can also be used with other cache designs <ref type="bibr" target="#b36">[37]</ref> by sampling a fraction of cache accesses at the UMON. Give m sizes. Although UCP was designed to work with way-partitioning, it can be used with other schemes <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref>. Instead of captur eir cache behavior: insensitive (n), cache-friendly (f), cache-fitting (t), and streaming (s) as in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr">Table 2]</ref>, and build random mixes of miss curves. Prior partitioning schemes use per-core UMONs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>, but this is  <ref type="bibr" target="#b33">[34]</ref>, th s is sufficient to partition small caches as in prior work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>, partitioning a large, multi-banked cache among many shares s, we use a similar methodology to prior partitioning work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>. We classify all 29 SPEC CPU2006 workloads into four types
e a fixed-work methodology and equalize sample lengths to avoid sample imbalance, similar to FIESTA <ref type="bibr" target="#b16">[17]</ref>: First, we run each application in isolation, and measure
get="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref> has proposed a variety of placement, migration, and replica get="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref>. They involve a combination of placement, migration, and re
rget="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>, but these schemes used to constrain the pages of a process to specific sets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b41">42]</ref>. While software-only, these schemes are incompatible with s
Virtual Hierarchies rely on a logical two-level directory to partition a cache at bank granularity <ref type="bibr" target="#b28">[29]</ref>, but this comes at the cost of doubling directory overhead
prior work has proposed a variety of partitioning techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar , virtual memory and page coloring can be used to constrain the pages of a process to specific sets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b41">42]</ref>. While software-on zes to maximize some metric, such as throughput <ref type="bibr" target="#b33">[34]</ref>, fairness <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b40">41]</ref>, or QoS <ref type=
head first finds all POIs in O(S) for each partition. This is inspired by the three coins algorithm <ref type="bibr" target="#b29">[30]</ref>. For example, we construct the convex hull ADHI in Fig. <r
d STB. Finally, increasing the number of banks would degrade NoC performance and increase overheads <ref type="bibr" target="#b25">[26]</ref>.</p><p>Page remapping: To classify pages dynamically (Sec.
caches. On the hardware side, we leverage recent prior work on efficient fine-grained partitioning <ref type="bibr" target="#b36">[37]</ref> to structure the last-level cache as a collection of distr by up to 2.2× (18.4% gmean) over a shared LRU LLC, up to 35% (9.4% gmean) over Vantage partitioning <ref type="bibr" target="#b36">[37]</ref>, up to 2.05× (11.4% gmean) over R-NUCA <ref type="bibr" ta e no guarantees and often require many more ways than partitions to work well. In contrast, Vantage <ref type="bibr" target="#b36">[37]</ref> leverages the statistical properties of skew-associative c erns the bank partitioning technique used (e.g., in Vantage, this requires changing a few registers <ref type="bibr" target="#b36">[37]</ref>), and is transparent to Jigsaw. Second, the share descript small but statistically significant number of sets. UMONs can also be used with other cache designs <ref type="bibr" target="#b36">[37]</ref> by sampling a fraction of cache accesses at the UMON. Give m sizes. Although UCP was designed to work with way-partitioning, it can be used with other schemes <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref>. Instead of captur eir cache behavior: insensitive (n), cache-friendly (f), cache-fitting (t), and streaming (s) as in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr">Table 2]</ref>, and build random mixes of miss curves. Prior partitioning schemes use per-core UMONs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>, but this is  <ref type="bibr" target="#b33">[34]</ref>, th s is sufficient to partition small caches as in prior work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>, partitioning a large, multi-banked cache among many shares s, we use a similar methodology to prior partitioning work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>. We classify all 29 SPEC CPU2006 workloads into four types
" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" tar " target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" tar ld on a sharedcache baseline and leverage virtual memory to perform adaptive placement. Cho and Jin <ref type="bibr" target="#b10">[11]</ref> use page coloring and a NUCA-aware allocator to map pages Jigsaw builds on a shared baseline. However, instead of mapping pages to locations as in prior work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>, we map pages to s
lation. To avoid losing associativity, some schemes can partition the cache by sets instead of ways <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref>, but they require
me schemes can partition the cache by sets instead of ways <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref>, but they require significant changes to cache arrays. Alte
lation. To avoid losing associativity, some schemes can partition the cache by sets instead of ways <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref>, but they require
adrant of the 64-core chip. Since IPC can be a misleading proxy for work in multithreaded workloads <ref type="bibr" target="#b0">[1]</ref>, we instrument each application with heartbeats that report </ref>. To achieve statistically significant results, we introduce small amounts of non-determinism <ref type="bibr" target="#b0">[1]</ref>, and perform enough runs to achieve 95% confidence intervals
holes, canneal, fluidanimate, swaptions), SPLASH-2 (barnes, ocean, fft, lu, radix), and BioParallel <ref type="bibr" target="#b18">[19]</ref> (svm). We simulate 40 random mixes of four workloads. Each
earch does not address both issues jointly. On one hand, prior non-uniform cache access (NUCA) work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target mic NUCA (D-NUCA) schemes improve on S-NUCA by adaptively placing data close to the requesting core <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target h is often also leveraged to implement NUCA techniques. For example, Adaptive Selective Replication <ref type="bibr" target="#b1">[2]</ref> controls replication by probabilistically deciding whether t g, selective replication, and adaptive spilling (DCC <ref type="bibr" target="#b14">[15]</ref>, ASR <ref type="bibr" target="#b1">[2]</ref>, and ECC <ref type="bibr" target="#b15">[16]</ref>), often b
et="#b2">[3]</ref>. On the other hand, prior work has proposed a variety of partitioning techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" targ y on restricting the locations where a line can reside depending on its partition. Way-partitioning <ref type="bibr" target="#b8">[9]</ref> restricts insertions from each partition to its assigned sub
ee ways. 1. Multipage mappings use one TLB entry to map multiple pages (e.g., 8-16 pages per entry) <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" ta es, and thus the OS can only allocate them when the available memory is size-aligned and contiguous <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b37">39]</ref>. In addition, many anslations for a variety of block sizes and exploits the clustering behavior of the buddy allocator <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b37">39]</ref>.</p><p>Finally, ea ="bibr" target="#b45">[47]</ref>, CoLT <ref type="bibr" target="#b37">[39]</ref> and Clustered TLBs <ref type="bibr" target="#b36">[38]</ref>, pack multiple Page  multiple of translations (e.g., 8-16) counters. (ii) We emulate multipage mappings in BadgerTrap. We implement the Clustered TLB approach <ref type="bibr" target="#b36">[38]</ref> of Pham et al., configured with 512 fullyassociative entri 2 fullyassociative entries. Each entry indexes up to an 8-page cluster, shown best by Clustered TLB <ref type="bibr" target="#b36">[38]</ref>. We use eager paging to increase the opportunities to form lable on x86-64 processors. All other configurations are emulated. The CTLB bars show Clustered TLB <ref type="bibr" target="#b36">[38]</ref> results. The DS bars show direct segments <ref type="bibr" ype="bibr" target="#b45">[47]</ref>, CoLT <ref type="bibr" target="#b37">[39]</ref>, Clustered TLBs <ref type="bibr" target="#b36">[38]</ref>), huge pages <ref type="bibr" target="#b0">[1,</ref><ref t
of hardware performance counters from an x86 execution and functional TLB simulation in BadgerTrap <ref type="bibr" target="#b20">[22]</ref>the same methodology as in prior TLB studies <ref type="bib ments from native executions with TLB performance emulation using a modified version of Bad-gerTrap <ref type="bibr" target="#b20">[22]</ref>. Compared to cycle-accurate simulation on these workloads,
memory protection <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b47">49]</ref>, in the sense that both exploit range behavior. However, in
38]</ref>), huge pages <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b34">36]</ref>, and direct segments <ref type="bibr" target="#b8">[10,</re
rformance because the range TLB contains only a few entries and it can use fast comparison circuits <ref type="bibr" target="#b30">[32]</ref>. Our results in Section 8 show that a 32-entry fully-assoc
is not uncommon, since a few pages scattered throughout memory can cause considerable fragmentation <ref type="bibr" target="#b16">[18]</ref>. In this case, the OS could perform full compaction <ref t compaction with techniques adapted from garbage collection <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b16">18]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n
of hardware performance counters from an x86 execution and functional TLB simulation in BadgerTrap <ref type="bibr" target="#b20">[22]</ref>the same methodology as in prior TLB studies <ref type="bib ments from native executions with TLB performance emulation using a modified version of Bad-gerTrap <ref type="bibr" target="#b20">[22]</ref>. Compared to cycle-accurate simulation on these workloads,
m.</p><p>Finally, our proposed architecture resembles prior works in fine-grained memory protection <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" ta
and standard paging for the remaining virtual address space <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b21">23]</ref>. For applications that can allocate and use a single segmen n prior TLB studies <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b21">23]</ref>. We compare RMM to standard paging, Clustered TLBs, huge (2 this same approach <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b21">23]</ref>.</p><p>BadgerTrap instruments x86-64 TLB misses. We add a f fraction of reduced page-walks, using a simple linear model <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b21">23]</ref> given in Table <ref type="table" target="#tab_14">6</ref>.< ef type="bibr" target="#b34">36]</ref>, and direct segments <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b21">23]</ref>, and Section 8 showed quantitatively that RMM substantially large page TLB entries, which further limits their benefit <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b29">31]</ref>.</p><p>Direct segme get="#b8">[10,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" tar
vance of their use <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b40">42]</ref>. However, the effectiveness of prefetching is limited by th
and standard paging for the remaining virtual address space <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b21">23]</ref>. For applications that can allocate and use a single segmen n prior TLB studies <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b21">23]</ref>. We compare RMM to standard paging, Clustered TLBs, huge (2 this same approach <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b21">23]</ref>.</p><p>BadgerTrap instruments x86-64 TLB misses. We add a f fraction of reduced page-walks, using a simple linear model <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b21">23]</ref> given in Table <ref type="table" target="#tab_14">6</ref>.< ef type="bibr" target="#b34">36]</ref>, and direct segments <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b21">23]</ref>, and Section 8 showed quantitatively that RMM substantially large page TLB entries, which further limits their benefit <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b29">31]</ref>.</p><p>Direct segme get="#b8">[10,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" tar
ted recurrent units, and residual architecture-based models. This work expands on our previous work <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" t nd investigate in more depth the gated recurrent unit (GRU) element introduced in our previous work <ref type="bibr" target="#b17">[18]</ref>. We also propose a new element called a residual GRU (rGRU
and speed up the training process of BLSTM. This includes context-sensitive-chunk BLSTM (CSC-BLSTM) <ref type="bibr" target="#b24">[25]</ref> and latency-controlled BLSTM (LC-BLSTM) <ref type="bibr" t

ng the rate of gradient decay through a gating mechanism.</p><p>LSTM units were first introduced in <ref type="bibr" target="#b11">[12]</ref>. A popular LSTM structure is shown in Fig. <ref type="figu
led the same as DNN.</p><p>Discriminative learning methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> can help to improve
ion less valuable <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Further, because an output gate is not used in GRUs, the
re the training time, all experiments have been implemented on a single NVIDIA Tesla K80 using CUDA <ref type="bibr" target="#b47">[48]</ref>.</p><p>From the training time data presented in Table <ref
to the vanishing and exploding gradients in the Stochastic Gradient Descent (SGD) training process <ref type="bibr" target="#b10">[11]</ref>.</p><p>The LSTM RNN topology is an advanced network struct d easily, thus reducing the difficulty due to vanishing or exploding gradients with respect to time <ref type="bibr" target="#b10">[11]</ref>.</p><p>The update gate helps the GRU to capture long term
BLSTM (CSC-BLSTM) <ref type="bibr" target="#b24">[25]</ref> and latency-controlled BLSTM (LC-BLSTM) <ref type="bibr" target="#b25">[26]</ref>. Figure <ref type="figure">2</ref> shows the differences a

ion less valuable <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Further, because an output gate is not used in GRUs, the

rectional LSTM</head><p>As discussed previously, the BLSTM <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> is able to make use of both the preceding and following con

="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. Experiments are carried out on the Babel benchmark datase ">[17]</ref>. For fair comparison, we only show the cross-entropy WER for fbank input models, as in <ref type="bibr" target="#b20">[21]</ref>.</p><p>We use the Kaldi decoder with the beam set to 11.0, and RNN can help to learn more precise time domain or frequency domain information. For example, in <ref type="bibr" target="#b20">[21]</ref>,  the convolutional maxout neural networks (CMNN) and recu
ns/1.0"><head n="4.6">Visualization for different models</head><p>The final experiment, inspired by <ref type="bibr" target="#b52">[54]</ref>, investigates the evolution of different models when perfo

rectional LSTM</head><p>As discussed previously, the BLSTM <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> is able to make use of both the preceding and following con
HMM) have become the dominant approach for acoustic modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, replacing the traditional Gaussian mixture modelhidden Marko

led the same as DNN.</p><p>Discriminative learning methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> can help to improve
to the vanishing and exploding gradients in the Stochastic Gradient Descent (SGD) training process <ref type="bibr" target="#b10">[11]</ref>.</p><p>The LSTM RNN topology is an advanced network struct d easily, thus reducing the difficulty due to vanishing or exploding gradients with respect to time <ref type="bibr" target="#b10">[11]</ref>.</p><p>The update gate helps the GRU to capture long term
ttp://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Transformer neural sequence model <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> has emerged as a popular alternative to r rg/ns/1.0"><head n="2.2">Multi-head Attention</head><p>The "Transformer" seuqence-to-sequence model <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> uses h different attention layers (heads) , hmv-&gt;hv " , weig hts , V) y = t f . einsum ( " hv , hdv-&gt;d " , o , P_o) r e t u r n y Note: <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> include a constant scaling factor on the values. In addition, we process a batch of b different non-interacting sequences at once. Following <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref>, in an autoregressive model, we can preve ing assumptions:</p><formula xml:id="formula_0">? m = n ? k = v = d</formula><p>h , as suggested by <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> ? n ? d</p><p>The total number of arithme allel. An example is a self-attention layer in an autoregressive language model such as Transformer <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref>. The queries produced at each position at </head><p>We introduce multi-query Attention as a variation of multi-head attention as described in <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref>. Multi-head attention consists of multipl </div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Following <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref>, we evaluate on the WMT 2014 English-Germ
ttending to a local neighborhood, or by otherwise compressing the number of memory positions, as in <ref type="bibr" target="#b2">[Liu et al., 2018]</ref>, <ref type="bibr" target="#b5">[Zhang et al.,
ttending to a local neighborhood, or by otherwise compressing the number of memory positions, as in <ref type="bibr" target="#b2">[Liu et al., 2018]</ref>, <ref type="bibr" target="#b5">[Zhang et al.,
r" target="#b2">[Liu et al., 2018]</ref>, <ref type="bibr" target="#b5">[Zhang et al., 2018]</ref>, <ref type="bibr" target="#b3">[Povey et al., 2018]</ref>. In this paper we present an orthogonal app
r" target="#b2">[Liu et al., 2018]</ref>, <ref type="bibr" target="#b5">[Zhang et al., 2018]</ref>, <ref type="bibr" target="#b3">[Povey et al., 2018]</ref>. In this paper we present an orthogonal app
ttending to a local neighborhood, or by otherwise compressing the number of memory positions, as in <ref type="bibr" target="#b2">[Liu et al., 2018]</ref>, <ref type="bibr" target="#b5">[Zhang et al.,
ttending to a local neighborhood, or by otherwise compressing the number of memory positions, as in <ref type="bibr" target="#b2">[Liu et al., 2018]</ref>, <ref type="bibr" target="#b5">[Zhang et al.,
r" target="#b2">[Liu et al., 2018]</ref>, <ref type="bibr" target="#b5">[Zhang et al., 2018]</ref>, <ref type="bibr" target="#b3">[Povey et al., 2018]</ref>. In this paper we present an orthogonal app
r" target="#b2">[Liu et al., 2018]</ref>, <ref type="bibr" target="#b5">[Zhang et al., 2018]</ref>, <ref type="bibr" target="#b3">[Povey et al., 2018]</ref>. In this paper we present an orthogonal app
ttending to a local neighborhood, or by otherwise compressing the number of memory positions, as in <ref type="bibr" target="#b2">[Liu et al., 2018]</ref>, <ref type="bibr" target="#b5">[Zhang et al.,
ttending to a local neighborhood, or by otherwise compressing the number of memory positions, as in <ref type="bibr" target="#b2">[Liu et al., 2018]</ref>, <ref type="bibr" target="#b5">[Zhang et al.,
t improvement of 3.2% compared to the two-bit TC-AGE baseline (analogue of SRRIP for exclusive LLC) <ref type="bibr" target="#b5">[5]</ref> while saving 66.6% data write transactions from the L2 cache ecisions and assign insertion ages to the non-bypassed blocks in the context of exclusive L3 caches <ref type="bibr" target="#b5">[5]</ref>. LLC insertion and replacement policies based on static and ache blocks presented in an earlier study in the context of a cache hierarchy with an exclusive LLC <ref type="bibr" target="#b5">[5]</ref>. According to the terminology used in that study, the set C0 st two extra state bits (S1, S0) per L2 cache block (as opposed to three bits per L2 cache block in <ref type="bibr" target="#b5">[5]</ref>). Figure <ref type="figure" target="#fig_4">4</ref>, through all L2 cache evictions and decides the insertion age of a block based on the two-bit TC-AGE policy <ref type="bibr" target="#b5">[5]</ref>. This policy is the analogue of SRRIP for exclusive LLCs. It in this discussion to conform to the terminology used in the prior work on exclusive LLC management<ref type="bibr" target="#b5">[5]</ref>. Age can be considered synonymous to RRPV in this discussion
get="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18]</ref>. These predictors usually require partial or full program c
manage LLCs shared between CPU workloads and GPGPU workloads in a CPU-GPU heterogeneous environment <ref type="bibr" target="#b17">[17]</ref>. Further, the PACMan family of policies is shown to outper
manage LLCs shared between CPU workloads and GPGPU workloads in a CPU-GPU heterogeneous environment <ref type="bibr" target="#b17">[17]</ref>. Further, the PACMan family of policies is shown to outper
get="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18]</ref>. These predictors usually require partial or full program c
get="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18]</ref>. These predictors usually require partial or full program c
br">[7]</ref>, thereby eliminating most of the negative effects of inclusion victims. A recent work <ref type="bibr" target="#b27">[27]</ref> explores an orthogonal dimension of the problem by proposi
che evictions, we conduct an oracle-assisted experiment where the LLC runs the two-bit SRRIP policy <ref type="bibr" target="#b8">[8]</ref> (this is our baseline in this paper). The two-bit SRRIP poli s based on static and dynamic re-reference interval prediction (SRRIP and DRRIP) have been explored <ref type="bibr" target="#b8">[8]</ref>. A recent proposal improves the re-reference interval predic </head><p>Figure <ref type="figure" target="#fig_8">7</ref> compares the performance of CHAR, DRRIP <ref type="bibr" target="#b8">[8]</ref>, CHAR-PC, SDBP <ref type="bibr" target="#b12">[12]</ref>, an n completely and compare the policies in terms of the execution time of the parallel computation.   <ref type="bibr" target="#b8">[8]</ref>, SDBP <ref type="bibr" target="#b12">[12]</ref>, SHiP-PC <re c>Figure9summarizes the normalized average throughput and LLC miss counts delivered by CHAR, TADRRIP<ref type="bibr" target="#b8">[8]</ref>, SDBP<ref type="bibr" target="#b12">[12]</ref>, SHiP-PC<ref www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>SRRIP is known to outperform NRU and LRU<ref type="bibr" target="#b8">[8]</ref>.</p></note> 			<note xmlns="http://www.tei-c.org/ns/1.0" pla
get="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18]</ref>. These predictors u
atures (SHiP-PC), memory region signatures (SHiP-Mem), and instruction trace signatures (SHiP-ISeq) <ref type="bibr" target="#b24">[24]</ref>. Another recent work shows how to extend RRIP to manage LL ing counter size, and the prediction threshold of zero have been borrowed from the SHiP-PC proposal <ref type="bibr" target="#b24">[24]</ref>. In Section 4, we show that CHAR-PC, which exploits the cr "bibr" target="#b8">[8]</ref>, CHAR-PC, SDBP <ref type="bibr" target="#b12">[12]</ref>, and SHiP-PC <ref type="bibr" target="#b24">[24]</ref>. The last three policies require the program counter of th <ref type="bibr" target="#b8">[8]</ref>, SDBP <ref type="bibr" target="#b12">[12]</ref>, SHiP-PC <ref type="bibr" target="#b24">[24]</ref>, ECI <ref type="bibr">[7]</ref>, and QBS <ref type="bibr"> DRRIP<ref type="bibr" target="#b8">[8]</ref>, SDBP<ref type="bibr" target="#b12">[12]</ref>, SHiP-PC<ref type="bibr" target="#b24">[24]</ref>, ECI[7], and QBS[7]. These data represent the average acro
light-weight dead block predictors exploiting the fill order of LLC blocks have also been proposed <ref type="bibr" target="#b2">[2]</ref>. Our basic CHAR proposal infers the death of an LLC block at
pe="bibr" target="#b30">[31]</ref>. A hybrid approach has been introduced to increase its viability <ref type="bibr" target="#b31">[32]</ref>.</p><p>The concept of STRAIGHT <ref type="bibr" target="#b
ple threads. Slipstream processors <ref type="bibr" target="#b22">[23]</ref> and Runahead execution <ref type="bibr" target="#b23">[24]</ref> are similar but more drastic technologies that utilize hel
ctures are examples of the representative approaches <ref type="bibr" target="#b12">[13]</ref> [19] <ref type="bibr" target="#b19">[20]</ref>. Their basic idea is to separate the wide-issue core into
the dataflow graph almost directly onto its ALU networks <ref type="bibr" target="#b29">[30]</ref>  <ref type="bibr" target="#b30">[31]</ref>. A hybrid approach has been introduced to increase its via
consumption <ref type="bibr" target="#b8">[9]</ref> [10] <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b11">[12]</ref>. It lowers the efficiency of the OoO CPU cores, with the r ref> introduces both OoO and in-order mechanisms into a core to enable rapid switching. Mirage Core <ref type="bibr" target="#b11">[12]</ref> virtually increases the number of OoO cores by transferrin
cing a specific ISA, the architecture maps the dataflow graph almost directly onto its ALU networks <ref type="bibr" target="#b29">[30]</ref>  <ref type="bibr" target="#b30">[31]</ref>. A hybrid appro
e the recovery penalty. To reveal this impact, we implemented TAGE predictor (8-component CBP-TAGE) <ref type="bibr" target="#b14">[15]</ref> to our simulator. Figure <ref type="figure" target="#fig_0
e="bibr" target="#b25">[26]</ref>. Composite cores <ref type="bibr" target="#b26">[27]</ref> or FXA <ref type="bibr" target="#b27">[28]</ref> introduces both OoO and in-order mechanisms into a core to
that even though the number of transistors can be increased, they cannot be switched simultaneously <ref type="bibr" target="#b1">[2]</ref>. In this scenario, the CPU is expected to effectively execut nstruction  I 2 means that the instruction uses the result value of the previous instruction, and " <ref type="bibr" target="#b1">[2]</ref>" means that the other operand is the result value of the sec and. There are two paths to this operand: from the BB0 and the BB2. The operand of SLTi is fixed as <ref type="bibr" target="#b1">[2]</ref> by adding RMOVs.</p><p>3) Distance Bounding: The STRAIGHT ar

nstruction is invoked. In the example, the instruction ADD <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b2">[3]</ref> in the callee always refers arg0 and arg1.</p><p>The return

r" target="#b27">Sutskever et al., 2014;</ref><ref type="bibr" target="#b7">Cho et al., 2014b;</ref><ref type="bibr" target="#b9">Forcada and Ñeco, 1997)</ref>. This neural machine translation approac
suffering too much from the vanishing effect <ref type="bibr" target="#b15">(Hochreiter, 1991;</ref><ref type="bibr" target="#b2">Bengio et al., 1994;</ref><ref type="bibr" target="#b20">Pascanu et al
dfellow et al., 2013)</ref> hidden layer to compute the conditional probability of each target word <ref type="bibr" target="#b22">(Pascanu et al., 2014)</ref>.</p><p>We use a minibatch stochastic gra ut probability (Eq. ( <ref type="formula" target="#formula_4">4</ref>)) as a multi-layered function <ref type="bibr" target="#b22">(Pascanu et al., 2014)</ref>. We use a single hidden layer of maxout r ti which is computed by</p><p>are weight matrices. This can be understood as having a deep output <ref type="bibr" target="#b22">(Pascanu et al., 2014)</ref> with a single maxout hidden layer <ref t
dfellow et al., 2013)</ref> hidden layer to compute the conditional probability of each target word <ref type="bibr" target="#b22">(Pascanu et al., 2014)</ref>.</p><p>We use a minibatch stochastic gra ut probability (Eq. ( <ref type="formula" target="#formula_4">4</ref>)) as a multi-layered function <ref type="bibr" target="#b22">(Pascanu et al., 2014)</ref>. We use a single hidden layer of maxout r ti which is computed by</p><p>are weight matrices. This can be understood as having a deep output <ref type="bibr" target="#b22">(Pascanu et al., 2014)</ref> with a single maxout hidden layer <ref t
org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the developers of Theano <ref type="bibr" target="#b4">(Bergstra et al., 2010;</ref><ref type="bibr" target="#b1">Bastien et
f>, we reduce the size of the combined corpus to have 348M words using the data selection method by <ref type="bibr" target="#b0">Axelrod et al. (2011)</ref>. <ref type="foot" target="#foot_2">5</ref>
="#b7">Cho et al. (2014b)</ref>. Unlike the traditional phrase-based translation system (see, e.g., <ref type="bibr" target="#b19">Koehn et al., 2003)</ref> which consists of many small sub-components
slation, recently proposed by <ref type="bibr" target="#b17">Kalchbrenner and Blunsom (2013)</ref>, <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> and <ref type="bibr" target="#b7">Cho e Despite being a quite new approach, neural machine translation has already shown promising results. <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> reported that the neural machine transl called RNN Encoder-Decoder, proposed by <ref type="bibr" target="#b6">Cho et al. (2014a)</ref> and <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> upon which we build a novel architectur is a vector generated from the sequence of the hidden states. f and q are some nonlinear functions. <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> used an LSTM as f and</p><formula xml:i ="#b11">Graves, 2012;</ref><ref type="bibr" target="#b5">Boulanger-Lewandowski et al., 2013)</ref>. <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> used this approach to generate translat use LSTM units instead of the gated hidden unit described here, as was done in a similar context by <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref>.</p><p>The new state s i of the RNN emp </p><p>Most of the proposed neural machine translation models belong to a family of encoderdecoders <ref type="bibr" target="#b27">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b6">Cho et al t="#b17">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b6">Cho et al., 2014a;</ref><ref type="bibr" target="#b27">Sutskever et al., 2014;</ref><ref type="bibr" target="#b7">Cho et al. rent neural networks (RNN) were used by <ref type="bibr" target="#b6">(Cho et al., 2014a)</ref> and <ref type="bibr" target="#b27">(Sutskever et al., 2014)</ref> to encode a variable-length source sen table <ref type="bibr" target="#b6">(Cho et al., 2014a)</ref> or to re-rank candidate translations <ref type="bibr" target="#b27">(Sutskever et al., 2014)</ref>, has allowed to surpass the previous s
slation, recently proposed by <ref type="bibr" target="#b17">Kalchbrenner and Blunsom (2013)</ref>, <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> and <ref type="bibr" target="#b7">Cho e Despite being a quite new approach, neural machine translation has already shown promising results. <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> reported that the neural machine transl called RNN Encoder-Decoder, proposed by <ref type="bibr" target="#b6">Cho et al. (2014a)</ref> and <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> upon which we build a novel architectur is a vector generated from the sequence of the hidden states. f and q are some nonlinear functions. <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> used an LSTM as f and</p><formula xml:i ="#b11">Graves, 2012;</ref><ref type="bibr" target="#b5">Boulanger-Lewandowski et al., 2013)</ref>. <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> used this approach to generate translat use LSTM units instead of the gated hidden unit described here, as was done in a similar context by <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref>.</p><p>The new state s i of the RNN emp </p><p>Most of the proposed neural machine translation models belong to a family of encoderdecoders <ref type="bibr" target="#b27">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b6">Cho et al t="#b17">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b6">Cho et al., 2014a;</ref><ref type="bibr" target="#b27">Sutskever et al., 2014;</ref><ref type="bibr" target="#b7">Cho et al. rent neural networks (RNN) were used by <ref type="bibr" target="#b6">(Cho et al., 2014a)</ref> and <ref type="bibr" target="#b27">(Sutskever et al., 2014)</ref> to encode a variable-length source sen table <ref type="bibr" target="#b6">(Cho et al., 2014a)</ref> or to re-rank candidate translations <ref type="bibr" target="#b27">(Sutskever et al., 2014)</ref>, has allowed to surpass the previous s
on a recent empirical study reported by <ref type="bibr" target="#b7">Cho et al. (2014b)</ref> and <ref type="bibr" target="#b23">Pouget-Abadie et al. (2014)</ref>.</p><p>In this paper, we proposed a
, the widespread use of such tools raises legitimate privacy concerns. For instance, Mislove et al. <ref type="bibr" target="#b22">[24]</ref> demonstrated how, by analysing Facebook's social network s
ithin C † to b − d nodes from outside of C † .</formula><p>This heuristic is inspired by modularity <ref type="bibr" target="#b24">[26]</ref>-a widely used index for measuring the quality of any given anguage (version 1.0.1), namely: Eigenvector <ref type="bibr" target="#b23">[25]</ref>, Betweenness <ref type="bibr" target="#b24">[26]</ref>, Walktrap <ref type="bibr" target="#b27">[29]</ref>, Louva
istic twice on the 9/11 terrorist network to hide Mohamed Atta-one of the ringleaders of the attack <ref type="bibr" target="#b17">[19]</ref>. The red link is the one to be to removed by the algorithm a). Covert organizations: we consider three terrorist network, responsible for the WTC 9/11 attacks <ref type="bibr" target="#b17">[19]</ref>; the 2002 Bali attack <ref type="bibr" target="#b11">[13]<
]</ref> is denoted by c clos , and the betweeness centrality <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b8">10]</ref> is denoted by c betw . Specifically, given a node v i ∈ V an
f type="bibr" target="#b24">[26]</ref>, Walktrap <ref type="bibr" target="#b27">[29]</ref>, Louvain <ref type="bibr" target="#b4">[6]</ref>, Greedy <ref type="bibr" target="#b5">[7]</ref>, Infomap <re
s of centrality measures <ref type="bibr" target="#b6">[8]</ref> and community detection algorithms <ref type="bibr" target="#b26">[28]</ref>; while such analyses typically consider the effects of sma
in <ref type="bibr" target="#b4">[6]</ref>, Greedy <ref type="bibr" target="#b5">[7]</ref>, Infomap <ref type="bibr" target="#b29">[31]</ref> and Spinglass <ref type="bibr" target="#b28">[30]</ref>. A on-deterministic, and may yield different results on the same network). As can be seen, the Infomap <ref type="bibr" target="#b29">[31]</ref> algorithm seems to be the most difficult to fool.</p></div
ook, Twitter and Google+. These fragments are taken from SNAPthe Stanford Network Analysis Platform <ref type="bibr" target="#b19">[21]</ref>.</p><p>We also study randomly-generated networks, namely: l undirected). all anonymized fragments were taken from SNAP-the Stanford Network Analysis Platform <ref type="bibr" target="#b19">[21]</ref>.</p><p>• Facebook: the small fragment consists of 61 nodes
<ref type="bibr" target="#b27">[29]</ref>, Louvain <ref type="bibr" target="#b4">[6]</ref>, Greedy <ref type="bibr" target="#b5">[7]</ref>, Infomap <ref type="bibr" target="#b29">[31]</ref> and Sping
in <ref type="bibr" target="#b4">[6]</ref>, Greedy <ref type="bibr" target="#b5">[7]</ref>, Infomap <ref type="bibr" target="#b29">[31]</ref> and Spinglass <ref type="bibr" target="#b28">[30]</ref>. A on-deterministic, and may yield different results on the same network). As can be seen, the Infomap <ref type="bibr" target="#b29">[31]</ref> algorithm seems to be the most difficult to fool.</p></div
ites and other internet content is policed, and anti-governmental blogs and activities are censored <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b16">18]</ref>.</p><p>Against thi
as designed to be representative of next-generation sharedmemory programs for chip-multiprocessors" <ref type="bibr" target="#b2">[3]</ref>. Our experiments show that for those programs, no matter how </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Benchmarks</head><p>We use PARSEC <ref type="bibr" target="#b2">[3]</ref> as the benchmark suite. It is a recently released suite desi Because there is no close-form expression for the equation, the program uses numerical computation <ref type="bibr" target="#b2">[3]</ref>.</p><p>The input data file of this benchmark includes an arr 6MB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* : see<ref type="bibr" target="#b2">[3]</ref> for detail.</note></figure> <figure xmlns="http://www.tei-c. , as well as systems applications that mimic large-scale multithreaded commercial programs. Studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> have shown that the su 0 {0 2 4 6},{1 3 5 7},{0 2 4 6},{1 3 5 7} 1 161.6 0 {0 2 1 3},{4 5 6 7},{0 2 1 3},{4 5 6 7} 4 161. <ref type="bibr" target="#b2">3</ref> No binding 165.7</p><p>In PARSEC, ferret and dedup are two suc urement are relevant to this current work. Bienia and others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> have shown a detailed exploration of the characterization of
target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref>, are on multiprogramming environments, attempting to allevi
many studies (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" tar
of independent jobs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref> as well as parallel t target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref> are concentrated on c target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" targe
measure running times, and employ the Performance Application Programming Interface (PAPI) library <ref type="bibr" target="#b3">[4]</ref> to read memory-related hardware performance counters, includ
target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref>, are on multiprogramming environments, attempting to allevi
some similarity with traditional locality optimization on NUMA architectures for main memory usage <ref type="bibr" target="#b10">[11]</ref>. Although both try to redistribute computation and data, o
target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref> as well as parallel threads inside certain classes of singl target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref> are concentrated on co-runs of independent programs, on whi programs, a contrast to previous results on independent jobs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref> and server programs <ref type="bibr" target="#b22">[23]</re target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" targe
mimic large-scale multithreaded commercial programs. Studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> have shown that the suite covers a wide range of working set d characterization and performance measurement are relevant to this current work. Bienia and others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> have shown a detailed
ure design and process/thread scheduling in OS.</p><p>In architecture research, many studies (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targ
target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref>, are on multiprogramm
#b11">[12]</ref>, which also proposed a combinatorial algorithm to compute embeddings. Ganea et al. <ref type="bibr" target="#b16">[17]</ref> and Gulcehre et al. <ref type="bibr" target="#b20">[21]</r taking its outputs. An alternative to prevent such collapse would be to introduce bias terms as in <ref type="bibr" target="#b16">[17]</ref>. Importantly, when applying the non-linearity directly on )y 1 + 2 x, y + x 2 y 2<label>(8)</label></formula><p>Similar to the Euclidean case, and following <ref type="bibr" target="#b16">[17]</ref>, we use x = x 0 . On the Poincaré ball, we employ pointwis
learning from graph-structured data <ref type="bibr" target="#b6">[7]</ref>. Originally proposed by <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref> as a method for le
n often be explained by assuming an underlying hierarchy which is well captured in hyperbolic space <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>. These insights le from the tangent space.</p><p>Hyperbolic geometry has also shown great promise in network science: <ref type="bibr" target="#b27">[28]</ref> showed that typical properties of complex networks such as </ref>. Furthermore, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28]</ref> exploited the property of hyperbolic embeddings to embed tr
been particularly impactful is in modeling chemical problems. Applications include molecular design <ref type="bibr" target="#b30">[31]</ref>, fingerprinting <ref type="bibr" target="#b13">[14]</ref> ph generation for molecules using machine learning methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>. However, see the appendix for results on QM8 <ref type="bi
arget="#b14">[15]</ref>, Barabási-Albert <ref type="bibr" target="#b1">[2]</ref> and Watts-Strogatz <ref type="bibr" target="#b45">[46]</ref> (see Figure <ref type="figure">1</ref>).</p><p>The graphs
les <ref type="bibr" target="#b17">[18]</ref>. A popular choice for this purpose is the QM9 dataset <ref type="bibr" target="#b36">[37]</ref>. Unfortunately, it is hard to compare to previous work on
ntum chemistry <ref type="bibr" target="#b17">[18]</ref> to modelling social and interaction graphs <ref type="bibr" target="#b49">[50]</ref>.</p><p>In this work, we are concerned with the representat
earn inductive models of graphs, GNNs have found promising applications in molecular fingerprinting <ref type="bibr" target="#b13">[14]</ref> and quantum chemistry <ref type="bibr" target="#b17">[18]< ms. Applications include molecular design <ref type="bibr" target="#b30">[31]</ref>, fingerprinting <ref type="bibr" target="#b13">[14]</ref> and poly pharmaceutical side-effect modeling <ref type="bi
bolic case. Hence, we propose an extension of the underlying idea of radial basis function networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref> to Riemannian manifo
rities. We use leaky ReLU as the activation function σ with the negative slope 0.5. We use RAMSGrad <ref type="bibr" target="#b3">[4]</ref> and AMSGrad for hyperbolic parameters and Euclidean paramete
m to compute embeddings. Ganea et al. <ref type="bibr" target="#b16">[17]</ref> and Gulcehre et al. <ref type="bibr" target="#b20">[21]</ref> proposed hyperbolic neural networks and hyperbolic attenti
s <ref type="bibr" target="#b14">[15]</ref>, dynamic predication based on frequently executed paths <ref type="bibr" target="#b13">[14]</ref>, and predicate prediction <ref type="bibr" target="#b25">[
ory-level parallelism with small cycle-critical structures <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23]</ref>. Their ability to form an effective large window is degrade
ded to the theoretical possibility of truly resolving branches in the fetch unit, and August et al. <ref type="bibr" target="#b2">[3]</ref> further explored opportunities for such earlyresolved branch
rget="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref>. We conservatively use 10 cycles for this parameter. We also
not specified as architectural registers: their physical counterparts are implementation-dependent. <ref type="bibr" target="#b3">4</ref>. The ISA provides mechanisms to save and restore the BQ state ware support.</p><p>Decoupled access/execute architectures <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4]</ref> are alternative implementations of OOO execution, and not a t
ed on frequently executed paths <ref type="bibr" target="#b13">[14]</ref>, and predicate prediction <ref type="bibr" target="#b25">[26]</ref>, to name a few. In this paper, predication (i.e., ifconver perblocks as these large scheduling regions yield more flexibility for code motion. Quinones et al. <ref type="bibr" target="#b25">[26]</ref> adapted the predicate register file for an OOO processor,
rget="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref>. We conservatively use 10 cycles for this parameter. We also
ory-level parallelism with small cycle-critical structures <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23]</ref>. Their ability to form an effective large window is degrade
arget="#b35">36,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. The key difference is that CFD preserves the simple sequenci ly, an automatic run-time microthread construction mechanism was proposed for preexecuting branches <ref type="bibr" target="#b8">[9]</ref>.</p><p>In the Branch Decoupled Architecture (BDA), proposed
not specified as architectural registers: their physical counterparts are implementation-dependent. <ref type="bibr" target="#b3">4</ref>. The ISA provides mechanisms to save and restore the BQ state ware support.</p><p>Decoupled access/execute architectures <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4]</ref> are alternative implementations of OOO execution, and not a t
echanism for applying CFD to partially separable branches.</p><p>CFD resembles branch pre-execution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" ta ng.</p><p>We now discuss several branch pre-execution solutions in more detail.</p><p>Farcy et. al. <ref type="bibr" target="#b10">[11]</ref> identified backward slices of applicable branches, and use
we propose Tacotron, an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) <ref type="bibr" target="#b17">(Sutskever et al., 2014)</ref> with attention paradigm <ref type="bib
nd a complex signal-processing-based vocoder <ref type="bibr" target="#b24">(Zen et al., 2009;</ref><ref type="bibr" target="#b1">Agiomyrgiannakis, 2015)</ref>. These components are based on extensive
d to a pre-net as is done in the encoder. Since we do not use techniques such as scheduled sampling <ref type="bibr" target="#b4">(Bengio et al., 2015)</ref> (we found it to hurt audio quality), the d
nd a complex signal-processing-based vocoder <ref type="bibr" target="#b24">(Zen et al., 2009;</ref><ref type="bibr" target="#b1">Agiomyrgiannakis, 2015)</ref>. These components are based on extensive
d to a pre-net as is done in the encoder. Since we do not use techniques such as scheduled sampling <ref type="bibr" target="#b4">(Bengio et al., 2015)</ref> (we found it to hurt audio quality), the d
d to a pre-net as is done in the encoder. Since we do not use techniques such as scheduled sampling <ref type="bibr" target="#b4">(Bengio et al., 2015)</ref> (we found it to hurt audio quality), the d
d to a pre-net as is done in the encoder. Since we do not use techniques such as scheduled sampling <ref type="bibr" target="#b4">(Bengio et al., 2015)</ref> (we found it to hurt audio quality), the d

d to a pre-net as is done in the encoder. Since we do not use techniques such as scheduled sampling <ref type="bibr" target="#b4">(Bengio et al., 2015)</ref> (we found it to hurt audio quality), the d
1-D convolutions, whose outputs are added with the original input sequence via residual connections <ref type="bibr" target="#b9">(He et al., 2016)</ref>. Batch normalization <ref type="bibr">(Ioffe &

by powerful baseline systems, such as the Fast/Faster R-CNN <ref type="bibr" target="#b14">[9,</ref><ref type="bibr" target="#b34">29]</ref> and Fully Convolutional Network (FCN) <ref type="bibr" targ target="#b14">[9]</ref>. N is 64 for the C4 backbone (as in <ref type="bibr" target="#b14">[9,</ref><ref type="bibr" target="#b34">29]</ref>) and 512 for FPN (as in <ref type="bibr" target="#b27">[22] state-of-the-art instance segmentation results. Our method, called Mask R-CNN, extends Faster R-CNN <ref type="bibr" target="#b34">[29]</ref> by adding a branch for predicting segmentation masks on ea ding to RoIs on feature maps using RoIPool, leading to fast speed and better accuracy. Faster R-CNN <ref type="bibr" target="#b34">[29]</ref> advanced this stream by learning the attention mechanism w e of Fast/Faster R-CNN.</p><p>Faster R-CNN: We begin by briefly reviewing the Faster R-CNN detector <ref type="bibr" target="#b34">[29]</ref>. Faster R-CNN consists of two stages. The first stage, cal are shareable.</p><p>Inference: At test time, the proposal number is 300 for the C4 backbone (as in <ref type="bibr" target="#b34">[29]</ref>) and 1000 for FPN (as in <ref type="bibr" target="#b27">[2 hares features between the RPN and Mask R-CNN stages, following the 4-step training of Faster R-CNN <ref type="bibr" target="#b34">[29]</ref>. This model runs at 195ms per image on an Nvidia Tesla M40 hyper-parameters following existing Fast/Faster R-CNN work <ref type="bibr" target="#b14">[9,</ref><ref type="bibr" target="#b34">29,</ref><ref type="bibr" target="#b27">22]</ref>. Although these dec decisions were made for object detection in original papers <ref type="bibr" target="#b14">[9,</ref><ref type="bibr" target="#b34">29,</ref><ref type="bibr" target="#b27">22]</ref>, we found our insta

segments <ref type="bibr" target="#b38">[33,</ref><ref type="bibr" target="#b7">2]</ref>. DeepMask <ref type="bibr" target="#b32">[27]</ref> and following works <ref type="bibr" target="#b33">[28,</r This is in contrast to most recent systems, where classification depends on mask predictions (e.g. <ref type="bibr" target="#b32">[27,</ref><ref type="bibr" target="#b12">7,</ref><ref type="bibr" tar that lacks spatial dimensions. Unlike previous methods that resort to fc layers for mask prediction <ref type="bibr" target="#b32">[27,</ref><ref type="bibr" target="#b33">28,</ref><ref type="bibr" ta

et="#b15">[10,</ref><ref type="bibr" target="#b17">12,</ref><ref type="bibr" target="#b18">13,</ref><ref type="bibr" target="#b11">6]</ref> resorted to bottom-up segments <ref type="bibr" target="#b38
s. Earlier methods <ref type="bibr" target="#b15">[10,</ref><ref type="bibr" target="#b17">12,</ref><ref type="bibr" target="#b18">13,</ref><ref type="bibr" target="#b11">6]</ref> resorted to bottom-u
re network-depth-features. We evaluate ResNet <ref type="bibr" target="#b20">[15]</ref> and ResNeXt <ref type="bibr" target="#b40">[35]</ref> networks of depth 50 or 101 layers. The original implement "http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We use the 64×4d variant of ResNeXt<ref type="bibr" target="#b40">[35]</ref>.</note> 		</body> 		<back> 			<div type="annex"> <div xmln
e methods, segmentation precedes recognition, which is slow and less accurate. Likewise, Dai et al. <ref type="bibr" target="#b12">[7]</ref> proposed a complex multiple-stage cascade that predicts seg eads to large improvements as we show in §4.2. We also compare to the RoIWarp operation proposed in <ref type="bibr" target="#b12">[7]</ref>. Unlike RoIAlign, RoIWarp overlooked the alignment issue an et="#b12">[7]</ref>. Unlike RoIAlign, RoIWarp overlooked the alignment issue and was implemented in <ref type="bibr" target="#b12">[7]</ref> as quantizing RoI just like RoIPool. So even though RoIWarp able <ref type="table" target="#tab_2">1</ref>. Instance segmentation mask AP on COCO test-dev. MNC <ref type="bibr" target="#b12">[7]</ref> and FCIS <ref type="bibr" target="#b26">[21]</ref> are the ns of our model outperform baseline variants of previous state-of-the-art models. This includes MNC <ref type="bibr" target="#b12">[7]</ref> and FCIS <ref type="bibr" target="#b26">[21]</ref>, the win p><p>Class-Specific vs. Class-Agnostic Masks: Additionally, we compare with RoIWarp proposed in MNC <ref type="bibr" target="#b12">[7]</ref> that also adopt bilinear sampling. As discussed in §3, RoIW ms, where classification depends on mask predictions (e.g. <ref type="bibr" target="#b32">[27,</ref><ref type="bibr" target="#b12">7,</ref><ref type="bibr" target="#b26">21]</ref>). Our approach follo s denoted by ResNet-50-C4. This is a common choice used in <ref type="bibr" target="#b20">[15,</ref><ref type="bibr" target="#b12">7,</ref><ref type="bibr" target="#b22">17,</ref><ref type="bibr" targ or mask prediction <ref type="bibr" target="#b32">[27,</ref><ref type="bibr" target="#b33">28,</ref><ref type="bibr" target="#b12">7]</ref>, our fully convolutional representation requires fewer param
re network-depth-features. We evaluate ResNet <ref type="bibr" target="#b20">[15]</ref> and ResNeXt <ref type="bibr" target="#b40">[35]</ref> networks of depth 50 or 101 layers. The original implement "http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We use the 64×4d variant of ResNeXt<ref type="bibr" target="#b40">[35]</ref>.</note> 		</body> 		<back> 			<div type="annex"> <div xmln
flexible and robust to many follow-up improvements (e.g., <ref type="bibr" target="#b35">[30,</ref><ref type="bibr" target="#b27">22,</ref><ref type="bibr" target="#b22">17]</ref>), and is the curren 6">31]</ref>.</p><p>We also explore another more effective backbone recently proposed by Lin et al. <ref type="bibr" target="#b27">[22]</ref>, called a Feature Pyramid Network (FPN). FPN uses a top-do N gives excellent gains in both accuracy and speed. For further details on FPN, we refer readers to <ref type="bibr" target="#b27">[22]</ref>.</p><p>For the network head we closely follow architecture extend the Faster R-CNN box heads from the ResNet <ref type="bibr" target="#b20">[15]</ref> and FPN <ref type="bibr" target="#b27">[22]</ref> papers. Details are shown in Figure <ref type="figure">3</ ibr" target="#b14">[9]</ref>. Images are resized such that their scale (shorter edge) is 800 pixels <ref type="bibr" target="#b27">[22]</ref>. Each mini-batch has 2 images per GPU and each image has N and each image has N sampled RoIs, with a ratio of 1:3 of positive to negatives Faster R-CNN w/ FPN <ref type="bibr" target="#b27">[22]</ref> Figure <ref type="figure">3</ref>. Head Architecture: We e w the heads for the ResNet C4 and FPN backbones, from <ref type="bibr" target="#b20">[15]</ref> and <ref type="bibr" target="#b27">[22]</ref>, respectively, to which a mask branch is added. Numbers de type="bibr" target="#b14">[9,</ref><ref type="bibr" target="#b34">29]</ref>) and 512 for FPN (as in <ref type="bibr" target="#b27">[22]</ref>). We train on 8 GPUs (so effective minibatch size is 16) f decay of 0.0001 and a momentum of 0.9. The RPN anchors span 5 scales and 3 aspect ratios, following <ref type="bibr" target="#b27">[22]</ref>. For convenient ablation, RPN is trained separately and do s 300 for the C4 backbone (as in <ref type="bibr" target="#b34">[29]</ref>) and 1000 for FPN (as in <ref type="bibr" target="#b27">[22]</ref>). We run the box prediction branch on these proposals, fol e <ref type="table" target="#tab_6">3</ref>. This model performs better than the model presented in <ref type="bibr" target="#b27">[22]</ref> due to RoIAlign. On the other hand, it is 0.9 points box A te-of-the-art models (the mask output is ignored in these experiments). The gains of Mask R-CNN over<ref type="bibr" target="#b27">[22]</ref> come from using RoIAlign (+1.1 AP bb ), multitask training t/Faster R-CNN work <ref type="bibr" target="#b14">[9,</ref><ref type="bibr" target="#b34">29,</ref><ref type="bibr" target="#b27">22]</ref>. Although these decisions were made for object detection in in original papers <ref type="bibr" target="#b14">[9,</ref><ref type="bibr" target="#b34">29,</ref><ref type="bibr" target="#b27">22]</ref>, we found our instance segmentation system is robust to the ad Architecture: We extend two existing Faster R-CNN heads <ref type="bibr" target="#b20">[15,</ref><ref type="bibr" target="#b27">22]</ref>. Left/Right panels show the heads for the ResNet C4 and FPN lts.</p><p>is evaluating using mask IoU. As in previous work <ref type="bibr" target="#b8">[3,</ref><ref type="bibr" target="#b27">22]</ref>, we train using the union of 80k train images and a 35k sub
f> to bounding-box object detection is to attend to a manageable number of candidate object regions <ref type="bibr" target="#b38">[33,</ref><ref type="bibr" target="#b21">16]</ref> and evaluate convo bibr" target="#b18">13,</ref><ref type="bibr" target="#b11">6]</ref> resorted to bottom-up segments <ref type="bibr" target="#b38">[33,</ref><ref type="bibr" target="#b7">2]</ref>. DeepMask <ref type=
ing. Various hardware <ref type="bibr" target="#b0">[2]</ref><ref type="bibr" target="#b1">[3]</ref><ref type="bibr" target="#b2">[4]</ref><ref type="bibr" target="#b9">11]</ref> are manufactured to c
t="#b14">16,</ref><ref type="bibr" target="#b15">17]</ref>.</p><p>Techniques based on visible light <ref type="bibr" target="#b42">[44]</ref> and depth imaging [1] have been proposed and commercialize
oftware-defined radios <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b11">13]</ref>, rendering them not ubiquitously applicable, or</p></div> < e noises, previous works proposed to apply linear fitting over multiple subcarriers for calibration <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b23">25]</ref>. However, this met rameters from raw CSIs.</p><p>Observing that the phase noises are linear across subcarriers, SpotFi <ref type="bibr" target="#b11">[13]</ref> proposes a ToF sanitization algorithm using linear fitting ef>, using fine-grained channel state information <ref type="bibr" target="#b41">[43]</ref>. SpotFi <ref type="bibr" target="#b11">[13]</ref> applies JADE <ref type="bibr" target="#b26">[28]</ref> alg
to the user attracts increasing research interests recently <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" targe
t="#b14">16,</ref><ref type="bibr" target="#b15">17]</ref>.</p><p>Techniques based on visible light <ref type="bibr" target="#b42">[44]</ref> and depth imaging [1] have been proposed and commercialize
3</ref>, form a binary integer program (BIP) problem, and can be solved via BIP solvers like YALMIP <ref type="bibr" target="#b16">[18]</ref>.</p><p>Since BIP is NP-complete, and signal parameters are
f><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b40">42]</ref>, millimetre wave <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b43">45]</ref> and Wi-Fi <ref typ
y linear fitting over multiple subcarriers for calibration <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b23">25]</ref>. However, this method is not applicable in passive tracking
cently. Many innovative applications have been designed, including activity and gesture recognition <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" ta
ing estimations, WiDeo <ref type="bibr" target="#b9">[11]</ref> proposes to use Hungarian algorithm <ref type="bibr" target="#b13">[15]</ref> to match parameters of adjacent estimations. However, this
p>Wi-Fi NICs measure channel discretely in time (packet), frequency (subcarrier) and space (sensor) <ref type="bibr" target="#b8">[10]</ref>. Denoting the discrete measurement at the i-th packet, j-th he air. The receiver has three antennas, which forms a uniform linear array. Linux 802.11n CSI Tool <ref type="bibr" target="#b8">[10]</ref> is installed in devices to collect CSI measurements. Device
their maximum achievable parallelism exceeds 100?. It may seem that thread-level speculation (TLS) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" ta alization: Prior TLS schemes enforce inorder commits by passing a token among ready-to-commit tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" ta eculation (TLS) schemes to parallelize sequential programs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" tar that caused the violation and all later speculative tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" tar ve multiversioning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" tar

nd ordered task commits. Swarm adapts prior eager version management and conflict detection schemes <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b78">79]</ref>, and features seve imitations discussed in Sec. 3.</p><p>Swarm's speculative execution borrows from LogTM and LogTM-SE <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" ta er versioning is common in hardware transactional memories <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b78">79]</ref>, which do not perfo a virtual time check (described below) fails, are checked against tasks in other tiles. As in LogTM <ref type="bibr" target="#b47">[48]</ref>, the L3 directory uses memory-backed sticky bits to only c
d-memory priority queues that scale with the number of cores <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b74">75]</ref>, but they do so by relaxing priority order. This restricts
ctive messages lower the cost of sending tasks among cores <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b72">73]</ref>. Hardware task schedulers such as Carbon <ref type="bibr" t
r Dijkstra's single-source shortest paths (sssp) algorithm <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>. sssp finds the shortest distance between some source node
61? at 64 cores. The software-parallel msf uses software speculation via deterministic reservations <ref type="bibr" target="#b6">[7]</ref>, and scales to 19? at 64 cores, 3.1? slower than Swarm. des:
cations, it is the only way to improve performance.</p><p>We focus on ordered irregular parallelism <ref type="bibr" target="#b54">[55]</ref>, which is often abundant but hard to exploit. Programs wit ered irregular parallelism have three main characteristics <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b54">55]</ref>. First, they consist of tasks that must follow a total or p e common in graph analytics, especially in search problems <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b54">55]</ref>. Second, they are important in simulating systems whose sta h as graph analytics, data mining, and in-memory databases <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b70">71]</ref>. Swarm hardware cou
ints, such as geo-replicated databases where transactions must appear to execute in timestamp order <ref type="bibr" target="#b13">[14]</ref>, or deterministic architectures <ref type="bibr" target="#
>. Second, they are important in simulating systems whose state evolves over time, such as circuits <ref type="bibr" target="#b46">[47]</ref>, computers <ref type="bibr" target="#b11">[12,</ref><ref t and by 57? at 64 cores. The software-parallel version uses the Chandy-Misra-Bryant (CMB) algorithm <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b66">67]</ref>. CMB exploits the
get="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" tar ful (&gt;1000?), but very large instruction windows are needed to exploit it (&gt;100K instructions <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b56">57]</ref>). Our oracle tool
" target="#b4">[5]</ref>, Superthreaded <ref type="bibr" target="#b19">[20]</ref>, Trace Processors <ref type="bibr" target="#b16">[17]</ref> [21], Speculative Multithreaded <ref type="bibr" target="#
not address the load balancing problem.</p><p>Clustering can also be applied to VLIW architectures <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b13">[14]</ref>. In this case the
he register file access time, which in turn is one of the critical delays of superscalar processors <ref type="bibr" target="#b6">[7]</ref>. This scheme performs at the same level when there is just o
ctional unit. However, since FP applications are rich in integer instructions, Palacharla and Smith <ref type="bibr" target="#b15">[16]</ref> addressed this drawback by proposing a more costeffective ead><p>The target processor microarchitecture is based on the proposal made by Palacharla and Smith <ref type="bibr" target="#b15">[16]</ref> and also investigated by Sastry, Palacharla and Smith <ref
he register file access time, which in turn is one of the critical delays of superscalar processors <ref type="bibr" target="#b6">[7]</ref>. This scheme performs at the same level when there is just o
artitioning with some run-time support to improve workload balance is the Multicluster architecture <ref type="bibr" target="#b5">[6]</ref>. In this case, the processor consists of several identical c

he register file access time, which in turn is one of the critical delays of superscalar processors <ref type="bibr" target="#b6">[7]</ref>. This scheme performs at the same level when there is just o
he register file access time, which in turn is one of the critical delays of superscalar processors <ref type="bibr" target="#b6">[7]</ref>. This scheme performs at the same level when there is just o
amples of such architectures are the Multiscalar <ref type="bibr" target="#b8">[9]</ref> [19], SPSM <ref type="bibr" target="#b4">[5]</ref>, Superthreaded <ref type="bibr" target="#b19">[20]</ref>, Tr

>(Simonyan &amp; Zisserman, 2015;</ref><ref type="bibr" target="#b44">Srivastava et al., 2015;</ref><ref type="bibr" target="#b18">He et al., 2016;</ref><ref type="bibr" target="#b21">Huang et al., 20 et (left) <ref type="bibr" target="#b34">(LeCun et al., 1998)</ref> with a 110-layer ResNet (right) <ref type="bibr" target="#b18">(He et al., 2016)</ref> on the CIFAR-100 dataset. The top row shows t e normalization techniques have enabled the development of very deep architectures, such as ResNets <ref type="bibr" target="#b18">(He et al., 2016)</ref> and DenseNets <ref type="bibr" target="#b22"> he past few years.</p><p>It is now common to see networks with hundreds, if not thousands of layers <ref type="bibr" target="#b18">(He et al., 2016;</ref><ref type="bibr" target="#b21">Huang et al., 2 ageNet models of 2015 all use an order of magnitude less weight decay than models of previous years <ref type="bibr" target="#b18">(He et al., 2016;</ref><ref type="bibr" target="#b41">Simonyan &amp;


al., 2001)</ref>. It is also referred to as the cross entropy loss in the context of deep learning <ref type="bibr" target="#b7">(Bengio et al., 2015)</ref>. Given a probabilistic model π(Y |X) and n
al., 2001)</ref>. It is also referred to as the cross entropy loss in the context of deep learning <ref type="bibr" target="#b7">(Bengio et al., 2015)</ref>. Given a probabilistic model π(Y |X) and n

al., 2001)</ref>. It is also referred to as the cross entropy loss in the context of deep learning <ref type="bibr" target="#b7">(Bengio et al., 2015)</ref>. Given a probabilistic model π(Y |X) and n

architectures, such as ResNets <ref type="bibr" target="#b18">(He et al., 2016)</ref> and DenseNets <ref type="bibr" target="#b22">(Huang et al., 2017)</ref>. It has been shown that Batch Normalizatio
n for object detection <ref type="bibr" target="#b27">(Kendall &amp; Cipolla, 2016)</ref>.</p><p>In <ref type="bibr" target="#b38">2005</ref><ref type="bibr" target="#b38">, Niculescu-Mizil &amp; Caru
r" target="#b44">Srivastava et al., 2015;</ref><ref type="bibr" target="#b18">He et al., 2016;</ref><ref type="bibr" target="#b21">Huang et al., 2016;</ref><ref type="bibr" target="#b4">2017)</ref>. A rks with hundreds, if not thousands of layers <ref type="bibr" target="#b18">(He et al., 2016;</ref><ref type="bibr" target="#b21">Huang et al., 2016)</ref> and hundreds of convolutional filters per l
zation (RCPO) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b5">[6]</ref> or devirtualizati ted languages <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" ircle class at runtime, the compiler can convert the indirect call to multiple guarded direct calls <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bib
">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b5">[6]</ref> or devirtualization <ref type="bibr" target="#b28">[28]</ref ">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b28">[28]</ref>. Ishizaki et al. direct calls <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b5">[6]</ref>, as shown in Fig. <ref type="figure" target="#fig_0">11b</re tion call and its devirtualized form. usually has higher accuracy than an indirect branch predictor <ref type="bibr" target="#b5">[6]</ref>. However, not all indirect calls can be converted to multipl form RCPO, the following conditions need to be fulfilled <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b5">[6]</ref>:</p><p>1. The number of frequent target addresses from a cal
f type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b5">[6]</ref> or devirtualization <ref type="bibr" target="#b28">[28]</ref>. This optimization statically converts an indirect branch ly a subset of indirect branches with a limited number of targets that can be determined statically <ref type="bibr" target="#b28">[28]</ref>. Our proposed VPC prediction mechanism provides the benefi 24">[24]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b28">[28]</ref>. Ishizaki et al. <ref type="bibr" target="#b28">[28]</ref> <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b28">[28]</ref>. Ishizaki et al. <ref type="bibr" target="#b28">[28]</ref> classify the devirtualization techniques into guarded devi on can overcome this limitation, but it requires an expensive mechanism called on-stack replacement <ref type="bibr" target="#b28">[28]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head tion based on static analysis requires type analysis, which in turn requires whole program analysis <ref type="bibr" target="#b28">[28]</ref>, and unsafe languages like Cþþ also require pointer alias or large applications. Due to the limited applicability of static devirtualization, Ishizaki et al. <ref type="bibr" target="#b28">[28]</ref> report only an average 40 percent reduction in the number et="#b23">[23]</ref>, and type feedback/devirtualization <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b28">[28]</ref>. As we show in Section 6, the benefit of devirtualization
on, called receiver class prediction optimization (RCPO) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bib ll with direct method calls in object-oriented languages <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bib morphic inline caches <ref type="bibr" target="#b23">[23]</ref>, and type feedback/devirtualization <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b28">[28]</ref>. As we show in
ation that the control-flow path leading to a branch is correlated with the direction of the branch <ref type="bibr" target="#b15">[16]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
mlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL METHODOLOGY</head><p>We use a Pin-based <ref type="bibr" target="#b38">[38]</ref> cycle-accurate x86 simulator to evaluate VPC prediction. T DNA <ref type="bibr" target="#b2">[3]</ref> is a dynamic binary instrumentation tool similar to Pin <ref type="bibr" target="#b38">[38]</ref>, but capable of tracing Java virtual machines. The DaCapo
table">3</ref> provides a brief description of the other two Cþþ benchmarks.</p><p>We use Pinpoints <ref type="bibr" target="#b45">[45]</ref> to select a representative simulation region for each benc
d that the performance benefit of supporting multiple predictions per cycle is not significant (see <ref type="bibr" target="#b35">[35,</ref><ref type="bibr">Section 5.4]</ref>).</p></div> <div xmlns= re already optimized with compiler-based devirtualization improves performance by 11.5 percent (see <ref type="bibr" target="#b35">[35,</ref><ref type="bibr">Section 6.3]</ref> for a detailed analysis xtensive performance characterization of VPC prediction on C/Cþþ applications in our previous paper <ref type="bibr" target="#b35">[35]</ref>. In particular, in <ref type="bibr" target="#b35">[35]</re Cþþ applications in our previous paper <ref type="bibr" target="#b35">[35]</ref>. In particular, in <ref type="bibr" target="#b35">[35]</ref>, we provided the characteristics of indirect branch target mpact the prediction accuracy of conditional branches in the benchmark set we examined, as shown in <ref type="bibr" target="#b35">[35]</ref>. Fig. <ref type="figure" target="#fig_8">7</ref> shows the branches can lead to mispredictions. We analyzed the effects of such contention and interference in <ref type="bibr" target="#b35">[35]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head igh in perlbmk, perlbench, and sjeng (shown in Fig. <ref type="figure" target="#fig_12">5</ref> and <ref type="bibr" target="#b35">[35]</ref>), the contention for virtual branch slots in the BTB is hi ion is that it enables compile-time code optimizations. However, as we showed in our previous paper <ref type="bibr" target="#b35">[35]</ref>, the two techniques can be used in combination and VPC pre versus on traditional C/Cþþ programs (which were evaluated briefly in Section 5 and extensively in <ref type="bibr" target="#b35">[35]</ref>).</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head ivalent to that provided by a 3-6 KB TTC predictor (similarly to the results for C/Cþþ applications <ref type="bibr" target="#b35">[35]</ref>). 12 Fig. <ref type="figure" target="#fig_14">16</ref> com the same performance as VPC prediction is smaller for Java applications than for C/Cþþ applications <ref type="bibr" target="#b35">[35]</ref>. In other words, TTC and cascaded predictors are relativel C prediction performance on Java applications. Similarly to what we observed for C/Cþþ applications <ref type="bibr" target="#b35">[35]</ref>,</p><p>VPC prediction provides higher performance improvem plications by 6.3 percent whereas it improves the performance of C/Cþþ applications by 18.5 percent <ref type="bibr" target="#b35">[35]</ref>. As Java applications have very large indirect branch and or, 4K-entry BTB, and 200-cycle memory latency. Similarly to our observation for C/Cþþ applications <ref type="bibr" target="#b35">[35]</ref>, since the less aggressive processor incurs a smaller pena logy Program of the Texas Higher Education Coordinating Board. This paper is an extended version of <ref type="bibr" target="#b35">[35]</ref>.</p></div> 			</div>  			<div type="annex"> <div xmlns="ht WORK</head><p>We have already discussed related work on indirect branch prediction in Section 2.2. <ref type="bibr" target="#b35">[35]</ref>, and Sections 5, 6, and 7 provide extensive comparisons of
ct. Therefore, other techniques like dynamic predication <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b30">[30]</ref> might be needed to complement VPC prediction to further re her related work in handling indirect branches.</p><p>We <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b30">[30]</ref> recently proposed handling hard-to-predict indirect branch
tten in modern high-level languages such as Java, Cþþ, and C#. These languages support polymorphism <ref type="bibr" target="#b7">[8]</ref>, which significantly eases the development and maintenance o
va applications are more difficult to predict. Therefore, other techniques like dynamic predication <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b30">[30]</ref> might be needed ch larger size. Here, we briefly discuss other related work in handling indirect branches.</p><p>We <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b30">[30]</ref> recently propos
mlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL METHODOLOGY</head><p>We use a Pin-based <ref type="bibr" target="#b38">[38]</ref> cycle-accurate x86 simulator to evaluate VPC prediction. T DNA <ref type="bibr" target="#b2">[3]</ref> is a dynamic binary instrumentation tool similar to Pin <ref type="bibr" target="#b38">[38]</ref>, but capable of tracing Java virtual machines. The DaCapo
">[33]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b47">[47]</ref> require large hardware resources to store the target addre ional muxes used to select the predicted target address.</p><p>In a recent work, Seznec and Michaud <ref type="bibr" target="#b47">[47]</ref> proposed extending their TAGE conditional branch predictor ">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b47">[47]</ref>, which further increases the overall complexity and develo
direct branches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b34">[34]</ref>. The BTB implicitly-and usually inaccurately-assumes that
gic (which includes conditional/indirect predictors and the BTB). We used the Wattch infrastructure <ref type="bibr" target="#b4">[5]</ref> to measure power/energy consumption, faithfully modeling eve
Previously proposed indirect branch prediction techniques <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bib ction accuracy <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[33]</ref>.</p><p>Chang et ed on improving the prediction accuracy by enhancing the indexing functions of two-level predictors <ref type="bibr" target="#b11">[12]</ref> and by combining multiple indirect branch predictors using
duced if the processor already supports the prediction of multiple conditional branches in parallel <ref type="bibr" target="#b51">[51]</ref>. The prediction logic can perform the calculation of VPCA
e method cache in Smalltalk-80 <ref type="bibr" target="#b10">[11]</ref>, polymorphic inline caches <ref type="bibr" target="#b23">[23]</ref>, and type feedback/devirtualization <ref type="bibr" targe
d that the performance benefit of supporting multiple predictions per cycle is not significant (see <ref type="bibr" target="#b35">[35,</ref><ref type="bibr">Section 5.4]</ref>).</p></div> <div xmlns= re already optimized with compiler-based devirtualization improves performance by 11.5 percent (see <ref type="bibr" target="#b35">[35,</ref><ref type="bibr">Section 6.3]</ref> for a detailed analysis xtensive performance characterization of VPC prediction on C/Cþþ applications in our previous paper <ref type="bibr" target="#b35">[35]</ref>. In particular, in <ref type="bibr" target="#b35">[35]</re Cþþ applications in our previous paper <ref type="bibr" target="#b35">[35]</ref>. In particular, in <ref type="bibr" target="#b35">[35]</ref>, we provided the characteristics of indirect branch target mpact the prediction accuracy of conditional branches in the benchmark set we examined, as shown in <ref type="bibr" target="#b35">[35]</ref>. Fig. <ref type="figure" target="#fig_8">7</ref> shows the branches can lead to mispredictions. We analyzed the effects of such contention and interference in <ref type="bibr" target="#b35">[35]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head igh in perlbmk, perlbench, and sjeng (shown in Fig. <ref type="figure" target="#fig_12">5</ref> and <ref type="bibr" target="#b35">[35]</ref>), the contention for virtual branch slots in the BTB is hi ion is that it enables compile-time code optimizations. However, as we showed in our previous paper <ref type="bibr" target="#b35">[35]</ref>, the two techniques can be used in combination and VPC pre versus on traditional C/Cþþ programs (which were evaluated briefly in Section 5 and extensively in <ref type="bibr" target="#b35">[35]</ref>).</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head ivalent to that provided by a 3-6 KB TTC predictor (similarly to the results for C/Cþþ applications <ref type="bibr" target="#b35">[35]</ref>). 12 Fig. <ref type="figure" target="#fig_14">16</ref> com the same performance as VPC prediction is smaller for Java applications than for C/Cþþ applications <ref type="bibr" target="#b35">[35]</ref>. In other words, TTC and cascaded predictors are relativel C prediction performance on Java applications. Similarly to what we observed for C/Cþþ applications <ref type="bibr" target="#b35">[35]</ref>,</p><p>VPC prediction provides higher performance improvem plications by 6.3 percent whereas it improves the performance of C/Cþþ applications by 18.5 percent <ref type="bibr" target="#b35">[35]</ref>. As Java applications have very large indirect branch and or, 4K-entry BTB, and 200-cycle memory latency. Similarly to our observation for C/Cþþ applications <ref type="bibr" target="#b35">[35]</ref>, since the less aggressive processor incurs a smaller pena logy Program of the Texas Higher Education Coordinating Board. This paper is an extended version of <ref type="bibr" target="#b35">[35]</ref>.</p></div> 			</div>  			<div type="annex"> <div xmlns="ht WORK</head><p>We have already discussed related work on indirect branch prediction in Section 2.2. <ref type="bibr" target="#b35">[35]</ref>, and Sections 5, 6, and 7 provide extensive comparisons of


orithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bib s the substitution of an indirect method call with direct method calls in object-oriented languages <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bib nce impact due to virtual function calls. These approaches include the method cache in Smalltalk-80 <ref type="bibr" target="#b10">[11]</ref>, polymorphic inline caches <ref type="bibr" target="#b23">
direct branches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b34">[34]</ref>. The BTB implicitly-and usually inaccurately-assumes that
va applications are more difficult to predict. Therefore, other techniques like dynamic predication <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b30">[30]</ref> might be needed ch larger size. Here, we briefly discuss other related work in handling indirect branches.</p><p>We <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b30">[30]</ref> recently propos
ediction. There are two types of indirect branch predictors: history based and precomputation based <ref type="bibr" target="#b46">[46]</ref>. The technique we introduce in this paper utilizes history ormance impact of indirect branches that are hard to predict with VPC prediction.</p><p>Roth et al. <ref type="bibr" target="#b46">[46]</ref> proposed dependence-based precomputation, which precompute
orithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bib s the substitution of an indirect method call with direct method calls in object-oriented languages <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bib nce impact due to virtual function calls. These approaches include the method cache in Smalltalk-80 <ref type="bibr" target="#b10">[11]</ref>, polymorphic inline caches <ref type="bibr" target="#b23">
gic (which includes conditional/indirect predictors and the BTB). We used the Wattch infrastructure <ref type="bibr" target="#b4">[5]</ref> to measure power/energy consumption, faithfully modeling eve
orithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bib s the substitution of an indirect method call with direct method calls in object-oriented languages <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bib nce impact due to virtual function calls. These approaches include the method cache in Smalltalk-80 <ref type="bibr" target="#b10">[11]</ref>, polymorphic inline caches <ref type="bibr" target="#b23">
ct. Therefore, other techniques like dynamic predication <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b30">[30]</ref> might be needed to complement VPC prediction to further re her related work in handling indirect branches.</p><p>We <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b30">[30]</ref> recently proposed handling hard-to-predict indirect branch
tten in modern high-level languages such as Java, Cþþ, and C#. These languages support polymorphism <ref type="bibr" target="#b7">[8]</ref>, which significantly eases the development and maintenance o
">[30]</ref> recently proposed handling hard-to-predict indirect branches using dynamic predication <ref type="bibr" target="#b36">[36]</ref>. In this technique, if the target address of an indirect b ruction set architecture, and significant hardware support for dynamic predication (as described in <ref type="bibr" target="#b36">[36]</ref>). However, the two approaches can be combined and used tog

xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Methodology</head><p>We have built an iDNA-based <ref type="bibr" target="#b2">[3]</ref> cycle-accurate x86 simulator to evaluate VPC prediction on J perimental branch, but not in the main branch <ref type="bibr" target="#b44">[44]</ref>.</p><p>iDNA <ref type="bibr" target="#b2">[3]</ref> is a dynamic binary instrumentation tool similar to Pin <ref

xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Methodology</head><p>We have built an iDNA-based <ref type="bibr" target="#b2">[3]</ref> cycle-accurate x86 simulator to evaluate VPC prediction on J perimental branch, but not in the main branch <ref type="bibr" target="#b44">[44]</ref>.</p><p>iDNA <ref type="bibr" target="#b2">[3]</ref> is a dynamic binary instrumentation tool similar to Pin <ref
ct. Therefore, other techniques like dynamic predication <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b30">[30]</ref> might be needed to complement VPC prediction to further re her related work in handling indirect branches.</p><p>We <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b30">[30]</ref> recently proposed handling hard-to-predict indirect branch
gic (which includes conditional/indirect predictors and the BTB). We used the Wattch infrastructure <ref type="bibr" target="#b4">[5]</ref> to measure power/energy consumption, faithfully modeling eve
gic (which includes conditional/indirect predictors and the BTB). We used the Wattch infrastructure <ref type="bibr" target="#b4">[5]</ref> to measure power/energy consumption, faithfully modeling eve
ediction. There are two types of indirect branch predictors: history based and precomputation based <ref type="bibr" target="#b46">[46]</ref>. The technique we introduce in this paper utilizes history ormance impact of indirect branches that are hard to predict with VPC prediction.</p><p>Roth et al. <ref type="bibr" target="#b46">[46]</ref> proposed dependence-based precomputation, which precompute
gic (which includes conditional/indirect predictors and the BTB). We used the Wattch infrastructure <ref type="bibr" target="#b4">[5]</ref> to measure power/energy consumption, faithfully modeling eve
y small-scale desktop applications.</p><p>Previously proposed indirect branch prediction techniques <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr equires only 2,048 bits but a 1,024-entry gsharelike indirect branch predictor (tagged target cache <ref type="bibr" target="#b9">[10]</ref>) needs at least 2,048 bytes along with additional tag stora hat the indirect branch will jump to the same target address it jumped to in its previous execution <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[33]</ref>. 3 To our knowle rget addresses of many indirect branches alternate rather than stay stable for long periods of time <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[33]</ref>.</p><p>4. Since ow path leading to an indirect branch is strongly correlated with the target of the indirect branch <ref type="bibr" target="#b9">[10]</ref>. This is very similar to modern conditional branch predicto predictor has low (about 50 percent) prediction accuracy <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[33]</ref>.</p><p>Chang et al. <ref type="bibr" target="#b9">[10]</ref> first proposed to use branch history information to disting 5">15</ref> compares the performance of VPC prediction with the tagged target cache (TTC) predictor <ref type="bibr" target="#b9">[10]</ref>. On average, VPC prediction provides performance improvemen
y small-scale desktop applications.</p><p>Previously proposed indirect branch prediction techniques <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr equires only 2,048 bits but a 1,024-entry gsharelike indirect branch predictor (tagged target cache <ref type="bibr" target="#b9">[10]</ref>) needs at least 2,048 bytes along with additional tag stora hat the indirect branch will jump to the same target address it jumped to in its previous execution <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[33]</ref>. 3 To our knowle rget addresses of many indirect branches alternate rather than stay stable for long periods of time <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[33]</ref>.</p><p>4. Since ow path leading to an indirect branch is strongly correlated with the target of the indirect branch <ref type="bibr" target="#b9">[10]</ref>. This is very similar to modern conditional branch predicto predictor has low (about 50 percent) prediction accuracy <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[33]</ref>.</p><p>Chang et al. <ref type="bibr" target="#b9">[10]</ref> first proposed to use branch history information to disting 5">15</ref> compares the performance of VPC prediction with the tagged target cache (TTC) predictor <ref type="bibr" target="#b9">[10]</ref>. On average, VPC prediction provides performance improvemen
Previously proposed indirect branch prediction techniques <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bib ction accuracy <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[33]</ref>.</p><p>Chang et ed on improving the prediction accuracy by enhancing the indexing functions of two-level predictors <ref type="bibr" target="#b11">[12]</ref> and by combining multiple indirect branch predictors using
ion techniques <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bib ame target address it jumped to in its previous execution <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[33]</ref>. 3 To our knowledge, only Intel Pentium M and AMD Barcelon ndirect branch predictor likely requires even more hardware resources to store the target addresses <ref type="bibr" target="#b33">[33]</ref>.</p><p>3. Previous research has shown that the prediction lternate rather than stay stable for long periods of time <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[33]</ref>.</p><p>4. Since we are using the outcomes of the existing 7">[37]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[33]</ref>.</p><p>Chang et al. <ref type="bibr" target="#b9">[10]</re ictor performed the best for a particular set of C and Cþþ benchmarks.</p><p>Kalamatianos and Kaeli <ref type="bibr" target="#b33">[33]</ref> proposed predicting indirect branches via data compression s complicated <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b47">[47]</ref>, which further
to improve performance. A bypass and insertion algorithm for exclusive LLCs was presented recently <ref type="bibr" target="#b7">[7]</ref>. It classifies blocks based on their access number in the L2

f the three Optimal Bypass Assertions, they are not as accurate as OBM.</p><p>Others: Victim caches <ref type="bibr" target="#b16">[16]</ref> use a small fully-associative buffer to improve direct-ass
ef><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b33">33]</ref> and address based <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" ta
="#b37">[37]</ref> combines PC based and address based methods to improve performance. Annex caches <ref type="bibr" target="#b14">[14]</ref> and PCC <ref type="bibr" target="#b34">[34]</ref> filter n
get="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" tar assifies blocks based on their access number in the L2 cache and the hit number in the LLC. NUcache <ref type="bibr" target="#b23">[23]</ref> dedicates a part of LLC to keep distant reuse blocks. Only
<ref type="bibr" target="#b12">[12]</ref>, SDBP <ref type="bibr" target="#b19">[19]</ref>, and DSB <ref type="bibr" target="#b6">[6]</ref>, OBM with NRU has superior performance while requiring less , and dueling segmented LRU with adaptive bypassing (DSB)<ref type="foot" target="#foot_3">4</ref>  <ref type="bibr" target="#b6">[6]</ref>. While the MPKI of NRU is similar to that of LRU, NRU+OBM re have shown, many blocks among them are actually useful and should be retained in caches.</p><p>DSB <ref type="bibr" target="#b6">[6]</ref> records the incoming block and the victim block for each set ey study the characteristic of accesses using metrics which are either similar to Assertion 1 and 2 <ref type="bibr" target="#b6">[6]</ref>, or similar to Assertion 1 and 3 <ref type="bibr" target="#b
="#b37">[37]</ref> combines PC based and address based methods to improve performance. Annex caches <ref type="bibr" target="#b14">[14]</ref> and PCC <ref type="bibr" target="#b34">[34]</ref> filter n
get="#b21">21,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37]</ref>, the shortened PC is delivered along with the request throu Assertion 1 and 3 <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37]</ref>. However, since such proposals do not assert the behavior o b15">15,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref>. LRF <ref type="bibr" target="#b37">[37]</ref> combines PC based and address based methods to improve per
and address based <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref>. LRF <ref type="bib
placement policies <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b38">38]</ref> attempt to insert distant reuse blocks into the LRU positio
as exponential service times, infinite buffers, and so on <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Likewise, analys et="#b6">[7]</ref> and hypercubes <ref type="bibr" target="#b22">[24]</ref>. The study presented in <ref type="bibr" target="#b8">[9]</ref> is not restricted to a particular topology, but it assumes a where T Bus is the service time of the Bus architecture and C Bus is the contention matrix given in <ref type="bibr" target="#b8">(9)</ref>. Finally, we note that when each buffer shown in Fig. <ref t
hich consist of a large number of heterogeneous components <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" ute C 2 with C 1 +δ, in lines (2) and (3). Next, we observe that (T C 2 ) n −(T C 1 ) n = n in line <ref type="bibr" target="#b3">(4)</ref>. Finally, we note that all the matrices in line ( <ref type=

compute the delay experienced at the intermediate routers, in our experimental work, we first apply <ref type="bibr" target="#b15">(17)</ref> by going through all the flows x sd and, following the rou ing time due to blocking) are computed using ( <ref type="formula" target="#formula_0">1</ref>) and <ref type="bibr" target="#b15">(17)</ref>. In order to find the average packet latency, we follow th packets blocked across several routers, similar to the recursive computation approach presented in <ref type="bibr" target="#b15">[17]</ref>. Extensions of our current framework for dealing with perf
uffers, and so on <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Likewise, analysis of power consumption of the NoC archit analysis techniques that can be used in optimization loops are extremely important. The authors in <ref type="bibr" target="#b12">[13]</ref> consider the buffer sizing problem and present a performan , network architecture synthesis <ref type="bibr" target="#b21">[23]</ref>, buffer space allocation <ref type="bibr" target="#b12">[13]</ref>) for fast and accurate performance estimations.</p></div>
er loop in Fig. <ref type="figure">1</ref>).</p><p>We note that for traffic with guaranteed service <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b19">[21]</ref>, accurate perform
eneous components <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr">[15]</ref>. NoCs target single chip mult
uffers, and so on <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Likewise, analysis of power consumption of the NoC archit analysis techniques that can be used in optimization loops are extremely important. The authors in <ref type="bibr" target="#b12">[13]</ref> consider the buffer sizing problem and present a performan , network architecture synthesis <ref type="bibr" target="#b21">[23]</ref>, buffer space allocation <ref type="bibr" target="#b12">[13]</ref>) for fast and accurate performance estimations.</p></div>
uffers, and so on <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Likewise, analysis of power consumption of the NoC archit analysis techniques that can be used in optimization loops are extremely important. The authors in <ref type="bibr" target="#b12">[13]</ref> consider the buffer sizing problem and present a performan , network architecture synthesis <ref type="bibr" target="#b21">[23]</ref>, buffer space allocation <ref type="bibr" target="#b12">[13]</ref>) for fast and accurate performance estimations.</p></div>

uffers, and so on <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Likewise, analysis of power consumption of the NoC archit analysis techniques that can be used in optimization loops are extremely important. The authors in <ref type="bibr" target="#b12">[13]</ref> consider the buffer sizing problem and present a performan , network architecture synthesis <ref type="bibr" target="#b21">[23]</ref>, buffer space allocation <ref type="bibr" target="#b12">[13]</ref>) for fast and accurate performance estimations.</p></div>
onal Networks (RGCNs) have been proposed as an extension of GCNs to the domain of relational graphs <ref type="bibr" target="#b28">(Schlichtkrull et al., 2018)</ref>. This model has achieved impressiv he kernels was beneficial for generalisation, although it comes at the cost of increased model bias <ref type="bibr" target="#b28">(Schlichtkrull et al., 2018)</ref>. We follow this approach, decompos j, 2015)</ref>, RDF2Vec <ref type="bibr" target="#b25">(Ristoski and Paulheim, 2016)</ref> and RGCN <ref type="bibr" target="#b28">(Schlichtkrull et al., 2018)</ref>, and (mean and standard deviation </p><p>We note that RGCN consistently outperforms RGAT on MUTAG, contrary to what might be expected <ref type="bibr" target="#b28">(Schlichtkrull et al., 2018)</ref>. The result is surprising given th j, 2015)</ref>, RDF2Vec <ref type="bibr" target="#b25">(Ristoski and Paulheim, 2016)</ref> and RGCN <ref type="bibr" target="#b28">(Schlichtkrull et al., 2018)</ref>, and (mean and standard deviation target="#b33">Veli?kovi? et al. (2017)</ref>, extending to the relational setting, using ideas from <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref>.</p></div> <div xmlns="http://www.t diate representations Different relations convey distinct pieces of information. The update rule of <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref> made this manifest by assigning eac uation ( <ref type="formula" target="#formula_8">7</ref>) with the neighborhood aggregation step of <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref> gives</p><formula xml:id="formula_9 ><p>We evaluate the models on transductive and inductive tasks. Following the experimental setup of <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref> for the transductive tasks, we eval ly.</p><p>Transductive baselines We consider as a baseline the recent state-of-the-art results from <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref> obtained with a two-layer RGCN mode conclusions, we compare against our own implementation of RGCN rather than the results reported in <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref>; <ref type="bibr" target="#b35">Wu evaluate RGAT on MUTAG and AIFB. With additive attention, WIRGAT outperforms ARGAT, consistent with <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref>. Interestingly, when employing mult
dden units and basis function decomposition. We also include the same challenging baselines of FEAT <ref type="bibr" target="#b23">(Paulheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29 2: (a) Entity classification results accuracy (mean and standard deviation over 10 seeds) for FEAT <ref type="bibr" target="#b23">(Paulheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29 <ref type="table">2</ref> we present a chart representation in Figure <ref type="figure">4</ref>.  <ref type="bibr" target="#b23">(Paulheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29
dden units and basis function decomposition. We also include the same challenging baselines of FEAT <ref type="bibr" target="#b23">(Paulheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29 2: (a) Entity classification results accuracy (mean and standard deviation over 10 seeds) for FEAT <ref type="bibr" target="#b23">(Paulheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29 <ref type="table">2</ref> we present a chart representation in Figure <ref type="figure">4</ref>.  <ref type="bibr" target="#b23">(Paulheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29
rget="#b26">Rustamov and Guibas, 2013;</ref><ref type="bibr" target="#b31">Szlam et al., 2005;</ref><ref type="bibr" target="#b11">Gavish et al., 2010)</ref>.</p><p>A recent approach that began with G
ref>. Incorporating gating mechanisms led to the development of Gated Graph Neural Networks (GGNNs) <ref type="bibr" target="#b27">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b0">Allamanis
ref>. Incorporating gating mechanisms led to the development of Gated Graph Neural Networks (GGNNs) <ref type="bibr" target="#b27">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b0">Allamanis
image captioning <ref type="bibr" target="#b8">(Donahue et al., 2017)</ref> and classifying videos <ref type="bibr" target="#b15">(Karpathy et al., 2014)</ref>. CNNs are successful because they assum
ermine if any of our model comparisons are significant, we employ the one-sided Mann-Whitney U test <ref type="bibr" target="#b20">Mann and Whitney (1947)</ref> as we are interested in the direction o
</ref>, deep bypass multitask networks <ref type="bibr" target="#b35">Wu et al. (2018)</ref>, Weave <ref type="bibr" target="#b16">Kearnes et al. (2016)</ref>, and a RGCN model whose relational struct Ramsundar et al., 2015)</ref>, Bypass <ref type="bibr" target="#b35">(Wu et al., 2018)</ref>, Weave <ref type="bibr" target="#b16">(Kearnes et al., 2016)</ref>, RGCN <ref type="bibr" target="#b1">(Alt Ramsundar et al., 2015)</ref>, Bypass <ref type="bibr" target="#b35">(Wu et al., 2018)</ref>, Weave <ref type="bibr" target="#b16">(Kearnes et al., 2016)</ref> </p></div> <div xmlns="http://www.tei-c. Ramsundar et al., 2015)</ref>, Bypass <ref type="bibr" target="#b35">(Wu et al., 2018)</ref>, Weave <ref type="bibr" target="#b16">(Kearnes et al., 2016)</ref> </p></div> <div xmlns="http://www.tei-c.
NNs) successfully solve a variety of tasks in Euclidean grid-like domains, such as image captioning <ref type="bibr" target="#b8">(Donahue et al., 2017)</ref> and classifying videos <ref type="bibr" t
lheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b6">de Vries and de Rooij, 2015)</ref> and RDF2Vec <ref type="bibr" target lheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b6">de Vries and de Rooij, 2015)</ref>, RDF2Vec <ref type="bibr" target="# lheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b6">de Vries and de Rooij, 2015)</ref>, RDF2Vec <ref type="bibr" target="#
ion in detail and then extrapolate to the entire execution <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>. A major challenge in sampled evaluation however is to quic is transferable across both hardware and software changes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>. Functional warming  warms up the microarchitecture state b es not allow for software changes. Functional warming (FW) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref> does not incur any storage overhead, allows for software ch ect number of representative detailed regions that are evaluated in detail to then extrapolate from <ref type="bibr" target="#b33">[34]</ref>. The key challenge in sampling is to get (i) the correct a unctional fast-forwarding, checkpointing and virtualized fastforwarding. Functional fast-forwarding <ref type="bibr" target="#b33">[34]</ref> leverages functional simulation to get to the next represe a detailed warm-up using a small number of instructions (e.g., 30,000) prior to the detailed region <ref type="bibr" target="#b33">[34]</ref>. With this small amount of warming, only a small part of t instructions. Prior research shows that the highest accuracy is achieved for small detailed regions <ref type="bibr" target="#b33">[34]</ref>; larger detailed regions will likely make DeLorean even mo d to keep the caches warm using functional simulation in-between detailed regions as done in SMARTS <ref type="bibr" target="#b33">[34]</ref>. • CoolSim: Randomized Statistical Warming (RSW) is employ ture state using all memory references between two consecutive detailed regions, which is very slow <ref type="bibr" target="#b33">[34]</ref>. Various approaches have been proposed to reduce the warm-
d does not incur any storage overhead and is transferable across both hardware and software changes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>. Functional warmin t, requires huge storage overhead, and does not allow for software changes. Functional warming (FW) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref> does not incur any on, which is fast but does not allow for changes to the software. Virtualized Fast-Forwarding (VFF) <ref type="bibr" target="#b25">[26]</ref> leverages hardware virtualization to quickly get to the ne 31]</ref> extend on the concept of BLRL using a form of hardware state checkpoints. Sandberg et al. <ref type="bibr" target="#b25">[26]</ref> propose a method that uses two parallel simulations, pessi
defined as the number of unique references (cachelines) between two accesses to the same cacheline <ref type="bibr" target="#b19">[20]</ref>. Stack distance allows for accurate modeling of fully-asso
e distributions can be used to statistically model caches considered caches with random replacement <ref type="bibr" target="#b5">[6]</ref>. Follow-on work demonstrates similar accuracy for LRU caches
each detailed region. Haskins and Skadron <ref type="bibr" target="#b11">[12]</ref> and Luo et al. <ref type="bibr" target="#b18">[19]</ref> use heuristics to find the minimum number of instructions
defined as the number of unique references (cachelines) between two accesses to the same cacheline <ref type="bibr" target="#b19">[20]</ref>. Stack distance allows for accurate modeling of fully-asso
have been proposed to reduce the warm-up length prior to each detailed region. Haskins and Skadron <ref type="bibr" target="#b11">[12]</ref> and Luo et al. <ref type="bibr" target="#b18">[19]</ref> u
imulates the cache for every memory access within the warm-up interval prior to the detailed region <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. More recently, Rand es a large enough cumulative distribution of MRRLs is used as the warming interval. Eeckhout et al. <ref type="bibr" target="#b8">[9]</ref> introduce the Boundary Line Reuse Latency (BLRL) which exten
of the proposed methods have to inspect all memory accesses and measure stack distance. Other works <ref type="bibr" target="#b28">[29]</ref> have proposed hardware-accelerated stack distance collecti
tance distribution -can be used to model other replacement algorithms as well. Beckmann and Sanchez <ref type="bibr" target="#b1">[2]</ref> propose probabilistic methods to model age-based replacement
valuate (a) small region(s) of the execution in detail and then extrapolate to the entire execution <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>. A major challenge pproach called memory hierarchy state (MHS) to minimize checkpoint size, in the context of SimPoint <ref type="bibr" target="#b27">[28]</ref>. Barr et al. <ref type="bibr" target="#b0">[1]</ref> propo
28">[29]</ref> have proposed hardware-accelerated stack distance collection. Liu and Mellor-Crummey <ref type="bibr" target="#b16">[17]</ref> use a technique based on shadow profiling that forks off a n application, instrumented by Pin, to measure the stack distances for a selected set of references <ref type="bibr" target="#b16">[17]</ref>.</p><p>A major limitation of stack distance analysis is th
imulates the cache for every memory access within the warm-up interval prior to the detailed region <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. More recently, Rand es a large enough cumulative distribution of MRRLs is used as the warming interval. Eeckhout et al. <ref type="bibr" target="#b8">[9]</ref> introduce the Boundary Line Reuse Latency (BLRL) which exten
he same cacheline. These accesses are typically handled as Miss Status Holding Register (MSHR) hits <ref type="bibr" target="#b2">[3]</ref>. DSW models MSHR hits as a cache hit (in case of cache simul mlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b2">3</ref>, warm-up cost can be amortized across multiple parallel simula
urately approximated by tracking a subset of randomly selected reuse distances and memory locations <ref type="bibr" target="#b4">[5]</ref>. Finally, statistical cache modeling has been generalized an ations and computes their reuses during the warm-up interval prior to a detailed region. Prior work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> uses watchpoints to
e distributions can be used to statistically model caches considered caches with random replacement <ref type="bibr" target="#b5">[6]</ref>. Follow-on work demonstrates similar accuracy for LRU caches
he same cacheline. These accesses are typically handled as Miss Status Holding Register (MSHR) hits <ref type="bibr" target="#b2">[3]</ref>. DSW models MSHR hits as a cache hit (in case of cache simul mlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b2">3</ref>, warm-up cost can be amortized across multiple parallel simula
every access and collect stack distance information. To improve performance, researchers use k-ary <ref type="bibr" target="#b3">[4]</ref> and AVL <ref type="bibr" target="#b23">[24]</ref> trees inst
tance distribution -can be used to model other replacement algorithms as well. Beckmann and Sanchez <ref type="bibr" target="#b1">[2]</ref> propose probabilistic methods to model age-based replacement
same memory location), see Figure <ref type="figure" target="#fig_1">1</ref>(b). Statistical models <ref type="bibr" target="#b10">[11]</ref> transform reuse distances into stack distances (i.e., numb every memory access between the two consecutive accesses to the same cacheline. Eklov and Hagersten <ref type="bibr" target="#b10">[11]</ref> provide a solution by showing that stack distance can be a tances and stack distance distributions, respectively, using the well-established statistical model <ref type="bibr" target="#b10">[11]</ref> as previously described in Section 2.2. If the stack dista ref type="bibr" target="#b5">[6]</ref>. Follow-on work demonstrates similar accuracy for LRU caches <ref type="bibr" target="#b10">[11]</ref>. More recent work extends statistical cache modeling to co same cacheline need to be inspected to compute the number of unique cachelines. Eklov and Hagersten <ref type="bibr" target="#b10">[11]</ref> demonstrate how reuse distance, which is computationally l
tation and profiling using tools such as Pin <ref type="bibr" target="#b17">[18]</ref> and Valgrind <ref type="bibr" target="#b20">[21]</ref> are useful to analyze a workload's characteristics, e.g.,
</ref> and reduce the space requirements for each checkpoint down to 142 KiB. Van Biesbrouck et al. <ref type="bibr" target="#b29">[30]</ref> propose a similar approach called memory hierarchy state (
the context of deep neural networks, and there is now a quickly growing body of work on this topic <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" targ e, the ∞ -ball around x has recently been studied as a natural notion for adversarial perturbations <ref type="bibr" target="#b10">[11]</ref>. While we focus on robustness against ∞ -bounded attacks i s. On the attack side, prior work has proposed methods such as the Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b10">[11]</ref> and multiple variations of it <ref type="bibr" target="#b1 rget="#b2">[3]</ref> for an overview of earlier work).</p><p>Adversarial training was introduced in <ref type="bibr" target="#b10">[11]</ref>, however the adversary utilized was quite weak-it relied o arial training discusses the phenomenon of transferability <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>-adversarial example
o want to note that the study of adversarial ML predates the widespread use of deep neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> (see <ref type="bibr"


sign goal. While trained models tend to be very effective in classifying benign inputs, recent work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" targ k. Unfortunately, ERM often does not yield models that are robust to adversarially crafted examples <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>. Formally, there are
r models are actually that robust. Indeed, subsequent work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> has found that PGD is in fact overestimating the 2 -robustn
f work on this topic <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar


arial ML predates the widespread use of deep neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> (see <ref type="bibr" target="#b2">[3]</ref> for an overview
inputs, recent work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22]</ref> shows that an adversary is often able to manipulate the inp

60.1 59.5 70.7 SemiEmb <ref type="bibr" target="#b23">(Weston et al., 2012)</ref> 59.6 59.0 71.1 LP <ref type="bibr" target="#b26">(Zhu et al., 2003)</ref> 45.3 68.0 63.0 DeepWalk <ref type="bibr" tar
orld Datasets:</head><p>The experiments with real-world datasets follow the methodology proposed in <ref type="bibr" target="#b24">Yang et al. (2016)</ref>. In addition to using the classic dataset sp ="table" target="#tab_2">2</ref>, we demonstrate how our model performs on common splits taken from <ref type="bibr" target="#b24">Yang et al. (2016)</ref>. Accuracy numbers above double-line are copi :</head><label>1</label><figDesc>Experiments run on Node Classification citation datasets created by<ref type="bibr" target="#b24">Yang et al. (2016)</ref>. ? The learned architecture for Citeseer is 67.2 65.3 ICA <ref type="bibr" target="#b17">(Lu &amp; Getoor, 2003)</ref> 69.1 75.1 73.9 Planetoid <ref type="bibr" target="#b24">(Yang et al., 2016)</ref> 64.7 75.7 77.2 Vanilla GCN <ref type="bibr" oth sets of dataset Table <ref type="table">3</ref>: Classification results on random partitions of <ref type="bibr" target="#b24">(Yang et al., 2016)</ref> datasets.</p><formula xml:id="formula_22">X
orld Datasets:</head><p>The experiments with real-world datasets follow the methodology proposed in <ref type="bibr" target="#b24">Yang et al. (2016)</ref>. In addition to using the classic dataset sp ="table" target="#tab_2">2</ref>, we demonstrate how our model performs on common splits taken from <ref type="bibr" target="#b24">Yang et al. (2016)</ref>. Accuracy numbers above double-line are copi :</head><label>1</label><figDesc>Experiments run on Node Classification citation datasets created by<ref type="bibr" target="#b24">Yang et al. (2016)</ref>. ? The learned architecture for Citeseer is 67.2 65.3 ICA <ref type="bibr" target="#b17">(Lu &amp; Getoor, 2003)</ref> 69.1 75.1 73.9 Planetoid <ref type="bibr" target="#b24">(Yang et al., 2016)</ref> 64.7 75.7 77.2 Vanilla GCN <ref type="bibr" oth sets of dataset Table <ref type="table">3</ref>: Classification results on random partitions of <ref type="bibr" target="#b24">(Yang et al., 2016)</ref> datasets.</p><formula xml:id="formula_22">X
="2.2.">Message Passing</head><p>Message Passing algorithms can be used to learn models over graphs <ref type="bibr" target="#b8">(Gilmer et al., 2017)</ref>. In such models, each graph node (and opti defined as an average of its neighbors' representations from step i -1, multiplied by W (i-1) . See <ref type="bibr" target="#b8">Gilmer et al. (2017)</ref>.</p><p>The vanilla GCN makes three simplify

60.1 59.5 70.7 SemiEmb <ref type="bibr" target="#b23">(Weston et al., 2012)</ref> 59.6 59.0 71.1 LP <ref type="bibr" target="#b26">(Zhu et al., 2003)</ref> 45.3 68.0 63.0 DeepWalk <ref type="bibr" tar
detectors.</p><p>The success of CNNs in Computer Vision and other domains has motivated researchers <ref type="bibr" target="#b4">(Bruna et al., 2014;</ref><ref type="bibr" target="#b7">Defferrard et eatures.</p><p>Early extensions of the graph convolution (GC) operator were theoretically motivated <ref type="bibr" target="#b4">(Bruna et al., 2014)</ref>, but (1) required quadratic computational c
for many Computer Vision applications <ref type="bibr" target="#b14">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b21">Szegedy et al., 2015)</ref>. CNNs consist of a series of convolutiona
="2.2.">Message Passing</head><p>Message Passing algorithms can be used to learn models over graphs <ref type="bibr" target="#b8">(Gilmer et al., 2017)</ref>. In such models, each graph node (and opti defined as an average of its neighbors' representations from step i -1, multiplied by W (i-1) . See <ref type="bibr" target="#b8">Gilmer et al. (2017)</ref>.</p><p>The vanilla GCN makes three simplify

></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Encoding Hints in the PC</head><p>Jim?nez <ref type="bibr" target="#b12">[14]</ref> proposes to encode branch hints in the program counter. By ich are placed as much as possible on cold code paths. Overall, placement is not exact. We refer to <ref type="bibr" target="#b12">[14]</ref> for details of the algorithm.</p><p>Figure <ref type="figu
target="#b1">[3]</ref>, branch targets <ref type="bibr" target="#b2">[4]</ref> and memory locality <ref type="bibr" target="#b3">[5]</ref>. On the other hand, researchers have indicated that hint bit
target="#b1">[3]</ref>, branch targets <ref type="bibr" target="#b2">[4]</ref> and memory locality <ref type="bibr" target="#b3">[5]</ref>. On the other hand, researchers have indicated that hint bit
>, early register release <ref type="bibr" target="#b6">[8]</ref> and criticallity-aware processors <ref type="bibr" target="#b7">[9]</ref>. Unfortunately, there are is no generic way to actually prov
target="#b1">[3]</ref>, branch targets <ref type="bibr" target="#b2">[4]</ref> and memory locality <ref type="bibr" target="#b3">[5]</ref>. On the other hand, researchers have indicated that hint bit
e use SimpleScalar for the Alpha <ref type="bibr" target="#b9">[11]</ref> and PTLsim for the x86-64 <ref type="bibr" target="#b10">[12]</ref>. Only the application part of the benchmarks is measured.
target="#b1">[3]</ref>, branch targets <ref type="bibr" target="#b2">[4]</ref> and memory locality <ref type="bibr" target="#b3">[5]</ref>. On the other hand, researchers have indicated that hint bit
chanism for hardware improvements such as value prediction <ref type="bibr" target="#b4">[6]</ref>, <ref type="bibr" target="#b5">[7]</ref>, early register release <ref type="bibr" target="#b6">[8]</r
dicated that hint bits are an enabling mechanism for hardware improvements such as value prediction <ref type="bibr" target="#b4">[6]</ref>, <ref type="bibr" target="#b5">[7]</ref>, early register rel
acy of the encoded hints is computed using functional simulation. We use SimpleScalar for the Alpha <ref type="bibr" target="#b9">[11]</ref> and PTLsim for the x86-64 <ref type="bibr" target="#b10">[1
type="bibr" target="#b4">[6]</ref>, <ref type="bibr" target="#b5">[7]</ref>, early register release <ref type="bibr" target="#b6">[8]</ref> and criticallity-aware processors <ref type="bibr" target="#
alistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27]  and EnhanceNet <ref type="bibr" target="#b37">[38]</ref>.</p></div> 			</abstract> 		</profileDesc> 	</teiHeader> e instead of pixel space. Ledig et al. <ref type="bibr" target="#b26">[27]</ref> and Sajjadi et al. <ref type="bibr" target="#b37">[38]</ref> further propose adversarial loss to encourage the network 7]</ref> introduce an adversarial loss, generating images with more natural details. Sajjadi et al. <ref type="bibr" target="#b37">[38]</ref> develop a similar approach and further explore the local t pe="figure">5</ref>. GAN-based methods (SRGAN <ref type="bibr" target="#b26">[27]</ref>, EnhanceNet <ref type="bibr" target="#b37">[38]</ref> and ours) clearly outperform PSNR-oriented approaches in t ef>, and GAN-based methods, such as SRGAN <ref type="bibr" target="#b26">[27]</ref> and En-hanceNet <ref type="bibr" target="#b37">[38]</ref>. More results are provided in the supplementary material. -GAN and the counterpart generated by SRGAN <ref type="bibr" target="#b26">[27]</ref> or EnhanceNet <ref type="bibr" target="#b37">[38]</ref>. The users were asked to pick the image with more natural on, our method is ranked higher than SRGAN <ref type="bibr" target="#b26">[27]</ref> and EnhanceNet <ref type="bibr" target="#b37">[38]</ref>, especially in building, animal, and grass categories. Com r studies, comparing our method with SRGAN <ref type="bibr" target="#b26">[27]</ref> and EnhanceNet <ref type="bibr" target="#b37">[38]</ref>. Second row: our methods produce visual results that are r ser studies, comparing our method with SRGAN<ref type="bibr" target="#b26">[27]</ref> and EnhanceNet<ref type="bibr" target="#b37">[38]</ref>. Second row: our methods produce visual results that are r ref type="bibr" target="#b2">3]</ref> and adversarial loss <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> are introduced to solve the regression-to-the-mean problem ur framework is based on adversarial learning, inspired by <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>. Specifically, it consists of one generator G θ and one dis d n="3.3.">Loss Function</head><p>We draw inspiration from <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> and apply perceptual loss and adversarial loss in our model leasing textures, outperforming previous GAN-based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Our work currently focuses on SR of outdoor scenes.
ef type="bibr" target="#b36">[37]</ref> uses semantic segmentation for video deblurring. Zhu et al. <ref type="bibr" target="#b51">[52]</ref> propose an approach to generate new clothing on a wearer. nal bias at the input layer.</p><p>2) Compositional mapping -This method is identical to Zhu et al. <ref type="bibr" target="#b51">[52]</ref>. It decomposes an LR image based on the predicted semantic
</ref><ref type="bibr" target="#b40">41]</ref>, statistics <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1]</ref> and internal patch recurrence <ref type="bibr" target="#b15">
age style transfer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>, visual question answering <ref type="bibr" target="#b5">[6
ansformation.</p><p>Semantic guidance. In image generation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5]</ref>, semantic segments are used as input conditions to generate n
" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar d has witnessed a variety of network architectures, such as a deeper network with residual learning <ref type="bibr" target="#b21">[22]</ref>, Laplacian pyramid structure <ref type="bibr" target="#b25 models including PSNR-oriented methods, such as SRCNN <ref type="bibr" target="#b6">[7]</ref>, VDSR <ref type="bibr" target="#b21">[22]</ref>, LapSRN <ref type="bibr" target="#b25">[26]</ref>, DRRN <r
and overly-smoothed results. Specifically, perceptual loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> is introduced to optimize a superresolution model in a featur tions and lead to overly-smooth results. Perceptual losses <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> are proposed to enhance the visual quality by minimizing the ><p>where (x i , y i ) are training pairs. Perceptual loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> and adversarial loss <ref type="bibr" target="#b26">[27,</ref
b46">[47]</ref>. They train specialized models for each semantic category on exemplar-based methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b45">46]</ref> and show that SR r iors by training specialized models separately for each semantic category on exemplar-based methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b45">46]</ref>. In contrast to th ing <ref type="bibr" target="#b3">[4]</ref>, sparse coding <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" tar main evaluation on standard benchmarks such as Set5 <ref type="bibr" target="#b1">[2]</ref>, Set14 <ref type="bibr" target="#b49">[50]</ref> and BSD100 <ref type="bibr" target="#b32">[33]</ref> since
get="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> constrain the solut f type="bibr" target="#b26">[27]</ref>, recursive learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref>, and densely connected network <ref type="bibr" target="#b4 not tried other architectures for the SR network, we believe many contemporary models such as DRRN <ref type="bibr" target="#b42">[43]</ref> and MemNet <ref type="bibr" target="#b43">[44]</ref> are a R <ref type="bibr" target="#b21">[22]</ref>, LapSRN <ref type="bibr" target="#b25">[26]</ref>, DRRN <ref type="bibr" target="#b42">[43]</ref>, MemNet <ref type="bibr" target="#b43">[44]</ref>, and GAN
" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar d has witnessed a variety of network architectures, such as a deeper network with residual learning <ref type="bibr" target="#b21">[22]</ref>, Laplacian pyramid structure <ref type="bibr" target="#b25 models including PSNR-oriented methods, such as SRCNN <ref type="bibr" target="#b6">[7]</ref>, VDSR <ref type="bibr" target="#b21">[22]</ref>, LapSRN <ref type="bibr" target="#b25">[26]</ref>, DRRN <r
k.</p><p>Contemporary SR algorithms are mostly learning-based methods, including neighbor embedding <ref type="bibr" target="#b3">[4]</ref>, sparse coding <ref type="bibr" target="#b48">[49,</ref><ref
odels for each semantic category on exemplar-based methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b45">46]</ref> and show that SR results can be improved by semantic priors ately for each semantic category on exemplar-based methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b45">46]</ref>. In contrast to these studies, we explore categorical prior et="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> and random forest <ref type="bibr" target="#b38">[39]</ref>
nd Lanczos resampling <ref type="bibr" target="#b10">[11]</ref>. Image priors such as edge features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>, statistics <ref t
age style transfer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>, visual question answering <ref type="bibr" target="#b5">[6
ds such as those based on deep convolutional neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target=
43">[44]</ref>. Multiscale guidance structure has also been proposed for depth map super-resolution <ref type="bibr" target="#b17">[18]</ref>. Different losses have also been proposed. Pixel-wise loss
n the COCO dataset <ref type="bibr" target="#b29">[30]</ref> and then fine-tuned on the ADE dataset <ref type="bibr" target="#b50">[51]</ref> with additional animal and mountain images. We train the n
n the COCO dataset <ref type="bibr" target="#b29">[30]</ref> and then fine-tuned on the ADE dataset <ref type="bibr" target="#b50">[51]</ref> with additional animal and mountain images. We train the n
lization (BN) is a widely used technique to ease network training by normalizing feature statistics <ref type="bibr" target="#b18">[19]</ref>. Conditional Normalization (CN) applies a learned function
<ref type="bibr" target="#b1">[2]</ref>, Set14 <ref type="bibr" target="#b49">[50]</ref> and BSD100 <ref type="bibr" target="#b32">[33]</ref> since these datasets are lack of regions with well-defined
nd Lanczos resampling <ref type="bibr" target="#b10">[11]</ref>. Image priors such as edge features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>, statistics <ref t
arget="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar roduced to optimize a superresolution model in a feature space instead of pixel space. Ledig et al. <ref type="bibr" target="#b26">[27]</ref> and Sajjadi et al. <ref type="bibr" target="#b37">[38]</re >[22]</ref>, Laplacian pyramid structure <ref type="bibr" target="#b25">[26]</ref>, residual blocks <ref type="bibr" target="#b26">[27]</ref>, recursive learning <ref type="bibr" target="#b22">[23,</r are proposed to enhance the visual quality by minimizing the error in a feature space. Ledig et al. <ref type="bibr" target="#b26">[27]</ref> introduce an adversarial loss, generating images with more ut and learn (γ, β) to modulate the feature maps by applying affine transformation. Skip connection <ref type="bibr" target="#b26">[27]</ref> is used to ease the training of deep CNN. We upsample feat i ) − φ(y i ) 2 2 . (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>Similar to <ref type="bibr" target="#b26">[27]</ref>, we use the feature maps obtained by the forth convolution ategory is used to encompass regions that do not appear in the aforementioned categories. Following <ref type="bibr" target="#b26">[27]</ref>, all experiments were performed with a scaling factor of × 8] SRGAN[27] SFT-GAN (ours) GT</head><p>Figure <ref type="figure">5</ref>. GAN-based methods (SRGAN <ref type="bibr" target="#b26">[27]</ref>, EnhanceNet <ref type="bibr" target="#b37">[38]</ref> and >[43]</ref>, MemNet <ref type="bibr" target="#b43">[44]</ref>, and GAN-based methods, such as SRGAN <ref type="bibr" target="#b26">[27]</ref> and En-hanceNet <ref type="bibr" target="#b37">[38]</ref>. son). Each pair consists of an image of the proposed SFT-GAN and the counterpart generated by SRGAN <ref type="bibr" target="#b26">[27]</ref> or EnhanceNet <ref type="bibr" target="#b37">[38]</ref>. T R causing confusion within the users. In the second session, our method is ranked higher than SRGAN <ref type="bibr" target="#b26">[27]</ref> and EnhanceNet <ref type="bibr" target="#b37">[38]</ref>, that we only present its . First row: the results of user studies, comparing our method with SRGAN <ref type="bibr" target="#b26">[27]</ref> and EnhanceNet <ref type="bibr" target="#b37">[38]</ref>. ur methods produce visual results that are ranked higher in all categories in comparison with SRGAN <ref type="bibr" target="#b26">[27]</ref>. first layer for this analysis. In the top row, we show an >7</label><figDesc>Figure 7. First row: the results of user studies, comparing our method with SRGAN<ref type="bibr" target="#b26">[27]</ref> and EnhanceNet<ref type="bibr" target="#b37">[38]</ref>. S our methods produce visual results that are ranked higher in all categories in comparison with SRGAN<ref type="bibr" target="#b26">[27]</ref>.</figDesc></figure> <figure xmlns="http://www.tei-c.org/ns ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> and adversarial loss <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> are introduced to 0"><head n="3.2.">Architecture</head><p>Our framework is based on adversarial learning, inspired by <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>. Specifically, it xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>We draw inspiration from <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> and apply perceptu AN in generating realistic and visually pleasing textures, outperforming previous GAN-based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Our work cu aining images. During training, we followed existing studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> to obtain LR images by downsampling HR images using MATLAB

nt weighted terms are then used to control the content-based approach for improved recommendations. <ref type="bibr" target="#b30">Seroussi et al.(2011)</ref> considered user demographic attributes th

ndations that match their interests and not whether the method can accurately predict rating scores <ref type="bibr" target="#b40">(Zhao et al., 2015)</ref>.</p><p>The work of <ref type="bibr" target=


edicting ratings more accurately than approaches that consider either ratings only or reviews only. <ref type="bibr" target="#b5">Chen et al.( 2017)</ref> fused user historical ratings with user laten

f corpus terms <ref type="bibr" target="#b29">(Sánchez &amp; Batet, 2013)</ref>.</p><p>Specifically <ref type="bibr" target="#b23">(Resnik, 1995)</ref> proposed evaluating the IC of the least common s
nt weighted terms are then used to control the content-based approach for improved recommendations. <ref type="bibr" target="#b30">Seroussi et al.(2011)</ref> considered user demographic attributes th

, an optimal relay selection strategy is developed in <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b11">[12]</ref>, the dynamism of electricity price and the deferrability o onding Nash value V i k N (XN , RN ) using ( <ref type="formula" target="#formula_15">10</ref>) and <ref type="bibr" target="#b11">(12)</ref>.</p><formula xml:id="formula_21">End For 1 ≤ n ≤ N − 1</fo sponding Nash value V i k n (Xn, Rn) using ( <ref type="formula" target="#formula_15">10</ref>) and <ref type="bibr" target="#b11">(12)</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>
ble to DT-ORS-Net applications. To this end, the two-player Markov stopping game (MSG) developed in <ref type="bibr" target="#b16">[17]</ref> can serve as a good starting point, which extends the clas e theoretic setting so as to handle the potential conflicts between the two players.</p><p>However, <ref type="bibr" target="#b16">[17]</ref> does not provide a systematic method to deal with a genera the optimal strategy for each player in such situations, the two-player MSG framework developed in <ref type="bibr" target="#b16">[17]</ref> may serve as a basis. Particularly, in the two-player MSG, yer. However, a systematic method for handling a general number of players in a MSG is missing from <ref type="bibr" target="#b16">[17]</ref>. Considering this, a general M-MSG is proposed in the next needed. Particularly, when two players coexist in the MSG, the randomized stopping time is used in <ref type="bibr" target="#b16">[17]</ref> to deal with the potential competition from the other play ynamism in the number of players as the game evolving, the concept of selection time is proposed in <ref type="bibr" target="#b16">[17]</ref>. However, constructing the selection time essentially requ
the contexts of different applications. For example, an optimal cloud accessing rule is derived in <ref type="bibr" target="#b9">[10]</ref> that can guide a company in choosing a suitable time to mig
stic spectrum resource exploited in cognitive radio networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, idle virtual machines may be offered by a cloud provider as cloud resource according to the corresponding multi-player randomized stopping time T ( P) given by <ref type="bibr" target="#b1">(2)</ref>.</p><p>Similarly, the participant recruitment problem descri /ref>, and so are X n 's. Similarly to the previous problem, a recruiter can invoke Algorithm 1 and <ref type="bibr" target="#b1">(2)</ref> to determine optimally when to hire a mobile user.</p><p>Thr
sources: the regular resource and the opportunistic resource <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. Particularly, the opportunistic resources can be a set of
y be offered by a cloud provider as an opportunistic computing resource in cloud-computing networks <ref type="bibr" target="#b2">[3]</ref>; lowprice harvested energy (e.g., wind or solar energy) may sidering that the front-end workload dynamism in a cloud can usually be modeled as a Markov process <ref type="bibr" target="#b2">[3]</ref>, it is reasonable to assume that the sequence of opportunist der can offer two types of computing resources: the regular resource and the opportunistic resource <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. Particularly, the o
optimal electricity utilization rule that can balance the electricity expense and waiting time. In <ref type="bibr" target="#b12">[13]</ref>, as a more economic-friendly alternative to the convention nt capabilities Q n 's of different mobile users are assumed to follow uniform i.i.d. distributions <ref type="bibr" target="#b12">[13]</ref>, and so are X n 's. Similarly to the previous problem, a r
users. For example, in a vehicle based crowdsourcing network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, companies can set up access points near a highway and ask
abilities provide another instantiation of opportunistic resources in mobile crowdsourcing networks <ref type="bibr" target="#b4">[5]</ref>. Two notable characteristics of such opportunistic resources the same shared pool of passing mobile users. For example, in a vehicle based crowdsourcing network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, companies can set u
users. For example, in a vehicle based crowdsourcing network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, companies can set up access points near a highway and ask
for primary-users in cognitive radio networks, an optimal relay selection strategy is developed in <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b11">[12]</ref>, the dynamis ally arrived rewards when there is only one decision-maker. In fact, it has already been noticed in <ref type="bibr" target="#b10">[11]</ref> that a substantial amount of collisions will occur when al ="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF PROPOSITION 1</head><p>Proof: To show that <ref type="bibr" target="#b10">(11)</ref> gives an NE strategy, suppose that R n players remain at t the strategy</p><formula xml:id="formula_24">Pi k ′ n = P i k ′ n • Pi k ′ n+1</formula><p>given by <ref type="bibr" target="#b10">(11)</ref>, except for player i k that plays according to an arbitrar
y be offered by a cloud provider as an opportunistic computing resource in cloud-computing networks <ref type="bibr" target="#b2">[3]</ref>; lowprice harvested energy (e.g., wind or solar energy) may sidering that the front-end workload dynamism in a cloud can usually be modeled as a Markov process <ref type="bibr" target="#b2">[3]</ref>, it is reasonable to assume that the sequence of opportunist der can offer two types of computing resources: the regular resource and the opportunistic resource <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. Particularly, the o
recursively compute an NE strategy for each player. For ease of presentation, only symmetric games <ref type="bibr" target="#b20">[21]</ref> will be considered in this work, in which the reward funct
abilities provide another instantiation of opportunistic resources in mobile crowdsourcing networks <ref type="bibr" target="#b4">[5]</ref>. Two notable characteristics of such opportunistic resources the same shared pool of passing mobile users. For example, in a vehicle based crowdsourcing network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, companies can set u
s, and is not suitable for solving MSG with a general number of players. This can be evidenced from <ref type="bibr" target="#b19">[20]</ref>, from which it can be seen that using the selection time t
users. For example, in a vehicle based crowdsourcing network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, companies can set up access points near a highway and ask
g multiple users to share the same physical resource on the cloud may incur security vulnerabilities<ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, exclusive resourc
or solar energy) may be provided by a power station as an opportunistic resource in power networks <ref type="bibr" target="#b3">[4]</ref>; and mobile users with different taskaccomplishment capabili
m considered in this work distinctive from the conventional distributed resource allocation problems<ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" targe
for primary-users in cognitive radio networks, an optimal relay selection strategy is developed in <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b11">[12]</ref>, the dynamis ally arrived rewards when there is only one decision-maker. In fact, it has already been noticed in <ref type="bibr" target="#b10">[11]</ref> that a substantial amount of collisions will occur when al ="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF PROPOSITION 1</head><p>Proof: To show that <ref type="bibr" target="#b10">(11)</ref> gives an NE strategy, suppose that R n players remain at t the strategy</p><formula xml:id="formula_24">Pi k ′ n = P i k ′ n • Pi k ′ n+1</formula><p>given by <ref type="bibr" target="#b10">(11)</ref>, except for player i k that plays according to an arbitrar
recursively compute an NE strategy for each player. For ease of presentation, only symmetric games <ref type="bibr" target="#b20">[21]</ref> will be considered in this work, in which the reward funct
dy is provided. To bypass this multiuser difficulty, a random contention access model is adopted in <ref type="bibr" target="#b15">[16]</ref> to simplify the multi-user problem into a single-user one,
vices is opening up new human-centered designs that blur the boundaries between humans and machines <ref type="bibr" target="#b0">[1]</ref>. Now, the frontier for research on data management is relate ng number of users and devices but also a diverse set of new applications and services. The work in <ref type="bibr" target="#b0">[1]</ref> introduced a system that can pervasively operate in any netw
and the edge cloud, and safe data storage on the edge cloud <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>.</p><p>However, t
e of the network.</p><p>There is some research on applying cognitive computing to edge computing in <ref type="bibr" target="#b15">[16]</ref>. The authors first introduced deep learning for IoTs into
nsume fewer resources and less energy to reduce the workload <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>The edge computing paradigm has multiple advantages. F ting cyber space and accordingly generates new information-humans also participate in this process; <ref type="bibr" target="#b2">(3)</ref> the machine has the cognition of a human, which provides a m
on the edge cloud <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>.</p><p>However, the above research on edge computing mostl
secure data sharing between the device and the edge cloud, and safe data storage on the edge cloud <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta
part in supporting latency-and privacysensitive applications <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Therefore, the security challenges relate to the protection
and the edge cloud, and safe data storage on the edge cloud <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>.</p><p>However, t
es and cloud servers leads to congestion in the underlying networks. Hence, to handle this problem, <ref type="bibr" target="#b6">[7]</ref> presented an SDN-based edge-cloud interplay to handle the st
part in supporting latency-and privacysensitive applications <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Therefore, the security challenges relate to the protection
e the facial expression (VGG <ref type="bibr" target="#b19">[20]</ref>) and speech emotion (AlexNet <ref type="bibr" target="#b18">[19]</ref>) simultaneously and carry out the simple decision fusion.
regions at multiple scales of the original image.</p><p>Lastly, inspired by the recent Transformer <ref type="bibr" target="#b22">[23]</ref> model in dealing with a number of difficult NLP tasks such idal way, following by corresponding pooling strategy. Moreover, inspired by the recent Transformer <ref type="bibr" target="#b22">[23]</ref> model in dealing with a number of difficult NLP tasks such iation between these local regions are still not explored. Inspired by the recent Transformer model <ref type="bibr" target="#b22">[23]</ref>, we implant a self attention module adapted to image featu g sentiment class number of datasets, and then tunes the parameters of all layers. • Self-Attention <ref type="bibr" target="#b22">[23]</ref> is a variant derived from the Transformer model. Transform
than the traditional hand-crafted feature based methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, making fundamental changes in the research of visual sent
f>, <ref type="bibr" target="#b8">[9]</ref>, Gist features <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and middle attribute Corresponding author: Xiaohao He, hex f>, <ref type="bibr" target="#b8">[9]</ref>, Gist features <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and so on. Besides, some works have tried to utilize middl
e. Local sentiment-related features from each region are incorporated for visual sentiment analysis <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. However, the w
years, the rise of convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> has made it possible to easily and automatically extract d rinciples-of-art (PAEF) for the image emotion classification and scoring task. • Fine-tuned AlexNet <ref type="bibr" target="#b13">[14]</ref>, VGG16 <ref type="bibr" target="#b20">[21]</ref>, Inceptio
backbone convolutional network (e.g., VGG-19 <ref type="bibr" target="#b20">[21]</ref>, ResNet-101 <ref type="bibr" target="#b21">[22]</ref>) to obtain the basic visual feature maps. Next, pyramidal egories as output. For an image, after going through a fully convolutional network (FCN) ResNet-101 <ref type="bibr" target="#b21">[22]</ref>, the model acquires its basic visual information and gener ula_7">)</formula><p>where M L ∈ R fnum×C and R ∈ R fnum×C . We add R to M L as residual connection <ref type="bibr" target="#b21">[22]</ref>, followed by batch normalization <ref type="bibr" target=" ="bibr" target="#b20">[21]</ref>, Inception-V3 <ref type="bibr" target="#b36">[37]</ref>, ResNet101 <ref type="bibr" target="#b21">[22]</ref> replaces the last fully connected layer, swaps it to the o Implementation Details</head><p>We implement the fully convolutional network (FCN) using ResNet-101 <ref type="bibr" target="#b21">[22]</ref>, whose performance is among the best in the most influenti
than the traditional hand-crafted feature based methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, making fundamental changes in the research of visual sent
visual sentiment taxonomy of "adjectivenoun pairs (ANP)" <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Fig. <ref type="figure">1</ref>. Images from the EmotionR tive-noun pairs (ANP)" to improve classification results <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>Deep learning, especially the rise of convolutional
l attention is attracted to the most informative regions <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>, some studies recently attempted to look into local areas
ns/1.0"><head>II. RELATED WORK</head><p>Sentiment analysis is a very important and challenging task <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Most of the pr
f>, <ref type="bibr" target="#b8">[9]</ref>, Gist features <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and middle attribute Corresponding author: Xiaohao He, hex f>, <ref type="bibr" target="#b8">[9]</ref>, Gist features <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and so on. Besides, some works have tried to utilize middl

nteger values. Adjusting (e.g., decreasing) η (i) min and η (i) max at every i-th restart (see also <ref type="bibr" target="#b16">Smith (2016)</ref>) could potentially improve performance, but we do
)</ref>, Adam <ref type="bibr" target="#b9">(Kingma &amp; Ba, 2014)</ref> and most recently AMSGrad <ref type="bibr" target="#b15">(Reddi et al., 2018)</ref> have become a default method of choice for

)</ref>, Adam <ref type="bibr" target="#b9">(Kingma &amp; Ba, 2014)</ref> and most recently AMSGrad <ref type="bibr" target="#b15">(Reddi et al., 2018)</ref> have become a default method of choice for
ref> have become a default method of choice for training feed-forward and recurrent neural networks <ref type="bibr" target="#b21">(Xu et al., 2015;</ref><ref type="bibr" target="#b13">Radford et al.,


/www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Adaptive gradient methods, such as AdaGrad <ref type="bibr" target="#b4">(Duchi et al., 2011)</ref>, <ref type="bibr">RMSProp (Tieleman &amp; H dam, we believe that similar results also hold for other adaptive gradient methods, such as AdaGrad <ref type="bibr" target="#b4">(Duchi et al., 2011) and</ref><ref type="bibr">AMSGrad (Reddi et al.,
/www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Adaptive gradient methods, such as AdaGrad <ref type="bibr" target="#b4">(Duchi et al., 2011)</ref>, <ref type="bibr">RMSProp (Tieleman &amp; H dam, we believe that similar results also hold for other adaptive gradient methods, such as AdaGrad <ref type="bibr" target="#b4">(Duchi et al., 2011) and</ref><ref type="bibr">AMSGrad (Reddi et al.,
4%). <ref type="bibr" target="#b14">Radford et al. (2018)</ref> employed AdamW to train Transformer <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref> architectures to obtain new state-of-the
arget="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" tar ing-based scheduling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref>. Sampling-based sch pling-based strategy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> that assumes runnin gs. They evaluate both static and dynamic scheduling policies. In their followon work, Kumar et al. <ref type="bibr" target="#b18">[19]</ref> study scheduling on heterogeneous multi-cores while runnin
y and power-efficiency <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" targe application scheduling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" targe de workload scheduling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" targe other hand is a dynamic scheduling algorithm that, in addition, is scalable.</p><p>Several studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" targ
p><p>Several studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> explore scheduling in heterogeneous systems by changing clo
" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar " target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar " target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar core in a one-big, multiple-small core configuration) quickly becomes a bottleneck. Bias scheduling <ref type="bibr" target="#b16">[17]</ref> is very similar to memorydominance scheduling. It schedule
div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We use CMP$im <ref type="bibr" target="#b12">[13]</ref> to conduct the simulation experiments in this paper. We co
e that the table counters do not need to be wide, because the dependency distance tends to be short <ref type="bibr" target="#b7">[8]</ref>; e.g., four bits per counter can capture 90% of the distance
target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar terogeneous multi-core is to apply sampling-based scheduling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar ment. We compare PIE scheduling to a sampling-based strategy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar ingle-ISA heterogeneous multicores, we only discuss this class of heterogeneity.</p><p>Kumar et al. <ref type="bibr" target="#b17">[18]</ref> made the case for heterogeneous single-ISA multi-core proc
" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar " target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar " target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar core in a one-big, multiple-small core configuration) quickly becomes a bottleneck. Bias scheduling <ref type="bibr" target="#b16">[17]</ref> is very similar to memorydominance scheduling. It schedule
r" target="#b28">29]</ref> study how to best integrate an MLP technique (such as runahead execution <ref type="bibr" target="#b23">[24]</ref>) into an asymmetric multi-core processor, i.e., should one
om the beginning of the simulation point when they reach the end. We report system throughput (STP) <ref type="bibr" target="#b5">[6]</ref> (also called weighted speedup <ref type="bibr" target="#b30"
" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar " target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar " target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar core in a one-big, multiple-small core configuration) quickly becomes a bottleneck. Bias scheduling <ref type="bibr" target="#b16">[17]</ref> is very similar to memorydominance scheduling. It schedule
that the cost benefits brought by spot offerings can be realized with intuitive bidding strategies <ref type="bibr" target="#b14">[15]</ref>. However, choosing between spot instances and bid levels a e on-demand price since the user only pays the spot price anyway, as is commonly advocated (e.g. in <ref type="bibr" target="#b14">[15]</ref>) and used in practice. Under the given model, the WSD assu
ed with the supply and demand for cloud resources. Thus, unlike prior works on bidding optimization <ref type="bibr" target="#b9">[10]</ref>, our model not only explicitly accounts for the interplay b old on 2017 spot data (see Figure <ref type="figure" target="#fig_4">3</ref>).</p><p>The authors of <ref type="bibr" target="#b9">[10]</ref> used a profit-maximization model to understand spot price d y explicitly considering job deadlines <ref type="bibr" target="#b15">[16]</ref>, cost minimization <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>, and task depend n-demand instances and a set B t ⊂ R + of bids from B t = |B t | spot instance requests. Many works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr f this weak assumption has basis in both previous analyses of spot markets and other auctions (e.g. <ref type="bibr" target="#b9">[10]</ref> assumes bids are drawn from U[π, π]) as well in the simple ese variables by the total number of instances, i.e. define n t = N b t instead of B t ), we follow <ref type="bibr" target="#b9">[10]</ref> and assume that all bids b ∈ B t are drawn independently fr family. A better strategy would be to consider the collective behavior of the spot prices over time <ref type="bibr" target="#b9">[10]</ref>, which we do in this section by accounting for their tempor bly ill-posed (it tends to −∞ as b t → 0), and the fact that we have constraints on the state space <ref type="bibr" target="#b9">(10)</ref>. Therefore, we need to resort to algorithms that support mo
nts CCF-1302518 and CCF-1527371, and ONR-N00014-16-1-2329. <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In this work, we propose another solution: using pricing po /1.0"><head>C. State and Parameter Estimation</head><p>Our proposed model on hidden state evolution <ref type="bibr" target="#b6">(7)</ref> and spot price observation (6) defines a nonlinear dynamical
state distributions; each sample is weighted by the probability that it generates the observed data <ref type="bibr" target="#b30">[31]</ref>.</p><p>While SMC has been most widely and successfully use
h bidding strategies for resources in the cloud spot market by explicitly considering job deadlines <ref type="bibr" target="#b15">[16]</ref>, cost minimization <ref type="bibr" target="#b9">[10]</ref
types <ref type="bibr" target="#b19">[20]</ref>. To limit the variance in the allocated resources, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> suggest that us
e unnormalized arrival-departure random variables are likely best captured by Poisson distributions <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, we find the cl
types <ref type="bibr" target="#b19">[20]</ref>. To limit the variance in the allocated resources, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> suggest that us
e unnormalized arrival-departure random variables are likely best captured by Poisson distributions <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, we find the cl
? Many works on cloud operations have attempted to address this issue, e.g., through job scheduling <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> or optimized virtua
01117C0052 and No. HR001117C0048, NSF grants CCF-1302518 and CCF-1527371, and ONR-N00014-16-1-2329. <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In this work, we p
iables are likely best captured by Poisson distributions <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, we find the closest counterparts for λ</p><formula xml:id
009, many works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> have characterized spot prices from a statistical perspect ince we do not want to bid below the lower bound π, when estimating the expected payment we correct <ref type="bibr" target="#b13">(14)</ref> by taking the maximum of it and π, yielding the following
, E π (i) τ π (i) t<label>(15)</label></formula><p>This approximation can then be substituted into <ref type="bibr" target="#b12">(13)</ref> in order to easily find the lowest-cost instance type to p
the Extended Kalman Filter <ref type="bibr" target="#b28">[29]</ref> or the Unscented Kalman Filter <ref type="bibr" target="#b29">[30]</ref>, they are likely to perform poorly on our model. The reaso
the Extended Kalman Filter <ref type="bibr" target="#b28">[29]</ref> or the Unscented Kalman Filter <ref type="bibr" target="#b29">[30]</ref>, they are likely to perform poorly on our model. The reaso
bids from B t = |B t | spot instance requests. Many works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> assume that the
009, many works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> have characterized spot prices from a statistical perspect ince we do not want to bid below the lower bound π, when estimating the expected payment we correct <ref type="bibr" target="#b13">(14)</ref> by taking the maximum of it and π, yielding the following
the advent of Amazon EC2 spot pricing in 2009, many works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> have characteri
ts. Many works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> assume that the CP's objective is to maximize its own prof
man filter to nonlinear dynamical systems via linearization, i.e., using the Extended Kalman Filter <ref type="bibr" target="#b28">[29]</ref> or the Unscented Kalman Filter <ref type="bibr" target="#b
empted to address this issue, e.g., through job scheduling <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> or optimized virtual machine placement The work in this pape
ry is not assumed; for a much more thorough treatment with all the measure theoretical details, see <ref type="bibr" target="#b0">Daley and Vere-Jones (2003)</ref> and <ref type="bibr" target="#b1">Da fies the mean number of events in a region conditional on the past. Here we use the notation * from <ref type="bibr" target="#b0">Daley and Vere-Jones (2003)</ref> to remind ourselves that this densit
easure theoretical details, see <ref type="bibr" target="#b0">Daley and Vere-Jones (2003)</ref> and <ref type="bibr" target="#b1">Daley and Vere-Jones (2008)</ref>.</p></div> <div xmlns="http://www.te
lue or a vector, depending on the choice of distribution). For more on the Hawkes process, see e.g. <ref type="bibr" target="#b3">Hawkes (1971b</ref><ref type="bibr" target="#b4">Hawkes ( ,a, 1972))</
lue or a vector, depending on the choice of distribution). For more on the Hawkes process, see e.g. <ref type="bibr" target="#b3">Hawkes (1971b</ref><ref type="bibr" target="#b4">Hawkes ( ,a, 1972))</
lue or a vector, depending on the choice of distribution). For more on the Hawkes process, see e.g. <ref type="bibr" target="#b3">Hawkes (1971b</ref><ref type="bibr" target="#b4">Hawkes ( ,a, 1972))</


l earthquakes. For more on the ETAS model, see e.g. <ref type="bibr" target="#b11">Ogata (1988</ref><ref type="bibr" target="#b12">Ogata ( , 1998))</ref>.</p><p>We sometimes make simplifying independe
ype="bibr" target="#b3">Hawkes (1971b</ref><ref type="bibr" target="#b4">Hawkes ( ,a, 1972))</ref>; <ref type="bibr" target="#b5">Hawkes and Oakes (1974)</ref>.</p><p>qq q q q q q qq q q q q qq q qq q
ype="bibr" target="#b3">Hawkes (1971b</ref><ref type="bibr" target="#b4">Hawkes ( ,a, 1972))</ref>; <ref type="bibr" target="#b5">Hawkes and Oakes (1974)</ref>.</p><p>qq q q q q q qq q q q q qq q qq q
lue or a vector, depending on the choice of distribution). For more on the Hawkes process, see e.g. <ref type="bibr" target="#b3">Hawkes (1971b</ref><ref type="bibr" target="#b4">Hawkes ( ,a, 1972))</
ibution). For more on the Hawkes process, see e.g. <ref type="bibr" target="#b3">Hawkes (1971b</ref><ref type="bibr" target="#b4">Hawkes ( ,a, 1972))</ref>; <ref type="bibr" target="#b5">Hawkes and Oa
e" target="#fig_1">3</ref>. This process is a special case of the so-called self-correcting process <ref type="bibr" target="#b6">(Isham and Westcott, 1979)</ref>.</p><p>q q q qq q q q q 0 2 4 Note th
1.0"><head n="4.2">Ogata's modified thinning algorithm</head><p>Ogata's modified thinning algorithm <ref type="bibr" target="#b10">(Ogata, 1981)</ref> is a thinning algorithm based on simulating homog

l earthquakes. For more on the ETAS model, see e.g. <ref type="bibr" target="#b11">Ogata (1988</ref><ref type="bibr" target="#b12">Ogata ( , 1998))</ref>.</p><p>We sometimes make simplifying independe
e" target="#fig_1">3</ref>. This process is a special case of the so-called self-correcting process <ref type="bibr" target="#b6">(Isham and Westcott, 1979)</ref>.</p><p>q q q qq q q q q 0 2 4 Note th
ibution). For more on the Hawkes process, see e.g. <ref type="bibr" target="#b3">Hawkes (1971b</ref><ref type="bibr" target="#b4">Hawkes ( ,a, 1972))</ref>; <ref type="bibr" target="#b5">Hawkes and Oa


ibution). For more on the Hawkes process, see e.g. <ref type="bibr" target="#b3">Hawkes (1971b</ref><ref type="bibr" target="#b4">Hawkes ( ,a, 1972))</ref>; <ref type="bibr" target="#b5">Hawkes and Oa
l earthquakes. For more on the ETAS model, see e.g. <ref type="bibr" target="#b11">Ogata (1988</ref><ref type="bibr" target="#b12">Ogata ( , 1998))</ref>.</p><p>We sometimes make simplifying independe
by m orthonormal matrices. We refer the readers to <ref type="bibr" target="#b24">(Wong, 1967;</ref><ref type="bibr" target="#b0">Absil et al., 2004)</ref> for details on the Riemannian geometry of th
n the literature.</p><p>Several subspace-based classification methods have been previously proposed <ref type="bibr" target="#b25">(Yamaguchi et al., 1998;</ref><ref type="bibr" target="#b15">Sakano, ion cos θ 1 (or the smallest principal angle θ 1 ). This max correlation was used in previous works <ref type="bibr" target="#b25">(Yamaguchi et al., 1998;</ref><ref type="bibr" target="#b15">Sakano, ttp://www.tei-c.org/ns/1.0"><head n="5.3.1.">Mutual Subspace Method (MSM)</head><p>The original MSM <ref type="bibr" target="#b25">(Yamaguchi et al., 1998)</ref> performs simple 1-NN classification wi

ariance with Yale Face Database</head><p>The Yale face database and the Extended Yale face database <ref type="bibr" target="#b7">(Georghiades et al., 2001)</ref> together consist of pictures of 38 su
methods have been previously proposed <ref type="bibr" target="#b25">(Yamaguchi et al., 1998;</ref><ref type="bibr" target="#b15">Sakano, 2000;</ref><ref type="bibr" target="#b5">Fukui &amp; Yamaguch correlation was used in previous works <ref type="bibr" target="#b25">(Yamaguchi et al., 1998;</ref><ref type="bibr" target="#b15">Sakano, 2000;</ref><ref type="bibr" target="#b5">Fukui &amp; Yamaguch d can be extended to any distance described in the paper. There are attempts to use kernels for MSM <ref type="bibr" target="#b15">(Sakano, 2000)</ref>. However, the kernel is used only to represent d

ariance with Yale Face Database</head><p>The Yale face database and the Extended Yale face database <ref type="bibr" target="#b7">(Georghiades et al., 2001)</ref> together consist of pictures of 38 su
he 2-norm of the sine of principal angles <ref type="bibr" target="#b3">(Edelman et al., 1999;</ref><ref type="bibr" target="#b22">Wang et al., 2006)</ref>.</p></div> <div xmlns="http://www.tei-c.org/

forts to use indefinite kernels for learning <ref type="bibr" target="#b14">(Ong et al., 2004;</ref><ref type="bibr" target="#b9">Haasdonk, 2005)</ref>, and several heuristics have been proposed to ma
n the literature.</p><p>Several subspace-based classification methods have been previously proposed <ref type="bibr" target="#b25">(Yamaguchi et al., 1998;</ref><ref type="bibr" target="#b15">Sakano, ion cos θ 1 (or the smallest principal angle θ 1 ). This max correlation was used in previous works <ref type="bibr" target="#b25">(Yamaguchi et al., 1998;</ref><ref type="bibr" target="#b15">Sakano, ttp://www.tei-c.org/ns/1.0"><head n="5.3.1.">Mutual Subspace Method (MSM)</head><p>The original MSM <ref type="bibr" target="#b25">(Yamaguchi et al., 1998)</ref> performs simple 1-NN classification wi
not visible at the node-level.</p><p>Graph kernels based on the k-WL have been proposed in the past <ref type="bibr" target="#b27">(Morris, Kersting, and Mutzel 2017)</ref>. However, a key advantage o ref type="bibr" target="#b35">(Shervashidze et al. 2011)</ref> as well as its higher-order variants <ref type="bibr" target="#b27">(Morris, Kersting, and Mutzel 2017)</ref>. Graphlet and Weisfeiler-Le <ref type="bibr" target="#b22">(Kriege, Giscard, and Wilson 2016)</ref>, and the global-local k-WL <ref type="bibr" target="#b27">(Morris, Kersting, and Mutzel 2017)</ref> with k in {2, 3} as kernel "foot_0">Note that the definition of the local neighborhood is different from the the one defined in<ref type="bibr" target="#b27">(Morris, Kersting, and Mutzel 2017)</ref> which is a superset of our e that we can scale our method to larger datasets by using sampling strategies introduced in, e.g., <ref type="bibr" target="#b27">(Morris, Kersting, and Mutzel 2017;</ref><ref type="bibr" target="#b1
s been completely characterized, see, e.g., <ref type="bibr" target="#b0">(Arvind et al. 2015;</ref><ref type="bibr" target="#b19">Kiefer, Schweitzer, and Selman 2015)</ref>, we can transfer these res
lson 2016;</ref><ref type="bibr" target="#b30">Nikolentzos, Meladianos, and Vazirgiannis 2017;</ref><ref type="bibr" target="#b16">Johansson and Dubhashi 2015)</ref>, spectral approaches <ref type="bi
mpleting the equivalence. Since the power of the 1-WL has been completely characterized, see, e.g., <ref type="bibr" target="#b0">(Arvind et al. 2015;</ref><ref type="bibr" target="#b19">Kiefer, Schwe comings of Both Approaches</head><p>The power of 1-WL has been completely characterized, see, e.g., <ref type="bibr" target="#b0">(Arvind et al. 2015)</ref>. Hence, by using Theorems 1 and 2, this cha
t al.'s (Li, Han, and Wu 2018)</ref> work connecting GNNs to a special form Laplacian smoothing and <ref type="bibr" target="#b23">Lei et al.'s (Lei et al. 2017)</ref> work showing that the feature ma
s. Neural Baselines. To compare GNNs to kernels we used the basic 1-GNN layer of Equation (5), DCNN <ref type="bibr" target="#b37">(Wang et al. 2018)</ref>, PatchySan <ref type="bibr" target="#b29">(N
et kernel <ref type="bibr" target="#b34">(Shervashidze et al. 2009)</ref>, the shortest-path kernel <ref type="bibr" target="#b2">(Borgwardt and Kriegel 2005)</ref>, the Weisfeiler-Lehman subtree kern
t al.'s (Li, Han, and Wu 2018)</ref> work connecting GNNs to a special form Laplacian smoothing and <ref type="bibr" target="#b23">Lei et al.'s (Lei et al. 2017)</ref> work showing that the feature ma
eCNN <ref type="bibr" target="#b8">(Fey et al. 2018)</ref>, and the spectral approaches proposed in <ref type="bibr" target="#b3">(Bruna et al. 2014;</ref><ref type="bibr" target="#b6">Defferrard, X.,
explicit feature maps. Prominent examples of this trend include kernels based on graphlet counting <ref type="bibr" target="#b34">(Shervashidze et al. 2009)</ref>, and, most notably, the Weisfeiler-L rnel and GNN methods as baselines for our experiments. Kernel Baselines. We use the Graphlet kernel <ref type="bibr" target="#b34">(Shervashidze et al. 2009)</ref>, the shortest-path kernel <ref type= ibr" target="#b39">(Ying et al. 2018a)</ref>, and the analysis of quantum interactions in molecules <ref type="bibr" target="#b34">(Schütt et al. 2017)</ref>. A survey of recent advancements in GNN te
datasets and offer benefits on real-world applications, we conduct experiments on the Q M 9 dataset <ref type="bibr" target="#b31">(Ramakrishnan et al. 2014;</ref><ref type="bibr" target="#b31">Ruddig uct experiments on the Q M 9 dataset <ref type="bibr" target="#b31">(Ramakrishnan et al. 2014;</ref><ref type="bibr" target="#b31">Ruddigkeit et al. 2012;</ref><ref type="bibr">Wu et al. 2018)</ref>,
ity and its performance drops dramatically for similarity based tasks, e.g. nearest neighbor search <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" ta s usually built on classifier weights <ref type="bibr" target="#b7">[8]</ref> or memorized features <ref type="bibr" target="#b45">[46]</ref>, which has limited efficiency and discriminability. We pro images and predefined noise signals, which constrains the distribution between raw data and noises <ref type="bibr" target="#b45">[46]</ref>. Bolztmann Machines (RBMs) <ref type="bibr" target="#b23"> discriminability. Softmax Embedding with Memory Bank. To improve the inferior efficiency, Wu et al. <ref type="bibr" target="#b45">[46]</ref> propose to set up a memory bank to store the instance feat tance feature rather than classifier weights <ref type="bibr" target="#b7">[8]</ref> or memory bank <ref type="bibr" target="#b45">[46]</ref>. To achieve the goal that features of the same instance un ="bibr" target="#b2">[3]</ref> 67.6 Exemplar <ref type="bibr" target="#b7">[8]</ref> 74.5 NPSoftmax <ref type="bibr" target="#b45">[46]</ref> 80.8 NCE <ref type="bibr" target="#b45">[46]</ref> 80. </p ype="bibr" target="#b7">[8]</ref> 74.5 NPSoftmax <ref type="bibr" target="#b45">[46]</ref> 80.8 NCE <ref type="bibr" target="#b45">[46]</ref> 80. </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><h ead n="4.1.">Experiments on Seen Testing Categories</head><p>We follow the experimental settings in <ref type="bibr" target="#b45">[46]</ref> to conduct the experiments on CIFAR-10 <ref type="bibr" ta ColorJitter, RandomHorizontalFlip) in PyTorch with default parameters are adopted.</p><p>Following <ref type="bibr" target="#b45">[46]</ref>, we adopt weighted kNN classifier to evaluate the performa 200) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label <ref type="bibr" target="#b45">[46]</ref>   plar CNN <ref type="bibr" target="#b7">[8]</ref>, NPSoft type="bibr" target="#b45">[46]</ref>   plar CNN <ref type="bibr" target="#b7">[8]</ref>, NPSoftmax <ref type="bibr" target="#b45">[46]</ref>, NCE <ref type="bibr" target="#b45">[46]</ref> and Triplet N <ref type="bibr" target="#b7">[8]</ref>, NPSoftmax <ref type="bibr" target="#b45">[46]</ref>, NCE <ref type="bibr" target="#b45">[46]</ref> and Triplet loss with and without hard mining. Triplet (ha and the margin parameter is set to 0.5. DeepCluster <ref type="bibr" target="#b2">[3]</ref> and NCE <ref type="bibr" target="#b45">[46]</ref> represent the state-of-the-art unsupervised feature learni classifier weights for training, the proposed method outperforms it by 9.1%. Compared to NPSoftmax <ref type="bibr" target="#b45">[46]</ref> and NCE <ref type="bibr" target="#b45">[46]</ref>, which u hod outperforms it by 9.1%. Compared to NPSoftmax <ref type="bibr" target="#b45">[46]</ref> and NCE <ref type="bibr" target="#b45">[46]</ref>, which use memorized feature for optimizing, the proposed target="#fig_4">3</ref>. The proposed method takes only 2 epochs to get a kNN accuracy of 60% while <ref type="bibr" target="#b45">[46]</ref> takes 25 epochs and <ref type="bibr" target="#b7">[8]</ref ance features rather than classifier weights <ref type="bibr" target="#b7">[8]</ref> or memory bank <ref type="bibr" target="#b45">[46]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head d images in ten The classifier is used to predict the label of test samples. We implement NPSoftmax <ref type="bibr" target="#b45">[46]</ref>, NCE <ref type="bibr" target="#b45">[46]</ref> and DeepClu ct the label of test samples. We implement NPSoftmax <ref type="bibr" target="#b45">[46]</ref>, NCE <ref type="bibr" target="#b45">[46]</ref> and DeepCluster <ref type="bibr" target="#b2">[3]</ref> (c the best accuracy with both classifiers (kNN: 74.1%, Linear: 69.5%), which are much better than NCE <ref type="bibr" target="#b45">[46]</ref> and DeepCluster <ref type="bibr" target="#b2">[3]</ref> un three state-of-the-art unsupervised methods (Exemplar <ref type="bibr" target="#b7">[8]</ref>, NCE <ref type="bibr" target="#b45">[46]</ref> and DeepCluster <ref type="bibr" target="#b2">[3]</ref>) o n Table <ref type="table">3</ref>.</p><p>Generally, the instance-wise feature learning methods (NCE <ref type="bibr" target="#b45">[46]</ref>, Examplar <ref type="bibr" target="#b7">[8]</ref>, Ours) o
stimates between-image labels using the clustering technique <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref> or kNN-based methods
pe="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> or triplet relationships <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>. In particular, se set to 64.</p><p>Evaluation Metrics. Following existing works on supervised deep embedding learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>, the retrieval per " target="#b15">[16]</ref>, semihard mining <ref type="bibr" target="#b34">[35]</ref>, smart mining <ref type="bibr" target="#b12">[13]</ref> and so on. In comparison, softmax embedding achieves compe
ove the performance, such as hard mining <ref type="bibr" target="#b15">[16]</ref>, semihard mining <ref type="bibr" target="#b34">[35]</ref>, smart mining <ref type="bibr" target="#b12">[13]</ref> an "> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>R@1 R@2 R@4 R@8 NMI Initial (FC) <ref type="bibr" target="#b34">35</ref> surement. Given a query image from the testing set, R@K meas
<ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref> or kNN-based methods <ref type="bibr" target="#b40">[41]</ref>, which provide label information. Then label information a
s should be invariant. For negative separation: since unlabelled data are usually highly imbalanced <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b48">49]</ref>, the number of neg
ing labels for training. The performances of some state-of-the-art unsupervised methods (k-MeansNet <ref type="bibr" target="#b4">[5]</ref>, HMP <ref type="bibr" target="#b1">[2]</ref>, Satck <ref typ
a-class variation and maximizing the inter-class variation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" tar g existing methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. A 128-dim fully connected layer with ℓ 2 normalization is
>[43]</ref>, Stanford Online Product (Product) <ref type="bibr" target="#b31">[32]</ref> and Car196 <ref type="bibr" target="#b21">[22]</ref>   Implementation Details. We implement the proposed method
from unlabelled data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. The learned featur local patches <ref type="bibr" target="#b5">[6]</ref>, the position of randomly rearranged patches <ref type="bibr" target="#b30">[31]</ref>, the missing pixels of an image <ref type="bibr" target="#
st neighbor search <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>The main challenge of unsupervised embedding learnin
a-class variation and maximizing the inter-class variation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" tar g existing methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. A 128-dim fully connected layer with ℓ 2 normalization is
emonstrate impressive capabilities in various vision tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">53]</ref>. However, annotated 6">47]</ref>. Most of them are designed on top of pairwise <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> or triplet relationships <ref type="bibr" target="#b12">[13 at size 227× 227 with random horizontal flipping following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>. Since the pre-trained network performs well on CUB200 data br" target="#b38">[39]</ref> on ImageNet is used as the backbone network following existing methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" ta tive. In testing phase, a single center-cropped image is adopted for fine-grained recognition as in <ref type="bibr" target="#b29">[30]</ref>. We adopt the SGD optimizer with 0.9 momentum. The initial
centrated, the embedding features of samples belonging to the same category are close to each other <ref type="bibr" target="#b31">[32]</ref>; 2) negative separated, the embedding features of samples ing strategy also takes full advantage of relationships among all instances sampled in a mini-batch <ref type="bibr" target="#b31">[32]</ref>. Theoretically, we could also use a multibranch network by ategories (unseen testing category). This setting is usually used for supervised embedding learning <ref type="bibr" target="#b31">[32]</ref>. Following <ref type="bibr" target="#b20">[21]</ref>, we d ng samples and testing samples are not overlapped. We follow the experimental settings described in <ref type="bibr" target="#b31">[32]</ref> to conduct experiments on CUB200-2011(CUB200) <ref type="b on CUB200-2011(CUB200) <ref type="bibr" target="#b42">[43]</ref>, Stanford Online Product (Product) <ref type="bibr" target="#b31">[32]</ref> and Car196 <ref type="bibr" target="#b21">[22]</ref>   Imp ility of any correct matching (with same category label) occurs in the top-k retrieved ranking list <ref type="bibr" target="#b31">[32]</ref>. The average score is reported for all testings samples. N embedding function by minimizing the intra-class variation and maximizing the inter-class variation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta is used as the backbone network following existing methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. A 128-dim fully co owing existing works on supervised deep embedding learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>, the retrieval performance and clustering quality of the te
st neighbor search <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>The main challenge of unsupervised embedding learnin
"bibr" target="#b45">[46]</ref>. Bolztmann Machines (RBMs) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>, Auto-encoders <ref type="bibr" target="#b19">[20,</ref><re
on between raw data and noises <ref type="bibr" target="#b45">[46]</ref>. Bolztmann Machines (RBMs) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>, Auto-encoders <re
et="#b31">[32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. Most of them are designed on top of pairwise <ref type="bi
learning usually aims at learning a good "intermediate" feature representation from unlabelled data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" targ arn the feature representations. The pretext task could be the context information of local patches <ref type="bibr" target="#b5">[6]</ref>, the position of randomly rearranged patches <ref type="bibr
an image <ref type="bibr" target="#b33">[34]</ref> or the color information from gray-scale images <ref type="bibr" target="#b50">[51]</ref>. Some attempts also use video information to provide weak
y for similarity based tasks, e.g. nearest neighbor search <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>The main cha
ove the performance, such as hard mining <ref type="bibr" target="#b15">[16]</ref>, semihard mining <ref type="bibr" target="#b34">[35]</ref>, smart mining <ref type="bibr" target="#b12">[13]</ref> an "> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>R@1 R@2 R@4 R@8 NMI Initial (FC) <ref type="bibr" target="#b34">35</ref> surement. Given a query image from the testing set, R@K meas
arget="#b91">94,</ref><ref type="bibr" target="#b55">58,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b60">63]</ref>. Here we make an attempt to actually find this structure. W arget="#b91">94,</ref><ref type="bibr" target="#b55">58,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b60">63]</ref>. Unlike multi-task learning, we explicitly model the relati
<ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b35">37]</ref>, few-shot learning <ref type="bibr" target="#b75">[78,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" ta
f><ref type="bibr" target="#b79">82,</ref><ref type="bibr" target="#b78">81]</ref>, functional maps <ref type="bibr" target="#b65">[68]</ref>, certain aspects of Bayesian inference and Dirichlet proce
ide Source Task Encoder Target Task Output meshes similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b95">98,</ref><ref type="bibr" target="#b11">12]</ref> enabling us to prog
arget="#b32">34]</ref>. It often addresses a shift in the input domain, e.g. webcam images to D-SLR <ref type="bibr" target="#b43">[45]</ref>, while the task is kept the same. In contrast, our framewo
tain aspects of Bayesian inference and Dirichlet processes <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b85">88,</ref><ref type="bibr" target="#b84">87,</ref><ref type="bibr" tar
t) compares the taxonomy suggested policy against some of the best existing self-supervised methods <ref type="bibr" target="#b90">[93,</ref><ref type="bibr" target="#b97">100,</ref><ref type="bibr" t
4">87]</ref>, homomorphic cryptography <ref type="bibr" target="#b38">[40]</ref>, lifelong learning <ref type="bibr" target="#b87">[90,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" ta ess and the reported advantages are another support for existence of a useful structure among tasks <ref type="bibr" target="#b87">[90,</ref><ref type="bibr" target="#b94">97,</ref><ref type="bibr" ta
transfer learning <ref type="bibr" target="#b69">[72,</ref><ref type="bibr" target="#b78">81,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b58">61,</ref><ref type="bibr" tar
l to the amount of time that an infinite-length random walk on W ′ t will spend at any given source <ref type="bibr" target="#b56">[59]</ref>. We stack the principal eigenvectors of W ′ t for all t ∈
rget="#b31">[33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" tar
raining. However, in this case three-fold data augmentation was applied prior to feature extraction <ref type="bibr" target="#b20">[21]</ref> and the acoustic features comprised 40-dimensional MFCCs (
train. Our TDNN-F was trained using the lattice-free maximum mutual information objective criterion <ref type="bibr" target="#b21">[22]</ref>. No parameter tuning was performed during neural network t
model with fewer parameters. The recently-introduced factorised time-delay neural networks (TDNN-F) <ref type="bibr" target="#b12">[13]</ref> utilise half the number of parameters than the hybrid netw to the parameter matrices of TDNN layers, ASR performance can be improved in lowresource situations <ref type="bibr" target="#b12">[13]</ref>. Consequently, a TDNN-F acoustic model (10 time-delay laye
r" target="#b8">[9]</ref>. Furthermore, two corpora taken from the Leipzig Corpora Collection (LCC) <ref type="bibr" target="#b17">[18]</ref> were included in our language model (LM) data collection.
een applied successfully in some other low-resource settings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" t M) neural network can lower language model perplexity when incorporated as additional training data <ref type="bibr" target="#b13">[14]</ref>. Hence we trained an LSTM network on the Somali acoustic t been shown that semi-supervised training can improve ASR performance in an under-resourced scenario<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. As we only have l
">Acoustic modelling</head><p>The Kaldi speech recognition toolkit was used for all ASR experiments <ref type="bibr" target="#b19">[20]</ref>. All experiments were performed using a PC with an 8-core
nd the difficulty of compiling speech data resulted in an available training set of just 1.57 hours <ref type="bibr" target="#b8">[9]</ref>. By leveraging available resources from better-resourced but using a hybrid neural network acoustic model was able to achieve a word error rate (WER) of 53.75% <ref type="bibr" target="#b8">[9]</ref>. While the additional language resources were beneficial to d by a high number of morphemes per word.</p><p>The multilingual Somali acoustic model described in <ref type="bibr" target="#b8">[9]</ref> uses a hybrid neural network that contains several million p were carefully manually cleaned and filtered, the Facebook comments consist of raw, unfiltered text <ref type="bibr" target="#b8">[9]</ref>. Furthermore, two corpora taken from the Leipzig Corpora Col ntranscribed data. To start the process, we used our best previouslyavailable Somali acoustic model <ref type="bibr" target="#b8">[9]</ref> labelled ASR1 in Figure <ref type="figure" target="#fig_0">2 our times. The resulting vocabulary consisted of 41.7k word types.</p><p>The language model used in <ref type="bibr" target="#b8">[9]</ref> was used as the baseline (LMbase). This model was trained on GPU. In our previous work, we found multilingual training to improve ASR performance substantially <ref type="bibr" target="#b8">[9]</ref>. For multilingual training, the training sets of the four la rd error rate (WER) for the various training approaches. In comparison with our previous ASR system <ref type="bibr" target="#b8">[9]</ref>, the improvement afforded by TDNN-F is clear (rows 1 and 2).
sufficient internet infrastructure indicate that this function is fulfilled by radio phone-in shows <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe
ecome an accepted platform for sharing opinions and concerns <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Surveys conducted by
sufficient internet infrastructure indicate that this function is fulfilled by radio phone-in shows <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe
r" target="#b8">[9]</ref>. Furthermore, two corpora taken from the Leipzig Corpora Collection (LCC) <ref type="bibr" target="#b17">[18]</ref> were included in our language model (LM) data collection.
een applied successfully in some other low-resource settings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" t M) neural network can lower language model perplexity when incorporated as additional training data <ref type="bibr" target="#b13">[14]</ref>. Hence we trained an LSTM network on the Somali acoustic t been shown that semi-supervised training can improve ASR performance in an under-resourced scenario<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. As we only have l
ther study on Somali automatic speech recognition (ASR) has so far been described in the literature <ref type="bibr" target="#b11">[12]</ref>.</p><p>Somali is an Afroasiatic language. It is the offici
able resources from better-resourced but unrelated languages <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, a system using a hy
ther study on Somali automatic speech recognition (ASR) has so far been described in the literature <ref type="bibr" target="#b11">[12]</ref>.</p><p>Somali is an Afroasiatic language. It is the offici
t unrelated languages <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, a system using a hybrid neural network acoustic model was
radio phone-in shows <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Therefore, to support its humanitarian relief efforts in ru
een applied successfully in some other low-resource settings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" t M) neural network can lower language model perplexity when incorporated as additional training data <ref type="bibr" target="#b13">[14]</ref>. Hence we trained an LSTM network on the Somali acoustic t been shown that semi-supervised training can improve ASR performance in an under-resourced scenario<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. As we only have l
r" target="#b8">[9]</ref>. Furthermore, two corpora taken from the Leipzig Corpora Collection (LCC) <ref type="bibr" target="#b17">[18]</ref> were included in our language model (LM) data collection.
cate that this function is fulfilled by radio phone-in shows <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Therefore, to suppor
d radio browsing systems in three of the country's languages <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>The success in Uganda served as a motivator for the de et="#b8">[9]</ref>. By leveraging available resources from better-resourced but unrelated languages <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe vised training, an approach which has been applied successfully in some other low-resource settings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" tar /div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Radio browsing system</head><p>Figure 1 <ref type="bibr" target="#b7">[8]</ref> shows the components of the radio browsing system. The prepr e xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2</head><label>2</label><figDesc>Figure 1<ref type="bibr" target="#b7">[8]</ref> shows the components of the radio browsing system. The prepr
sufficient internet infrastructure indicate that this function is fulfilled by radio phone-in shows <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe
">Acoustic modelling</head><p>The Kaldi speech recognition toolkit was used for all ASR experiments <ref type="bibr" target="#b19">[20]</ref>. All experiments were performed using a PC with an 8-core
w-resource settings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>.</p></div> <div x can improve ASR performance in an under-resourced scenario<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. As we only have less than two hours of transcribed Somali
">Acoustic modelling</head><p>The Kaldi speech recognition toolkit was used for all ASR experiments <ref type="bibr" target="#b19">[20]</ref>. All experiments were performed using a PC with an 8-core
oped parts of Uganda, the UN has piloted radio browsing systems in three of the country's languages <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>The success in
able resources from better-resourced but unrelated languages <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, a system using a hy
able resources from better-resourced but unrelated languages <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, a system using a hy
een applied successfully in some other low-resource settings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" t M) neural network can lower language model perplexity when incorporated as additional training data <ref type="bibr" target="#b13">[14]</ref>. Hence we trained an LSTM network on the Somali acoustic t been shown that semi-supervised training can improve ASR performance in an under-resourced scenario<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. As we only have l
nd the difficulty of compiling speech data resulted in an available training set of just 1.57 hours <ref type="bibr" target="#b8">[9]</ref>. By leveraging available resources from better-resourced but using a hybrid neural network acoustic model was able to achieve a word error rate (WER) of 53.75% <ref type="bibr" target="#b8">[9]</ref>. While the additional language resources were beneficial to d by a high number of morphemes per word.</p><p>The multilingual Somali acoustic model described in <ref type="bibr" target="#b8">[9]</ref> uses a hybrid neural network that contains several million p were carefully manually cleaned and filtered, the Facebook comments consist of raw, unfiltered text <ref type="bibr" target="#b8">[9]</ref>. Furthermore, two corpora taken from the Leipzig Corpora Col ntranscribed data. To start the process, we used our best previouslyavailable Somali acoustic model <ref type="bibr" target="#b8">[9]</ref> labelled ASR1 in Figure <ref type="figure" target="#fig_0">2 our times. The resulting vocabulary consisted of 41.7k word types.</p><p>The language model used in <ref type="bibr" target="#b8">[9]</ref> was used as the baseline (LMbase). This model was trained on GPU. In our previous work, we found multilingual training to improve ASR performance substantially <ref type="bibr" target="#b8">[9]</ref>. For multilingual training, the training sets of the four la rd error rate (WER) for the various training approaches. In comparison with our previous ASR system <ref type="bibr" target="#b8">[9]</ref>, the improvement afforded by TDNN-F is clear (rows 1 and 2).
ompiled from national radio news bulletins and consists of a mix of prepared and spontaneous speech <ref type="bibr" target="#b16">[17]</ref>. The total transcribed multilingual speech data available
radio phone-in shows <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Therefore, to support its humanitarian relief efforts in ru
een applied successfully in some other low-resource settings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" t M) neural network can lower language model perplexity when incorporated as additional training data <ref type="bibr" target="#b13">[14]</ref>. Hence we trained an LSTM network on the Somali acoustic t been shown that semi-supervised training can improve ASR performance in an under-resourced scenario<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. As we only have l
="#b2">2,</ref><ref type="bibr" target="#b3">3]</ref>. Parallel research exists in vocal expression <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>. Many prosodic feature ees with an earlier study examining gender differences in emotion detection using the voice channel <ref type="bibr" target="#b4">[4]</ref>. More detailed studies can be performed in the future.</p><p
but no mixed audio-visual data with corresponding ratings <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b37">36]</ref>. IEMOCAP <ref type="bibr" target="#b38">[37]</ref> and the
ble Dataset</head><p>A concern in crowd-sourcing data acquisition is "cheating" or "poor" responses <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" ta
ated emotional displays as well as mismatched emotions. That dataset only compares fear and disgust <ref type="bibr" target="#b34">[33]</ref>. The AV Synthetic Character study uses animated video clip
improve recognition <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11]</ref>. Research on bimodal perception of emotional expressions in

get="#b48">47,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b52">51]</ref>.</p><p>Our study wa

.</p><p>Datasets consisting of emotion expressions are available that contain rated visual displays <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" ta
ing modalities. However, the clips come from only 10 actors with 1,260 rated clips. De Silva et al. <ref type="bibr" target="#b33">[32]</ref> investigated the interactions be- AV Integration data set
tion, recent computational emotion recognition systems use spectral features to improve recognition <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" targ
ms that are applied to object segmentation based on CNNs <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Because the performance of deep learning algorithms depen
eir original image size. The encoder part of FCN consists of visual geometry group network (VGGNet) <ref type="bibr" target="#b25">[26]</ref> that is a famous CNN classification model and the decoder
Ns) <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b16">[17]</ref>, and U-Net <ref type="bibr" target="#b17">[18]</ref> algorithms have been used.</p><p>Although it is possible f er for upsampling.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>B. U-Net</head><p>U-Net <ref type="bibr" target="#b17">[18]</ref> is a modified FCN for yielding more precise segmentation. formation such as boundaries of objects. To overcome this problem, skip connection methods are used <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The features e U-Net. It is important to note that the compared U-Net is not the original architecture proposed in <ref type="bibr" target="#b17">[18]</ref> but a highly calibrated model for the enhanced capability
iction by reinterpretation of fully connected layers of the classifier as a fully convolution layer <ref type="bibr" target="#b24">[25]</ref>. The FCN consists of an encoder of input images and a deco overcome this problem, skip connection methods are used <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The features extracted from PPL are upsampled and concate
vide valuable information on areas that are difficult for people to access or access nonintrusively <ref type="bibr" target="#b0">[1]</ref>. The information obtained using aerial images is used in a v
to generate geospatial data but takes much labor and time to extract or identify features manually <ref type="bibr" target="#b3">[4]</ref>. With the development of the optical sensor technology, it i
lized as performance indices. The experiments were implemented using the public platform Tensorflow <ref type="bibr" target="#b31">[32]</ref> and run on an Intel core 6 i7-7820X CPU at 3.6 GHz with 2
al images is widely used in segmenting roads <ref type="bibr" target="#b5">[6]</ref>, and buildings <ref type="bibr" target="#b6">[7]</ref>. Also, traditional machine learning techniques such as suppo
"bibr" target="#b15">[16]</ref>. In addition, they have used many data from the OpenStreetMap (OSM) <ref type="bibr" target="#b28">[29]</ref>. However, the OSM has several defects (e.g., buildings mat
rning techniques such as support vector machine and artificial neural networks have been studied in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref>. The automatic m
s/rights/index.html for more information.</p><p>research <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. To address these problems, we make a large scale of the d egment objects such as roads and buildings have been carried out using aerial images in rural areas <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. In addition, t ing segmentation due to the lack of multiclass data sets <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Moreover, classifying several classes at once can solve t
ed much attention to segment objects in satellite images <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. To segment roads and buildings from satellite images, sev
al images is widely used in segmenting roads <ref type="bibr" target="#b5">[6]</ref>, and buildings <ref type="bibr" target="#b6">[7]</ref>. Also, traditional machine learning techniques such as suppo
h there are the large-scale data sets for classification <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Second, due to the lack of multiclass data sets, binary d
utional neural networks (CNNs) have attracted much attention to segment objects in satellite images <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. To segment roa
. The numbers of the filter channels are increased. In this process, we utilize batch normalization <ref type="bibr" target="#b27">[28]</ref> to prevent the overfitting problem as shown in Fig. <ref t
variety of industries including land inventory, vegetation monitoring, and environmental assessment <ref type="bibr" target="#b1">[2]</ref>. In particular, extraction of manufactured features such as
al images is widely used in segmenting roads <ref type="bibr" target="#b5">[6]</ref>, and buildings <ref type="bibr" target="#b6">[7]</ref>. Also, traditional machine learning techniques such as suppo
ine and artificial neural networks have been studied in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref>. The automatic methods are objective without human intervent
lized as performance indices. The experiments were implemented using the public platform Tensorflow <ref type="bibr" target="#b31">[32]</ref> and run on an Intel core 6 i7-7820X CPU at 3.6 GHz with 2
al images is widely used in segmenting roads <ref type="bibr" target="#b5">[6]</ref>, and buildings <ref type="bibr" target="#b6">[7]</ref>. Also, traditional machine learning techniques such as suppo


variety of industries including land inventory, vegetation monitoring, and environmental assessment <ref type="bibr" target="#b1">[2]</ref>. In particular, extraction of manufactured features such as
ine and artificial neural networks have been studied in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref>. The automatic methods are objective without human intervent
SYSTEMS</head><p>There are several algorithms that are applied to object segmentation based on CNNs <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Because the pe
vide valuable information on areas that are difficult for people to access or access nonintrusively <ref type="bibr" target="#b0">[1]</ref>. The information obtained using aerial images is used in a v
al images is widely used in segmenting roads <ref type="bibr" target="#b5">[6]</ref>, and buildings <ref type="bibr" target="#b6">[7]</ref>. Also, traditional machine learning techniques such as suppo

main types of research for detecting objects from aerial images: semiautomatic and fully automatic <ref type="bibr" target="#b4">[5]</ref>. However, because the semiautomatic methods require prior kn
al images is widely used in segmenting roads <ref type="bibr" target="#b5">[6]</ref>, and buildings <ref type="bibr" target="#b6">[7]</ref>. Also, traditional machine learning techniques such as suppo
rning techniques such as support vector machine and artificial neural networks have been studied in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref>. The automatic m
utional neural networks (CNNs) have attracted much attention to segment objects in satellite images <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. To segment roa
have been carried out using aerial images in rural areas <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. In addition, they have used many data from the OpenStreet

SYSTEMS</head><p>There are several algorithms that are applied to object segmentation based on CNNs <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Because the pe
esults have generally been discouraging due to various adverse factors (image noise, shadows, etc.) <ref type="bibr" target="#b9">[10]</ref>.</p><p>Recently, as the deep learning technology has evolve
h there are the large-scale data sets for classification <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Second, due to the lack of multiclass data sets, binary d
utional neural networks (CNNs) have attracted much attention to segment objects in satellite images <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. To segment roa
lized as performance indices. The experiments were implemented using the public platform Tensorflow <ref type="bibr" target="#b31">[32]</ref> and run on an Intel core 6 i7-7820X CPU at 3.6 GHz with 2
variety of industries including land inventory, vegetation monitoring, and environmental assessment <ref type="bibr" target="#b1">[2]</ref>. In particular, extraction of manufactured features such as
have been carried out using aerial images in rural areas <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. In addition, they have used many data from the OpenStreet
al images is widely used in segmenting roads <ref type="bibr" target="#b5">[6]</ref>, and buildings <ref type="bibr" target="#b6">[7]</ref>. Also, traditional machine learning techniques such as suppo
variety of industries including land inventory, vegetation monitoring, and environmental assessment <ref type="bibr" target="#b1">[2]</ref>. In particular, extraction of manufactured features such as
accepted method of determining the critical path of program execution was proposed by Fields et al. <ref type="bibr" target="#b5">[5]</ref>. A graph of dynamic instructions is constructed, modeling ea sing directed graphs, and proposed a token-based hardware mechanism to approximate this in hardware <ref type="bibr" target="#b5">[5]</ref>. Runahead <ref type="bibr" target="#b3">[3,</ref><ref type="
re processor-, cache-, and memory-sensitive, respectively</p><ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b7">7]</ref></p>.</p>AELV ammp -ep -lu -vpr C</p></note></figure> <figure
es a detailed DDR3 DRAM model, we show that pairing this mechanism up with a lean FR-FCFS scheduler <ref type="bibr" target="#b22">[22]</ref> can improve performance by 9.3%, on average, for parallel lity into FR-FCFS</head><p>We add our concept of load criticality into the FR-FCFS memory scheduler <ref type="bibr" target="#b22">[22]</ref>. The basic FR-FCFS algorithm calls for CAS commands to be atively show that pairing this mechanism with a novel criticality-aware scheduler, based on FR-FCFS <ref type="bibr" target="#b22">[22]</ref>, can improve performance by 9.3%, on average, for parallel
es a detailed DDR3 DRAM model, we show that pairing this mechanism up with a lean FR-FCFS scheduler <ref type="bibr" target="#b22">[22]</ref> can improve performance by 9.3%, on average, for parallel lity into FR-FCFS</head><p>We add our concept of load criticality into the FR-FCFS memory scheduler <ref type="bibr" target="#b22">[22]</ref>. The basic FR-FCFS algorithm calls for CAS commands to be atively show that pairing this mechanism with a novel criticality-aware scheduler, based on FR-FCFS <ref type="bibr" target="#b22">[22]</ref>, can improve performance by 9.3%, on average, for parallel
w that guaranteeing that these loads hit in the L1 cache increases performance by an average of 40% <ref type="bibr" target="#b27">[27]</ref>. For an online implementation, they mark loads as critical
he notion of load criticality used in earlier memory scheduling proposals <ref type="bibr">[9,</ref><ref type="bibr" target="#b10">10]</ref> is typically from the memory's perspective and tends to be rness. The Minimalist Open-page scheduler also ranks threads based on the importance of the request <ref type="bibr" target="#b10">[10]</ref>. Threads with low memory-level parallelism (MLP) are ranke
-ranked DDR3-2133 memory subsystem. Our memory model is based on the Micron DDR3 DRAM specification <ref type="bibr" target="#b15">[15]</ref>. The microarchitectural features of the baseline processor
nnels used so far. We also cut the number of L2 MSHR entries in half.</p><p>We use weighted speedup <ref type="bibr" target="#b24">[24]</ref> to quantify the schedulers' effects on throughput. To calc
he notion of load criticality used in earlier memory scheduling proposals <ref type="bibr">[9,</ref><ref type="bibr" target="#b10">10]</ref> is typically from the memory's perspective and tends to be rness. The Minimalist Open-page scheduler also ranks threads based on the importance of the request <ref type="bibr" target="#b10">[10]</ref>. Threads with low memory-level parallelism (MLP) are ranke
width.</p><p>Tune et al. use a number of statistics to determine whether an instruction is critical <ref type="bibr" target="#b31">[31]</ref>, based on a series of profiling observations. They flag an
that the scheduler can prioritize among the L2 misses marked critical. Based on Runahead and CLEAR <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" targ that are in the load's dependency chain (easily detectable by "poisoning" the destination register) <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b18">18]</ref>. After the load comp ardware mechanism to approximate this in hardware <ref type="bibr" target="#b5">[5]</ref>. Runahead <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b18">18]</ref> and CLEAR <ref type=
huge boost in performance using Deep-Learning based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target yer. The network input is interpolated to the output size. As done in previous CNN-based SR methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" targe orted numerical were produced using the evaluation script of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n=
a single image has recently received a huge boost in performance using Deep-Learning based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe CNN-based SR methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b3">4]</ref>, we only learn the residual between the interpolated LR and i haustively trained for these conditions. In fact, ZSSR is significantly better than the older SRCNN <ref type="bibr" target="#b3">[4]</ref>, and in some cases achieves comparable or better results tha
, high-quality imaging conditions), current supervised SR methods achieve an incredible performance <ref type="bibr" target="#b19">[20]</ref>. In practice, however, the acquisition process tends to ch
ref type="bibr" target="#b14">[15]</ref> (when the downscaling kernel is unknown), Blind-Deblurring <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2]</ref>, Blind-Dehazing <ref
ns of recognition/classification. Note however, that unlike these approaches for Zero-Shot Learning <ref type="bibr" target="#b22">[23]</ref> or One-shot Learning <ref type="bibr" target="#b18">[19]</
, high-quality imaging conditions), current supervised SR methods achieve an incredible performance <ref type="bibr" target="#b19">[20]</ref>. In practice, however, the acquisition process tends to ch
se approaches for Zero-Shot Learning <ref type="bibr" target="#b22">[23]</ref> or One-shot Learning <ref type="bibr" target="#b18">[19]</ref>, our approach does not require any side information/attrib
fact that image-specific information tends to repeat across scales, whereas noise artifacts do not <ref type="bibr" target="#b24">[25]</ref>. Adding a bit of synthetic noise to the LR sons (but not t
ns of recognition/classification. Note however, that unlike these approaches for Zero-Shot Learning <ref type="bibr" target="#b22">[23]</ref> or One-shot Learning <ref type="bibr" target="#b18">[19]</
these 8 outputs rather than their mean. We further combine it with the back-projection technique of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5]</ref>, so that each of the 8
target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. The recent SotA (State of the Art) method <ref type="bibr" "#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. The recent SotA (State of the Art) method <ref type="bibr" target="#b12">[13]</ref> exceeds previous non-Deep SR methods (supervised <ref type r synthesize ones.</p><p>Lastly, we use a method similar to the geometric self-ensemble proposed in <ref type="bibr" target="#b12">[13]</ref> (which generates 8 different outputs for the 8 rotations+f ut quality, which is up to the user to choose.</p><p>For compariosn, the test-time of leading EDSR+ <ref type="bibr" target="#b12">[13]</ref> grows quadratically with the image size. While it is fast f degradation types, the image-specific CNN obtains significantly better SR results than SotA EDSR+ <ref type="bibr" target="#b12">[13]</ref> (see Sec. 4). Similarly, in the case of non-ideal downscal gly, ZSSR produces competitive results (although VDSR <ref type="bibr" target="#b8">[9]</ref> EDSR+ <ref type="bibr" target="#b12">[13]</ref> Blind-SR <ref type="bibr" target="#b14">[15]</ref> ZSSR [e ails.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth VDSR [9]</head><p>EDSR+ <ref type="bibr" target="#b12">[13]</ref> ZSSR (ours) (PSNR, SSIM) (20.11, 0.9136) (25.29 / 0.9627) ing).</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic interpolation</head><p>EDSR+ <ref type="bibr" target="#b12">[13]</ref> ZSSR (ours) 27.9216 / 0.7504 27.5600 / 0.7135 28.6148 / 0. l. Table <ref type="table">2</ref> compares our against the leading externallysupervised SR methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9]</ref>. We also compared ou
b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, Blind-SR <ref type="bibr" target="#b14">[15]</ref> (when the downscaling kernel is unknown), Blind-Deblurring 5), using K-  The unknown image-specific kernel is estimated directly from the LR test image using <ref type="bibr" target="#b14">[15]</ref>, and fed into our image-specific CNN as the downscaling ke able and provided (e.g., the downscaling kernel can be estimated directly from the test image using <ref type="bibr" target="#b14">[15]</ref>), our image-specific CNN can make good use of this at test timate of the kernel can be computed directly from the test image itself (e.g., using the method of <ref type="bibr" target="#b14">[15]</ref>). Such rough kernel estimations suffice to obtain +1dB imp SR <ref type="bibr" target="#b8">[9]</ref> EDSR+ <ref type="bibr" target="#b12">[13]</ref> Blind-SR <ref type="bibr" target="#b14">[15]</ref> ZSSR [estimated kernel] (ours) ZSSR [true kernel] (ours) 2 ibr" target="#b8">9]</ref>. We also compared our performance to the unsupervised Blind-SR method of <ref type="bibr" target="#b14">[15]</ref>. We considered two cases for applying ZSSR: (i) The more r applying ZSSR: (i) The more realistic scenario of unknown downscaling kernel. For this mode we used <ref type="bibr" target="#b14">[15]</ref> to evaluate the kernel directly from the test image and fe e the kernel directly from the test image and fed it to ZSSR. The unknown SR kernel is estimated in <ref type="bibr" target="#b14">[15]</ref> by seeking a non-parametric downscaling kernel which maxim ref type="figure">2</ref>, and project website). Interestingly, the unsupervised Blind-SR method of <ref type="bibr" target="#b14">[15]</ref>, which does not use deep learning, also outperforms SotA S
target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. The recent SotA (State of the Art) method <ref type="bibr" "#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. The recent SotA (State of the Art) method <ref type="bibr" target="#b12">[13]</ref> exceeds previous non-Deep SR methods (supervised <ref type r synthesize ones.</p><p>Lastly, we use a method similar to the geometric self-ensemble proposed in <ref type="bibr" target="#b12">[13]</ref> (which generates 8 different outputs for the 8 rotations+f ut quality, which is up to the user to choose.</p><p>For compariosn, the test-time of leading EDSR+ <ref type="bibr" target="#b12">[13]</ref> grows quadratically with the image size. While it is fast f degradation types, the image-specific CNN obtains significantly better SR results than SotA EDSR+ <ref type="bibr" target="#b12">[13]</ref> (see Sec. 4). Similarly, in the case of non-ideal downscal gly, ZSSR produces competitive results (although VDSR <ref type="bibr" target="#b8">[9]</ref> EDSR+ <ref type="bibr" target="#b12">[13]</ref> Blind-SR <ref type="bibr" target="#b14">[15]</ref> ZSSR [e ails.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth VDSR [9]</head><p>EDSR+ <ref type="bibr" target="#b12">[13]</ref> ZSSR (ours) (PSNR, SSIM) (20.11, 0.9136) (25.29 / 0.9627) ing).</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic interpolation</head><p>EDSR+ <ref type="bibr" target="#b12">[13]</ref> ZSSR (ours) 27.9216 / 0.7504 27.5600 / 0.7135 28.6148 / 0. l. Table <ref type="table">2</ref> compares our against the leading externallysupervised SR methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9]</ref>. We also compared ou
ven from very small LR images, the SR is performed gradually <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. Our algorithm is applied for several intermediate scale-fa
rong under growing uncertainty and image degradations (see <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref> for details).</p></div> <div xmlns="http://www.tei-c.org/ns
ability to numerically evaluate the results. For this purpose we created a new dataset from BSD100 <ref type="bibr" target="#b13">[14]</ref> by downscaling the HR images using random (but reasonably . To test the robustness of ZSSR in coping with unknown damage, we chose for each image from BSD100 <ref type="bibr" target="#b13">[14]</ref> a random type of degradation out of 3 degradations: (i) Ga
ge, was shown to be a very strong property of natural images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>. This formed the basis for many unsupervised image enhancem t image scales. This observation was empirically verified by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref> using hundreds of natural images, and was shown to be true isual entropy inside a single image is much smaller than in a general external collection of images <ref type="bibr" target="#b23">[24]</ref>, a small and simple CNN suffices for this image-specific t y natural image <ref type="bibr" target="#b4">[5]</ref>.</p><p>In fact, it was empirically shown by <ref type="bibr" target="#b23">[24]</ref> that the internal entropy of patches inside a single image e was further shown to be particularly strong under growing uncertainty and image degradations (see <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref> for details).</p><
b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, Blind-SR <ref type="bibr" target="#b14">[15]</ref> (when the downscaling kernel is unknown), Blind-Deblurring 5), using K-  The unknown image-specific kernel is estimated directly from the LR test image using <ref type="bibr" target="#b14">[15]</ref>, and fed into our image-specific CNN as the downscaling ke able and provided (e.g., the downscaling kernel can be estimated directly from the test image using <ref type="bibr" target="#b14">[15]</ref>), our image-specific CNN can make good use of this at test timate of the kernel can be computed directly from the test image itself (e.g., using the method of <ref type="bibr" target="#b14">[15]</ref>). Such rough kernel estimations suffice to obtain +1dB imp SR <ref type="bibr" target="#b8">[9]</ref> EDSR+ <ref type="bibr" target="#b12">[13]</ref> Blind-SR <ref type="bibr" target="#b14">[15]</ref> ZSSR [estimated kernel] (ours) ZSSR [true kernel] (ours) 2 ibr" target="#b8">9]</ref>. We also compared our performance to the unsupervised Blind-SR method of <ref type="bibr" target="#b14">[15]</ref>. We considered two cases for applying ZSSR: (i) The more r applying ZSSR: (i) The more realistic scenario of unknown downscaling kernel. For this mode we used <ref type="bibr" target="#b14">[15]</ref> to evaluate the kernel directly from the test image and fe e the kernel directly from the test image and fed it to ZSSR. The unknown SR kernel is estimated in <ref type="bibr" target="#b14">[15]</ref> by seeking a non-parametric downscaling kernel which maxim ref type="figure">2</ref>, and project website). Interestingly, the unsupervised Blind-SR method of <ref type="bibr" target="#b14">[15]</ref>, which does not use deep learning, also outperforms SotA S
fact that image-specific information tends to repeat across scales, whereas noise artifacts do not <ref type="bibr" target="#b24">[25]</ref>. Adding a bit of synthetic noise to the LR sons (but not t
use deep learning, also outperforms SotA SR methods. This supports the analysis and observations of <ref type="bibr" target="#b17">[18]</ref>, that (i) an accurate downscaling model is more important
ing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2]</ref>, Blind-Dehazing <ref type="bibr" target="#b2">[3]</ref>, and more. While such unsupervised methods can exploit image
(when the downscaling kernel is unknown), Blind-Deblurring <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2]</ref>, Blind-Dehazing <ref type="bibr" target="#b2">[3]</ref>, and
s guaranteed to converge to a local optimum of it.</p><p>Inspired by the previous work on CycleGANs <ref type="bibr" target="#b29">(Zhu et al., 2017)</ref> and dual learning <ref type="bibr" target="#
ora go back to statistical decipherment <ref type="bibr" target="#b23">(Ravi and Knight, 2011;</ref><ref type="bibr" target="#b5">Dou and Knight, 2012)</ref>. These methods see the source language as
autoencoding and back-translation to train a dual model ini-tialized with cross-lingual embeddings <ref type="bibr" target="#b3">(Artetxe et al., 2018c;</ref><ref type="bibr" target="#b14">Lample et evel translation.</p><p>More recently, the task got a renewed interest after the concurrent work of <ref type="bibr" target="#b3">Artetxe et al. (2018c)</ref> and <ref type="bibr" target="#b14">Lample
odel and a distortion model. This initial system is then refined through iterative back-translation <ref type="bibr" target="#b25">(Sennrich et al., 2016)</ref> which, in the case of <ref type="bibr" site directions, uses one of them to generates a synthetic parallel corpus through back-translation <ref type="bibr" target="#b25">(Sennrich et al., 2016)</ref>, and applies MERT to tune the model in rection by performing a single pass over a synthetic parallel corpus built through back-translation <ref type="bibr" target="#b25">(Sennrich et al., 2016)</ref>. <ref type="foot" target="#foot_4">9</r
or Bayesian inference. This basic approach was later improved by incorporating syntactic knowledge <ref type="bibr" target="#b6">(Dou and Knight, 2013)</ref> and word embeddings <ref type="bibr" targ
T implementation is based on Moses<ref type="foot" target="#foot_5">10</ref> , and we use the KenLM <ref type="bibr" target="#b12">(Heafield et al., 2013)</ref> tool included in it to estimate our 5-g
report detokenized BLEU scores as computed by SacreBLEU<ref type="foot" target="#foot_8">13</ref>  <ref type="bibr" target="#b22">(Post, 2018)</ref>, which is equivalent to the official mteval-v13a.p
n a parallel validation corpus, which is typically done through Minimum Error Rate Training or MERT <ref type="bibr" target="#b20">(Och, 2003)</ref>. Needless to say, this cannot be done in strictly u
, 2017</ref><ref type="bibr" target="#b1">(Artetxe et al., , 2018a) )</ref> or adversarial training <ref type="bibr" target="#b4">(Conneau et al., 2018)</ref>. The resulting crosslingual embeddings ar
arget="#b14">Lample et al. (2018a)</ref>, adversarial training. This method was further improved by <ref type="bibr" target="#b27">Yang et al. (2018)</ref>, who use two language-specific encoders shar
arget="#b14">Lample et al. (2018a)</ref>, adversarial training. This method was further improved by <ref type="bibr" target="#b27">Yang et al. (2018)</ref>, who use two language-specific encoders shar
19">(Schuster and Nakajima, 2012;</ref><ref type="bibr" target="#b4">Chitnis and DeNero, 2015;</ref><ref type="bibr" target="#b21">Sennrich et al., 2016;</ref><ref type="bibr" target="#b31">Wu et al., ref type="table">1</ref>: Multiple subword sequences encoding the same sentence "Hello World" (BPE) <ref type="bibr" target="#b21">(Sennrich et al., 2016)</ref> is a de facto standard subword segmenta type="bibr" target="#b16">(Nong et al., 2009)</ref>, where T is the size of the corpus. Similar to <ref type="bibr" target="#b21">(Sennrich et al., 2016)</ref>, we do not consider subwords that cross 2004)</ref>. The same mark is used in Table <ref type="table">4</ref> and<ref type="table">6</ref>. <ref type="bibr" target="#b21">(Sennrich et al., 2016)</ref> and our unigram model with or without s bword segmentations with language model 3.1 Byte-Pair-Encoding (BPE)</p><p>Byte-Pair-Encoding (BPE) <ref type="bibr" target="#b21">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b19">Schuster
on approach for dealing with the open vocabulary issue is to break up rare words into subword units <ref type="bibr" target="#b19">(Schuster and Nakajima, 2012;</ref><ref type="bibr" target="#b4">Chit ng (BPE)</p><p>Byte-Pair-Encoding (BPE) <ref type="bibr" target="#b21">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b19">Schuster and Nakajima, 2012</ref>) is a subword segmentation algorith abilities. The unigram language model makes an assumption that 1 Strictly speaking, wordpiece model <ref type="bibr" target="#b19">(Schuster and Nakajima, 2012)</ref> is different from BPE. We conside
slation to make the encoder truly learn the compositionality of input sentences.</p><p>Word dropout <ref type="bibr" target="#b9">(Iyyer et al., 2015)</ref> is a simple approach for a bag-of-words rep pe="table">6</ref>: Comparison on different regularization strategies (IWSLT15/17, l = 64, ? = 0.1) <ref type="bibr" target="#b9">(Iyyer et al., 2015)</ref> and image caption generation <ref type="bib
most from unsegmented raw sentences in these languages.</p><p>We used the case sensitive BLEU score <ref type="bibr" target="#b17">(Papineni et al., 2002)</ref> as an evaluation metric. As the output
chitectures.</p><p>Lattice-to-sequence models <ref type="bibr" target="#b24">(Su et al., 2017;</ref><ref type="bibr" target="#b22">Sperber et al., 2017)</ref> are natural extension of sequence-to-sequ
segmentation candidates built from the input sentence X. x * is obtained with the Viterbi algorithm <ref type="bibr" target="#b30">(Viterbi, 1967)</ref>.</p><p>If the vocabulary V is given, subword oc
g Auto Encoder <ref type="bibr" target="#b27">(Vincent et al., 2008)</ref> and Adversarial Training <ref type="bibr" target="#b8">(Goodfellow et al., 2015)</ref>.</p></div><figure xmlns="http://www.te
e idea of noise injection has previously been used in the context of Denoising Auto-Encoders (DAEs) <ref type="bibr" target="#b27">(Vincent et al., 2008)</ref>, where noise is added to the inputs and re the application of subword regularization for machine learning, including Denoising Auto Encoder <ref type="bibr" target="#b27">(Vincent et al., 2008)</ref> and Adversarial Training <ref type="bibr
algorithm applied to many NMT systems and achieving top translation quality in several shared tasks <ref type="bibr" target="#b5">(Denkowski and Neubig, 2017;</ref><ref type="bibr" target="#b15">Nakaz
bstrings can be enumerated in O(T ) time and O(20T ) space with the Enhanced Suffix Array algorithm <ref type="bibr" target="#b16">(Nong et al., 2009)</ref>, where T is the size of the corpus. Similar
rom all possible segmentations, we use the Forward-Filtering and Backward-Sampling algorithm (FFBS) <ref type="bibr" target="#b20">(Scott, 2002)</ref>, a variant of the dynamic programming originally
ing network inference algorithms from the biological network, involving various biomedical entities <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
ge number of similarity measures have been used to compute the similarities between the drugs pairs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Vilar et al. <ref ted how important heterogeneous features are for DDI prediction task that use multiple data sources <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>. Zhang et al. <ref
s biomedical entities <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Zhang et al. <ref type="bibr" target="#b11">[12]</ref> con
7">[38]</ref>, and the treatment relationship between drugs and diseases from the work of Li and Lu <ref type="bibr" target="#b38">[39]</ref>. Each dataset is used to construct a feature network. The
system tries to learn from different learning algorithms to enhance the overall prediction accuracy <ref type="bibr" target="#b32">[33]</ref>. It is about learning from learned knowledge. Learned know The two main approaches towards meta-learning are stacked generalization and cascade generalization <ref type="bibr" target="#b32">[33]</ref>. In stacked generalization, the base learners are learnt f
tegy discriminates the positive set, "P", from the unlabeled set, "U". Weighted logistic regression <ref type="bibr" target="#b28">[29]</ref> and biased SVM <ref type="bibr" target="#b27">[28]</ref> a
ide-effect similarity among drug pairs and used it to predict drug interaction. Later, Vilar et al. <ref type="bibr" target="#b17">[18]</ref> modelled an interaction profile fingerprint as a similarit ures, the feature networks are constructed.</p><p>The one hot-representation used in previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref> has the disadvanta
we have a lot of data sources like DrugBank <ref type="bibr" target="#b23">[24]</ref> and TWOSIDES <ref type="bibr" target="#b24">[25]</ref>, from which labeled data can be accessed. However, most of pe="bibr">(1: 44)</ref>. By increasing the number of positive DDIs from other sources like TWOSIDES <ref type="bibr" target="#b24">[25]</ref> and KEGG <ref type="bibr" target="#b49">[50]</ref>, thereb #b39">[40]</ref>, bioinformatics and clinical applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref>. DrugBank <ref type
s biomedical entities <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Zhang et al. <ref type="bibr" target="#b11">[12]</ref> con
d, similarity-based and network-based inferences. Knowledge-based methods <ref type="bibr">[4,</ref><ref type="bibr" target="#b4">5]</ref> utilize scientific literature documents (MEDLINE), Electronic
we have a lot of data sources like DrugBank <ref type="bibr" target="#b23">[24]</ref> and TWOSIDES <ref type="bibr" target="#b24">[25]</ref>, from which labeled data can be accessed. However, most of pe="bibr">(1: 44)</ref>. By increasing the number of positive DDIs from other sources like TWOSIDES <ref type="bibr" target="#b24">[25]</ref> and KEGG <ref type="bibr" target="#b49">[50]</ref>, thereb #b39">[40]</ref>, bioinformatics and clinical applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref>. DrugBank <ref type
uccessful use of CNNs in image tasks, a newly proposed sequential recommender, referred to as Caser <ref type="bibr" target="#b28">[29]</ref>, abandoned RNN structures, proposing instead a convolution la">3</ref>) ( see <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>).</p><formula xml:i
item x i conditioned on all the previous items x 0:i−1 . A similar setup has been explored by NADE <ref type="bibr" target="#b18">[19]</ref>, PixelRNN/CNN <ref type="bibr" target="#b22">[23,</ref><re
rix factorization <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref> approaches. Compa
idual mapping F (E) is much easier than the original, unreferenced mapping H (E).</p><p>Inspired by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>, we introduce two
item x i conditioned on all the previous items x 0:i−1 . A similar setup has been explored by NADE <ref type="bibr" target="#b18">[19]</ref>, PixelRNN/CNN <ref type="bibr" target="#b22">[23,</ref><re
"#fig_1">2</ref>). Specifically, each block is made up of the normalization, activation (e.g., ReLU <ref type="bibr" target="#b20">[21]</ref>), convolutional layers and a skip connection in a specific
DE <ref type="bibr" target="#b18">[19]</ref>, PixelRNN/CNN <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> in biological and image domains.</p><p>Owing to the ability
item x i conditioned on all the previous items x 0:i−1 . A similar setup has been explored by NADE <ref type="bibr" target="#b18">[19]</ref>, PixelRNN/CNN <ref type="bibr" target="#b22">[23,</ref><re
on the markov chain <ref type="bibr" target="#b4">[5]</ref> and feature-based matrix factorization <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" t
ities represent the most relevant recommendations. While effective, these RNN-based models, such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>, depend on a hidden
item x i conditioned on all the previous items x 0:i−1 . A similar setup has been explored by NADE <ref type="bibr" target="#b18">[19]</ref>, PixelRNN/CNN <ref type="bibr" target="#b22">[23,</ref><re
ead n="3.2">Tucker Decomposition</head><p>Tucker decomposition, named after Ledyard R.</p><p>Tucker <ref type="bibr" target="#b26">(Tucker, 1964)</ref>, decomposes a tensor into a set of matrices and
tion so far have been linear, based on various methods of factorizing the third-order binary tensor <ref type="bibr" target="#b16">(Nickel et al., 2011;</ref><ref type="bibr" target="#b28">Yang et al. expressiveness.</p><p>Finally, we show that several previous stateof-the-art linear models, RESCAL <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref>, DistMult <ref type="bibr" target="#b28"> lated Work</head><p>Several linear models for link prediction have previously been proposed: RESCAL <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref>  HypER <ref type="bibr" target="#b0">(Bal d><p>Several previous tensor factorization models can be viewed as a special case of TuckER: RESCAL <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref> Following the notation introduced in Sect
br" target="#b16">(Nickel et al., 2011;</ref><ref type="bibr" target="#b28">Yang et al., 2015;</ref><ref type="bibr" target="#b25">Trouillon et al., 2016;</ref><ref type="bibr" target="#b9">Kazemi and kel et al., 2011)</ref>, DistMult <ref type="bibr" target="#b28">(Yang et al., 2015)</ref>, ComplEx <ref type="bibr" target="#b25">(Trouillon et al., 2016)</ref> and <ref type="bibr">SimplE (Kazemi an ject and object entity mode, DistMult cannot learn to represent asymmetric relations.</p><p>ComplEx <ref type="bibr" target="#b25">(Trouillon et al., 2016)</ref> Bilinear models represent subject and
st entity and relation embeddings that accurately separate true triples from the false. As shown in <ref type="bibr" target="#b24">(Trouillon et al., 2017)</ref> Proof. Let e s and e o be the n e -dim
four standard link prediction datasets (see Table <ref type="table" target="#tab_4">2</ref>): FB15k <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> is a subset of Freebase, a large database ning set from validation and test sets, making it more difficult for simple models to do well. WN18 <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> is a subset of Word-Net, a hierarchical da elation pair with all possible entities E, ranking the scores obtained. We use the filtered setting <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, i.e. all known true triples are removed f essive: DistMult, because it cannot model asymmetric relations; and transitive models such as TransE<ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> and its variants FTransE<ref type="bibr" t
" target="#b28">Yang et al., 2015;</ref><ref type="bibr" target="#b25">Trouillon et al., 2016;</ref><ref type="bibr" target="#b9">Kazemi and Poole, 2018)</ref>. Recently, state-of-the-art results have s equivalent to a hard regularization of the core tensor of TuckER in the real domain.</p><p>SimplE <ref type="bibr" target="#b9">(Kazemi and Poole, 2018)</ref> The authors show that SimplE belongs to
order SVD in the special case where matrices are orthogonal and the core tensor is "all-orthogonal" <ref type="bibr" target="#b12">(Kroonenberg and De Leeuw, 1980)</ref>. In our case, rows of the matr
a strong baseline for more elaborate models. Tucker decomposition, used widely in machine learning <ref type="bibr" target="#b18">(Schein et al., 2016;</ref><ref type="bibr" target="#b1">Ben-Younes e
st entity and relation embeddings that accurately separate true triples from the false. As shown in <ref type="bibr" target="#b24">(Trouillon et al., 2017)</ref> Proof. Let e s and e o be the n e -dim
e use batch normalization <ref type="bibr" target="#b8">(Ioffe and Szegedy, 2015)</ref> and dropout <ref type="bibr" target="#b21">(Srivastava et al., 2014)</ref> to speed up training. We find that lo
dix A for a complete list of hyper-parameter values on each dataset). We train the model using Adam <ref type="bibr" target="#b10">(Kingma and Ba, 2015)</ref> with the batch size 128.</p><p>At evaluat
elations compared to FB15k and FB15k-237, we set d e = 200 and d r = 30. We use batch normalization <ref type="bibr" target="#b8">(Ioffe and Szegedy, 2015)</ref> and dropout <ref type="bibr" target="#
a strong baseline for more elaborate models. Tucker decomposition, used widely in machine learning <ref type="bibr" target="#b18">(Schein et al., 2016;</ref><ref type="bibr" target="#b1">Ben-Younes e
-c.org/ns/1.0"><head n="6.2">Implementation and Experiments</head><p>We implement TuckER in PyTorch <ref type="bibr" target="#b17">(Paszke et al., 2017)</ref> and make our code available on GitHub. 1
factorizing the third-order binary tensor <ref type="bibr" target="#b16">(Nickel et al., 2011;</ref><ref type="bibr" target="#b28">Yang et al., 2015;</ref><ref type="bibr" target="#b25">Trouillon et a -the-art linear models, RESCAL <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref>, DistMult <ref type="bibr" target="#b28">(Yang et al., 2015)</ref>, ComplEx <ref type="bibr" target="#b25">(Tr quadratically in the entity embedding dimension d e as the number of relations increases. DistMult <ref type="bibr" target="#b28">(Yang et al., 2015)</ref> The scoring function of DistMult (see Table
-c.org/ns/1.0"><head n="6.2">Implementation and Experiments</head><p>We implement TuckER in PyTorch <ref type="bibr" target="#b17">(Paszke et al., 2017)</ref> and make our code available on GitHub. 1
st entity and relation embeddings that accurately separate true triples from the false. As shown in <ref type="bibr" target="#b24">(Trouillon et al., 2017)</ref> Proof. Let e s and e o be the n e -dim
ed using non-linear convolutional models <ref type="bibr" target="#b4">(Dettmers et al., 2018;</ref><ref type="bibr" target="#b0">Balažević et al., 2019)</ref>. Despite achieving very good performance previously been proposed: RESCAL <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref>  HypER <ref type="bibr" target="#b0">(Balažević et al., 2019)</ref> is a simplified convolutional model, th
/ref>  Table <ref type="table">3</ref>: Link prediction results on WN18RR and FB15k-237. The RotatE <ref type="bibr" target="#b22">(Sun et al., 2019)</ref> results are reported without their self-adve
factorizing the third-order binary tensor <ref type="bibr" target="#b16">(Nickel et al., 2011;</ref><ref type="bibr" target="#b28">Yang et al., 2015;</ref><ref type="bibr" target="#b25">Trouillon et a -the-art linear models, RESCAL <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref>, DistMult <ref type="bibr" target="#b28">(Yang et al., 2015)</ref>, ComplEx <ref type="bibr" target="#b25">(Tr quadratically in the entity embedding dimension d e as the number of relations increases. DistMult <ref type="bibr" target="#b28">(Yang et al., 2015)</ref> The scoring function of DistMult (see Table
Bordes et al., 2013)</ref> is a subset of Freebase, a large database of real world facts. FB15k-237 <ref type="bibr" target="#b23">(Toutanova et al., 2015)</ref> was created from FB15k by removing the
arget="#b18">(Schein et al., 2016;</ref><ref type="bibr" target="#b1">Ben-Younes et al., 2017;</ref><ref type="bibr" target="#b30">Yang and Hospedales, 2017)</ref>, factorizes a tensor into a core ten
r any combination of input parameters ( § 3).</p><p>• Based on the red-blue pebble game abstraction <ref type="bibr" target="#b34">[34]</ref>, we provide a new method of deriving I/O lower bounds (Lem o parents (or no children, respectively). Red-Blue Pebble Game Hong and Kung's red-blue pebble game <ref type="bibr" target="#b34">[34]</ref> models an execution of an algorithm in a two-level memory achinery for deriving I/O lower bounds for general CDAGs. We extend the main lemma by Hong and Kung <ref type="bibr" target="#b34">[34]</ref>, which provides a method to nd an I/O lower bound for a gi rresponding pebbling. Hong and Kung use a speci c variant of this partition, denoted as S-partition <ref type="bibr" target="#b34">[34]</ref>.</p><p>We rst introduce our generalization of S-partition, ation that each V i performs at least S I/O operations, we phrase the lemma by Hong and Kung: L 1 ( <ref type="bibr" target="#b34">[34]</ref>). e minimal number Q of I/O operations for any valid execu ulation of the CDAG, where a calculation is a sequence of allowed moves in the red-blue pebble game <ref type="bibr" target="#b34">[34]</ref>. Divide the complete calculation into h consecutive subcom d Lemma. We now use the above de nitions and observations to generalize the result of Hong and Kung <ref type="bibr" target="#b34">[34]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> he remaining properties of a valid X -partition S(X ), we use the same reasoning as originally done <ref type="bibr" target="#b34">[34]</ref>.</p><p>erefore, a complete calculation performing q &gt; ( , and therefore may be seen as the last step in the long sequence of improved bounds. Hong and Kung <ref type="bibr" target="#b34">[34]</ref> derived an asymptotic bound Ω n 3 / √ S for the sequential xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">General I/O Lower Bounds</head><p>Hong and Kung <ref type="bibr" target="#b34">[34]</ref> analyzed the I/O complexity for general CDAGs in their the
various techniques for loop fusion and proved that in general this problem is NP-hard. Later, Darte <ref type="bibr" target="#b20">[20]</ref> identi ed cases when this problem has polynomial complexit
="#b13">[13]</ref>, triangular solvers <ref type="bibr" target="#b15">[15]</ref>), machine learning <ref type="bibr" target="#b6">[6]</ref>, graph processing <ref type="bibr" target="#b4">[4,</ref><re
f Cannon <ref type="bibr" target="#b10">[10]</ref>, which has been analyzed and extended many times <ref type="bibr" target="#b30">[30]</ref>  <ref type="bibr" target="#b39">[39]</ref>. In the presenc
rassen-like routines <ref type="bibr" target="#b54">[54]</ref>, as in practice they are o en slower <ref type="bibr" target="#b19">[19]</ref>. optimized for square matrices o en perform poorly when ma
es as loop tiling and skewing <ref type="bibr" target="#b59">[59]</ref>, interchanging and reversal <ref type="bibr" target="#b58">[58]</ref>. For programs with multiple loop nests, Kennedy and McKinl
es as loop tiling and skewing <ref type="bibr" target="#b59">[59]</ref>, interchanging and reversal <ref type="bibr" target="#b58">[58]</ref>. For programs with multiple loop nests, Kennedy and McKinl
rocessors is usually determined by the available hardware resources. Cyclops Tensor Framework (CTF) <ref type="bibr" target="#b50">[50]</ref> (an implementation of the 2.5D decomposition) can utilize his work) ScaLAPACK <ref type="bibr" target="#b14">[14]</ref>   # of cores MB communicated per core <ref type="bibr" target="#b50">50</ref> 22</p><formula xml:id="formula_71">(b) Limited memory,m = n
et al. <ref type="bibr" target="#b9">[9]</ref> and extended to rectangular matrices by Frigo et al. <ref type="bibr" target="#b26">[26]</ref>. Demmel el al. <ref type="bibr" target="#b22">[22]</ref> i
factorization algorithms, where updating Schur complements is performed as a rank-k gemm operation <ref type="bibr" target="#b31">[31]</ref>.</p><p>Unfavorable Number of Processors Due to the process
ices and square processor decompositions. Subsequent works <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref> generalized the MMM algorithm to rectangular matrices, di e
structure in graphs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar graph-based ML tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref>. DeepGL <ref type=" -1.</formula><p>Our work differs from previous approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" tar /ref>] in several key points. Specifically, in contrast to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" tar neral class of graph convolution networks which utilize weighted multi-hop motif adjacency matrices <ref type="bibr" target="#b32">[33]</ref> to capture higher-order neighborhoods in graphs. The weigh neighborhoods in graphs. The weighted adjacency matrices are computed using various network motifs <ref type="bibr" target="#b32">[33]</ref>. Fig. <ref type="figure" target="#fig_3">1</ref> shows an ions of relational operators applied to a base graph function such as triangle counts. Rossi et al. <ref type="bibr" target="#b32">[33]</ref> proposed the notion of higher-order network embeddings and ased matrix formulation as a function ? : R N ?N ? R N ?N over a motif adjacency A t ? A similar to <ref type="bibr" target="#b32">[33]</ref>. Given a function ?, we can obtain motif-based matrices ?t While the K-step motif-based adjacencies defined here share some similarity to that of Rossi et al. <ref type="bibr" target="#b32">[33]</ref> we would like to point out that there is an important dist
y, with the rise of deep learning and the success of models such as recursive neural networks (RNN) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b47">48]</ref> for sequential dat
o allow each node to focus on features in its neighborhood that were more relevant while Lee et al. <ref type="bibr" target="#b21">[22]</ref> used attention to guide a walk in the graph to learn an em ing training. Finally, to reduce model variance we can also include an advantage term (see Eq. 2 in <ref type="bibr" target="#b21">[22]</ref>, for instance). Our final loss can then be written as:</p>
perturbations correlated highly to the appearance of certain motif patterns while Paranjape et al. <ref type="bibr" target="#b29">[30]</ref> looked at motifs in temporal networks showing that graphs
the structure and the function of many complex systems that are represented as graphs. Prill et al. <ref type="bibr" target="#b31">[32]</ref> studied motifs in biological networks showing that the dyn
aph to learn an embedding for the graph. More specialized methods of graph attention models include <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> with Choi et al. <re de <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> with Choi et al. <ref type="bibr" target="#b6">[7]</ref> using attention on a medical ontology graph for medical diag
. More specialized methods of graph attention models include <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> with Choi et al. <ref type="bibr" target="#b6">[7]</ref> us et="#b6">[7]</ref> using attention on a medical ontology graph for medical diagnosis and Han et al. <ref type="bibr" target="#b13">[14]</ref> using attention on a knowledge graph for the task of entit
applied directly to many realworld problems whose data come in the form of graphs -social networks <ref type="bibr" target="#b30">[31]</ref> or collaboration/citation networks <ref type="bibr" target vised learning.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>? DeepWalk <ref type="bibr" target="#b30">[31]</ref>: An unsupervised network embedding approach which uses the
ref>: A structured logistic regression model which leverages links between objects.</p><p>? ManiReg <ref type="bibr" target="#b4">[5]</ref>: A framework that can be used for semi-supervised classifica
the tasks of image classification <ref type="bibr" target="#b20">[21]</ref>, image super-resolution <ref type="bibr" target="#b18">[19]</ref>, and video action recognition <ref type="bibr" target="#b9
as a way for models to attend to important parts of the data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. The technique has been successfully adopted by models solv successfully adopted by models solving a variety of tasks. For instance, it was used by Mnih et al. <ref type="bibr" target="#b25">[26]</ref> to take glimpses of relevant parts of an input image for i ctions affect the outcome, we continue this process until we reach the first layer. Please refer to <ref type="bibr" target="#b25">[26]</ref> for a more detailed explanation of the REINFORCE rule for
ETS</head><p>Dilated convolutions were originally proposed for the computation of wavelet transform <ref type="bibr" target="#b37">[38]</ref> and employed in the deep learning context (as an alternati
segmentation <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In dilated convolutional layers, filter weights are emplo
">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref> incorporate the benefits of the multi-context paradigm in ction. Others <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref> extract and merge features from distinct layers in order t ">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref> have been proposed to perform semantic segmentation in rem target="#b35">[36]</ref>, with the final layer having connections to the previous ones. Wang et al. <ref type="bibr" target="#b26">[27]</ref> proposed to extract features from distinct layers of the n of overall accuracy, was 90.3% achieved by DLR 9 <ref type="bibr" target="#b21">[22]</ref> and GSN3 <ref type="bibr" target="#b26">[27]</ref>. Our best result (UFMG 4) appears in fifth place by yieldi
ed to previous state-of-the-art methods <ref type="bibr" target="#b11">[12]</ref>, such as lowlevel <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref> and mid-level (
ision making of a wide range of fields, including environmental monitoring, intelligent agriculture <ref type="bibr" target="#b4">[5]</ref>, disaster relief <ref type="bibr" target="#b5">[6]</ref>, <r lines</head><p>For the coffee dataset, we employed the Cascaded Convolutional Neural Network (CCNN) <ref type="bibr" target="#b4">[5]</ref> as baseline. This method employs a multi-context strategy by
Some of them <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref> train several distinct layers or networks, one for each co ters is really huge allowing them to use only small values of batch size (10 patches per batch). In <ref type="bibr" target="#b25">[26]</ref>, the authors proposed a multi-context semantic segmentatio experiments to adjust such size (as done in several works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b40">[41]</ref>), in this secti
f the image. For the GRSS Data Fusion Dataset, we employed, as baseline, the work of Santana et al. <ref type="bibr" target="#b45">[46]</ref>. Their algorithm extracts features with many levels of con though all of them achieved comparable results, the best outcome was yielded by the Deep Contextual <ref type="bibr" target="#b45">[46]</ref>. This method also leverages from multicontext information, proposed approach, which improves the results for all metrics when compared to the Deep Contextual <ref type="bibr" target="#b45">[46]</ref> approach. This reaffirms the effectiveness of the proposed
has been proven to be essential for segmentation methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, given that it allows the model to extract and capture pat this order), which are the most useful and representative ones for discriminating vegetation areas <ref type="bibr" target="#b22">[23]</ref>. More specifically, the dataset consists of 1,250,000 pixe
larger ones (for instance, 256 × 256).</p><p>Considering this, a full set of experiments (guided by <ref type="bibr" target="#b39">[40]</ref>) was performed in order to define the best architectures. .04.3 LTS was used as operating system.</p><p>As previously stated, a set of experiments (guided by <ref type="bibr" target="#b39">[40]</ref>) was executed to define the hyperparameters. After all the
he effectiveness of the proposed method, we carried out experiments on four high-resolution remote  <ref type="bibr" target="#b40">[41]</ref> is composed of 5 images taken by the SPOT sensor in 2005 o side this, for both aforementioned datasets, we also considered as baseline the method conceived by <ref type="bibr" target="#b40">[41]</ref>, in which specific networks are used to perform labeling s N) <ref type="bibr" target="#b16">[17]</ref>. In this case, the pixelwise architectures proposed by <ref type="bibr" target="#b40">[41]</ref> were converted into fully convolutional network and exploi 8]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Again, the pixelwise architectures proposed by <ref type="bibr" target="#b40">[41]</ref> were converted into deconvolutional network (based on the e trained traditionally using patches of constant size defined according to a set of experiments of <ref type="bibr" target="#b40">[41]</ref>, i.e., patches of 7×7 and 25×25 for Coffee and GRSS Data F iv> <div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Protocol</head><p>For the Coffee <ref type="bibr" target="#b40">[41]</ref> and the GRSS Data Fusion <ref type="bibr" target="#b41">[4 SS Data Fusion <ref type="bibr" target="#b41">[42]</ref> datasets, we employed the same protocol of <ref type="bibr" target="#b40">[41]</ref>. Specifically, for the former dataset, we conducted a five "table" target="#tab_3">IV</ref>. Note that all experiments were performed using the Coffee dataset <ref type="bibr" target="#b40">[41]</ref>, Dilated6 network (Figure <ref type="figure">4a</ref>), ac aining the remaining ones. Specifically, these experiments were conducted using: the Coffee dataset <ref type="bibr" target="#b40">[41]</ref>, Dilated6 network (Figure <ref type="figure">4a</ref>), un n several works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b40">[41]</ref>), in this section, the patch size range is analyzed in ord ated over several ranges, selected based on previous works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Specifically, each dataset was evaluated in a large range some insights about how the proposed method behaves during such step.</p><p>For the Coffee dataset <ref type="bibr" target="#b40">[41]</ref>, obtained results are all very similar making it difficult lysis performed on previous sections, we have conducted several experiments over the Coffee dataset <ref type="bibr" target="#b40">[41]</ref>. Results of these experiments, as well as the state-of-the #fig_5">6</ref>.</p><p>Overall, all baselines produced similar results. While the pixelwise network <ref type="bibr" target="#b40">[41]</ref> yielded a slightly worse result with a higher standard dev s on four high-resolution remote sensing datasets with very distinct properties: (i) Coffee dataset <ref type="bibr" target="#b40">[41]</ref>, composed multispectral high-resolution scenes of coffee c
e <ref type="bibr" target="#b4">[5]</ref>, disaster relief <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, urban planning <ref type="bibr" target="#b7">[8]</ref>. K. pe="bibr" target="#b41">[42]</ref>, the best result was obtained when considering the largest range <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ
labels) is the great advantage of deep learning when compared to previous state-of-the-art methods <ref type="bibr" target="#b11">[12]</ref>, such as lowlevel <ref type="bibr" target="#b12">[13]</ref
ity <ref type="bibr" target="#b0">[1]</ref>, allowing a better understanding of the Earth's surface <ref type="bibr" target="#b1">[2]</ref>. Towards such understanding, one of the most important task
p><p>2) GRSS Data Fusion Dataset: Proposed for the 2014 IEEE GRSS Data Fusion Contest, this dataset <ref type="bibr" target="#b41">[42]</ref> is composed of two (training and testing) fine-resolution Protocol</head><p>For the Coffee <ref type="bibr" target="#b40">[41]</ref> and the GRSS Data Fusion <ref type="bibr" target="#b41">[42]</ref> datasets, we employed the same protocol of <ref type="bibr For remaining datasets, a specific range achieved the best result. For the GRSS Data Fusion dataset <ref type="bibr" target="#b41">[42]</ref>, the best result was obtained when considering the largest Data Fusion Dataset: We also performed several experiments on the GRSS Data Fusion Contest dataset <ref type="bibr" target="#b41">[42]</ref> considering all analysis carried out in previous sections. spectral high-resolution scenes of coffee crops and non-coffee areas, (ii) GRSS Data Fusion dataset <ref type="bibr" target="#b41">[42]</ref>, consisting of very high-resolution of visible spectrum im get="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr">56,
vision tasks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref>. Towards a better understanding of the Earth's surface, a
on data provided by new sensor technologies has opened new horizons to the remote sensing community <ref type="bibr" target="#b0">[1]</ref>, allowing a better understanding of the Earth's surface <ref
on data provided by new sensor technologies has opened new horizons to the remote sensing community <ref type="bibr" target="#b0">[1]</ref>, allowing a better understanding of the Earth's surface <ref


">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref> incorporate the benefits of the multi-context paradigm in ction. Others <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref> extract and merge features from distinct layers in order t ">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref> have been proposed to perform semantic segmentation in rem target="#b35">[36]</ref>, with the final layer having connections to the previous ones. Wang et al. <ref type="bibr" target="#b26">[27]</ref> proposed to extract features from distinct layers of the n of overall accuracy, was 90.3% achieved by DLR 9 <ref type="bibr" target="#b21">[22]</ref> and GSN3 <ref type="bibr" target="#b26">[27]</ref>. Our best result (UFMG 4) appears in fifth place by yieldi
on data provided by new sensor technologies has opened new horizons to the remote sensing community <ref type="bibr" target="#b0">[1]</ref>, allowing a better understanding of the Earth's surface <ref
o a semantic class. Based on this idea, several networks <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref> achieved stateof-the-art for the labeling task in the comp type="bibr" target="#b16">[17]</ref> and deconvolutions <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. These networks can also process patches of varying size, and exploited as baseline. (ii) Deconvolutional networks <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Again, the pixelwise architectures proposed by <ref type=
.</p><p>Recently, van Renesse and Schneider presented a chain replication method for object storage <ref type="bibr" target="#b46">[47]</ref> over fail-stop servers, designed to provide strong consist bject storage system that, while maintaining the strong consistency properties of chain replication <ref type="bibr" target="#b46">[47]</ref>, provides lower latency and higher throughput for read ope ain, with transmission costs equally spread over all nodes. The simulation results of previous work <ref type="bibr" target="#b46">[47]</ref> showed competitive or superior throughput for CR compared cular, CRAQ's choice of allowing a node to join anywhere in a chain (as opposed only to at its tail <ref type="bibr" target="#b46">[47]</ref>), as well as properly handling failures during recovery, r An alternative approach, taken by GFS <ref type="bibr" target="#b21">[22]</ref> and promoted in CR <ref type="bibr" target="#b46">[47]</ref>, is to use the membership management service as a director lti-chain multi-object updates was heavily influenced by Sinfonia.</p><p>CRAQ and Chain Replication <ref type="bibr" target="#b46">[47]</ref> are both examples of object-based storage systems that exp
rea; recent examples include both Chain Replication and the ring-based protocol of Guerraoui et al. <ref type="bibr" target="#b24">[25]</ref>, which uses a two-phase write protocol and delays reads du
e explores how to improve reliability while minimizing replica maintenance under transient failures <ref type="bibr" target="#b10">[11]</ref>. Strongly-consistent mutable data is considered by OceanSt
the figure by the dotted blue arrow-which returns its known version number for the requested object <ref type="bibr" target="#b0">(1)</ref>. The dirty node then returns the old object value (V 1 ) ass <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref> and quorum systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref>. These protocols pro
the figure by the dotted blue arrow-which returns its known version number for the requested object <ref type="bibr" target="#b0">(1)</ref>. The dirty node then returns the old object value (V 1 ) ass <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref> and quorum systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref>. These protocols pro
d to malicious settings, both for state machine replication <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref> and quorum systems <ref type="bibr" target="#b0">[1,</ref><
/ref>.</p><p>Focusing on peer-to-peer settings, CFS provides a read-only filesystem on top of a DHT <ref type="bibr" target="#b13">[14]</ref>; Carbonite explores how to improve reliability while minim
><ref type="bibr" target="#b33">34]</ref> and quorum systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref>. These protocols provide linearizability across all operati
erance was considered by BASE <ref type="bibr" target="#b18">[19]</ref> and Brewer's CAP conjecture <ref type="bibr" target="#b8">[9]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n=
) to the massively-scalable systems being deployed in commercial datacenters (e.g., Amazon's Dynamo <ref type="bibr" target="#b14">[15]</ref>, Facebook's Cassandra <ref type="bibr" target="#b15">[16]< antics due to their perceived costs (as at Google <ref type="bibr" target="#b21">[22]</ref>, Amazon <ref type="bibr" target="#b14">[15]</ref>, eBay <ref type="bibr" target="#b45">[46]</ref>, and Faceb conciliation. A similar gossip-style antientropy protocol is used in Amazon's Dynamo object service <ref type="bibr" target="#b14">[15]</ref>, to support "always-on" writes and continued operation whe ifiers to a single head node. This is similar to a growing number of datacenter-based object stores <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. An alternative ap
usally related. Causal consistency was studied both for optimistic concurrency control in databases <ref type="bibr" target="#b6">[7]</ref> and for ordered messaging layers for distributed systems <re
/ref>, but it has recently become more common to use models based on the "Transformer" architecture <ref type="bibr" target="#b108">[Vaswani et al., 2017]</ref>. The Transformer was initially shown to ding a comprehensive definition of this model, we refer the interested reader to the original paper <ref type="bibr" target="#b108">[Vaswani et al., 2017]</ref> or follow-up tutorials 3,4 for a more d verall, our encoder-decoder Transformer implementation closely follows its originally-proposed form <ref type="bibr" target="#b108">[Vaswani et al., 2017]</ref>. First, an input sequence of tokens is to generate the answer token-by-token. For WMT English to German, we use the same training data as <ref type="bibr" target="#b108">[Vaswani et al., 2017]</ref> (i.e. News Commentary v13, Common Crawl ="3.1.1">Model</head><p>For our model, we use a standard encoder-decoder Transformer as proposed by <ref type="bibr" target="#b108">Vaswani et al. [2017]</ref>. While many modern approaches to transfe


1.4. We include the language modeling objective due to its historic use as a pre-training objective <ref type="bibr" target="#b20">[Dai and Le, 2015;</ref><ref type="bibr" target="#b85">Ramachandran e y to downstream tasks. This has led to the development of a wide variety of pre-training objectives <ref type="bibr" target="#b20">[Dai and Le, 2015;</ref><ref type="bibr" target="#b85">Ramachandran e
"bibr" target="#b64">Yang et al., 2019;</ref><ref type="bibr" target="#b63">Liu et al., 2019b;</ref><ref type="bibr" target="#b110">Wang et al., 2019a;</ref><ref type="bibr" target="#b100">Song et al. es that combine concepts from multiple common approaches. An analogous exploration was performed by <ref type="bibr" target="#b110">Wang et al. [2019a]</ref>.</p><p>Overall, all of our objectives inge es for setting the proportion of data coming from each task. A similar exploration was performed by <ref type="bibr" target="#b110">Wang et al. [2019a]</ref>.</p><p>Examples-proportional mixing A majo
improves efficiency by pre-training a model to distinguish between real and machine-generated text <ref type="bibr" target="#b2">[Anonymous, 2019]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns
but is then fine-tuned on the individual supervised tasks. This is the method used by the "MT-DNN" <ref type="bibr" target="#b62">[Liu et al., 2015</ref><ref type="bibr" target="#b63">[Liu et al., , well as pre-training on the unsupervised task alone. This is the approach advocated by the "MT-DNN" <ref type="bibr" target="#b62">[Liu et al., 2015</ref><ref type="bibr" target="#b63">[Liu et al., ,
reviously been used as a source of text data for NLP, for example to train an n-gram language model <ref type="bibr" target="#b12">[Buck et al., 2014]</ref>, for commonsense reasoning <ref type="bibr"
NN/Daily Mail, we find the performance of models on the ROUGE-1-F, ROUGE-2-F, and ROUGE-L-F metrics <ref type="bibr" target="#b59">[Lin, 2004]</ref> to be highly correlated so we report the ROUGE-2-F
"bibr" target="#b64">Yang et al., 2019;</ref><ref type="bibr" target="#b63">Liu et al., 2019b;</ref><ref type="bibr" target="#b110">Wang et al., 2019a;</ref><ref type="bibr" target="#b100">Song et al. es that combine concepts from multiple common approaches. An analogous exploration was performed by <ref type="bibr" target="#b110">Wang et al. [2019a]</ref>.</p><p>Overall, all of our objectives inge es for setting the proportion of data coming from each task. A similar exploration was performed by <ref type="bibr" target="#b110">Wang et al. [2019a]</ref>.</p><p>Examples-proportional mixing A majo
but is then fine-tuned on the individual supervised tasks. This is the method used by the "MT-DNN" <ref type="bibr" target="#b62">[Liu et al., 2015</ref><ref type="bibr" target="#b63">[Liu et al., , well as pre-training on the unsupervised task alone. This is the approach advocated by the "MT-DNN" <ref type="bibr" target="#b62">[Liu et al., 2015</ref><ref type="bibr" target="#b63">[Liu et al., ,
blem is apparent in Figure <ref type="figure" target="#fig_0">2</ref>. or "Bezier patches," Gouraud <ref type="bibr" target="#b10">[11]</ref> developed an algorithm to shade curved surfaces. With his

tion for curved surfaces; these algorithms have been summarized and classified by Sutherland et al. <ref type="bibr" target="#b5">[6]</ref>. The next section discusses their influence on the way shadi
have been made by Watkins <ref type="bibr" target="#b8">[9]</ref> and by Newell, Newell, and Sancha <ref type="bibr" target="#b9">[10]</ref>. Watkins generates the displayed picture scan line by scan
n the way shading is performed. For example Warnock, who developed one of the first such algorithms <ref type="bibr" target="#b7">[8]</ref>, computed display data by a binary subdivision process: this

have been made by Watkins <ref type="bibr" target="#b8">[9]</ref> and by Newell, Newell, and Sancha <ref type="bibr" target="#b9">[10]</ref>. Watkins generates the displayed picture scan line by scan
have been made by Watkins <ref type="bibr" target="#b8">[9]</ref> and by Newell, Newell, and Sancha <ref type="bibr" target="#b9">[10]</ref>. Watkins generates the displayed picture scan line by scan

tion for curved surfaces; these algorithms have been summarized and classified by Sutherland et al. <ref type="bibr" target="#b5">[6]</ref>. The next section discusses their influence on the way shadi
e two major advances in the development of fast hidden surface algorithms have been made by Watkins <ref type="bibr" target="#b8">[9]</ref> and by Newell, Newell, and Sancha <ref type="bibr" target="#
o-rank (LTR) is challenging due to its biased nature. To address this bias problem, Joachims et al. <ref type="bibr" target="#b19">[19]</ref> proposed a counterfactual inference approach, providing an f type="bibr" target="#b18">[18]</ref>.</p><p>To handle biases in a principled way, Joachims et al. <ref type="bibr" target="#b19">[19]</ref> introduced an unbiased learning-to-rank framework, which i ecting the examination bias in learning to rank from implicit feedback. As shown by Joachims et al. <ref type="bibr" target="#b19">[19]</ref>, the parameters of the PBM can serve as propensity estimat ent d for query q.</p><p>While Pr(E = 1|k) can be used as an estimate of the examination propensity <ref type="bibr" target="#b19">[19]</ref>, it is a rather simplistic model since it assumes that exa max ]. In this case, randomly swapping results at positions k and k ′ before presenting the ranking <ref type="bibr" target="#b19">[19]</ref> makes the expected relevance of results at the two positio ck-through rates is a consistent estimator of the relative propensities p k and p k ′ under the PBM <ref type="bibr" target="#b19">[19]</ref>. Note that knowing the relative propensities with respect sufficient, since the counterfactual ERM learning objective is invariant to multiplicative scaling <ref type="bibr" target="#b19">[19]</ref>.</p><p>While this ratio estimator is a sensible approach f interventions were then used to get a gold-standard estimate of the propensities via the methods in <ref type="bibr" target="#b19">[19]</ref>. To avoid any confounding due to changes in the query dist <ref type="table" target="#tab_0">1</ref>. We then use the gold-standard propensity estimator from <ref type="bibr" target="#b19">[19]</ref> to learn two PBM models from the swap intervention data, o be expected, given that AllPairs makes more efficient use of the data than the ratio-estimates from <ref type="bibr" target="#b19">[19]</ref>.</p><p>Can AllPairs learn CPBM models with many context fe nce compared to using the propensities from the PBM.</p><p>We trained a Clipped Propensity SVM-Rank <ref type="bibr" target="#b19">[19]</ref> for each of the following three propensity models: PBM est ed via cross-validation. For rank r &gt; 21, we impute the propensity p r (x) = p 21 (x). Following <ref type="bibr" target="#b19">[19]</ref>, we measure test-set ranking performance via the average s imitations of existing propensity estimation methods for LTR <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b27">27]</ref>. First, existing me or LTR algorithms like <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">19]</ref>. We evaluate the fidelity of the CPBM model and the effecti <ref type="bibr" target="#b23">[23]</ref>. The most effective methods use randomized interventions <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b26">26]</ref>, which unfortunate first review how explicit interventions have been used for estimating p k := Pr(E = 1|k) in the PBM <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b26">26]</ref>. The PBM requires
ver, they were designed for inferring relevance, not propensities. One example is the Cascade model <ref type="bibr" target="#b10">[10]</ref>, where users scan documents top-down until a relevant docu
ing deep network can then be used to compute context-dependent propensities for LTR algorithms like <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target ="bibr" target="#b24">24]</ref>. IPS has been commonly adopted for unbiased evaluation and learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target
>. The most effective methods use randomized interventions <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b26">26]</ref>, which unfortunately degrade the user's search experience. ave been used for estimating p k := Pr(E = 1|k) in the PBM <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b26">26]</ref>. The PBM requires estimating a single vector p = [p 1 , p 2
" target="#b27">27]</ref>. First, existing methods are restricted to the Position-Based Model (PBM) <ref type="bibr" target="#b9">[9]</ref>, which only models how examination changes with the rank of dom variable denoting whether the user examines d.</p><p>Then according to the Position-Based Model <ref type="bibr" target="#b9">[9]</ref>,</p><formula xml:id="formula_0">Pr(C = 1|q, d, k) = Pr(E = 1
biased feedback. It relies on IPS weighting first developed in causal inference and survey sampling <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b24">24]</ref>. IPS has been comm
type of data suffers from various biases due to both the system and the user, such as position bias <ref type="bibr" target="#b17">[17]</ref>, presentation bias <ref type="bibr" target="#b22">[22]</re
arget="#b17">[17]</ref>, presentation bias <ref type="bibr" target="#b22">[22]</ref> and trust bias <ref type="bibr" target="#b18">[18]</ref>.</p><p>To handle biases in a principled way, Joachims et a
unbiased LTR.</p><p>There are two key limitations of existing propensity estimation methods for LTR <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" targ specifically that there is no dependency between the context and the choice of ranking function f i <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">20]</ref>,</p><formula xml:id= on arbitrary context vectors through a deep network. Furthermore, we present an AllPairs estimator <ref type="bibr" target="#b2">[3]</ref> for learning CPBM models from log data. For training, our es te relevance model, which is just as hard as the LTR problem itself. The approach of Agarwal et al. <ref type="bibr" target="#b2">[3]</ref> avoids both randomized interventions and relevance modeling atent, and even for the simpler PBM model it was found that the propensity estimates can be far off <ref type="bibr" target="#b2">[3]</ref>. A key shortcoming of this approach is that it requires lear ntervention Harvesting for the CPBM</head><p>Instead of explicitly swapping results, Agarwal et al. <ref type="bibr" target="#b2">[3]</ref> have recently shown for the PBM how interventions similar to imator for the CPBM using this data. The key challenge compared to analogous estimators for the PBM <ref type="bibr" target="#b2">[3]</ref> lies in modeling the dependence on context. We start by cons on-uniform assignment mechanism to the uniform intervention distribution in each interventional set <ref type="bibr" target="#b2">[3]</ref>.</p><formula xml:id="formula_15">ĉj k,k ′ (k) := 1 [(x j ,d k,k ′ (k) is proportional to the product of examination propensity p k and average relevance r k,k <ref type="bibr" target="#b2">[3]</ref>, just like for the explicit swap interventions mentioned abo l methods, specifically the intervention harvesting approach previously used for estimating the PBM <ref type="bibr" target="#b2">[3]</ref>.</p><formula xml:id="formula_17">ĥCP BM := argmax h,д j ∈L k t="#tab_1">2</ref> shows the test-set performance. The baseline is a PBM model trained according to <ref type="bibr" target="#b2">[3]</ref>, which is essentially a CPBM model without features and a re
target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b25">25]</ref>. However, because t
unbiased LTR.</p><p>There are two key limitations of existing propensity estimation methods for LTR <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" targ specifically that there is no dependency between the context and the choice of ranking function f i <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">20]</ref>,</p><formula xml:id= on arbitrary context vectors through a deep network. Furthermore, we present an AllPairs estimator <ref type="bibr" target="#b2">[3]</ref> for learning CPBM models from log data. For training, our es te relevance model, which is just as hard as the LTR problem itself. The approach of Agarwal et al. <ref type="bibr" target="#b2">[3]</ref> avoids both randomized interventions and relevance modeling atent, and even for the simpler PBM model it was found that the propensity estimates can be far off <ref type="bibr" target="#b2">[3]</ref>. A key shortcoming of this approach is that it requires lear ntervention Harvesting for the CPBM</head><p>Instead of explicitly swapping results, Agarwal et al. <ref type="bibr" target="#b2">[3]</ref> have recently shown for the PBM how interventions similar to imator for the CPBM using this data. The key challenge compared to analogous estimators for the PBM <ref type="bibr" target="#b2">[3]</ref> lies in modeling the dependence on context. We start by cons on-uniform assignment mechanism to the uniform intervention distribution in each interventional set <ref type="bibr" target="#b2">[3]</ref>.</p><formula xml:id="formula_15">ĉj k,k ′ (k) := 1 [(x j ,d k,k ′ (k) is proportional to the product of examination propensity p k and average relevance r k,k <ref type="bibr" target="#b2">[3]</ref>, just like for the explicit swap interventions mentioned abo l methods, specifically the intervention harvesting approach previously used for estimating the PBM <ref type="bibr" target="#b2">[3]</ref>.</p><formula xml:id="formula_17">ĥCP BM := argmax h,д j ∈L k t="#tab_1">2</ref> shows the test-set performance. The baseline is a PBM model trained according to <ref type="bibr" target="#b2">[3]</ref>, which is essentially a CPBM model without features and a re
al influence. Indeed, extensive work has been done on social influence prediction in the literature <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" ta ial equations extended from the classic 'Susceptible-Infected' (SI) model; Most recently, Li et al. <ref type="bibr" target="#b25">[26]</ref> proposed an end-toend predictor for inferring cascade size efforts to detect those global patterns automatically using deep learning, e.g., the DeepCas model <ref type="bibr" target="#b25">[26]</ref> which formulate cascade prediction as a sequence problem a
t of user-level influence prediction models, most of which <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> consider complicate k, while external sources are assumed to be not present.</p><p>Problem 1. Social Influence Locality <ref type="bibr" target="#b52">[53]</ref> Social influence locality models the probability of v's ac get="#b53">54]</ref> Weibo 6 is the most popular Chinese microblogging service. The dataset is from <ref type="bibr" target="#b52">[53]</ref> and can be downloaded here. 7 The complete dataset contain and the social action is defined to be whether a user retweets "Higgs" related tweets.</p><p>Weibo <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> Weibo 6 is the mos .</p><p>Data Preparation We process the above four datasets following the practice in existing work <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. More concretely,
with Glorot initialization <ref type="bibr" target="#b17">[18]</ref> and trained using the Adagrad <ref type="bibr" target="#b15">[16]</ref> optimizer with learning rate 0.1 (0.05 for Digg dataset),
<ref type="bibr" target="#b53">[54]</ref> as well as the state-ofthe-art graph classification model <ref type="bibr" target="#b33">[34]</ref>. Experimental results suggest that the DeepInf model can s s the classification model. The model use the same features as logistic regression (LR).</p><p>PSCN <ref type="bibr" target="#b33">[34]</ref> As we model social influence locality prediction as a grap ation problem, we compare our framework with the state-of-the-art graph classification models, PSCN <ref type="bibr" target="#b33">[34]</ref>. For each graph, PSCN selects w vertices according to a us ref>, deep graph kernel <ref type="bibr" target="#b51">[52]</ref>, and state-of-the-art method PSCN <ref type="bibr" target="#b33">[34]</ref>. Recently, there have been several attempts to incorporate
on Performance We evaluate the predictive performance of DeepInf in terms of Area Under Curve (AUC) <ref type="bibr" target="#b7">[8]</ref>, Precision (Prec.), Recall (Rec.), and F1-Measure (F1).</p><
ections <ref type="bibr" target="#b6">[7]</ref>, advertising <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, and innovation adoption <ref type="bibr" target="#b41">[42 ks of many real-world problems and applications. For example, in the influence maximization problem <ref type="bibr" target="#b23">[24]</ref>, both independent cascade and linear threshold models assu
with Glorot initialization <ref type="bibr" target="#b17">[18]</ref> and trained using the Adagrad <ref type="bibr" target="#b15">[16]</ref> optimizer with learning rate 0.1 (0.05 for Digg dataset),
s nonlinearity (function д in Eq. 5). All the parameters are initialized with Glorot initialization <ref type="bibr" target="#b17">[18]</ref> and trained using the Adagrad <ref type="bibr" target="#b1
umber of vertices) of G r v 's can be very large due to the small-world property in social networks <ref type="bibr" target="#b49">[50]</ref>. Such variously sized data is unsuited to most deep learni </p><p>Eigenvector Centrality <ref type="bibr" target="#b5">[6]</ref>.</p><p>Clustering Coefficient <ref type="bibr" target="#b49">[50]</ref>. Rarity (reciprocal of ego user's degree) <ref type="bibr"
with Glorot initialization <ref type="bibr" target="#b17">[18]</ref> and trained using the Adagrad <ref type="bibr" target="#b15">[16]</ref> optimizer with learning rate 0.1 (0.05 for Digg dataset),
d are listed in Table <ref type="table" target="#tab_3">2</ref>.</p><p>Support Vector Machine (SVM) <ref type="bibr" target="#b16">[17]</ref> We also use support vector machine (SVM) with linear kerne
been done on social influence prediction in the literature <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" tar ibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. For example, Matsubara et al. <ref type="bibr" target="#b31">[32]</ref> studied the dynamics of social influence by carefully desi ts of a cascade and their correlation with the final cascade size, e.g., the rise-and-fall patterns <ref type="bibr" target="#b31">[32]</ref>, external influence sources <ref type="bibr" target="#b32"
et="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. For example, Matsubara et al. <ref type="bibr" target="#b3 f>, external influence sources <ref type="bibr" target="#b32">[33]</ref>, and conformity phenomenon <ref type="bibr" target="#b42">[43]</ref>. Recently, there have been efforts to detect those global
>[4]</ref>. Pagerank <ref type="bibr" target="#b34">[35]</ref>.</p><p>Hub score and authority score <ref type="bibr" target="#b8">[9]</ref>.</p><p>Eigenvector Centrality <ref type="bibr" target="#b5">
d are listed in Table <ref type="table" target="#tab_3">2</ref>.</p><p>Support Vector Machine (SVM) <ref type="bibr" target="#b16">[17]</ref> We also use support vector machine (SVM) with linear kerne
on Performance We evaluate the predictive performance of DeepInf in terms of Area Under Curve (AUC) <ref type="bibr" target="#b7">[8]</ref>, Precision (Prec.), Recall (Rec.), and F1-Measure (F1).</p><
on Performance We evaluate the predictive performance of DeepInf in terms of Area Under Curve (AUC) <ref type="bibr" target="#b7">[8]</ref>, Precision (Prec.), Recall (Rec.), and F1-Measure (F1).</p><
ach user is only influenced by her near neighbors. Examples of such work include pairwise influence <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>, topic-level influ
"#b14">[15]</ref> and AMiner <ref type="bibr" target="#b43">[44]</ref>. Similar to the treatment in <ref type="bibr" target="#b12">[13]</ref>, we choose 20 popular conferences from data mining, inform ="bibr" target="#b40">[41]</ref>, node2vec <ref type="bibr" target="#b19">[20]</ref>, metap-ath2vec <ref type="bibr" target="#b12">[13]</ref>, NetMF <ref type="bibr" target="#b36">[37]</ref>, etc. Ano
<ref type="bibr" target="#b35">[36]</ref>, LINE <ref type="bibr" target="#b40">[41]</ref>, node2vec <ref type="bibr" target="#b19">[20]</ref>, metap-ath2vec <ref type="bibr" target="#b12">[13]</ref>,
tion as suggested by Velickovic et al. <ref type="bibr" target="#b48">[49]</ref> and Vaswani et al. <ref type="bibr" target="#b47">[48]</ref>. The multi-head attention mechanism performs K independent
Hub score and authority score <ref type="bibr" target="#b8">[9]</ref>.</p><p>Eigenvector Centrality <ref type="bibr" target="#b5">[6]</ref>.</p><p>Clustering Coefficient <ref type="bibr" target="#b49"
global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type="bibr" target="#b0">[1]</ref>, and it was used in <ref type="bibr" target="#b1">[2]</ref> nd of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type="bibr" target="#b0">(1)</ref>. Because of memory limitations on our machines, when |S| exc rting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type="bibr" target="#b0">(1)</ref>, and so now we generate results by performing a second pass
X, the one that provided the final prediction, and only that counter. This is the classical method <ref type="bibr" target="#b5">[6]</ref> : the counter is incremented if the branch is taken, decreme
tion. The leftmost bank on Figure <ref type="figure" target="#fig_0">1</ref> is a bimodal predictor <ref type="bibr" target="#b4">[5]</ref>. We refer to this bank as bank 0. It has 4k entries, and is of groups of consecutive history bits, then it is XORed with the branch PC as in a gshare predictor <ref type="bibr" target="#b4">[5]</ref>. For example, bank 3 is indexed with 40 history bits, and th
tion. The leftmost bank on Figure <ref type="figure" target="#fig_0">1</ref> is a bimodal predictor <ref type="bibr" target="#b4">[5]</ref>. We refer to this bank as bank 0. It has 4k entries, and is of groups of consecutive history bits, then it is XORed with the branch PC as in a gshare predictor <ref type="bibr" target="#b4">[5]</ref>. For example, bank 3 is indexed with 40 history bits, and th
ginally introduced for text compression <ref type="bibr" target="#b0">[1]</ref>, and it was used in <ref type="bibr" target="#b1">[2]</ref> for branch prediction. Figure <ref type="figure" target="#fi
a synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type="bibr" target="#b2">[3]</ref>, while YAGS <ref type="bibr" target="#b3">[4]</ref>, which i are not folding a random value, but a global history value derived from the previous history value <ref type="bibr" target="#b2">[3]</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows two
tion. The leftmost bank on Figure <ref type="figure" target="#fig_0">1</ref> is a bimodal predictor <ref type="bibr" target="#b4">[5]</ref>. We refer to this bank as bank 0. It has 4k entries, and is of groups of consecutive history bits, then it is XORed with the branch PC as in a gshare predictor <ref type="bibr" target="#b4">[5]</ref>. For example, bank 3 is indexed with 40 history bits, and th
a synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type="bibr" target="#b2">[3]</ref>, while YAGS <ref type="bibr" target="#b3">[4]</ref>, which i are not folding a random value, but a global history value derived from the previous history value <ref type="bibr" target="#b2">[3]</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows two
X, the one that provided the final prediction, and only that counter. This is the classical method <ref type="bibr" target="#b5">[6]</ref> : the counter is incremented if the branch is taken, decreme
be viewed as a 4 th order approximation to PPM <ref type="bibr" target="#b2">[3]</ref>, while YAGS <ref type="bibr" target="#b3">[4]</ref>, which is a GPPM predictor too, can be viewed as a 1 st orde
re exists a longer matching sequence in T , i.e., s∈T f (s.0 u ). More explanations can be found in <ref type="bibr" target="#b6">[7]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n=
ur suite of open-source benchmark circuits. This kind of coverage has been used in related academic <ref type="bibr" target="#b12">[13]</ref> and industrial <ref type="bibr" target="#b4">[5]</ref> wor tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>The prior work most similar to rfuzz is MicroGP <ref type="bibr" target="#b12">[13]</ref> which focuses on maximizing statement coverage in the HDL
rior industrial work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> uses functional coverage models manually specified by verif
rior industrial work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> uses functional coverage models manually specified by verif
and harness generation component, which works on arbitrary RTL circuits described in the FIRRTL IR <ref type="bibr" target="#b5">[6]</ref>. It automatically generates a test harness for software or F Our tool is language-agnostic since it can work on arbitrary RTL designs expressed in the FIRRTL IR <ref type="bibr" target="#b5">[6]</ref>. Once a target design is translated into FIRRTL IR from its
and harness generation component, which works on arbitrary RTL circuits described in the FIRRTL IR <ref type="bibr" target="#b5">[6]</ref>. It automatically generates a test harness for software or F Our tool is language-agnostic since it can work on arbitrary RTL designs expressed in the FIRRTL IR <ref type="bibr" target="#b5">[6]</ref>. Once a target design is translated into FIRRTL IR from its
and harness generation component, which works on arbitrary RTL circuits described in the FIRRTL IR <ref type="bibr" target="#b5">[6]</ref>. It automatically generates a test harness for software or F Our tool is language-agnostic since it can work on arbitrary RTL designs expressed in the FIRRTL IR <ref type="bibr" target="#b5">[6]</ref>. Once a target design is translated into FIRRTL IR from its
m input generator. Our work on the other hand does not assume that a generator exists. Nativ et al. <ref type="bibr" target="#b9">[10]</ref> propose a system that uses coverage feedback to direct a ra
m input generator. Our work on the other hand does not assume that a generator exists. Nativ et al. <ref type="bibr" target="#b9">[10]</ref> propose a system that uses coverage feedback to direct a ra
rior industrial work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> uses functional coverage models manually specified by verif
fication progress and has been used in the past to evaluate automated stimuli generation approaches <ref type="bibr" target="#b4">[5]</ref>.</p><p>When using a software simulator for dynamic verificat coverage has been used in related academic <ref type="bibr" target="#b12">[13]</ref> and industrial <ref type="bibr" target="#b4">[5]</ref> work.</p><p>Most automatic coverage definitions focus on the al evaluation reports that simulation takes 30 times more resources than the core genetic algorithm <ref type="bibr" target="#b4">[5]</ref>. rfuzz on the other hand is geared towards fast FPGA-acceler
network topology is DUT specific and needs to be designed by a verification engineer. Wang et tal. <ref type="bibr" target="#b15">[16]</ref> use a manually designed abstract model of the DUT to autom
ased on re-reference interval prediction and updates the predicted age on a hit in an inclusive LLC <ref type="bibr" target="#b15">[15]</ref>. Although such an option of age update is nonexistent in a he TC-AGE policy is similar to SRRIP-style static algorithms originally proposed for inclusive LLCs <ref type="bibr" target="#b15">[15]</ref>. In our age assignment setting where a lower age correspon licy shows one way to implement DRRIP-style dynamic policies originally proposed for inclusive LLCs <ref type="bibr" target="#b15">[15]</ref>.</p><p>The TC-AGE policy improves performance by more than
nformation of the source instruction of a to-be-filled block <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b13">13]</ref>. Also, several of these studies usually identify the insert ge with LRU, MRU, or other access recency positions in a set <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" tar
ctor that exploits the reuse probabilities to improve the replacement decisions in an inclusive LLC <ref type="bibr" target="#b4">[4]</ref>. On the other hand, our bypass algorithms identify a block t
nformation of the source instruction of a to-be-filled block <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b13">13]</ref>. Also, several of these studies usually identify the insert ge with LRU, MRU, or other access recency positions in a set <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" tar

" target="#b7">[7]</ref>.</p><p>Exclusive caches are functionally equivalent to large victim caches <ref type="bibr" target="#b18">[18]</ref>. Selective victim caching (analogous to an exclusive LLC w
get="#b19">19,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25]</ref> have close connecti
LCs. Some of these studies require PC information of the source instruction of a to-be-filled block <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b13">13]</ref>. Also, several of th tudies usually identify the insertion age with LRU, MRU, or other access recency positions in a set <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" targ
ls have explored bypass algorithms for LLCs. <ref type="foot" target="#foot_2">3</ref> One proposal <ref type="bibr" target="#b6">[6]</ref> remembers the tag of a bypassed block (if the incoming block
context of small victim caches that work well with L1 caches <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b11">11]</ref>. A design of a large victim cache with selective insertion bedded" victim cache <ref type="bibr" target="#b21">[21]</ref>.</p><p>Dead block prediction schemes <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" ta

have a longer access time, and this may increase the critical path length and penalize performance <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this paper we propose a novel register renaming ap
common solutions to provide the rename storage locations:</p><p>• The entries of the reorder buffer <ref type="bibr" target="#b10">[11]</ref>. In this case, the result of every instruction is kept in
used. In this paper we focus on dynamically scheduled processors that implement precise exceptions <ref type="bibr" target="#b8">[9]</ref>. In such processors, instructions are committed in-order. Af
ncy.</p><p>Three cache memory ports and the memory disambiguation scheme implemented in the PA-8000 <ref type="bibr" target="#b5">[6]</ref> have been assumed in this experiment. Up to 8 instructions c
get="#b11">[12]</ref>, and the HAL SPARC64 <ref type="bibr" target="#b2">[3]</ref>. The MIPS R10000 <ref type="bibr" target="#b14">[15]</ref>, and the DEC 21264 <ref type="bibr" target="#b3">[4]</ref> herwise it is reset.</p><p>A mechanism based on checkpointing similar to the one used by the R10000 <ref type="bibr" target="#b14">[15]</ref> could be used to recover from branches in just one cycle.< have been simulated. The first one is the conventional register renaming scheme used by the R10000 <ref type="bibr" target="#b14">[15]</ref> among others, which is based on a physical register file a


1.0"><head>Functional Unit Count Latency</head><p>Simple The processor has a lookup-free data cache <ref type="bibr" target="#b6">[7]</ref> that allows up to 8 pending misses to different cache lines.
ming</head><p>Register renaming was first implemented for the floating-point unit of the IBM 360/91 <ref type="bibr" target="#b13">[14]</ref>. Register renaming is a key issue for the performance of o
on for a DEC AlphaStation 600 5/266 with a DEC 21164 processor, is instrumented using the Atom tool <ref type="bibr" target="#b12">[13]</ref>. The instrumented program is executed and the trace genera
ming</head><p>Register renaming was first implemented for the floating-point unit of the IBM 360/91 <ref type="bibr" target="#b13">[14]</ref>. Register renaming is a key issue for the performance of o
ser intent.</p><p>As a general information modeling method, Heterogeneous Information Network (HIN) <ref type="bibr" target="#b17">[18]</ref>, consisting of multiple types of objects and links, has be y data mining tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. In this paper, we propose to model the intent recommendati
which generates object embeddings based on local neighbors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>, we first propose a UIQ path UQI path metapath-guided heter
<head n="4.3">Detailed Implementation</head><p>We implement the proposed method based on Tensorflow <ref type="bibr" target="#b0">[1]</ref>. For our method, we set the dimension of term embedding as 6
zon, usually extract handcrafted features, and then feed these features to a classifier, e.g., GBDT <ref type="bibr" target="#b6">[7]</ref> and XG-Boost <ref type="bibr" target="#b3">[4]</ref>. These the same input setting as LR, we implement the deep neural network with 3 layers MLP.</p><p>• GBDT <ref type="bibr" target="#b6">[7]</ref>: It is a scalable tree-based model for feature learning and
s have been proposed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>, they mainly employ metapath based features through exploit
<head n="4.3">Detailed Implementation</head><p>We implement the proposed method based on Tensorflow <ref type="bibr" target="#b0">[1]</ref>. For our method, we set the dimension of term embedding as 6
nd links, has been widely applied to many data mining tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. In this paper, we

s have been proposed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>, they mainly employ metapath based features through exploit
eed these features to a classifier, e.g., GBDT <ref type="bibr" target="#b6">[7]</ref> and XG-Boost <ref type="bibr" target="#b3">[4]</ref>. These methods heavily rely on domain knowledge and need lab
gh, some HIN based recommendation methods have been proposed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>, they mainly employ
is load balancing; its use is a complex trade-off between its cost and its benefits. Work stealing <ref type="bibr" target="#b3">[4]</ref> can be considered the most widely used load balancing techni
et="#b7">[8]</ref>, it is also possible to implement these concepts as a library. For instance, HPX <ref type="bibr" target="#b16">[17]</ref> and Charm++ <ref type="bibr" target="#b17">[18]</ref> are
lly designed to improve shared memory performance of existing language extensions, such as Qthreads <ref type="bibr" target="#b26">[27]</ref> or Argobots <ref type="bibr" target="#b23">[24]</ref>; thi
hese concepts as a library. For instance, HPX <ref type="bibr" target="#b16">[17]</ref> and Charm++ <ref type="bibr" target="#b17">[18]</ref> are asynchronous GAS runtimes.</p><p>This already very div
xisting language extensions, such as Qthreads <ref type="bibr" target="#b26">[27]</ref> or Argobots <ref type="bibr" target="#b23">[24]</ref>; this topic is of significant importance, considering the
hardware have also naturally developed with the emergence of accelerator and GPU computing; StarPU <ref type="bibr" target="#b1">[2]</ref> is an example of such an environment.</p><p>In addition, tas cies based on list scheduling and performance models are employed in some many-task runtime systems <ref type="bibr" target="#b1">[2]</ref>. Additionally, hybrid policies, which integrate static and d lelism, it is capable of generating MPI communication from a given task graph and data distribution <ref type="bibr" target="#b1">[2]</ref>; hence, it is marked with explicit support for distributed m 4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref> or offline <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target
erformance observation, performance monitoring software is either generating data to be used online <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target
xisting language extensions, such as Qthreads <ref type="bibr" target="#b26">[27]</ref> or Argobots <ref type="bibr" target="#b23">[24]</ref>; this topic is of significant importance, considering the
xmlns="http://www.tei-c.org/ns/1.0"><p>The Cilk language <ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b4">[5]</ref> allows task-focused parallel programming and is an early exa
are specifically designed languages such as Chapel <ref type="bibr" target="#b6">[7]</ref> and X10 <ref type="bibr" target="#b7">[8]</ref>, it is also possible to implement these concepts as a librar
arget="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref> or offline <ref type="bibr" target="#b1">[2,</ref><ref type
ef>.</p><p>To address this disparity, researchers have designed heterogeneous multi-core processors <ref type="bibr" target="#b2">[2]</ref> in which an application is mapped to the most efficient core gher switching costs of coarse-grained heterogeneous systems <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref> enforce switching granularities of the order of milli-seconds s big</p></note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p><ref type="bibr" target="#b2">2</ref> OinO stands for an InO core appearing to be OoO</p></note>
e for each trace and perfectly predictable control-flow. These results corroborate with recent work <ref type="bibr" target="#b8">[8]</ref> that credit the OoO's ability to create good static schedule
ogether as a trace in a trace cache.</p><p>Compilers use profile-based static scheduling mechanisms <ref type="bibr" target="#b32">[32]</ref> or run-time binary optimization <ref type="bibr" target="#
ogether as a trace in a trace cache.</p><p>Compilers use profile-based static scheduling mechanisms <ref type="bibr" target="#b32">[32]</ref> or run-time binary optimization <ref type="bibr" target="#
ance <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. Recent work <ref type="bibr" target="#b31">[31]</ref> shows that microarchitectural heterogeneity, which can alt
hereby reducing the overall energy consumption. Prior work has proposed heterogeneous architectures <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target seconds. Novel architectures minimize migration costs by either sharing of structures between cores <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b7">7]</ref> or reducing the distan performance loss as compared to execution on big. Our core architecture, based on a Composite Core <ref type="bibr" target="#b4">[4]</ref>, executes a single application thread, and includes both a b /www.tei-c.org/ns/1.0"><head n="3.5">Switching overheads</head><p>DynaMOS adopts the Composite Core <ref type="bibr" target="#b4">[4]</ref> architecture, which tightly couples the big and little cores ducing the distance between them using 3D technology <ref type="bibr" target="#b6">[6]</ref>. Cores <ref type="bibr" target="#b4">[4]</ref> shares access to L1 caches, TLBs, fetch unit and branch pred
p><p>? big detects traces with repeatable schedules and stores them in a Schedule Trace-Cache (STC) <ref type="bibr" target="#b14">[14]</ref> (Section 3.3).</p><p>? An online controller <ref type="bib they are fetched from the ICache and InO mode executes.</p><p>A fill buffer similar to that used in <ref type="bibr" target="#b14">[14]</ref> is used to combine instructions in issue-order on the big. g instructions in logical dependency order rather than the order in which they are stored in memory <ref type="bibr" target="#b14">[14]</ref>. They aim to improve efficiency of fetch by grouping instr erleaving conditional forward branches (TraceID). This definition is synonymous with previous works <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref> and represents a s
ype="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>, and heterogeneous ISAs <ref type="bibr" target="#b25">[25]</ref>. Commercial products include ARM's big.LITTLE <ref type="b
ogeneous architectures <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7]</ref> that enable fast switch ef><ref type="bibr" target="#b7">7]</ref> or reducing the distance between them using 3D technology <ref type="bibr" target="#b6">[6]</ref>. Cores <ref type="bibr" target="#b4">[4]</ref> shares access
ance <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. Recent work <ref type="bibr" target="#b31">[31]</ref> shows that microarchitectural heterogeneity, which can alt
postdecode traces, thus allowing expensive CISC decoders to be turned off <ref type="bibr">[9,</ref><ref type="bibr" target="#b34">34]</ref>. Section 5 compares DynaMOS 's behavior to relevant schemes
tic scheduling mechanisms <ref type="bibr" target="#b32">[32]</ref> or run-time binary optimization <ref type="bibr" target="#b33">[33]</ref> to create optimized instruction schedules. Most compilers
="#b22">22]</ref>, specialized hardware for specific codes <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>, and heterogeneous ISAs <ref type="bibr" target="#b25">[25]
tic scheduling mechanisms <ref type="bibr" target="#b32">[32]</ref> or run-time binary optimization <ref type="bibr" target="#b33">[33]</ref> to create optimized instruction schedules. Most compilers
postdecode traces, thus allowing expensive CISC decoders to be turned off <ref type="bibr">[9,</ref><ref type="bibr" target="#b34">34]</ref>. Section 5 compares DynaMOS 's behavior to relevant schemes
the core's energy efficiency at the expense of performance <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. Recent work <ref type="bibr" target="#b31">[31]</ref> show
p><p>? big detects traces with repeatable schedules and stores them in a Schedule Trace-Cache (STC) <ref type="bibr" target="#b14">[14]</ref> (Section 3.3).</p><p>? An online controller <ref type="bib they are fetched from the ICache and InO mode executes.</p><p>A fill buffer similar to that used in <ref type="bibr" target="#b14">[14]</ref> is used to combine instructions in issue-order on the big. g instructions in logical dependency order rather than the order in which they are stored in memory <ref type="bibr" target="#b14">[14]</ref>. They aim to improve efficiency of fetch by grouping instr erleaving conditional forward branches (TraceID). This definition is synonymous with previous works <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref> and represents a s
ance <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. Recent work <ref type="bibr" target="#b31">[31]</ref> shows that microarchitectural heterogeneity, which can alt
ases that can utilize it, and to switch to the energy-efficient InO core for low performance phases <ref type="bibr" target="#b3">[3]</ref>, thereby reducing the overall energy consumption. Prior work
ance <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. Recent work <ref type="bibr" target="#b31">[31]</ref> shows that microarchitectural heterogeneity, which can alt
postdecode traces, thus allowing expensive CISC decoders to be turned off <ref type="bibr">[9,</ref><ref type="bibr" target="#b34">34]</ref>. Section 5 compares DynaMOS 's behavior to relevant schemes
phase behavior in a subset of h64ref 's execution, a compute-intensive benchmark from SPECInt 2006 <ref type="bibr" target="#b13">[13]</ref> exhibiting predictable control and data-flow. Each point r k, there can be many traces that spawn from a common header PC. Its high branch misprediction rates <ref type="bibr" target="#b13">[13]</ref> indicate that these traces are not predictably repeatable.
e rank range from 1 to K. As pointed out above, this scheme pre-determines the weight. Rendle et al <ref type="bibr" target="#b28">[29]</ref> proposed an empirical weight for sampling a single positio
al <ref type="bibr" target="#b35">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref type="bibr" target="#b15">[16]</ref>. As described in Algorithm 1, for the r -th document π r ,
cores for all documents. To get the student predicted rank for this document, we apply Weston et al <ref type="bibr" target="#b35">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref
et="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>. However, the size of such models (in terms of the number o get="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, three different evaluation metrics used are Precision@n (P
ing has been widely used in the field of recommender systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>. These methods mainly focus on how to transfer knowledge (e
br" target="#b20">[21]</ref>. Recently, with the great impact of neural networks on computer vision <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref> and natural langua
for candidate generation and applying time-consuming models to the candidates for online inferences <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>. These methods eithe
r" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref> and natural language processing <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>, a new branch of I
bibr" target="#b41">42]</ref>. Discrete hashing techniques <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> and binary coding o
for candidate generation and applying time-consuming models to the candidates for online inferences <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>. These methods eithe
ta to construct test collections <ref type="bibr" target="#b1">[2]</ref>, to provide extra features <ref type="bibr" target="#b10">[11]</ref> and labels <ref type="bibr" target="#b9">[10]</ref> for ra
g for Recommender System Transfer learning has been widely used in the field of recommender systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>. These methods mainl
ent factor models and neural networks are usually non-convex <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, which means that their training processes are more challen
by other approaches. Several successful ranking models with neural networks have been investigated <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" ta ss list-wise loss in this work. The point-wise loss is widely used when relevance labels are binary <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>. One typical point
imilarity measure and set the number of nearest neighbor to 20. Bayesian personalized ranking (BPR) <ref type="bibr" target="#b29">[30]</ref>. Clearly, the performance of these non-sequential baseline
4</ref> was constructed by <ref type="bibr" target="#b6">[7]</ref> and Foursquare was obtained from <ref type="bibr" target="#b37">[38]</ref>. These data sets contain sequences of implicit feedbacks t nd the immediately next item as the groundtruth. Following <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38]</ref>, we hold the first 70% of actions in each user's sequence a
sers' sequences) based item recommendation (POP), the item based Collaborative Filtering 5 (ItemCF) <ref type="bibr" target="#b31">[32]</ref>, and the 5 We use Jaccard similarity measure and set the n
et="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Discrete hashing t
users and items. Semi-Supervised Learning Another related research area is semisupervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43]</ref>. Unlike the teacher-
ent factor models and neural networks are usually non-convex <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, which means that their training processes are more challen
et="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Discrete hashing t
imilarity measure and set the number of nearest neighbor to 20. Bayesian personalized ranking (BPR) <ref type="bibr" target="#b29">[30]</ref>. Clearly, the performance of these non-sequential baseline
users and items. Semi-Supervised Learning Another related research area is semisupervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43]</ref>. Unlike the teacher-
ranking models with neural networks have been investigated <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar -wise loss is widely used when relevance labels are binary <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>. One typical point-wise loss is taking the negative logarit s work, as they contain numerous sequential signals and thus suitable for sequential recommendation <ref type="bibr" target="#b32">[33]</ref>. Their statistics are described in Table <ref type="table" www.tei-c.org/ns/1.0"><head>Gowalla Model</head><p>Prec@3 Prec@5 Prec@10 nDCG@3 nDCG@5 nDCG@10 MAP  <ref type="bibr" target="#b32">[33]</ref> incorporates the Convolutional Neural Network and latent f (L = 5) from her sequence as S (u,t ) , and the immediately next item as the groundtruth. Following <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38]</ref>, we hold the first
et="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>. However, the size of such models (in terms of the number o get="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, three different evaluation metrics used are Precision@n (P
br" target="#b20">[21]</ref>. Recently, with the great impact of neural networks on computer vision <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref> and natural langua
by other approaches. Several successful ranking models with neural networks have been investigated <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" ta ss list-wise loss in this work. The point-wise loss is widely used when relevance labels are binary <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>. One typical point
tiveness and efficiency has been a line of recent research <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" tar runing and indexing to speed-up retrieval of related items <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>, using fast models for candidate generation and applying ti
larger model size.</p><p>Balancing effectiveness and efficiency has been a line of recent research <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" ta us on database-related methods, such as pruning and indexing to speed-up retrieval of related items <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>, using fast models
et="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>. However, the size of such models (in terms of the number o get="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, three different evaluation metrics used are Precision@n (P
ent factor models and neural networks are usually non-convex <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, which means that their training processes are more challen
vex optimization. The objectives of latent factor models and neural networks are usually non-convex <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, which means that th
vex optimization. The objectives of latent factor models and neural networks are usually non-convex <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, which means that th
://www.tei-c.org/ns/1.0"><head n="3.2">Deep Supervision</head><p>We propose to use deep supervision <ref type="bibr" target="#b5">[6]</ref> in UNet++, enabling the model to operate in two modes: (1) a
he other two recent related works are GridNet <ref type="bibr" target="#b2">[3]</ref> and Mask-RCNN <ref type="bibr" target="#b3">[4]</ref>. GridNet is an encoder-decoder architecture wherein the feat
chitecture like U-Net <ref type="bibr" target="#b8">[9]</ref> and fully convolutional network (FCN) <ref type="bibr" target="#b7">[8]</ref>. These encoder-decoder networks used for segmentation share t.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Long et al. <ref type="bibr" target="#b7">[8]</ref> first introduced fully convolutional networks (FCN), while U
ef> proposed H-denseunet for liver and liver tumor segmentation. In the same spirit, Drozdzalet al. <ref type="bibr" target="#b1">[2]</ref> systematically investigated the importance of skip connectio
he other two recent related works are GridNet <ref type="bibr" target="#b2">[3]</ref> and Mask-RCNN <ref type="bibr" target="#b3">[4]</ref>. GridNet is an encoder-decoder architecture wherein the feat

g fully convolutional methods suitable for semantic segmentation. Inspired by DenseNet architecture <ref type="bibr" target="#b4">[5]</ref>, Li et al. <ref type="bibr" target="#b6">[7]</ref> proposed
ef> proposed H-denseunet for liver and liver tumor segmentation. In the same spirit, Drozdzalet al. <ref type="bibr" target="#b1">[2]</ref> systematically investigated the importance of skip connectio
ef> proposed H-denseunet for liver and liver tumor segmentation. In the same spirit, Drozdzalet al. <ref type="bibr" target="#b1">[2]</ref> systematically investigated the importance of skip connectio
he other two recent related works are GridNet <ref type="bibr" target="#b2">[3]</ref> and Mask-RCNN <ref type="bibr" target="#b3">[4]</ref>. GridNet is an encoder-decoder architecture wherein the feat
ef> proposed H-denseunet for liver and liver tumor segmentation. In the same spirit, Drozdzalet al. <ref type="bibr" target="#b1">[2]</ref> systematically investigated the importance of skip connectio
hesizing it from scratch. It is most similar to recent work on sequence-tosequence voice conversion <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" t #b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. <ref type="bibr" target="#b15">[16]</ref> uses a similar end-toend model, conditioned on speaker ide dard speech. In the future, we plan to test it on other speech disorders, and adopt techniques from <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref> to preserve the sp
>, speech recognition <ref type="bibr" target="#b1">[2]</ref>, and even combined speech translation <ref type="bibr" target="#b2">[3]</ref>. They have also achieved state-of-the-art results in end-to-
techniques have also been applied to improving intelligibility for speakers with vocal disabilities <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, and hearing-impai
lar to recent work on sequence-tosequence voice conversion <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. <ref type="bibr" r identities, to transform word segments from multiple speakers into multiple target voices. Unlike <ref type="bibr" target="#b16">[17]</ref>, which trained separate models for each source-target spea a pretrained speech recognizer to more explicitly capture phonemic information in the source speech <ref type="bibr" target="#b16">[17]</ref>. However, we do find it helpful to multitask train the mod
an mixture models <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Recent work has also addressed accent conversion <ref typ
an mixture models <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Recent work has also addressed accent conversion <ref typ
d n="2.1.">Spectrogram encoder</head><p>The base encoder configuration is similar to the encoder in <ref type="bibr" target="#b23">[24]</ref>, and some variations are evaluated in Section 3.1. From th
type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, dynamic frequency warping <ref type="bibr" target="#b9">[10]</ref>, and Gaussian mixture models <ref type="bibr" target="#b10"
ollowed by an inverse STFT. However, when conducting human listening tests we instead use a WaveRNN <ref type="bibr" target="#b28">[29]</ref> neural vocoder which has been shown to significantly impro
f>, dynamic frequency warping <ref type="bibr" target="#b9">[10]</ref>, and Gaussian mixture models <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t
ralizing beyond the training set. Furthermore, unlike state-of-the-art speech separation techniques <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, Parrotron generat
chers' noisy votes; for this purpose, we use the state-of-the-art moments accountant technique from <ref type="bibr" target="#b0">Abadi et al. (2016)</ref>, which tightens the privacy bound when the t effect, limiting applicability to logistic regression with convex loss. Also, unlike the methods of <ref type="bibr" target="#b0">Abadi et al. (2016)</ref>, which represent the state-of-the-art in dif e a privacy/utility tradeoff that equals or improves upon bespoke learning methods such as those of <ref type="bibr" target="#b0">Abadi et al. (2016)</ref>.</p><p>Section 5 further discusses the relat g the need for supervision. • We present a new application of the moments accountant technique from <ref type="bibr" target="#b0">Abadi et al. (2016)</ref> for improving the differential-privacy analy (8.19, 10 −6 ) for SVHN, respectively with accuracy of 98.00% and 90.66%. In comparison, for MNIST, <ref type="bibr" target="#b0">Abadi et al. (2016)</ref> obtain a looser (8, 10 −5 ) privacy bound an y cost, we use recent advances in privacy cost accounting. The moments accountant was introduced by <ref type="bibr" target="#b0">Abadi et al. (2016)</ref>, building on previous work <ref type="bibr" rivacy loss random variable.</p><p>The following properties of the moments accountant are proved in <ref type="bibr" target="#b0">Abadi et al. (2016)</ref>.</p><p>Theorem 1. 1. [Composability] Suppose 00% and 90.66%. These results improve the differential privacy state-of-the-art for these datasets. <ref type="bibr" target="#b0">Abadi et al. (2016)</ref> previously obtained 97% accuracy with a (8, he large number of parameters prevents the technique from providing a meaningful privacy guarantee. <ref type="bibr" target="#b0">Abadi et al. (2016)</ref> provided stricter bounds on the privacy loss we keep track of the privacy budget throughout the student's training using the moments accountant <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. When teachers reach a strong quorum, this e cost of assuming that nonprivate unlabeled data is available, an assumption that is not shared by <ref type="bibr" target="#b0">(Abadi et al., 2016;</ref><ref type="bibr" target="#b33">Shokri &amp;
ves upon a specific, structured application of the techniques of knowledge aggregation and transfer <ref type="bibr" target="#b5">(Breiman, 1994)</ref>, previously explored by <ref type="bibr" target=
Bassily et al. (2014)</ref>; <ref type="bibr" target="#b9">Chaudhuri &amp; Monteleoni (2009)</ref>; <ref type="bibr" target="#b30">Pathak et al. (2011)</ref>; <ref type="bibr" target="#b34">Song et al
sonal contacts, private photographs or correspondence, or even medical records or genetic sequences <ref type="bibr" target="#b2">(Alipanahi et al., 2015;</ref><ref type="bibr" target="#b23">Kannan et
SIS OF PATE</head><p>Differential privacy <ref type="bibr" target="#b16">(Dwork et al., 2006b;</ref><ref type="bibr" target="#b12">Dwork, 2011)</ref>  </p><formula xml:id="formula_2">Pr[M(d) ∈ S] ≤ e
man, 1994)</ref>, previously explored by <ref type="bibr" target="#b28">Nissim et al. (2007)</ref>, <ref type="bibr" target="#b29">Pathak et al. (2010)</ref>, and particularly <ref type="bibr" target= "#b0">(Abadi et al., 2016;</ref><ref type="bibr" target="#b33">Shokri &amp; Shmatikov, 2015)</ref>. <ref type="bibr" target="#b29">Pathak et al. (2010)</ref> first discussed secure multi-party aggrega
target="#b2">(Alipanahi et al., 2015;</ref><ref type="bibr" target="#b23">Kannan et al., 2016;</ref><ref type="bibr" target="#b25">Kononenko, 2001;</ref><ref type="bibr" target="#b35">Sweeney, 1997)</
3.1">DIFFERENTIAL PRIVACY PRELIMINARIES AND A SIMPLE ANALYSIS OF PATE</head><p>Differential privacy <ref type="bibr" target="#b16">(Dwork et al., 2006b;</ref><ref type="bibr" target="#b12">Dwork, 2011 n, differential privacy, established itself as a rigorous standard for providing privacy guarantees <ref type="bibr" target="#b16">(Dwork et al., 2006b)</ref>. In contrast to k-anonymity, differential
Chaudhuri &amp; Monteleoni (2009)</ref>; <ref type="bibr" target="#b30">Pathak et al. (2011)</ref>; <ref type="bibr" target="#b34">Song et al. (2013), and</ref><ref type="bibr" target="#b36">Wainwrigh
ning set that have a high potential to contribute to learning <ref type="bibr">(Angluin, 1988;</ref><ref type="bibr" target="#b4">Baum, 1991)</ref>. If the label of an input in the student's training
ning set that have a high potential to contribute to learning <ref type="bibr">(Angluin, 1988;</ref><ref type="bibr" target="#b4">Baum, 1991)</ref>. If the label of an input in the student's training
type="bibr" target="#b5">(Gao et al. 2012</ref>). However, traditional hypergraph learning methods <ref type="bibr" target="#b23">(Zhou, Huang, and Schölkopf 2007)</ref> suffer from their high comput ployed to model high-order correlation among data.</p><p>Hypergraph learning is first introduced in <ref type="bibr" target="#b23">(Zhou, Huang, and Schölkopf 2007)</ref>, as a propagation process on the hypergraph structure. The task can be formulated as a regularization framework as introduced by <ref type="bibr" target="#b23">(Zhou, Huang, and Schölkopf 2007)</ref>:</p><formula xml:id="formula_
s to classify citation data. Here, two widely used citation network datasets, i.e., Cora and Pubmed <ref type="bibr" target="#b17">(Sen et al. 2008</ref>) are employed. The experimental setup follows
ph, extending neural networks to graph structure has attracted great attention from researchers. In <ref type="bibr" target="#b7">(Gori, Monfardini, and Scarselli 2005)</ref> and <ref type="bibr" targ
bibr" target="#b21">(Wu et al. 2015)</ref> and the National Taiwan University (NTU) 3D model dataset<ref type="bibr" target="#b2">(Chen et al. 2003)</ref>, as shown in Table 3. The ModelNet40 dataset
s to classify citation data. Here, two widely used citation network datasets, i.e., Cora and Pubmed <ref type="bibr" target="#b17">(Sen et al. 2008</ref>) are employed. The experimental setup follows
Liu, and Metaxas 2009)</ref>, hypergraph learning is further employed in video object segmentation. <ref type="bibr" target="#b9">(Huang et al. 2010</ref>) used the hypergraph structure to model image
p>For spatial approaches, the convolution operation is defined in groups of spatial close nodes. In <ref type="bibr" target="#b0">(Atwood and Towsley 2016)</ref>, the powers of a transition matrix is
/p><p>For spectral approaches, the convolution operation is formulated in spectral domain of graph. <ref type="bibr" target="#b1">(Bruna et al. 2014)</ref> introduces the first graph CNN, which uses t
from researchers. In <ref type="bibr" target="#b7">(Gori, Monfardini, and Scarselli 2005)</ref> and <ref type="bibr" target="#b16">(Scarselli et al. 2009</ref>), the neural network on graph is first i

s to classify citation data. Here, two widely used citation network datasets, i.e., Cora and Pubmed <ref type="bibr" target="#b17">(Sen et al. 2008</ref>) are employed. The experimental setup follows
get="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar milar recent proposals <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>, a sequence of succ target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar arate the storage of address sequences and correlation data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. A history buffer r -proposed prefetch meta-data organization where misses are logged continuously in a circular buffer <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. By separating ind ddress-correlating prefetchers beyond our idealized implementation of the proposed prior techniques <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. However, improved et="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. However, correlation table storage requirements are propor e="bibr" target="#b26">[27]</ref> and the predictor organization of the Global History Buffer (GHB) <ref type="bibr" target="#b20">[21]</ref>. After we describe the basic hardware operation, the follo ssor equipped with STMS. STMS comprises on-chip prefetch buffers and queues and offchip index table <ref type="bibr" target="#b20">[21]</ref> and history buffers <ref type="bibr" target="#b20">[21]</r rs and queues and offchip index table <ref type="bibr" target="#b20">[21]</ref> and history buffers <ref type="bibr" target="#b20">[21]</ref> 1 . The prefetch buffers and queues, located along side th associative table design limits maximum prefetch sequence length (referred to as the prefetch depth <ref type="bibr" target="#b20">[21]</ref>) based on the size of a table entry. Because storage cost,
r" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ ce of successor misses <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" targ iate a miss address with a set of possible successor misses, or, in Temporal Memory Streaming (TMS) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar storage efficiency, several designs separate the storage of address sequences and correlation data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar
target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref> and remains an essential strategy to address the processor- sing access patterns <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In contrast to stride-based approaches, addresscorre get="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> are effective for repetitive, yet arbitrarily-irregular acc target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Addresscorrelating prefetchers associate a miss address wi ory Streaming (TMS) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> and similar recent proposals <ref type="bibr" target="#b5"> target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. Shifting correlation tables to main memory eliminates on-c several misses ahead in the anticipated future miss sequence <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. Second, the extra memory traffic used to lookup and mainta where misses are logged continuously in a circular buffer <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. By separating indexing and logging, this prefetcher organi ict long missaddress sequences of up to hundreds of misses <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. We evaluate our practical design, Sampled Temporal Memory e commercial workloads <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Pair-wise-correlating prefetchers. The Markov prefet rget="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. We adopt the terminology of <ref type="bibr" target="#b25" nd correlation data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. A history buffer records the application's recent miss-add target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Nevertheless, stride prefetchers are widely implemented, w uires megabytes of correlation table storage to be effective <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. Figure <ref type="figure">1</ref> (left) shows the number target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>, where multi-megabyte tables can easily be accommodated. Ho idealized implementation of the proposed prior techniques <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. However, improved predictors can leverage the basic mechan uffer size, as it has been studied extensively in prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. In addition, STMS uses a shared 8KB bucket buffer to store st be spent to locate and retrieve the miss address sequence <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>We estimate lost prefetch opportunity by examining w hasing applications such as the studied commercial workloads <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>As discussed in Section 3, when temporal streams are eir published results<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cel n Prefetcher (EBCP <ref type="bibr" target="#b5">[6]</ref>), and the Temporal Streaming Engine (TSE <ref type="bibr" target="#b26">[27]</ref>), based on their published results. Overhead traffic is no cher that draws heavily from the stream-following mechanisms of the Temporal Streaming Engine (TSE) <ref type="bibr" target="#b26">[27]</ref> and the predictor organization of the Global History Buffe up cost <ref type="bibr" target="#b8">[9]</ref>. However, unlike those applications, prior research <ref type="bibr" target="#b26">[27]</ref> and our results (see Section 5.4) indicate that half of th g, continuing only if the annotated address is explicitly requested by the core. In contrast to TSE <ref type="bibr" target="#b26">[27]</ref>, the STMS stream-end detection is highly bandwidth-efficie
rage <ref type="bibr" target="#b12">[13]</ref>, or trigger prefetchers earlier to improve lookahead <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>The key lim
y access latency continues to pose a crucial performance bottleneck for commercial server workloads <ref type="bibr" target="#b10">[11]</ref>. System designers employ a variety of strategies to bridge proving throughput when abundant software threads are available, but does not improve response time <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Prefetching
" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target ="bibr" target="#b26">27]</ref> and similar recent proposals <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" targe relating prefetchers store meta-data off chip in main memory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" targe single correlation to predict a sequence of successor misses <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target ssor addresses. However, offline analyses of miss repetition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref> have shown that tempo ecent prefetchers locate correlation metadata in main memory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" targe rgeting desktop/engineering applications rely on long temporal streams to overcome the startup cost <ref type="bibr" target="#b8">[9]</ref>. However, unlike those applications, prior research <ref typ ck-sized buffer accumulates entries which are then written to main memory as a group as proposed in <ref type="bibr" target="#b8">[9]</ref>. As history buffer entries are created, the index table entr
" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar capture correlations is proportional to an application's data set and requires megabytes of storage <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta irements. Initial addresscorrelating prefetcher designs located correlation tables entirely on-chip <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta air-wisecorrelating prefetchers build upon this simple design to optimize correlation table storage <ref type="bibr" target="#b12">[13]</ref>, or trigger prefetchers earlier to improve lookahead <ref application's working set. Hence, for commercial workloads, even the most storage-efficient design <ref type="bibr" target="#b12">[13]</ref> requires megabytes of correlation table storage to be effe
/p><p>Prefetching improves both throughput and response time by increasing memory level parallelism <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target "bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> and similar recent proposals <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target mprove practicality, recent address-correlating prefetchers store meta-data off chip in main memory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target lookup latency by targeting prefetches several misses ahead in the anticipated future miss sequence <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. Second, the extra m t addresscorrelating prefetchers use a single correlation to predict a sequence of successor misses <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target successors) rather than only a single future access in each set-associative correlation table entry <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>. The primary shortco ype="bibr" target="#b12">[13]</ref> requires megabytes of correlation table storage to be effective <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. Figure <ref type="f lation tables impractical.</p><p>More recent prefetchers locate correlation metadata in main memory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target do not report sensitivity to prefetch buffer size, as it has been studied extensively in prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. In addition, STMS u ortunity on each lookup because time must be spent to locate and retrieve the miss address sequence <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>We estimate l right graph shows the memory traffic overheads of existing designs based on their published results<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" targ orrelating prefetchers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target= the pointer-chasing access patterns of commercial workloads <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" targe ove the performance of pointerintensive commercial workloads <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Pair-wise-corr ads, whereas stride prefetchers provide only minimal benefit <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" targe ress-correlating prefetcher we analyze in detail in Section 5.2. Our result corroborates prior work <ref type="bibr" target="#b5">[6]</ref>: to achieve maximum coverage in commercial workloads, correl ed to account for long correlation table lookup latency. Epoch-based correlation prefetching (EBCP) <ref type="bibr" target="#b5">[6]</ref> explicitly accounts for off-chip lookup latency and the memo read (ULMT <ref type="bibr" target="#b22">[23]</ref>), the Epoch-Based Correlation Prefetcher (EBCP <ref type="bibr" target="#b5">[6]</ref>), and the Temporal Streaming Engine (TSE <ref type="bibr" ta this time result in lost prefetch opportunity, even if they comprise a predictable temporal stream <ref type="bibr" target="#b5">[6]</ref>. Prior prefetchers targeting desktop/engineering application er of off-chip loads issued while at least one such load is outstanding. As discussed in prior work <ref type="bibr" target="#b5">[6]</ref>, a predictor that retrieves meta-data from main memory loses ction 3, when temporal streams are stored in a single set-associative correlation table (as in EBCP <ref type="bibr" target="#b5">[6]</ref> and ULMT <ref type="bibr" target="#b22">[23]</ref>), lookups commercial server workloads. One million correlation table entries can require up to 64MB of storage<ref type="bibr" target="#b5">[6]</ref>.The right graph shows the memory traffic overheads of existi
ing), which are dominated by pointer-chasing access patterns <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In contrast
rget="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar s entirely on-chip <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. However, correlati gabytes of storage <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>To improve practicality, recent address-correlating /ref>, or trigger prefetchers earlier to improve lookahead <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>The key limitation of pairwise-correlating prefetche
dress the processor-memory performance gap. Today's systems employ spatial/stride based prefetchers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref> because they are p type="table" target="#tab_3">1</ref>.</p><p>We include a stride-based prefetcher in our base system <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref>. All results repor
2017;</ref><ref type="bibr" target="#b39">Veličković et al., 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b33">Qu et al., 2019;</ref><ref type="bibr" target="#b13">Gao &amp; Ji, 20 rs and the edge weights between them correspond to the degree of trust between the users. Following <ref type="bibr" target="#b33">(Qu et al., 2019)</ref>, we treat edges with weights greater than 3 a t state-of-the-art methods GAT <ref type="bibr" target="#b39">(Veličković et al., 2018)</ref>, GMNN <ref type="bibr" target="#b33">(Qu et al., 2019)</ref> and Graph U-Net <ref type="bibr" target="#b13 the results of GraphMix(GCN) are comparable with the recently proposed state-of-the-art method GMNN <ref type="bibr" target="#b33">(Qu et al., 2019)</ref>. Since GraphMix consists of various component ± 0.3% GraphScan <ref type="bibr" target="#b11">(Ding et al., 2018)</ref> 83.3 ±1.3 73.1±1.8 -GMNN <ref type="bibr" target="#b33">(Qu et al., 2019)</ref> 83.7% 73.1% 81.8% DisenGCN <ref type="bibr" t ; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b39">(Veličković et al., 2018)</ref> and GMNN <ref type="bibr" target="#b33">(Qu et al., 2019)</ref>, among others. This architecture has one hidd hence much of the recent attention is dedicated to proposing architectural changes to these methods <ref type="bibr" target="#b33">(Qu et al., 2019;</ref><ref type="bibr" target="#b13">Gao &amp; Ji, 2
t="#b8">Defferrard et al., 2016;</ref><ref type="bibr" target="#b20">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b15">Gilmer et al., 2017;</ref><ref type="bibr" target="#b18">Hamilton et ref type="bibr" target="#b39">(Veličković et al., 2018)</ref>, or any general message passing layer <ref type="bibr" target="#b15">(Gilmer et al., 2017)</ref>. Formally, let H l ∈ R n×k be a matrix co

izes interpolation based data augmentation <ref type="bibr" target="#b46">(Zhang et al., 2018;</ref><ref type="bibr" target="#b41">Verma et al., 2019a)</ref> and self-targetprediction based data-augme d on interpolation-based data augmentation <ref type="bibr" target="#b46">(Zhang et al., 2018;</ref><ref type="bibr" target="#b41">Verma et al., 2019a)</ref> has seen sizable improvements in regulariz ref> trains a neural network on the convex combination of input and targets, whereas Manifold Mixup <ref type="bibr" target="#b41">(Verma et al., 2019a</ref>) trains a neural network on the convex com erations. For Manifold Mixup training of FCN, we apply mixup only in the hidden layer. Note that in <ref type="bibr" target="#b41">Verma et al. (2019a)</ref>, the authors recommended applying mixing i samples are generated by interpolating the samples and their corresponding targets. Manifold Mixup <ref type="bibr" target="#b41">(Verma et al., 2019a)</ref> proposes to augment the data in the hidde
org/ns/1.0"> <ref type="bibr" target="#b1">(Belkin et al., 2006)</ref> <p>59.5% 60.1% 70.7% SemiEmb <ref type="bibr" target="#b43">(Weston et al., 2012)</ref> 59.0% 59.6% 71.7% LP <ref type="bibr">(Zh
is to predict the labels of the remaining links, given a graph and labels of a few links. Following <ref type="bibr" target="#b37">(Taskar et al., 2004)</ref>, we can formulate the link classification
5">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b36">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b42">Verma et al., 2019b;</ref><ref type="bibr">Berthelot et al., 2019)</r d-targets based algorithms which can be also interpreted as a form of data-augmentation techniques) <ref type="bibr" target="#b42">(Verma et al., 2019b;</ref><ref type="bibr">Berthelot et al., 2019)</ "#b29">Miyato et al., 2018;</ref><ref type="bibr" target="#b36">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b42">Verma et al., 2019b)</ref> are based on the cluster assumption <ref t tes and shows that it learns more discriminative features for supervised learning. Furthermore, ICT <ref type="bibr" target="#b42">(Verma et al., 2019b)</ref> and MixMatch <ref type="bibr">(Berthelot
is to predict the labels of the remaining links, given a graph and labels of a few links. Following <ref type="bibr" target="#b37">(Taskar et al., 2004)</ref>, we can formulate the link classification
><p>For semi-supervised link classification, we use two datasets Bitcoin Alpha and Bitcoin OTC from <ref type="bibr" target="#b23">(Kumar et al., 2016;</ref><ref type="bibr">2018)</ref>. The nodes in
t="#b8">Defferrard et al., 2016;</ref><ref type="bibr" target="#b20">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b15">Gilmer et al., 2017;</ref><ref type="bibr" target="#b18">Hamilton et ref type="bibr" target="#b39">(Veličković et al., 2018)</ref>, or any general message passing layer <ref type="bibr" target="#b15">(Gilmer et al., 2017)</ref>. Formally, let H l ∈ R n×k be a matrix co
n be used as an input to any classifier, or learns the node embedding and target prediction jointly <ref type="bibr" target="#b45">(Yang et al., 2016)</ref>. Many of the recent Graph Neural Network ba 65.3% ICA <ref type="bibr" target="#b27">(Lu &amp; Getoor, 2003)</ref> 75.1% 69.1% 73.9% Planetoid <ref type="bibr" target="#b45">(Yang et al., 2016)</ref> 75.7% 64.7% 77.2% Chebyshev <ref type="bibr
e for the 'happy' category while the others not. In this paper, inspired by the attention mechanism <ref type="bibr" target="#b13">[14]</ref> of machine translation and the neural aggregation networks
ation in videos. The Long Short-Term Memory (LSTM) <ref type="bibr" target="#b5">[6]</ref>, and C3D <ref type="bibr" target="#b6">[7]</ref> are two widely-used spatialtemporal methods for video-based
toolbox. By default, for feature embedding, we use the ResNet18 which is pre-trained on MS-Celeb-1M <ref type="bibr" target="#b20">[21]</ref> face recognition dataset and FER Plus expression dataset <
d-crafted <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and learned <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target nary patterns (LBP) and LBP histogram for facial feature extraction. For the learned features, Tang <ref type="bibr" target="#b2">[3]</ref> utilizes deep CNNs for feature extraction, and win the FER20
ame aggregation operation is necessary for staticbased methods. For frame aggregation, Kahou et al. <ref type="bibr" target="#b12">[13]</ref> concatenate the n-class probability vectors of 10 segments
for video action recognition, is also popular in the EmotiW challenge.</p><p>Geometry based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> aim to model the m s which only leverage the geometry locations of facial landmarks in every video frames. Jung et al. <ref type="bibr" target="#b11">[12]</ref> propose a deep temporal appearance-geometry network (DTAGN model and a temporal network, where the spatial CNN model only uses the last peak frame. Jung et al <ref type="bibr" target="#b11">[12]</ref> select a fixed length sequence for each video with a lipre arget="#b22">[23]</ref> S: the last frame T: all frames S: the last frame T: all frames 98.50 DTAGN <ref type="bibr" target="#b11">[12]</ref> Fixed length Fixed length 97.25</p><p>CNN+Island loss <ref
expression to the peak expression, most of the methods conduct data selection manually. Zhang et al <ref type="bibr" target="#b22">[23]</ref> propose to combine a spatial CNN model and a temporal netw s="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training data Test data Acc.</p><p>ST network <ref type="bibr" target="#b22">[23]</ref> S: the last frame T: all frames S: the last frame T: all f
. This pipeline is widely-used in the EmotiW challenge, e.g. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target
target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. C3D, which is originally developed for video action recogn ular in the EmotiW challenge.</p><p>Geometry based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> aim to model the motions of key points in faces which only se normalized points over time for a one-dimensional trajectory signal of each sequence. Yan et al. <ref type="bibr" target="#b10">[11]</ref> construct an image-like map by stretching all the normaliz
toolbox. By default, for feature embedding, we use the ResNet18 which is pre-trained on MS-Celeb-1M <ref type="bibr" target="#b20">[21]</ref> face recognition dataset and FER Plus expression dataset <
ation in videos. The Long Short-Term Memory (LSTM) <ref type="bibr" target="#b5">[6]</ref>, and C3D <ref type="bibr" target="#b6">[7]</ref> are two widely-used spatialtemporal methods for video-based
icit partitioning of the cache among co-running applications for throughput or fairness improvement <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta
ey adept in handling thrashing or scan-type access streams <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" tar data, with separate parameter tuning for each application <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" tar pecific insertion policy for each of the competing applications based on its memory access behavior <ref type="bibr" target="#b18">[19]</ref>. The key insight behind this scheme is that for a referenc
n be obviated if the dependency resolution is implemented in hardware, as proposed by Etsion et al. <ref type="bibr" target="#b14">[15]</ref>. The LLC tags carry task-ids, but for thread-based partiti
unning applications for throughput or fairness improvement <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar
the memory hierarchy that uses the Simics full-system simulator as its functional simulation engine <ref type="bibr" target="#b13">[14]</ref>. We model a multicore chip with a highlyassociative shared
get="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Wang et al. propose a software replacement hint in t to reduce the cache pollution effects of zero-intializtions in virtual machine-managed applications <ref type="bibr" target="#b38">[39]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
euse threads on other threads, nor are they adept in handling thrashing or scan-type access streams <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta
ave explored compiler and profile-based techniques to improve cache utilization through these hints <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe hniques to generate target cache hints for loop-based applications running on the IA64 architecture <ref type="bibr" target="#b7">[8]</ref>. Target cache hints inform the hardware about the highest(fa
n through these hints <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" tar string operations with poor reuse behavior to improve cache utilization for datacenter applications <ref type="bibr" target="#b32">[33]</ref>. Yang et al. explored the benefits of using non-temporal a
get="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Shared LLC management techniques for multicore proce get="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>. Proposals for replacement policy modification have concent load has been the focus of several partitioning techniques <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref>. A partitioning scheme that always allocates space to a thr eve that utility. The winning thread is then allocated the minimum number of ways.</p><p>Suh et al. <ref type="bibr" target="#b35">[36]</ref> compute the marginal utility of each thread from the actua
ts and mapping, with an aim to ensure balanced progress for all threads while optimizing throughput <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In this wor ng co-running threads in order to maximize throughput and achieve balanced progress for all threads <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" ta ate the problem of partitioning a shared L2 cache among the threads of a multi-threaded application <ref type="bibr" target="#b25">[26]</ref>. The authors proposed a dynamic partitioning scheme that f ing the slowest running thread (critical path thread) faster so that the application becomes faster <ref type="bibr" target="#b25">[26]</ref>. The proposed technique involves dividing the entire execu
ithreaded SPL fabric shared by four single issue out-of-order processor cores. In our previous work <ref type="bibr" target="#b44">[43]</ref>, we evaluated the use of SPL with a range of in-order and re possible, but this consideration is beyond the scope of this paper.</p><p>Each SPL, adopted from <ref type="bibr" target="#b44">[43]</ref> and shown in more detail in Figures <ref type="figure" tar n 2, we provide an overview of the SPL microarchitecture. A more complete treatment is available in <ref type="bibr" target="#b44">[43]</ref>. </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head d n="2.1">SPL Hardware Microarchitecture</head><p>We adopt the row-based fabric of our earlier work <ref type="bibr" target="#b44">[43]</ref> in which the space of shared SPL configurations was explor ately available after the initial configuration overhead is paid.</p><p>Using our analytical models <ref type="bibr" target="#b44">[43]</ref>, we arrive at the area and power results for eight single ficient as well. While one might consider shrinking the private SPL even further, our previous work <ref type="bibr" target="#b44">[43]</ref> has shown that that this yields poor performance.</p></div e four processor cores, as well as spatial partitioning to permit private or semi-private operation <ref type="bibr" target="#b44">[43]</ref>. Spatial partitioning is enabled by inserting additional m arger speedups than the product of the two techniques applied in isolation.</p><p>Our previous work <ref type="bibr" target="#b44">[43]</ref> identifies a number of characteristics of past reconfigura e impact of incorporating the fabric with processors of different complexity. While the emphasis of <ref type="bibr" target="#b44">[43]</ref> is on the fabric design, this paper proposes a complete ha he four processor cores, as well as spatial partitioning to permit private or semi-private operation<ref type="bibr" target="#b44">[43]</ref>. Spatial partitioning is enabled by inserting additional m
e-aware scheduling <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b41">40]</ref>, thermal management <ref type="bibr" target="#b12">[11]</re
8">[37]</ref>, one from SPEC2000 <ref type="bibr" target="#b37">[36]</ref>, and one from MediaBench <ref type="bibr" target="#b30">[29]</ref>. Our multithreaded workloads consist of two benchmarks fro
for a number of purposes, including cache-aware scheduling <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b41">40]</ref>, thermal management
ial power and area costs relative to the fixed hardware functionality of commercial microprocessors <ref type="bibr" target="#b29">[28]</ref>. These costs are especially important given the disparity
to SMT processors where adding additional contexts provides limited benefit beyond a certain point <ref type="bibr" target="#b43">[42]</ref>, sharing an SPL among four cores was shown to be the best ref><ref type="bibr" target="#b10">10]</ref> or indirectly <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b43">42]</ref> control the amount of processor resources that any thread c
both spatially and temporally. A number of research efforts <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b21">20]</ref> have investigated t
multi-threaded/multiprogrammed workload phase tracker. We use the phase tracker of Sherwood et al. <ref type="bibr" target="#b34">[33]</ref> to identify phases for each thread. The phase tracker repo n="4.1">Phase Tracking</head><p>We use the same parameters for our phase tracker as Sherwood et al. <ref type="bibr" target="#b34">[33]</ref> with the exception of the phase interval length. We use a in optimizing resource allocation during different phases <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b34">33]</ref> only address single applications. In our multithreaded envi
Configurations can map to these blocks both spatially and temporally. A number of research efforts <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" targ
bibr" target="#b30">[29]</ref>. Our multithreaded workloads consist of two benchmarks from ALPBench <ref type="bibr" target="#b31">[30]</ref> and a version of the JavaGrande <ref type="bibr" target="#
lly designed for more efficient integration with general purpose processors than conventional FPGAs <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" targ s can produce good mappings for reconfigurable architectures <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b46">45]</ref>. Since dynamic thread
br" target="#b21">(Levy et al., 2017;</ref><ref type="bibr" target="#b28">McCann et al., 2018;</ref><ref type="bibr" target="#b22">Li et al., 2019)</ref>, we propose a new framework that is capable of formalized as answering the question "What is the summary?". Our work is significantly inspired by <ref type="bibr" target="#b22">Li et al. (2019)</ref>, which formalized the task of entity-relation sk of entity-relation extraction as a multi-turn question answering task. Different from this work, <ref type="bibr" target="#b22">Li et al. (2019)</ref> focused on relation extraction rather than NER target="#b22">Li et al. (2019)</ref> focused on relation extraction rather than NER. Additionally, <ref type="bibr" target="#b22">Li et al. (2019)</ref> utilized a template-based procedure for constr nt influence on the final results. Different ways have been proposed for question generation, e.g., <ref type="bibr" target="#b22">Li et al. (2019)</ref> utilized a template-based procedure for constr
and Cardie (2018) used neural models to learn the hyper-graph representations for nested entities. <ref type="bibr" target="#b15">Ju et al. (2018)</ref> dynamically stacked flat NER layers in a hiera
rchitecture by modeling and leveraging the head-driven phrase structures of nested entity mentions. <ref type="bibr" target="#b26">Luan et al. (2019)</ref> built a span enumeration approach by selecti
ref> provided inference model that extracts entities iteratively from outermost ones to inner ones. <ref type="bibr" target="#b44">Straková et al. (2019)</ref> viewed nested NER as a sequence-tosequen
07)</ref> as a backbone for NER. The first work using neural models for NER goes back to 2003, when <ref type="bibr" target="#b12">Hammerton (2003)</ref> attempted to solve the problem using unidirect
ntextual embeddings such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> and ELMo <ref type="bibr" target="#b32">(Peters et al., 2018b)</ref>. <ref type="bibr" target="#b11">Fisher a
" target="#b29">(Muis and Lu, 2017;</ref><ref type="bibr" target="#b43">Sohrab and Miwa, 2018;</ref><ref type="bibr" target="#b56">Zheng et al., 2019)</ref> also proposed various methods to tackle the


07)</ref> as a backbone for NER. The first work using neural models for NER goes back to 2003, when <ref type="bibr" target="#b12">Hammerton (2003)</ref> attempted to solve the problem using unidirect
"bibr" target="#b1">Byrne, 2007;</ref><ref type="bibr" target="#b10">Finkel and Manning, 2009;</ref><ref type="bibr" target="#b25">Lu and Roth, 2015;</ref><ref type="bibr" target="#b16">Katiyar and Ca trees. They made the assumption that one mention is fully contained by the other when they overlap. <ref type="bibr" target="#b25">Lu and Roth (2015)</ref> proposed to use mention hyper-graphs for rec
e="bibr" target="#b3">[3]</ref>, on which the OS kernel (Linux 4.2) and the embedded network GARNET <ref type="bibr" target="#b1">[1]</ref> are enhanced according to our technique. The details of the
et="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr">8,< et="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22]</ref>), or by predicting et="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr">8,<
execution leads to serialization and puts a fundamental limit on parallel programs' performance. In <ref type="bibr" target="#b5">[5]</ref>, contention probability and hot critical section are used to
ction (OCOR) technique with timing-detailed full-system simulations using both PARSEC (11 programs) <ref type="bibr" target="#b2">[2]</ref> and SPEC OMP2012 (14 programs) <ref type="bibr" target="#b18

ng both PARSEC (11 programs) <ref type="bibr" target="#b2">[2]</ref> and SPEC OMP2012 (14 programs) <ref type="bibr" target="#b18">[18]</ref> benchmarks. We implement and integrate our design in GEM5
ng both PARSEC (11 programs) <ref type="bibr" target="#b2">[2]</ref> and SPEC OMP2012 (14 programs) <ref type="bibr" target="#b18">[18]</ref> benchmarks. We implement and integrate our design in GEM5
g that only one thread can get access to a critical section at a time. As shown in previous studies <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" ta loiting various mechanisms such as running serialized codes on the fat cores in an asymmetric CMP ( <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" ta technique is exploited to speed up threads that hold locks.</p><p>In contrast to the above studies <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" ta oltage/frequency scaling to improve performances.</p><p>Accelerating critical section execution: In <ref type="bibr" target="#b21">[21]</ref>, the authors propose a mechanism to accelerate critical se
n previous studies <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" tar n asymmetric CMP ( <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" tar the above studies <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" tar bottleneck, and the one that keeps CPUs waiting for the longest time is the critical bottleneck. In <ref type="bibr" target="#b13">[13]</ref>, utility-based acceleration of multi-threaded applications
et="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr">8,< et="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22]</ref>), or by predicting et="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr">8,<
n previous studies <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" tar n asymmetric CMP ( <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" tar the above studies <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" tar bottleneck, and the one that keeps CPUs waiting for the longest time is the critical bottleneck. In <ref type="bibr" target="#b13">[13]</ref>, utility-based acceleration of multi-threaded applications
pe="bibr" target="#b53">53]</ref> in the network to infer important features. More advanced methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">45]</ref> produce explanatio does it assume the features are independent. These are two key assumptions made by existing models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">45]</ref> which are often vi ddress this problem through approximation. However, this sacrifices the fidelity of the explanation <ref type="bibr" target="#b33">[34]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head thus LIME can pinpoint important features based on the regression coefficients. A recent work SHAP <ref type="bibr" target="#b33">[34]</ref> tries to extend LIME by adding weights to the artificially <note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We have tested SHAP<ref type="bibr" target="#b33">[34]</ref>, which is an extension of LIME. We find that SHAP is very
"#b25">[26]</ref>, Sparse Coding <ref type="bibr" target="#b38">[39]</ref> or Chi-square Statistics <ref type="bibr" target="#b49">[49]</ref>. Explanation methods aim to identify the key features of a
r" target="#b71">71]</ref> and network intrusion detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b62">62]</ref>, which all achieved an exceptionally high accuracy.</p><p>W
]</ref>. We follow the standard method to transform the feature values into a binary representation <ref type="bibr" target="#b40">[41]</ref> (i.e., nonzero feature values are converted to 1), which h
sis, binary code sequence analysis). Then, we integrate fused lasso into a mixture regression model <ref type="bibr" target="#b27">[28]</ref> to approximate locally nonlinear decision boundaries to su
68]</ref> propose an adversarial resistant neural network for detecting malware based on audit logs <ref type="bibr" target="#b6">[7]</ref>.</p><p>A key observation is that RNN and MLP are more widely label>(13)</label></formula><p>Recall that we re-compute parameter ? 1:K by minimizing the Equation <ref type="bibr" target="#b6">(7)</ref> shown in Section ?4. While it can be resolved by using MLE,
get="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b65">65]</ref>. A representative method is "machine unlearning" <ref type=
ble to craft adversarial examples for adversarial training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" tar
ral networks to train classifiers for malware classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar
get="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b65">65]</ref>. A representative method is "machine unlearning" <ref type=
sis, binary code sequence analysis). Then, we integrate fused lasso into a mixture regression model <ref type="bibr" target="#b27">[28]</ref> to approximate locally nonlinear decision boundaries to su
fense strategies into consideration <ref type="bibr">(Athalye et al., 2018b)</ref>.</p><p>Recently, <ref type="bibr" target="#b13">Shafahi et al. (2019)</ref> showed that, for two classes of data dist for general classifiers, and their relationship to some recent works in literature.</p><p>Recently, <ref type="bibr" target="#b13">Shafahi et al. (2019)</ref> shows that no classifier can achieve low
la">12</ref>) and replace θ with its asymptotic limit as given by solving (θ, t) from the equations <ref type="bibr" target="#b15">(Huang, 2017)</ref>:</p><formula xml:id="formula_23">sin 2 θ = N d t
o its great performance gains in recent years. Meanwhile adversarial examples, first pointed out by <ref type="bibr" target="#b0">Szegedy et al. (2014)</ref>, emerges as a novel peculiar security thre efense methods have also been proposed to prevent adversarial example attacks: Adversarial training <ref type="bibr" target="#b0">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b1">Goodfellow e
l., 2014;</ref><ref type="bibr" target="#b1">Goodfellow et al., 2015)</ref>; Defensive distillation <ref type="bibr" target="#b9">Papernot et al. (2016)</ref>; Minmax robust training <ref type="bibr"

ve also been demonstrated to misled DNN based classification systems in physical world applications <ref type="bibr" target="#b5">(Sharif et al., 2016;</ref><ref type="bibr" target="#b6">Brown et al.,
l., 2014;</ref><ref type="bibr" target="#b1">Goodfellow et al., 2015)</ref>; Defensive distillation <ref type="bibr" target="#b9">Papernot et al. (2016)</ref>; Minmax robust training <ref type="bibr"
la">12</ref>) and replace θ with its asymptotic limit as given by solving (θ, t) from the equations <ref type="bibr" target="#b15">(Huang, 2017)</ref>:</p><formula xml:id="formula_23">sin 2 θ = N d t
et al. (2016)</ref>; Minmax robust training <ref type="bibr" target="#b4">(Madry et al., 2018;</ref><ref type="bibr" target="#b10">Sinha et al., 2018)</ref>; Input transformation <ref type="bibr" targ certifiable adversarial-robust classifiers <ref type="bibr" target="#b4">(Madry et al., 2018;</ref><ref type="bibr" target="#b10">Sinha et al., 2018)</ref>, the robustness is achieved only for the pe
o its great performance gains in recent years. Meanwhile adversarial examples, first pointed out by <ref type="bibr" target="#b0">Szegedy et al. (2014)</ref>, emerges as a novel peculiar security thre efense methods have also been proposed to prevent adversarial example attacks: Adversarial training <ref type="bibr" target="#b0">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b1">Goodfellow e

f type="bibr" target="#b15">[16]</ref>, we use the SEQUITUR hierarchical data compression algorithm <ref type="bibr" target="#b9">[10]</ref> to identify repetitive sub-sequences within the miss traces
al server applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" targe ointerbased structures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target= single system organization, focusing either on uniprocessors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target= titive).</p><p>Discussion. Our results confirm prior studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref>: a substantial miss f plications are dominated by pointer-based data structures with complex, non-strided access patterns <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" targ usefulness of temporal streams. Long streams amortize prefetch costs (e.g., off-chip lookup latency <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target ong-in all cases the median stream length exceeds the fixed prefetch depths of many prior proposals <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" targ -level memory thread <ref type="bibr" target="#b20">[21]</ref>, epoch-based correlation prefetching <ref type="bibr" target="#b7">[8]</ref>, and last-touch correlated data streaming <ref type="bibr" t
arget="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar
in database applications, enables the efficient insertion, removal, and search for database records <ref type="bibr" target="#b3">[4]</ref>. A B+-tree maintains a sorted index of records according to
ulation infrastructure <ref type="bibr" target="#b25">[26]</ref>. FLEXUS builds on Virtutech Simics <ref type="bibr" target="#b16">[17]</ref> and supports both rapid tracebased and detailed cycle-accu
hat 8MB is sufficient to capture first-order temporal locality in the applications' data footprints <ref type="bibr" target="#b10">[11]</ref>. We model an MSI coherence protocol.</p><p>Our single-chip
ulation infrastructure <ref type="bibr" target="#b25">[26]</ref>. FLEXUS builds on Virtutech Simics <ref type="bibr" target="#b16">[17]</ref> and supports both rapid tracebased and detailed cycle-accu
cross system organizations. We classify miss behavior using categories similar to the "4 C's" model <ref type="bibr" target="#b11">[12]</ref>. Our results confirm prior observations that up to 80% of cation</head><p>We begin by classifying misses using a categorization based on the "four C's model" <ref type="bibr" target="#b11">[12]</ref>. These high-level breakdowns demonstrate the substantial d
ulation infrastructure <ref type="bibr" target="#b25">[26]</ref>. FLEXUS builds on Virtutech Simics <ref type="bibr" target="#b16">[17]</ref> and supports both rapid tracebased and detailed cycle-accu
" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. Extensive research ed data structures with complex, non-strided access patterns <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>To improve p
al server applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" targe ointerbased structures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target= single system organization, focusing either on uniprocessors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target= titive).</p><p>Discussion. Our results confirm prior studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref>: a substantial miss f plications are dominated by pointer-based data structures with complex, non-strided access patterns <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" targ usefulness of temporal streams. Long streams amortize prefetch costs (e.g., off-chip lookup latency <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target ong-in all cases the median stream length exceeds the fixed prefetch depths of many prior proposals <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" targ -level memory thread <ref type="bibr" target="#b20">[21]</ref>, epoch-based correlation prefetching <ref type="bibr" target="#b7">[8]</ref>, and last-touch correlated data streaming <ref type="bibr" t
search of graph embedding ( <ref type="bibr" target="#b15">Perozzi, Al-Rfou, and Skiena, 2014;</ref><ref type="bibr" target="#b4">Grover and Leskovec, 2016)</ref>, where graph topology and node relati he performance of our algorithm against several unsupervised graph learning counter-parts: Node2Vec <ref type="bibr" target="#b4">(Grover and Leskovec, 2016)</ref>, VGAE <ref type="bibr" target="#b9"> ds. DeepWalk <ref type="bibr" target="#b15">(Perozzi, Al-Rfou, and Skiena, 2014)</ref> and Node2vec <ref type="bibr" target="#b4">(Grover and Leskovec, 2016)</ref> are representative random walk-based ns from nodes' raw features, without using any graph structure information incorporated. • Node2Vec <ref type="bibr" target="#b4">(Grover and Leskovec, 2016)</ref>: This approach is an extension of Wo

p><p>Early works on graphical structure data can be dated back to the research of graph embedding ( <ref type="bibr" target="#b15">Perozzi, Al-Rfou, and Skiena, 2014;</ref><ref type="bibr" target="#b4 wo groups: random walksbased methods, and graph convolutional networks (GCN)based methods. DeepWalk <ref type="bibr" target="#b15">(Perozzi, Al-Rfou, and Skiena, 2014)</ref> and Node2vec <ref type="bi
correlations between them plays an important role in accurate and effective recommendation services <ref type="bibr" target="#b12">(Linden, Smith, and York, 2003)</ref>.</p><p>Early works on graphical
><p>Graphs that characterize relations among data arise in various domains including drug discovery <ref type="bibr" target="#b23">(You et al., 2018;</ref><ref type="bibr" target="#b8">Jin, Barzilay,
f>, Graph-SAGE <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec, 2017)</ref>, and AS-GCN <ref type="bibr" target="#b7">(Huang et al., 2018)</ref>. For the evaluation, we apply a large-scale br" target="#b5">(Hamilton, Ying, and Leskovec, 2017)</ref> and the ayerwise sampling method AS-GCN <ref type="bibr" target="#b7">(Huang et al., 2018)</ref>. Differ from these methods, our BGNN has be Leskovec, 2017)</ref> directly samples neighbor nodes, while layer-wise sampling method like AS-GCN <ref type="bibr" target="#b7">(Huang et al., 2018)</ref> uses adaptive sampling to fix the number of
f>, Graph-SAGE <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec, 2017)</ref>, and AS-GCN <ref type="bibr" target="#b7">(Huang et al., 2018)</ref>. For the evaluation, we apply a large-scale br" target="#b5">(Hamilton, Ying, and Leskovec, 2017)</ref> and the ayerwise sampling method AS-GCN <ref type="bibr" target="#b7">(Huang et al., 2018)</ref>. Differ from these methods, our BGNN has be Leskovec, 2017)</ref> directly samples neighbor nodes, while layer-wise sampling method like AS-GCN <ref type="bibr" target="#b7">(Huang et al., 2018)</ref> uses adaptive sampling to fix the number of
also construct three synthesized datasets based on the citation networks Cora, Citeseer, and PubMed <ref type="bibr" target="#b18">(Sen et al., 2008)</ref>. Upon all benchmarks, our method exhibits mo thetic bipartite graphs dataset generated from citation network datasets: Cora, Citeseer and PubMed <ref type="bibr" target="#b18">(Sen et al., 2008)</ref>. Documents and citation links between them a
ubbed as Graph Neural Networks (GNNs) <ref type="bibr">(Gori, Monfardini, and Scarselli, 2005;</ref><ref type="bibr" target="#b17">Scarselli et al., 2009)</ref>. To their essential characteristics, GN
p><p>Early works on graphical structure data can be dated back to the research of graph embedding ( <ref type="bibr" target="#b15">Perozzi, Al-Rfou, and Skiena, 2014;</ref><ref type="bibr" target="#b4 wo groups: random walksbased methods, and graph convolutional networks (GCN)based methods. DeepWalk <ref type="bibr" target="#b15">(Perozzi, Al-Rfou, and Skiena, 2014)</ref> and Node2vec <ref type="bi
nd Zhu, 2016;</ref><ref type="bibr" target="#b16">Qiu et al., 2018)</ref>, and visual understanding <ref type="bibr" target="#b22">(Yang et al., 2018;</ref><ref type="bibr" target="#b20">Wan et al., 2
et="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targe >19,</ref><ref type="bibr" target="#b19">20]</ref> for GP. Very recently, the authors of PowerGraph <ref type="bibr" target="#b5">[6]</ref> find that the vertex-cut methods can achieve better performa rtex-cut has attracted more and more attention from DGC research community. For example, PowerGraph <ref type="bibr" target="#b5">[6]</ref> adopts a random vertex-cut method and two greedy variants fo sly guarantee good workload balance. • DBH can be implemented as an execution engine for PowerGraph <ref type="bibr" target="#b5">[6]</ref>, and hence all PowerGraph applications can be seamlessly sup hines. Hence, |A(v)| is the number of replicas of v among different machines. Similar to PowerGraph <ref type="bibr" target="#b5">[6]</ref>, one of the replicas of a vertex is chosen as the master and the α is, the more skewed a graph will be. This power-law degree distribution makes GP challenging <ref type="bibr" target="#b5">[6]</ref>. Although vertex-cut methods can achieve better performance though vertex-cut methods can achieve better performance than edge-cut methods for power-law graphs <ref type="bibr" target="#b5">[6]</ref>, existing vertex-cut methods, such as random method in Power ysis for our DBH method. For comparison, the random vertex-cut method (called Random) of PowerGraph <ref type="bibr" target="#b5">[6]</ref> and the grid-based constrained solution (called Grid) of Gra ly to the p machines via a randomized hash function. The result can be directly got from PowerGraph <ref type="bibr" target="#b5">[6]</ref>. Lemma 1. Assume that we have a sequence of n vertices {v i theorem says that our DBH method has smaller expected replication factor than Random of PowerGraph <ref type="bibr" target="#b5">[6]</ref>.</p><p>Next we turn to the analysis of the balance constrain "5.2">Baselines and Evaluation Metric</head><p>In our experiment, we adopt the Random of PowerGraph <ref type="bibr" target="#b5">[6]</ref> and the Grid of GraphBuilder [8]<ref type="foot" target="#fo
#b5">[6]</ref> adopts a random vertex-cut method and two greedy variants for GP.</p><p>GraphBuilder <ref type="bibr" target="#b7">[8]</ref> provides some heuristics, such as the grid-based constrained sting vertex-cut methods, such as random method in PowerGraph and grid-based method in GraphBuilder <ref type="bibr" target="#b7">[8]</ref>, cannot make effective use of the powerlaw distribution to a "bibr" target="#b5">[6]</ref> and the grid-based constrained solution (called Grid) of GraphBuilder <ref type="bibr" target="#b7">[8]</ref> are adopted as baselines. Our analysis is based on randomiza
arget="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>. To perform distributed (parallel) graph-computing on cluster
balance factors. We de- The degrees of natural graphs usually follow skewed power-law distributions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>:</p><formula xml:id="f
his problem is well studied in the model of uniformly throwing n balls into p bins when n p(ln p) 3 <ref type="bibr" target="#b16">[17]</ref>.</p><p>Lemma 2. The maximum number of master vertices for
use edge-cut methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> for GP. Very recent
target="#b12">[13]</ref> and Pregel <ref type="bibr" target="#b14">[15]</ref>, use edge-cut methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targ

rget="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> for GP. Very recently, the authors of PowerGraph <ref type=
his problem is well studied in the model of uniformly throwing n balls into p bins when n p(ln p) 3 <ref type="bibr" target="#b16">[17]</ref>.</p><p>Lemma 2. The maximum number of master vertices for
balance factors. We de- The degrees of natural graphs usually follow skewed power-law distributions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>:</p><formula xml:id="f
arget="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>. To perform distributed (parallel) graph-computing on cluster
use edge-cut methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> for GP. Very recent
ed much attention from big data machine learning community <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" tar t traditional DGC frameworks, such as GraphLab <ref type="bibr" target="#b12">[13]</ref> and Pregel <ref type="bibr" target="#b14">[15]</ref>, use edge-cut methods <ref type="bibr" target="#b8">[9,</r
arget="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>. To perform distributed (parallel) graph-computing on cluster
natural graphs usually follow skewed power-law distributions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>:</p><formula xml:id="formula_2">fine 1 n n i=1 |A(v i )| as r
arget="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>. To perform distribut
natural graphs usually follow skewed power-law distributions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>:</p><formula xml:id="formula_2">fine 1 n n i=1 |A(v i )| as r
learning community <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" targe
ed much attention from big data machine learning community <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" tar t traditional DGC frameworks, such as GraphLab <ref type="bibr" target="#b12">[13]</ref> and Pregel <ref type="bibr" target="#b14">[15]</ref>, use edge-cut methods <ref type="bibr" target="#b8">[9,</r

use edge-cut methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> for GP. Very recent
<ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18]</ref> and machine learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4]</ref> communities exposed bstitute. We expect the target DNN to misclassify them due to transferability between architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4]</ref> To understand the di " target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>, or (b) an independently collected training set to fit an a fit an auxiliary model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref>. This limited their applicability to strong adversaries cap t the case in practice <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n st adversarial attacks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref> is to approximate its solution using gradient-based optimiz the greatest empirical success so far: adversarial training <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>, and defensive distillation for DNNs <ref type="bibr" targe adversarial samples transfer between different architectures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>. Here, we build an intuition behind transferability based o to simultaneously provide all of these three key properties <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar robustness of significantly descriptive models, such as DNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>. We implemented an bution than the oracle could train a model with a different architecture and use it as a substitute <ref type="bibr" target="#b13">[14]</ref>: adversarial examples designed to manipulate the substitut
target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>, and defensive distillation for DNNs <ref type="bibr" target="#b9">[10]</ref>.</p><p>Adversarial training: It was shown that injecting ad esimal perturbations, can be evaded by the black-box approach. T is the temperature of distillation <ref type="bibr" target="#b9">[10]</ref>. Curves marked by (direct) indicate baseline attacks comput allows us to evade it.</p><p>Defensive distillation: Due to space constraints, we refer readers to <ref type="bibr" target="#b9">[10]</ref> for a detailed presentation of defensive distillation, whic ate this defense on a locally trained oracle. Therefore, we train a distilled model as described in <ref type="bibr" target="#b9">[10]</ref> to act as our MNIST oracle.</p><p>We train several variants e Goodfellow algorithm at varying ? misclassified by the oracle.T is the temperature of distillation<ref type="bibr" target="#b9">[10]</ref>. Curves marked by (direct) indicate baseline attacks comput
at it yielded substantial improvements.</p><p>Reducing Oracle Querying: We apply reservoir sampling <ref type="bibr" target="#b15">[16]</ref> to reduce the number of queries made to the oracle. This i
them to the benign or malware class. Efforts in the security <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target= ft adversarial examples using either: (a) detailed knowledge of the DNN architecture and parameters <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target " target="#b13">14]</ref>, or (b) an independently collected training set to fit an auxiliary model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target
models, such as DNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>. We implemented an approximation of this defense using the
target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>, and defensive distillation for DNNs <ref type="bibr" target="#b9">[10]</ref>.</p><p>Adversarial training: It was shown that injecting ad esimal perturbations, can be evaded by the black-box approach. T is the temperature of distillation <ref type="bibr" target="#b9">[10]</ref>. Curves marked by (direct) indicate baseline attacks comput allows us to evade it.</p><p>Defensive distillation: Due to space constraints, we refer readers to <ref type="bibr" target="#b9">[10]</ref> for a detailed presentation of defensive distillation, whic ate this defense on a locally trained oracle. Therefore, we train a distilled model as described in <ref type="bibr" target="#b9">[10]</ref> to act as our MNIST oracle.</p><p>We train several variants e Goodfellow algorithm at varying ? misclassified by the oracle.T is the temperature of distillation<ref type="bibr" target="#b9">[10]</ref>. Curves marked by (direct) indicate baseline attacks comput
models, such as DNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>. We implemented an approximation of this defense using the
target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>, and defensive distillation for DNNs <ref type="bibr" target="#b9">[10]</ref>.</p><p>Adversarial training: It was shown that injecting ad esimal perturbations, can be evaded by the black-box approach. T is the temperature of distillation <ref type="bibr" target="#b9">[10]</ref>. Curves marked by (direct) indicate baseline attacks comput allows us to evade it.</p><p>Defensive distillation: Due to space constraints, we refer readers to <ref type="bibr" target="#b9">[10]</ref> for a detailed presentation of defensive distillation, whic ate this defense on a locally trained oracle. Therefore, we train a distilled model as described in <ref type="bibr" target="#b9">[10]</ref> to act as our MNIST oracle.</p><p>We train several variants e Goodfellow algorithm at varying ? misclassified by the oracle.T is the temperature of distillation<ref type="bibr" target="#b9">[10]</ref>. Curves marked by (direct) indicate baseline attacks comput
http://www.tei-c.org/ns/1.0"><head n="8.">DEFENSE STRATEGIES</head><p>The two types of defense are: <ref type="bibr" target="#b0">(1)</ref> reactive where one seeks to detect adversarial examples, and
at it yielded substantial improvements.</p><p>Reducing Oracle Querying: We apply reservoir sampling <ref type="bibr" target="#b15">[16]</ref> to reduce the number of queries made to the oracle. This i
fforts in the security <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18]</ref> and machine learning tecture and parameters <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>, or (b) an independen adversarial examples showed this is not the case in practice <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div> <div xmlns L model: e.g., a DNN. The basis for most adversarial attacks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref> is to approximate its 0"><head n="3.">THREAT MODEL</head><p>A taxonomy of adversaries against DNN classifiers is found in <ref type="bibr" target="#b8">[9]</ref>. In our work, the adversary seeks to force a classifier to m where adversaries seek to take samples from any legitimate source class to any chosen target class <ref type="bibr" target="#b8">[9]</ref>. Misclassification attacks are a special case of source-targ our running set of architectures. Performances are comparable with some DNNs performing better 5 In <ref type="bibr" target="#b8">[9]</ref>, the algorithm stopped perturbing when the input reached the he adversary can use one of the previously described attacks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref> to craft adversarial samples misclassified by F . As long as plementing two previously introduced approaches described in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. We provide an overview of the two approaches, namely the Goo ial samples produced by each algorithm introduced previously <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, to elect the strongest technique under our threat model.</p>
pe="bibr" target="#b34">31]</ref>, ASPP <ref type="bibr" target="#b6">[3]</ref>, and Deformable CNN <ref type="bibr" target="#b7">[4]</ref>.</p><p>The Inception block adopts multiple branches with dif all the positions equally, probably leading to confusion between object and context. Deformable CNN <ref type="bibr" target="#b7">[4]</ref> learns distinctive resolutions of individual objects, unfort pe="bibr" target="#b36">[33]</ref>, ASPP <ref type="bibr" target="#b6">[3]</ref> and Deformable CNN <ref type="bibr" target="#b7">[4]</ref>. For Inception, besides the original version, we change its
same backbone network as in SSD <ref type="bibr" target="#b25">[22]</ref>. In brief, it is a VGG16 <ref type="bibr" target="#b33">[30]</ref> architecture pre-trained on the ILSVRC CLS-LOC dataset <re
umber of effective extensions are proposed to further improve the detection accuracy, such as R-FCN <ref type="bibr" target="#b20">[17]</ref>, FPN <ref type="bibr" target="#b22">[19]</ref>, Mask R-CNN in some reputable object detectors, such as SSD <ref type="bibr" target="#b25">[22]</ref> and R-FCN <ref type="bibr" target="#b20">[17]</ref>, to elevate speed or/and accuracy.</p><p>In this paper, we of 80.5%, while keeping the real-time speed as SSD300. It even reaches the same accuracy with R-FCN <ref type="bibr" target="#b20">[17]</ref>, the advanced model under the two-stage framework. RFB Net ef> VGG 07+12 73.2 7 Faster <ref type="bibr" target="#b14">[11]</ref> ResNet-101 07+12 76.4 5 R-FCN <ref type="bibr" target="#b20">[17]</ref> ResNet-101 07+12 80.5 9 YOLOv2 544 <ref type="bibr" target which surpasses the baseline score of SSD300* with a large margin, and even equals to that of R-FCN <ref type="bibr" target="#b20">[17]</ref> which employs ResNet-101 as the base net with a larger inp (denoted as RFB Net512-E), while the computational cost only marginally ascends.  21.6 25 [B] R-FCN <ref type="bibr" target="#b20">[17]</ref> 29.9 110 [C] SSD512* <ref type="bibr" target="#b25">[22]</
n in the following section of experiments. All new conv-layers are initialized with the MSRA method <ref type="bibr" target="#b13">[10]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
b11">[8]</ref> straightforwardly combines the steps of cropping box proposals like Selective Search <ref type="bibr" target="#b38">[35]</ref> and classifying them through a CN-N model, yielding a sign
tive updated descendants, e.g. Fast R-CNN <ref type="bibr" target="#b10">[7]</ref> and Faster R-CNN <ref type="bibr" target="#b29">[26]</ref>, have persistently promoted the performance of object dete detection. Its descendants (e.g., Fast R-CNN <ref type="bibr" target="#b10">[7]</ref>, Faster R-CNN <ref type="bibr" target="#b29">[26]</ref>) update the two-stage framework and achieve dominant perfo div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Data mAP(%) FPS Faster <ref type="bibr" target="#b29">[26]</ref> VGG 07+12 73.2 7 Faster <ref type="bibr" target="#b14">[11
f type="bibr" target="#b8">[5]</ref>, MS COCO <ref type="bibr" target="#b24">[21]</ref>, and ILSVRC <ref type="bibr" target="#b30">[27]</ref>. They formulate this issue as a two-stage problem and buil 16 <ref type="bibr" target="#b33">[30]</ref> architecture pre-trained on the ILSVRC CLS-LOC dataset <ref type="bibr" target="#b30">[27]</ref>, where its fc6 and fc7 layers are converted to convolution
tive updated descendants, e.g. Fast R-CNN <ref type="bibr" target="#b10">[7]</ref> and Faster R-CNN <ref type="bibr" target="#b29">[26]</ref>, have persistently promoted the performance of object dete detection. Its descendants (e.g., Fast R-CNN <ref type="bibr" target="#b10">[7]</ref>, Faster R-CNN <ref type="bibr" target="#b29">[26]</ref>) update the two-stage framework and achieve dominant perfo div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Data mAP(%) FPS Faster <ref type="bibr" target="#b29">[26]</ref> VGG 07+12 73.2 7 Faster <ref type="bibr" target="#b14">[11
promoted the performance of object detection on major challenges and benchmarks, such as Pascal VOC <ref type="bibr" target="#b8">[5]</ref>, MS COCO <ref type="bibr" target="#b24">[21]</ref>, and ILSV ww.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on the Pascal VOC 2007 <ref type="bibr" target="#b8">[5]</ref> and MS COCO <ref type="bibr" target="#b24">[21]</ref> datase

b11">[8]</ref> straightforwardly combines the steps of cropping box proposals like Selective Search <ref type="bibr" target="#b38">[35]</ref> and classifying them through a CN-N model, yielding a sign
/head><p>Perceptron learning for microarchitectural prediction was introduced for branch prediction <ref type="bibr" target="#b19">[20]</ref>. Our predictor uses a version of microarchitectural percep on all levels of cache hierarchies. Branch prediction is done using the perceptron branch predictor <ref type="bibr" target="#b19">[20]</ref>. The page size is 4KB. ChampSim operates all the prefetche org/ns/1.0"><head n="7.5">Perceptrons in Cache Management</head><p>In addition to branch prediction <ref type="bibr" target="#b19">[20]</ref>, perceptron-based learning has been applied to the area of
reonly prefetchers: Best Offset Prefetcher (BOP), DRAM Aware -Access Map Pattern Matching (DA-AMPM) <ref type="bibr" target="#b31">[32]</ref> and Signature Path Prefetcher (SPP). BOP was the winner of can be identified and prefetched that is centered on the current access. DRAM-Aware AMPM (DA-AMPM) <ref type="bibr" target="#b31">[32]</ref> is a variant of AMPM that delays some prefetches so they c
weights are then thresholded to generate a prediction. When a block from one of a few sampled sets <ref type="bibr" target="#b43">[44]</ref> is reused or evicted, the corresponding weights are decrem
e more sophisticated as they look into past memory behavior <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, locality <ref type="bibr" target="#b11">[12]</ref><ref typ ial footprint is prefetched. Ishii et al. propose the Access Map Pattern Matching prefetcher (AMPM) <ref type="bibr" target="#b10">[11]</ref>, which creates a map of all accessed lines within a region
significantly harm performance. To minimize interference from prefetching, Wu et al. propose PACMan <ref type="bibr" target="#b36">[37]</ref>, a prefetch-aware cache management policy. PACMan dedicate
0,</ref><ref type="bibr" target="#b10">11]</ref>, locality <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" t essed. In addition to these simpler examples, Somogyi et al. propose Spatial Memory Streaming (SMS) <ref type="bibr" target="#b12">[13]</ref>.</p><p>SMS works by learning the spatial footprint of all
a version of microarchitectural perceptron prediction known as the "hashed perceptron" organization <ref type="bibr" target="#b20">[21]</ref>. As an abstract idea, a hashed perceptron predictor hashes
hes in their MIN algorithm-inspired Hawkeye cache management system. Ebrahimi et al. introduce HPAC <ref type="bibr" target="#b40">[41]</ref> which provides a coordinated control between multiple pref
for hardware data prefetching.</p><p>Previous work in this area either relied on program semantics <ref type="bibr" target="#b3">[4]</ref> or were application specific <ref type="bibr" target="#b4">[ rcement Learning and dynamically scaling the magnitude of feedback given to the baseline prefetcher <ref type="bibr" target="#b3">[4]</ref>. The prefetcher relies on compiler support to receive featur
</ref> illustrates the above scenario. Here we consider a stateof-the-art lookahead prefetcher -SPP <ref type="bibr" target="#b1">[2]</ref>. Lookahead prefetchers such as SPP provide a mechanism to sp the-art prefetcher that we use to evaluate PPF in this paper is the Signature Path Prefetcher (SPP) <ref type="bibr" target="#b1">[2]</ref>, however as we describe, PPF can be designed to benefit any her other aspects to detect complex memory access patterns. See Section 7 for other relevant work.  <ref type="bibr" target="#b1">[2]</ref>, a confidencebased lookahead prefetcher. SPP creates a signa account for DRAM row buffer locality. SPP has been shown to outperform BOP on SPEC CPU 2006 traces <ref type="bibr" target="#b1">[2]</ref>. For each of these, we compare their speedups taking the no of deltas between cache line accesses within memory pages with the next delta within that page. SPP <ref type="bibr" target="#b1">[2]</ref> and KPC's prefetching component <ref type="bibr" target="#b3 Table</p></note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p><ref type="bibr" target="#b1">2</ref> The Reject Table does not need to maintain the useful bit as t
]</ref>, locality <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" t
xt layer. This greedy layer-wise training process forms the model of the Stacked Sparse Autoencoder <ref type="bibr" target="#b23">(Vincent et al. 2010)</ref>. When all the layers are trained in this

eep learning have been developed and successfully adopted in real tasks, such as speech recognition <ref type="bibr" target="#b8">(Dahl et al. 2012)</ref>, image classification <ref type="bibr" target eech recognition, and natural language processing <ref type="bibr" target="#b1">(Bengio 2009</ref>) <ref type="bibr" target="#b8">(Dahl et al. 2012</ref>) <ref type="bibr" target="#b7">(Collobert et a
bibr" target="#b13">(Krizhevsky, Sutskever, and Hinton 2012)</ref>, and natural language processing <ref type="bibr" target="#b7">(Collobert et al. 2011)</ref>. However, to our knowledge, the adoption type="bibr" target="#b1">(Bengio 2009</ref>) <ref type="bibr" target="#b8">(Dahl et al. 2012</ref>) <ref type="bibr" target="#b7">(Collobert et al. 2011)</ref>  <ref type="bibr" target="#b13">(Krizhev
the discrete optimization problem in ( <ref type="formula" target="#formula_2">2</ref>) is NP-Hard <ref type="bibr" target="#b25">(Wagner and Wagner 1993)</ref>. Therefore, spectral clustering turns
s image segmentation <ref type="bibr" target="#b19">(Shi and Malik 2000)</ref>, community detection <ref type="bibr" target="#b20">(Smyth and White 2005)</ref>, and VLSI design <ref type="bibr" target
ion function (e.g., sigmoid or tanh) or some regularization on features (e.g., sparsity constraints <ref type="bibr" target="#b15">(Poultney et al. 2006))</ref>. By stacking these non-linear single la nodes in the graph, when the graph has some special sparse and dense structure like Toeplitz matrix <ref type="bibr" target="#b15">(Pan and Chen 1999)</ref>. In contrast, it is not difficult to see th

ng method, in terms of what they actually optimize. Among many existing graph clustering algorithms <ref type="bibr" target="#b12">(Karypis and Kumar 1998)</ref> <ref type="bibr" target="#b19">(Shi an
bibr" target="#b13">(Krizhevsky, Sutskever, and Hinton 2012)</ref>, and natural language processing <ref type="bibr" target="#b7">(Collobert et al. 2011)</ref>. However, to our knowledge, the adoption type="bibr" target="#b1">(Bengio 2009</ref>) <ref type="bibr" target="#b8">(Dahl et al. 2012</ref>) <ref type="bibr" target="#b7">(Collobert et al. 2011)</ref>  <ref type="bibr" target="#b13">(Krizhev
many applications such as image classification, speech recognition, and natural language processing <ref type="bibr" target="#b1">(Bengio 2009</ref>) <ref type="bibr" target="#b8">(Dahl et al. 2012</r

ertical convolutional filters. Chen et al. <ref type="bibr" target="#b2">[3]</ref> and Huang et al. <ref type="bibr" target="#b18">[19]</ref> employ Memory Network to improve sequential recommendation ative items that the user has not interacted with. To make the sampling reliable and representative <ref type="bibr" target="#b18">[19]</ref>, these 100 negative items are sampled according to their p
rget="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref>. The basic paradigm get="#b14">15,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref>. The basic idea of
el users' preferences based on their interaction histories <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b43">43]</ref>. Among various CF methods, Matrix Factorization (MF) is the et="#b19">[20,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b43">43]</ref>. They estimate a user's preference on an item via measuring
_2">3</ref> : This is a series of product review datasets crawled from Amazon.com by McAuley et al. <ref type="bibr" target="#b34">[34]</ref>. They split the data into separate datasets according to t
om different representation subspaces at different positions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b52">52]</ref>. Thus, we here adop
rget="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref>. The basic paradigm get="#b14">15,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref>. The basic idea of
ence on an item by the inner product between their vectors <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b41">41]</ref>. Another line of wo
>. Another line of work is item-based neighborhood methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" tar
ence on an item by the inner product between their vectors <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b41">41]</ref>. Another line of wo
pe="bibr" target="#b10">11]</ref>.</p><p>Recently, RNN and its variants, Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">[4]</ref> and Long Short-Term Memory (LSTM) <ref type="bibr" target="#
ate a variable number of features to a fixed-length vector. Fisher Vectors were first introduced in <ref type="bibr" target="#b15">(Jaakkola and Haussler 1999)</ref> to combine the advantages of gener

tion detection.</p><p>Another line of work attempts to find behavioral cues for deception detection <ref type="bibr" target="#b6">(DePaulo et al. 2003)</ref>. These cues are faint behavioral residues,
behavioral residues, which are difficult for untrained people to detect. For example, according to <ref type="bibr" target="#b9">(Ekman et al. 1969;</ref><ref type="bibr" target="#b10">Ekman 2009)</r e prone to overfitting and did not generalize to new subjects. Based on Ekman's psychology research <ref type="bibr" target="#b9">(Ekman et al. 1969;</ref><ref type="bibr" target="#b10">Ekman 2009</re Here, we introduce the high-level features used to represent facial micro-expressions. According to <ref type="bibr" target="#b9">(Ekman et al. 1969;</ref><ref type="bibr" target="#b10">Ekman 2009)</r
lly much easier for humans to detect a subtle facial expression from videos than from static images <ref type="bibr" target="#b13">(Grill-Spector et al. 1998)</ref>. For example, Fig. <ref type="figur
behavioral residues, which are difficult for untrained people to detect. For example, according to <ref type="bibr" target="#b9">(Ekman et al. 1969;</ref><ref type="bibr" target="#b10">Ekman 2009)</r e prone to overfitting and did not generalize to new subjects. Based on Ekman's psychology research <ref type="bibr" target="#b9">(Ekman et al. 1969;</ref><ref type="bibr" target="#b10">Ekman 2009</re Here, we introduce the high-level features used to represent facial micro-expressions. According to <ref type="bibr" target="#b9">(Ekman et al. 1969;</ref><ref type="bibr" target="#b10">Ekman 2009)</r
method should integrate information from more than one modality. Taking motivation from prior work <ref type="bibr" target="#b23">(Pérez-Rosas et al. 2015;</ref><ref type="bibr" target="#b16">Jaiswal ceptive behavior. We also include simple verbal and audio features, as other multi-modal approaches <ref type="bibr" target="#b23">(Pérez-Rosas et al. 2015;</ref><ref type="bibr" target="#b16">Jaiswal ed more on high stake deception detection, so that experiments are closer to real life settings. In <ref type="bibr" target="#b23">(Pérez-Rosas et al. 2015)</ref>, a new dataset containing reallife tr >We evaluate our automated deception detection approach on a real-life deception detection database <ref type="bibr" target="#b23">(Pérez-Rosas et al. 2015)</ref>. This database consists of 121 court nge or human editing. In the experiments shown below, we do not report the results, as described in <ref type="bibr" target="#b23">(Pérez-Rosas et al. 2015)</ref>. Instead, we re-implement the method is how the performance will be affected if we use the Ground Truth micro-expression features, as in <ref type="bibr" target="#b23">(Pérez-Rosas et al. 2015)</ref>. In the following experiment, we use
re based methods are overt and could be disrupted by the subject's counter preparation and behavior <ref type="bibr" target="#b12">(Ganis et al. 2011</ref>).</p><p>Among the covert systems, computer v
audio features equally and use our feature encoding method to encode the whole sequence, similar to <ref type="bibr" target="#b2">(Campbell, Sturim, and Reynolds 2006)</ref>. This is because we are no
ed methods play an important role. Early works <ref type="bibr" target="#b19">(Lu et al. 2005;</ref><ref type="bibr" target="#b26">Tsechpenakis et al. 2005</ref>) used blob analysis to track head and ace may not always be viewed frontally or centered. Here, we employ IDT (Improved Dense Trajectory) <ref type="bibr" target="#b26">(Wang et al. 2016</ref>) features due to their excellent performance
amics for recognizing facial micro-expressions. This coincides with the psychological insights from <ref type="bibr" target="#b7">(Duran et al. 2013)</ref>, in which the authors suggest focusing on dy patterns, e.g. micro facial expression, itself is a challenging problem. In addition, Duran et al. <ref type="bibr" target="#b7">(Duran et al. 2013)</ref> suggested that research should focus more on
.</p><p>Among the covert systems, computer vision based methods play an important role. Early works <ref type="bibr" target="#b19">(Lu et al. 2005;</ref><ref type="bibr" target="#b26">Tsechpenakis et
jectory, the method computes HOG (histogram of oriented gradients), HOF (histogram of optical flow) <ref type="bibr" target="#b18">(Laptev et al. 2008)</ref>, MBH (motion bountary histogram) <ref type
image sequence when compared to the static image, as shown by the two green dotted lines. deception <ref type="bibr" target="#b11">(Farah et al. 2014)</ref>. Additionally, the cost of the equipment an pe="bibr">(Kozel et al. 2005;</ref><ref type="bibr" target="#b17">Langleben and Moriarty 2013;</ref><ref type="bibr" target="#b11">Farah et al. 2014)</ref>, important questions related to the working are still open research problems <ref type="bibr" target="#b17">(Langleben and Moriarty 2013;</ref><ref type="bibr" target="#b11">Farah et al. 2014)</ref>. Furthermore, the above mentioned physiologi
lassification (Perronnin and Dance 2007), action recognition (Wang et al. 2016) and video retrieval <ref type="bibr" target="#b14">(Han et al. 2017)</ref>.</p><p>Fisher Vector encoding first builds a
elated with deception. Although it achieves high accuracy <ref type="bibr">(Kozel et al. 2005;</ref><ref type="bibr" target="#b17">Langleben and Moriarty 2013;</ref><ref type="bibr" target="#b11">Fara to the working mechanism, reliability and the experimental setting are still open research problems <ref type="bibr" target="#b17">(Langleben and Moriarty 2013;</ref><ref type="bibr" target="#b11">Far
image sequence when compared to the static image, as shown by the two green dotted lines. deception <ref type="bibr" target="#b11">(Farah et al. 2014)</ref>. Additionally, the cost of the equipment an pe="bibr">(Kozel et al. 2005;</ref><ref type="bibr" target="#b17">Langleben and Moriarty 2013;</ref><ref type="bibr" target="#b11">Farah et al. 2014)</ref>, important questions related to the working are still open research problems <ref type="bibr" target="#b17">(Langleben and Moriarty 2013;</ref><ref type="bibr" target="#b11">Farah et al. 2014)</ref>. Furthermore, the above mentioned physiologi
lly much easier for humans to detect a subtle facial expression from videos than from static images <ref type="bibr" target="#b13">(Grill-Spector et al. 1998)</ref>. For example, Fig. <ref type="figur
audio features equally and use our feature encoding method to encode the whole sequence, similar to <ref type="bibr" target="#b2">(Campbell, Sturim, and Reynolds 2006)</ref>. This is because we are no
image sequence when compared to the static image, as shown by the two green dotted lines. deception <ref type="bibr" target="#b11">(Farah et al. 2014)</ref>. Additionally, the cost of the equipment an pe="bibr">(Kozel et al. 2005;</ref><ref type="bibr" target="#b17">Langleben and Moriarty 2013;</ref><ref type="bibr" target="#b11">Farah et al. 2014)</ref>, important questions related to the working are still open research problems <ref type="bibr" target="#b17">(Langleben and Moriarty 2013;</ref><ref type="bibr" target="#b11">Farah et al. 2014)</ref>. Furthermore, the above mentioned physiologi
ons capture meaningful semantic structure. Compared to other text-based deception detection methods <ref type="bibr" target="#b24">(Porter and Brinke 2010)</ref>, Glove is more widely applicable in un
ty. Taking motivation from prior work <ref type="bibr" target="#b23">(Pérez-Rosas et al. 2015;</ref><ref type="bibr" target="#b16">Jaiswal, Tabibu, and Bajpai 2016)</ref>, we also include features fro ures, as other multi-modal approaches <ref type="bibr" target="#b23">(Pérez-Rosas et al. 2015;</ref><ref type="bibr" target="#b16">Jaiswal, Tabibu, and Bajpai 2016)</ref>, into the overall covert auto
ns is determined by dynamic external customers. The Sharing Architecture is inspired by Core Fusion <ref type="bibr" target="#b22">[24]</ref>, WiD-GET <ref type="bibr" target="#b62">[64]</ref>, the Di e fetch stage of the pipeline, it stalls all of the Slices in a VCore.</p><p>Similar to Core Fusion <ref type="bibr" target="#b22">[24]</ref>, a distributed branch predictor is used, thus the effectiv issue processors. The Sharing Architecture  <ref type="bibr" target="#b6">[8]</ref> and Core Fusion <ref type="bibr" target="#b22">[24]</ref> require LSQ bank prediction because a memory operation's e distribute Reorder Buffer (ROB) entries across Slices. We leverage the approach used by Core Fusion <ref type="bibr" target="#b22">[24]</ref> which uses a pre-commit pointer to guarantee that all ROBs potential to exploit high ILP.</p><p>The Sharing Architecture leverages many ideas from Core Fusion <ref type="bibr" target="#b22">[24]</ref> on how to distribute resources across cores, but unlike Co
and established companies moving computationally intensive <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b44">46]</ref> and data serving tasks into the Cloud.</p><p>In this work,
Sharing Architecture is inspired by Core Fusion <ref type="bibr" target="#b22">[24]</ref>, WiD-GET <ref type="bibr" target="#b62">[64]</ref>, the Distributed ILP mechanisms in the TRIPS architecture d to better efficiency and provides a way to partition cache traffic and working sets.</p><p>WiDGET <ref type="bibr" target="#b62">[64]</ref> decouples computation resources to enable flexibility in p
arget="#b4">6,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b53">55]</ref>. The auto-tuner would slowly search the configuration space
operational expenditures. In contrast to instance-based pricing, private IaaS Cloud systems such   <ref type="bibr" target="#b29">[31]</ref> system provide a richer ability to configure a VM instance 2</label><figDesc>Figure 2. Resource Utility Functions for Different Applications as VMWare's VCloud<ref type="bibr" target="#b29">[31]</ref> system provide a richer ability to configure a VM instance
isticated control theory based or machine learning based software adaptation engine, such as PTRADE <ref type="bibr" target="#b21">[23]</ref>. This software engine asks application developers to speci
and performance benefit of heterogeneous multicore systems <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b32">34]</ref>, but is built out of a large homogeneous fabric. Applicatio ngle application may have different utility functions as a function of time of day or program phase <ref type="bibr" target="#b32">[34]</ref>. Programs commonly have different phases and if properly e ingle application may have different utility functions as a function of time of day or program phase<ref type="bibr" target="#b32">[34]</ref>. Programs commonly have different phases and if properly e
ds. Paritioning a shared LLC potentially mitigates the negative performance effects of coscheduling <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" ta
get="#b17">19,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b54">56]</ref>. These Cloud provid
get="#b9">[11,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b51">53]</ref>, and how ILP is map >[65]</ref>, TRIPS <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b48">50]</ref>, and Wavescalar <ref type="bibr" target="#b56">[58]</ref>.
itecture has much of the same motivation and performance benefit of heterogeneous multicore systems <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b32">34]</ref>, but is built out Sharing Architecture, MorphCore is unable to scale up and down flexibly.</p><p>A heterogeneous CMP <ref type="bibr" target="#b31">[33]</ref> can combine superscalar cores for ILP with simple cores fo
ition pattern is more complicated. • The recent approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref>, which divide the u the fairness and the convenience of comparison, we follow <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref> to filter out sessi , s,i −1 ] with the last item s,i −1 as l abel . Following <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref>, for the Yoochoose tention on the last item after encoding with a RNN. To alleviate the influence of time order, STAMP <ref type="bibr" target="#b18">[19]</ref> only utilizes the self-attention mechanism without RNN. SR to sum up as the session embedding. To further alleviate the bias introduced by time series, STAMP <ref type="bibr" target="#b18">[19]</ref> entirely replaces the recurrent encoder with an attention h enables the model to explicitly emphasize on the more important parts of the input.</p><p>• STAMP <ref type="bibr" target="#b18">[19]</ref> uses attention layers to replace all RNN encoders in previ ith different lengths because the length varies greatly within one dataset. Following previous work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref>, sessions in Yooch
rget="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref> work under the mech s, some baseline methods on GNN, for example, GCN <ref type="bibr" target="#b11">[12]</ref> and GAT <ref type="bibr" target="#b32">[33]</ref>, are demonstrated to be capable of extracting features of t can be used to generate node embeddings, e.g., GCN <ref type="bibr" target="#b11">[12]</ref>, GAT <ref type="bibr" target="#b32">[33]</ref> and gated graph networks <ref type="bibr" target="#b17">[1 get="#b19">[20]</ref>.</p><p>As suggested in previous work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, the multi-head attention can help to stabilize the trainin
their learned feature and uses a normal neural network layer to process the sorted nodes. DiffPool <ref type="bibr" target="#b39">[40]</ref> designs two sets of GNN for every layer to learn a new den
ed as the recent preference of a user. Sequential recommendation is based on the Markov chain model <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42]</ref>, which learns the target="#b41">[42]</ref> proposed to encode the state of the item transition pattern. Shani et al. <ref type="bibr" target="#b27">[28]</ref> made use of a Markov Decision Process (MDP) to compute the
get="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref> work under the mechanism that is similar to message passing s, for example, graph classification and graph isomorphism <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39]</ref>. Set2Set <ref type="bibr" target="#b33">[34]</ref> assigns
their learned feature and uses a normal neural network layer to process the sorted nodes. DiffPool <ref type="bibr" target="#b39">[40]</ref> designs two sets of GNN for every layer to learn a new den
y, GNN is applied to the simple situation on directed graphs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. In recent years, many GNN methods <ref type="bibr" target=
" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. In recent years, many GNN methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ y node i in G s , in a WGAT layer, all attention coefficients of their neighbors can be computed as <ref type="bibr" target="#b5">(6)</ref>. To utilize these attention coefficients, a linear combinati
ols, for instance, zeroshot learning and domain adaptation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. These methods all depend on the identification of users an
N model, some work makes improvements on the architectural choice and the objective function design <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>. In addition to RNN, la><p>where K is the number of heads and for every head, there is a different set of parameters. in <ref type="bibr" target="#b7">(8)</ref> stands for the concatenation of all heads. As a result, afte
popular recently with the boom of recurrent neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target t al. <ref type="bibr" target="#b8">[9]</ref> proposed the GRU4REC, which applies a multi layer GRU <ref type="bibr" target="#b1">[2]</ref> to simply treat the data as time series. Based on the RNN mo
of RNN <ref type="bibr" target="#b9">[10]</ref> applied on the sequence data, for instance, GRU4REC <ref type="bibr" target="#b8">[9]</ref> and NARM <ref type="bibr" target="#b15">[16]</ref> mainly mo target="#b35">36]</ref>, which is naturally designed for processing sequential data. Hidasi et al. <ref type="bibr" target="#b8">[9]</ref> proposed the GRU4REC, which applies a multi layer GRU <ref t arget="#b15">[16]</ref>, we omit the user feature directly because of the unavailability. • GRU4REC <ref type="bibr" target="#b8">[9]</ref> stacks multiple GRU layers to encode the session sequence in
]</ref> utilized 3D convolutional neural networks to learn more accurate representations. Wu et al. <ref type="bibr" target="#b36">[37]</ref> proposed a list-wise deep neural network model to train a
Regularization is also introduced to avoid the rare high similarities for unvisited items. • BPR-MF <ref type="bibr" target="#b21">[22]</ref> proposes a BPR objective function which calculates a pairw
target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>, which is naturally designed for processing sequential data
morphism <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39]</ref>. Set2Set <ref type="bibr" target="#b33">[34]</ref> assigns each node in the graph a descriptor as the order f apacity for learning a representative session embedding for the session graph. In contrast, Set2Set <ref type="bibr" target="#b33">[34]</ref> is a graph level feature extractor which learns a query ve
ed method to capture cooccurrence signals. Incorporating content features of items, Tuan and Phuong <ref type="bibr" target="#b30">[31]</ref> utilized 3D convolutional neural networks to learn more ac
items to potential users in today's flood of web information. In recent years, the content-based RS <ref type="bibr" target="#b20">[21]</ref> and the collaborative filtering RS <ref type="bibr" target
modified to session-based recommendation by using mean latent vectors of items in a session. • FPMC <ref type="bibr" target="#b22">[23]</ref> is a hybrid model for the next-basket recommendation and i
y, GNN is applied to the simple situation on directed graphs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. In recent years, many GNN methods <ref type="bibr" target=
popular recently with the boom of recurrent neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target t al. <ref type="bibr" target="#b8">[9]</ref> proposed the GRU4REC, which applies a multi layer GRU <ref type="bibr" target="#b1">[2]</ref> to simply treat the data as time series. Based on the RNN mo
ed method to capture cooccurrence signals. Incorporating content features of items, Tuan and Phuong <ref type="bibr" target="#b30">[31]</ref> utilized 3D convolutional neural networks to learn more ac
uts a human player would have. In contrast to previous work <ref type="bibr" target="#b18">24,</ref><ref type="bibr" target="#b20">26</ref> , our approach incorporates 'end-to-end' reinforcement learn as inputs to the neural network by some previous approaches <ref type="bibr" target="#b18">24,</ref><ref type="bibr" target="#b20">26</ref> . The main drawback of this type of architecture is that a s
r training neural networks in the reinforcement learning setting, such as neural fitted Q-iteration <ref type="bibr" target="#b18">24</ref> , these methods involve the repeated training of networks de ters on each game, privy only to the inputs a human player would have. In contrast to previous work <ref type="bibr" target="#b18">24,</ref><ref type="bibr" target="#b20">26</ref> , our approach incor e history and the action have been used as inputs to the neural network by some previous approaches <ref type="bibr" target="#b18">24,</ref><ref type="bibr" target="#b20">26</ref> . The main drawback
which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay <ref type="bibr" target="#b15">[21]</ref><ref type="bibr" target="#b16">[22]</ref><ref type="bibr" t with deep network architectures was critically dependent on our incorporation of a replay algorithm <ref type="bibr" target="#b15">[21]</ref><ref type="bibr" target="#b16">[22]</ref><ref type="bibr" t n, with the timecompressed reactivation of recently experienced trajectories during offline periods <ref type="bibr" target="#b15">21,</ref><ref type="bibr" target="#b16">22</ref> (for example, waking
nvolves 32 filters of 8 3 8 with stride 4 with the input image and applies a rectifier nonlinearity <ref type="bibr" target="#b25">31,</ref><ref type="bibr" target="#b26">32</ref> . The second hidden
lay towards salient events, a phenomenon that characterizes empirically observed hippocampal replay <ref type="bibr" target="#b23">29</ref> , and relates to the notion of 'prioritized sweeping' <ref t
nvolves 32 filters of 8 3 8 with stride 4 with the input image and applies a rectifier nonlinearity <ref type="bibr" target="#b25">31,</ref><ref type="bibr" target="#b26">32</ref> . The second hidden
roximator such as a neural network is used to represent the action-value (also known as Q) function <ref type="bibr" target="#b14">20</ref> . This instability has several causes: the correlations pres arise and the parameters could get stuck in a poor local minimum, or even diverge catastrophically <ref type="bibr" target="#b14">20</ref> . By using experience replay the behaviour distribution is a
which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay <ref type="bibr" target="#b15">[21]</ref><ref type="bibr" target="#b16">[22]</ref><ref type="bibr" t with deep network architectures was critically dependent on our incorporation of a replay algorithm <ref type="bibr" target="#b15">[21]</ref><ref type="bibr" target="#b16">[22]</ref><ref type="bibr" t n, with the timecompressed reactivation of recently experienced trajectories during offline periods <ref type="bibr" target="#b15">21,</ref><ref type="bibr" target="#b16">22</ref> (for example, waking
"bibr" target="#b7">13</ref> that has eluded previous efforts <ref type="bibr" target="#b2">8,</ref><ref type="bibr" target="#b8">14,</ref><ref type="bibr" target="#b9">15</ref> . To achieve this, we
rithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains <ref type="bibr" target="#b0">[6]</ref><ref type="bibr" target="#b1">[7]</ref><ref type="bibr" targe
nvolves 32 filters of 8 3 8 with stride 4 with the input image and applies a rectifier nonlinearity <ref type="bibr" target="#b25">31,</ref><ref type="bibr" target="#b26">32</ref> . The second hidden
m raw sensory data. We use one particularly successful architecture, the deep convolutional network <ref type="bibr" target="#b11">17</ref> , which uses hierarchical layers of tiled convolutional filt
eplay <ref type="bibr" target="#b23">29</ref> , and relates to the notion of 'prioritized sweeping' <ref type="bibr" target="#b24">30</ref> in reinforcement learning. Taken together, our work illustra ategy might emphasize transitions from which we can learn the most, similar to prioritized sweeping <ref type="bibr" target="#b24">30</ref> .</p><p>The second modification to online Q-learning aimed a
rithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains <ref type="bibr" target="#b0">[6]</ref><ref type="bibr" target="#b1">[7]</ref><ref type="bibr" targe
Here we use recent advances in training deep neural networks <ref type="bibr" target="#b3">[9]</ref><ref type="bibr" target="#b4">[10]</ref><ref type="bibr" target="#b5">[11]</ref> to develop a novel l networks. Notably, recent advances in deep neural networks <ref type="bibr" target="#b3">[9]</ref><ref type="bibr" target="#b4">[10]</ref><ref type="bibr" target="#b5">[11]</ref> , in which several
agents have achieved some successes in a variety of domains <ref type="bibr" target="#b0">[6]</ref><ref type="bibr" target="#b1">[7]</ref><ref type="bibr" target="#b2">[8]</ref> , their applicability
deep neural networks <ref type="bibr" target="#b3">[9]</ref><ref type="bibr" target="#b4">[10]</ref><ref type="bibr" target="#b5">[11]</ref> to develop a novel artificial agent, termed a deep Q-networ deep neural networks <ref type="bibr" target="#b3">[9]</ref><ref type="bibr" target="#b4">[10]</ref><ref type="bibr" target="#b5">[11]</ref> , in which several layers of nodes are used to build up pro
luded previous efforts <ref type="bibr" target="#b2">8,</ref><ref type="bibr" target="#b8">14,</ref><ref type="bibr" target="#b9">15</ref> . To achieve this, we developed a novel agent, a deep Q-netwo ning literature on the 49 games where results were available <ref type="bibr" target="#b6">12,</ref><ref type="bibr" target="#b9">15</ref> . In addition to the learned agents, we also report scores fo ng shooters (River Raid) to boxing games (Boxing) and three-dimensional car-racing games (Enduro).  <ref type="bibr" target="#b9">15</ref> in the literature. The performance of DQN is normalized with here results were available for all other comparable methods <ref type="bibr" target="#b6">12,</ref><ref type="bibr" target="#b9">15</ref> . A different network was trained on each game: the same netw wing previous approaches to playing Atari 2600 games, we also use a simple frame-skipping technique <ref type="bibr" target="#b9">15</ref> . More precisely, the agent sees and selects actions on every ores obtained by DQN agents with methods from the literature <ref type="bibr" target="#b6">12,</ref><ref type="bibr" target="#b9">15</ref> and a professional human games tester Best Linear Learner is
m raw sensory data. We use one particularly successful architecture, the deep convolutional network <ref type="bibr" target="#b11">17</ref> , which uses hierarchical layers of tiled convolutional filt
a biologically inspired mechanism termed experience replay <ref type="bibr" target="#b15">[21]</ref><ref type="bibr" target="#b16">[22]</ref><ref type="bibr" target="#b17">[23]</ref> that randomizes o cally dependent on our incorporation of a replay algorithm <ref type="bibr" target="#b15">[21]</ref><ref type="bibr" target="#b16">[22]</ref><ref type="bibr" target="#b17">[23]</ref> involving the sto of recently experienced trajectories during offline periods <ref type="bibr" target="#b15">21,</ref><ref type="bibr" target="#b16">22</ref> (for example, waking rest) providing a putative mechanism by ism by which value functions may be efficiently updated through interactions with the basal ganglia <ref type="bibr" target="#b16">22</ref> . In the future, it will be important to explore the potenti
acteristics of representations within primate visual cortex <ref type="bibr" target="#b21">27,</ref><ref type="bibr" target="#b22">28</ref> . Notably, the successful integration of reinforcement learn

" target="#b19">Huang et al., 2015;</ref><ref type="bibr" target="#b7">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b22">Lample et al., 2016;</ref><ref type="bibr" target="#b46">Yang et al., model we use to perform NER. We will first describe the basic hierarchical neural CRF tagging model <ref type="bibr" target="#b22">(Lample et al., 2016;</ref><ref type="bibr" target="#b27">Ma and Hovy labels and performs inference.</p><p>In this paper, we closely follow the architecture proposed by <ref type="bibr" target="#b22">Lample et al. (2016)</ref>, and use bidirectional LSTMs for both the
get="#b30">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b15">Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b2">Artetxe et al., 2016;</ref><ref type="bibr" target="#b38">Smith et al. om the dictionary. Following previous work <ref type="bibr" target="#b50">(Zhang et al., 2016;</ref><ref type="bibr" target="#b2">Artetxe et al., 2016;</ref><ref type="bibr" target="#b38">Smith et al. every training pair an equal contribution to the objective and improving word translation accuracy <ref type="bibr" target="#b2">(Artetxe et al., 2016)</ref>. When training the NER model, however, we
any approaches in the past have leveraged the shared embedding space for cross-lingual applications <ref type="bibr" target="#b17">(Guo et al., 2015;</ref><ref type="bibr" target="#b1">Ammar et al., 2 s trained directly on data using the source Instead of directly modeling the shared embedding space <ref type="bibr" target="#b17">(Guo et al., 2015;</ref><ref type="bibr" target="#b50">Zhang et al.,
is trained jointly across different languages by sharing parameters to allow for knowledge transfer <ref type="bibr" target="#b0">(Ammar et al., 2016a;</ref><ref type="bibr" target="#b47">Yang et al.,
target="#b8">(Collobert et al., 2011;</ref><ref type="bibr" target="#b19">Huang et al., 2015;</ref><ref type="bibr" target="#b7">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b22">Lample et
r" target="#b21">Kim et al., 2012;</ref><ref type="bibr" target="#b45">Wang and Manning, 2014;</ref><ref type="bibr" target="#b32">Ni et al., 2017)</ref>. Since parallel corpora may not be always avai ng and Cohn, 2017)</ref>, including NER <ref type="bibr" target="#b4">(Bharadwaj et al., 2016;</ref><ref type="bibr" target="#b32">Ni et al., 2017)</ref>. The minimal dependency on parallel resources resources, including parallel corpora <ref type="bibr" target="#b40">(Täckström et al., 2012;</ref><ref type="bibr" target="#b32">Ni et al., 2017)</ref>, Wikipedia <ref type="bibr" target="#b33">(Not br" target="#b50">Zhang et al., 2016;</ref><ref type="bibr" target="#b14">Fang and Cohn, 2017;</ref><ref type="bibr" target="#b32">Ni et al., 2017)</ref>, we leverage the shared embedding space for wo ref>, Wikipedia <ref type="bibr" target="#b33">(Nothman et al., 2013)</ref>, and large dictionaries <ref type="bibr" target="#b32">(Ni et al., 2017;</ref><ref type="bibr" target="#b28">Mayhew et al., s comparable to other methods such as <ref type="bibr" target="#b28">Mayhew et al. (2017)</ref> and <ref type="bibr" target="#b32">Ni et al. (2017)</ref>:</p><p>• Labeled training data in the source l entries and uses the entry category as features to train language independent NER models. Recently, <ref type="bibr" target="#b32">Ni et al. (2017)</ref> propose to project word embeddings into a comm s the most common setting for using bilingual word embeddings, and has recently been applied in NER <ref type="bibr" target="#b32">(Ni et al., 2017)</ref>. In short, the source and target word embeddi
röm et al., 2013;</ref><ref type="bibr" target="#b13">Fang and Cohn, 2016)</ref>, mention detection <ref type="bibr" target="#b52">(Zitouni and Florian, 2008)</ref> and parsing <ref type="bibr" target
" target="#b47">Yang et al., 2017;</ref><ref type="bibr" target="#b9">Cotterell and Duh, 2017;</ref><ref type="bibr" target="#b24">Lin et al., 2018)</ref>. However, such approaches usually require som
ur models 5 times using different seeds and report the mean and standard deviation, as suggested by <ref type="bibr" target="#b37">Reimers and Gurevych (2017)</ref>.</p><p>Word Embeddings For all lang
nchmark CoNLL 2002 and 2003 NER datasets <ref type="bibr" target="#b41">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b42">Tjong Kim Sang and De Meulder, 2003)</ref>, which contain 4 European

benchmarks are an important tool to analyze specific aspects of computer systems. In previous work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> we presented micro- t L3 slice or obtains a copy from another core's L1 or L2 cache as indicated by the core valid bits <ref type="bibr" target="#b6">[7]</ref>. In case of an L3 miss, the caching agent forwards the reque obenchmark Design</head><p>We use an extended version of the synthetic microbenchmarks presented in <ref type="bibr" target="#b6">[7]</ref>. They include data placement and coherence state control mec
vers use point-to-point connections between the processors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. The available memory bandwidth scales with the number of pr he distributed caches in Haswell-EP based systems are kept coherent using a snooping based protocol <ref type="bibr" target="#b1">[2]</ref>. Two snooping modes-source snooping and home snooping-are av > describe the directory assisted snoop broadcast protocol (DAS)-an extension of the MESIF protocol <ref type="bibr" target="#b1">[2]</ref>. The DAS protocol uses an in-memory directory to accelerate /1.0"><head>A. MESIF Implementation</head><p>Cache coherence is maintained using the MESIF protocol <ref type="bibr" target="#b1">[2]</ref>. It inherits the modified, exclusive, shared, and invalid st ei-c.org/ns/1.0"><head>B. Source Snoop Mode</head><p>In the source snoop mode of the MESIF protocol <ref type="bibr" target="#b1">[2]</ref>, snoops are sent by the caching agents. In case of an L3 mis /div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Home Snoop Mode</head><p>The MESIF protocol <ref type="bibr" target="#b1">[2]</ref> also supports a home snoop mode in which snoops are sent by rwarded by the other node 8 as expected according to the state specifications in the MESIF protocol <ref type="bibr" target="#b1">[2]</ref>. For 2.5 MiB 5 The behavior doesn't change if hardware prefe
11]</ref>. Furthermore, the uncore frequency is adapted to the workload dynamically by the hardware <ref type="bibr" target="#b11">[12]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> /1.0" place="foot" n="4" xml:id="foot_2"><p>Uncore frequency scaling automatically adjusts frequency<ref type="bibr" target="#b11">[12]</ref> </p></note> 		</body> 		<back> 			<div type="references">

yze specific aspects of computer systems. In previous work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> we presented micro-benchmarks to measure the characteristics
.</p><p>Contemporary multi-socket x86 servers use point-to-point connections between the processors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. The available memo

yze specific aspects of computer systems. In previous work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> we presented micro-benchmarks to measure the characteristics
11]</ref>. Furthermore, the uncore frequency is adapted to the workload dynamically by the hardware <ref type="bibr" target="#b11">[12]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> /1.0" place="foot" n="4" xml:id="foot_2"><p>Uncore frequency scaling automatically adjusts frequency<ref type="bibr" target="#b11">[12]</ref> </p></note> 		</body> 		<back> 			<div type="references">
11]</ref>. Furthermore, the uncore frequency is adapted to the workload dynamically by the hardware <ref type="bibr" target="#b11">[12]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> /1.0" place="foot" n="4" xml:id="foot_2"><p>Uncore frequency scaling automatically adjusts frequency<ref type="bibr" target="#b11">[12]</ref> </p></note> 		</body> 		<back> 			<div type="references">
ten used to analyze the performance of parallel architectures. M?ller et al. introduce SPEC OMP2012 <ref type="bibr" target="#b8">[9]</ref>-a benchmark suite for shared memory systems. It contains 14
hes in drug discovery <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. We have shown previously that a large semantic network of d f>.</p><p>In this work, we apply a random walk-based link prediction algorithm based on Chen et al. <ref type="bibr" target="#b2">[3]</ref> to a more extensive drug-target network and evaluated its pe rug transition matrix. The calculation of each of the transition matrix in discussed in Chen et al. <ref type="bibr" target="#b2">[3]</ref>. The random walk is implemented on the heterogeneous network #b14">[16]</ref>. Because of these evidences, we here simply adopt the previously used value of 0.3 <ref type="bibr" target="#b2">[3]</ref>. Second, the robustness of ? (jumping probability) has alrea >. It has been shown that the weight parameters w d and w t are robust among the prediction results <ref type="bibr" target="#b2">[3]</ref>.</p><p>In our drug target network 684 (94%) drugs have at le arget network into account without separating protein categories, in contrast to the previous study <ref type="bibr" target="#b2">[3]</ref>. The following estimation corroborates our approach. Our dru network and (S ab equals 1 if node a and b are connected, 0 otherwise) K a denotes the degree of a. <ref type="bibr" target="#b2">(3)</ref> With the probability c, the walker goes back to a. (4) After is the first time that the random walk-based method is evaluated using a binding assay dataset (cf. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>).</p></div> <div xmlns
et="#b6">[7,</ref><ref type="bibr" target="#b12">[14]</ref><ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref>. Because of these evidences, we here simply adopt the prev ness of ? (jumping probability) has already been discussed <ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref><ref type="bibr" target="#b15">[17]</ref>. It has been show the performance of the prediction. The enrichment criterion is evaluated by enrichment factor (EF) <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17]</ref>. EF reflects the c
ic way to measure importance of nodes in a network. The most widely known is the PageRank algorithm <ref type="bibr" target="#b6">[7]</ref>. PageRank, developed for ranking web pages, measures page cl br" target="#b13">15]</ref>. It has been shown that the prediction results from RWR are also robust <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">[14]</ref><ref type="bibr" tar
des information on atom-centered fragments that is derived from the variant of the Morgan algorithm <ref type="bibr" target="#b10">[12]</ref>. ECFPs are generated using the neighborhood of each non-hy
f type="bibr" target="#b16">[19]</ref>, PDSP <ref type="bibr" target="#b17">[20]</ref>, and Pubchem <ref type="bibr" target="#b18">[21]</ref> using the binding coefficients like IC50 and Ki. Table <re
re highly insensitive to the choice of restart probability <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>. It has been shown that the prediction results from RWR are RWR are also robust <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">[14]</ref><ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref>. Because of these arget="#b2">[3]</ref>. Second, the robustness of ? (jumping probability) has already been discussed <ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref><ref type="bibr" t
twork performs surprisingly well, even without training datasets or incorporating target preference <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this work, we apply a random walk-based link predi based method is evaluated using a binding assay dataset (cf. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>).</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Ca
f type="bibr" target="#b7">[8]</ref>, ECFP6 fingerprints (extended connectivity fingerprint path 6) <ref type="bibr" target="#b8">[9]</ref>, 2D Pharmacophore fingerprints (PHFP4) [10] and ROCS program
et="#b6">[7,</ref><ref type="bibr" target="#b12">[14]</ref><ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref>. Because of these evidences, we here simply adopt the prev ness of ? (jumping probability) has already been discussed <ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref><ref type="bibr" target="#b15">[17]</ref>. It has been show the performance of the prediction. The enrichment criterion is evaluated by enrichment factor (EF) <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17]</ref>. EF reflects the c
n.com/rocs/shape_theory.html] defined by SMARTS. Conformers for the data set is created using OMEGA <ref type="bibr" target="#b11">[13]</ref>, about 250 conformers with RMSD threshold of 0.6 is genera
f type="bibr" target="#b7">[8]</ref>, ECFP6 fingerprints (extended connectivity fingerprint path 6) <ref type="bibr" target="#b8">[9]</ref>, 2D Pharmacophore fingerprints (PHFP4) [10] and ROCS program
"#b19">Suganuma et al., 2017;</ref><ref type="bibr">Zoph &amp; Le, 2017)</ref>. More recent studies <ref type="bibr" target="#b3">(Brock et al., 2018;</ref><ref type="bibr" target="#b18">Shirakawa et he most likely architecture, ? = argmax c p ? (c), from scratch, which is a commonly used technique <ref type="bibr" target="#b3">(Brock et al., 2018;</ref><ref type="bibr" target="#b9">Liu et al., 20 hree different existing approaches are reported. The 1st category is based on a meta-network. SMASH <ref type="bibr" target="#b3">(Brock et al., 2018)</ref> employs HyperNet that takes an architecture
pe="bibr" target="#b9">Liu et al., 2019;</ref><ref type="bibr" target="#b22">Xie et al., 2019;</ref><ref type="bibr" target="#b4">Cai et al., 2019)</ref>, on the other hand, optimize the weights and t nal work is followed by further improvements <ref type="bibr" target="#b22">(Xie et al., 2019;</ref><ref type="bibr" target="#b4">Cai et al., 2019)</ref>. An advantage of ASNG-NAS is that DARTS requir
g of the weights under a fixed architecture <ref type="bibr" target="#b17">(Real et al., 2017;</ref><ref type="bibr" target="#b19">Suganuma et al., 2017;</ref><ref type="bibr">Zoph &amp; Le, 2017)</re
gathering more potential applications.</p><p>Research directions of NAS fall into three categories <ref type="bibr" target="#b6">(Elsken et al., 2019)</ref>: performance estimation (how to estimate t
mization framework for our stochastic relaxation based on the so-called stochastic natural gradient <ref type="bibr" target="#b1">(Amari, 1998)</ref>. Our theoretical investigation derives a condition
adds them. Our search space includes 5 operations: identity, 3 ? 3 and 5 ? 5 separable convolutions <ref type="bibr" target="#b5">(Chollet, 2017)</ref>, and 3 ? 3 average and max poolings. The separab

" target="#b18">Shirakawa et al., 2018;</ref><ref type="bibr" target="#b16">Pham et al., 2018;</ref><ref type="bibr" target="#b9">Liu et al., 2019;</ref><ref type="bibr" target="#b22">Xie et al., 2019 scratch, which is a commonly used technique <ref type="bibr" target="#b3">(Brock et al., 2018;</ref><ref type="bibr" target="#b9">Liu et al., 2019;</ref><ref type="bibr" target="#b16">Pham et al., 201 and architectures into optimization of a differentiable objective by means of continuous relaxation <ref type="bibr" target="#b9">(Liu et al., 2019;</ref><ref type="bibr" target="#b22">Xie et al., 201 set and adopt the standard preprocessing and data augmentation as done in the previous works, e.g., <ref type="bibr" target="#b9">Liu et al. (2019)</ref>; <ref type="bibr" target="#b16">Pham et al. (2 g the architecture search, we split the training dataset into halves as D = {D x , D ? } as done in <ref type="bibr" target="#b9">Liu et al. (2019)</ref>. The gradients ( <ref type="formula" target="# each layer, along with a skip connection. The 2nd category is based on continuous relaxation. DARTS <ref type="bibr" target="#b9">(Liu et al., 2019)</ref> extends essentially categorical architecture

gathering more potential applications.</p><p>Research directions of NAS fall into three categories <ref type="bibr" target="#b6">(Elsken et al., 2019)</ref>: performance estimation (how to estimate t
tochastic relaxation (2) of function f (c) is known as the information geometric optimization (IGO) <ref type="bibr" target="#b13">(Ollivier et al., 2017)</ref> algorithm. For the case of exponential
domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> to address the lack of highquality, large- eters et al., 2018)</ref>, GPT <ref type="bibr" target="#b25">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, unsupervised pretraining of language mode s="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Background The BERT model architecture <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> is based on a multilayer bidirectional Tra ead n="3.3">Pretrained BERT Variants</head><p>BERT-Base We use the pretrained weights for BERT-Base <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> released with the original BERT code. <ref 9">(Howard and Ruder, 2018)</ref> which is equivalent to the linear warmup followed by linear decay <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>. For each dataset and BERT variant, we pic h using AllenNLP <ref type="bibr" target="#b8">(Gardner et al., 2017)</ref>.</p><p>Casing We follow <ref type="bibr" target="#b5">Devlin et al. (2019)</ref> in using the cased models for NER and the u T</head><p>We mostly follow the same architecture, optimization, and hyperparameter choices used in <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>. For text classification (i.e. CLS and REL)
="bibr" target="#b5">(Devlin et al., 2019)</ref> is based on a multilayer bidirectional Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>. Instead of the traditional left-to-righ
="bibr" target="#b5">(Devlin et al., 2019)</ref> is based on a multilayer bidirectional Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>. Instead of the traditional left-to-righ
science abstracts. ACL-ARC <ref type="bibr" target="#b13">(Jurgens et al., 2018)</ref> and SciCite <ref type="bibr" target="#b2">(Cohan et al., 2019)</ref> assign intent labels (e.g. <ref type="bibr" F1 with finetuning and +1.13 F1 without). In addition, SCIBERT achieves new SOTA results on ACL-ARC <ref type="bibr" target="#b2">(Cohan et al., 2019)</ref>, and the NER part of SciERC <ref type="bibr



ans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper <ref type="bibr" target="#b15">(Kim et al., 2011)</ref>. REL is a special case of text classificatio
er datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP <ref type="bibr" target="#b23">(Nye et al., 2018)</ref> annotates PICO spans in clinical trial abstr results on BC5CDR and ChemProt <ref type="bibr" target="#b18">(Lee et al., 2019)</ref>, and EBM-NLP <ref type="bibr" target="#b23">(Nye et al., 2018)</ref>.</p><p>SCIBERT performs slightly worse than
atch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule <ref type="bibr" target="#b9">(Howard and Ruder, 2018)</ref> which is equivalent to the linear warmu

to a conventional design due to an increase in the access time on a filter cache miss. The L-Cache <ref type="bibr" target="#b5">[6]</ref> similarly reduces switching activity by holding loop-nested
frequently disabled. In addition, frequent cache flushing can significantly impact cache miss rate <ref type="bibr" target="#b3">[4]</ref>, and considerable energy may be expended.</p><p>An alternati
ally true for the data array and for set associative caches for which the wordlines can be exceed-  <ref type="bibr" target="#b20">[21]</ref>. Our analysis using the Cacti cache cycle time model <ref
ed during high-load periods. The operating system or a continuous profiling and optimization system <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> could effectively co s profiling of the application. For example, the Digital Continuous Profiling Infrastructure (DCPI) <ref type="bibr" target="#b2">[3]</ref> periodically samples performance counters as programs normal
ling tool developed by Ammons et al. <ref type="bibr" target="#b1">[2]</ref> and Compaq's ProfileMe <ref type="bibr" target="#b9">[10]</ref>, leverage the on-chip performance counters found on modern
frequently disabled. In addition, frequent cache flushing can significantly impact cache miss rate <ref type="bibr" target="#b3">[4]</ref>, and considerable energy may be expended.</p><p>An alternati
2">3</ref>, roughly correspond to those in a current high-end microprocessor such as the HP PA-8000 <ref type="bibr" target="#b18">[19]</ref> and Alpha 21264 <ref type="bibr" target="#b16">[17]</ref>.
ing system or a continuous profiling and optimization system <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> could effectively control the PDT by changing the number of information can be later examined to identify sources of performance degradation. The Morph system <ref type="bibr" target="#b25">[26]</ref> is designed to provide automatic program optimization via
ing system or a continuous profiling and optimization system <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> could effectively control the PDT by changing the number of information can be later examined to identify sources of performance degradation. The Morph system <ref type="bibr" target="#b25">[26]</ref> is designed to provide automatic program optimization via


motivated the development and release of high performance stacked DRAM from several leading vendors <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" t tom layer of the chip has many-core compute engine and the top layer has wide I/O DRAMs (similar to <ref type="bibr" target="#b12">[13]</ref>) which are used as LLC shared by all cores. To optimize th
with SRAM caches.  <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b43">45]</ref> Area saving for integrating more cores <ref type="bibr" tar f type="bibr" target="#b52">54</ref>] Cache mapping scheme <ref type="bibr" target="#b42">[44]</ref><ref type="bibr" target="#b43">[45]</ref><ref type="bibr" target="#b44">[46]</ref> Use of additional cache management <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">[44]</ref><ref type="bibr" target="#b43">[45]</ref><ref type="bibr" target="#b44">[46]</ref>.</p><p>Sim et al. .org/ns/1.0"><head n="3.6">Addressing issues related to multicore processors</head><p>Hameed et al. <ref type="bibr" target="#b43">[45]</ref> note that application-unaware bank-mapping policies in lar eues are limitations of this technique.</p><p>The difference between the technique of Hameed et al. <ref type="bibr" target="#b43">[45]</ref> and Loh <ref type="bibr" target="#b40">[42]</ref> is that
ly stored. Due to this, soft-error rate in a 4-level PCM may be 10 6 times higher than that in DRAM <ref type="bibr" target="#b63">[65]</ref>.</p><p>Furthermore, the write endurance of NVMs is orders
get="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b56">58]</ref> Thermal management <ref type="bibr" target="#b1">[2,</ref>< get="#b37">39,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b56">58]</ref> Predictor for selecting useful/hot blocks <ref type="bibr"
han the off-chip DRAM <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">34]</ref>, for example, in Intel's Knights Landing system, the capaci br" target="#b31">[33,</ref><ref type="bibr" target="#b35">37]</ref>, while that of DRAM is 60-80ns <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b35">37]</ref>, the large differe get="#b11">12,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" tar
pe="bibr" target="#b4">5,</ref><ref type="bibr" target="#b60">62]</ref> Refresh overhead management <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b59">61]</ref> Dynamically optimi ns="http://www.tei-c.org/ns/1.0"><head n="3.8">Refresh management techniques</head><p>Jaksic et al. <ref type="bibr" target="#b57">[59]</ref> present a DRAM-based L1 and L2 coherent cache design. To r has recently seen a read/write access.</p><p>The difference between the technique of Jaksic et al. <ref type="bibr" target="#b57">[59]</ref> and Ghosh et al. <ref type="bibr" target="#b59">[61]</ref>
b_0">1</ref>). Since the latency of SRAM is less than 10ns <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b35">37]</ref>, while that of DRAM is 60-80ns <ref type="bibr" target="#b3 br" target="#b35">37]</ref>, while that of DRAM is 60-80ns <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b35">37]</ref>, the large difference between the latency of SRAM and DRAM such caches. Conventional reconfiguration approaches (e.g. <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b35">37]</ref>) remap the data stored in sets of both powered-on and power data already mapped to the powered-on banks remain where they were. While previous techniques e.g. <ref type="bibr" target="#b35">[37]</ref> allow reconfiguring the cache to power-of-two setcounts on computing which seeks 10 18 operations/second computation capability within a power budget of 20MW <ref type="bibr" target="#b35">[37]</ref>. To accomplish this goal, solutions spanning across the wh
="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> and as such, stac arget="#b2">(3,</ref><ref type="bibr" target="#b7">8)</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b15">16)</ref>. Thus, the set-associativity (=X+Y) can vary dynamically on
get="#b29">31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" tar 1,</ref><ref type="bibr" target="#b51">53]</ref> Prefetching <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" tar s="http://www.tei-c.org/ns/1.0"><head n="3.2">Techniques for managing metadata</head><p>Zhao et al. <ref type="bibr" target="#b37">[39]</ref> evaluate latency/area/bandwidth tradeoffs of multiple DRAM
que RAM) and ReRAM (resistive RAM) have also been considered for designing last level caches (LLCs) <ref type="bibr" target="#b18">[20]</ref>, they have yet to reach the level of fabrication maturity r, however, their write latency and energy are significantly higher than that of SRAM and even DRAM <ref type="bibr" target="#b18">[20]</ref>. The density of STT-RAM is close to DRAM and is 4X compare For STT-RAM, this value is expected to be 10 15 , although best results so far show only 4 ? 10 12 <ref type="bibr" target="#b18">[20]</ref>. This not only limits the lifetime of NVM devices, but als e), whereas, DRAM has 1T1C (one transistor and one capacitor) structure with a cell size of 6-10F 2 <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b21">23]</ref>. Clearly, DRAM pro he write endurance of DRAM is 10 16 , this value for PCM and ReRAM is 10 8 and 10 11 , respectively <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" ta
he mapping scheme <ref type="bibr" target="#b42">[44]</ref><ref type="bibr" target="#b43">[45]</ref><ref type="bibr" target="#b44">[46]</ref> Use of additional structures/approaches Hit/miss or locati ="#b10">[11,</ref><ref type="bibr" target="#b42">[44]</ref><ref type="bibr" target="#b43">[45]</ref><ref type="bibr" target="#b44">[46]</ref>.</p><p>Sim et al. <ref type="bibr" target="#b33">[35]</ref d the tag read is overlapped with data read to avoid tag serialization latency.</p><p>Hameed et al. <ref type="bibr" target="#b44">[46]</ref> propose a DRAM cache architecture where each DRAM row comp
. This observation has led to a flurry of ASIC proposals for DNN accelerators over the recent years <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" targe 3]</ref> have proposed meshes due to their flexibility to support all-to-all communication. Diannao <ref type="bibr" target="#b2">[3]</ref> and Shidiannao <ref type="bibr" target="#b43">[44]</ref> rel accelerator domain with an example of convolution accelerator for image processing domain. Diannao <ref type="bibr" target="#b2">[3]</ref>, DaDiannao <ref type="bibr" target="#b14">[15]</ref>, ShiDia ary trees are well-suited for performing reductions and have been used in prior DNN implementations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target
This is the approach most of the early DNN accelerators took <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. However, recently t almost every DNN accelerator has used a hierarchy of buses <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref> and/or trees <ref t
calculation. Each dataflow is essentially some kind of transformation of this multidimensional loop <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. We propose to des arget="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> to implement adder trees within the PE clusters described i <ref type="bibr" target="#b43">[44]</ref> are early spatial DNN accelerators. A recent work in FPGA <ref type="bibr" target="#b20">[21]</ref> proposed constraint aware optimization tool for FPGA based
rkable degree of accuracy in recognition and classification of images, exceeding human capabilities <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>. Each convolution
assic topology for providing non-blocking bandwidth, and is used extensively in datacenter networks <ref type="bibr" target="#b37">[38]</ref>. However, such a topology is infeasible to build on-chip s
>[16]</ref>, kernels <ref type="bibr" target="#b16">[17]</ref> and hybrid (kernel, channel, output) <ref type="bibr" target="#b17">[18]</ref>. Among these, for e.g., Fused CNN <ref type="bibr" target= across DNN accelerator design.</p><p>DNN accelerators for flexible dataflows: FlexFlow [34] and DNA <ref type="bibr" target="#b17">[18]</ref> are two recent DNN ASIC proposals with similar motivation
us convolution layer) and produces output activations by sliding a set of filters over input images <ref type="bibr" target="#b24">[25]</ref>. Convolutions account for almost 90% of the computations i . Maeri tries to optimize and get the best of the three kinds of dataflows described in the Eyeriss <ref type="bibr" target="#b24">[25]</ref> taxonomy. Each multiplier switch acts as a weight stationa to read different data every cycle to each row and column, which results in high energy consumption <ref type="bibr" target="#b24">[25]</ref>. Therefore, the systolic array need to read 1,323 times fr gn used simple adder trees within their computation engine, which can benefit from our ART. Eyeriss <ref type="bibr" target="#b24">[25]</ref> analyzed data flow patterns in existing CNN accelerators a /label><figDesc>Figure 12. Total latency and compute unit utilization of systolic array(SA), Eyeriss<ref type="bibr" target="#b24">[25]</ref> style row-stationary accelerator, and Maeri with 64 PEs (m
of LSTMs on FPGAs <ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" t
arget="#b57">[58]</ref> which has been implemented on FPGA <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>. An ASIC implementation for accelerating the control of RNN
="#b56">[57]</ref> has been explored. Maeda et al. suggested new learning algorithm of Hopfield RNN <ref type="bibr" target="#b57">[58]</ref> which has been implemented on FPGA <ref type="bibr" target
s the idea of sparsity <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> and has been popula target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> to implement adder a recent accelerator for sparse CNNs, that leverages sparsity in activations and weights. Cnvlutin <ref type="bibr" target="#b18">[19]</ref> compresses activation values based on the ReLU operator. C
to support all-to-all communication. Diannao <ref type="bibr" target="#b2">[3]</ref> and Shidiannao <ref type="bibr" target="#b43">[44]</ref> relied on mesh-based interconnects for data transfer. Howe type="bibr" target="#b2">[3]</ref>, DaDiannao <ref type="bibr" target="#b14">[15]</ref>, ShiDiannao <ref type="bibr" target="#b43">[44]</ref> are early spatial DNN accelerators. A recent work in FPGA
>[16]</ref>, kernels <ref type="bibr" target="#b16">[17]</ref> and hybrid (kernel, channel, output) <ref type="bibr" target="#b17">[18]</ref>. Among these, for e.g., Fused CNN <ref type="bibr" target= across DNN accelerator design.</p><p>DNN accelerators for flexible dataflows: FlexFlow [34] and DNA <ref type="bibr" target="#b17">[18]</ref> are two recent DNN ASIC proposals with similar motivation
="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref> has been explored. Maeda et al. suggested new learning alg
t explore additional data reuse opportunities by changing the partition have emerged -across layers <ref type="bibr" target="#b15">[16]</ref>, kernels <ref type="bibr" target="#b16">[17]</ref> and hyb ernel, channel, output) <ref type="bibr" target="#b17">[18]</ref>. Among these, for e.g., Fused CNN <ref type="bibr" target="#b15">[16]</ref> retains intermediate outputs of a layer and calculate the ach partition, as Figure <ref type="figure" target="#fig_0">2</ref> shows. For example, Fused-layer <ref type="bibr" target="#b15">[16]</ref> modified the computation order (loop order) in CNNs to max <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref>. Fused-layer CNN <ref type="bibr" target="#b15">[16]</ref> and others <ref type="bibr" target="#b34">[35,</ref><ref t iven optimization goal <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref>. This makes the har
us convolution layer) and produces output activations by sliding a set of filters over input images <ref type="bibr" target="#b24">[25]</ref>. Convolutions account for almost 90% of the computations i . Maeri tries to optimize and get the best of the three kinds of dataflows described in the Eyeriss <ref type="bibr" target="#b24">[25]</ref> taxonomy. Each multiplier switch acts as a weight stationa to read different data every cycle to each row and column, which results in high energy consumption <ref type="bibr" target="#b24">[25]</ref>. Therefore, the systolic array need to read 1,323 times fr gn used simple adder trees within their computation engine, which can benefit from our ART. Eyeriss <ref type="bibr" target="#b24">[25]</ref> analyzed data flow patterns in existing CNN accelerators a /label><figDesc>Figure 12. Total latency and compute unit utilization of systolic array(SA), Eyeriss<ref type="bibr" target="#b24">[25]</ref> style row-stationary accelerator, and Maeri with 64 PEs (m
rkable degree of accuracy in recognition and classification of images, exceeding human capabilities <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>. Each convolution
lgorithm of RNNs has been discussed for more than 20 years <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" t
us convolution layer) and produces output activations by sliding a set of filters over input images <ref type="bibr" target="#b24">[25]</ref>. Convolutions account for almost 90% of the computations i . Maeri tries to optimize and get the best of the three kinds of dataflows described in the Eyeriss <ref type="bibr" target="#b24">[25]</ref> taxonomy. Each multiplier switch acts as a weight stationa to read different data every cycle to each row and column, which results in high energy consumption <ref type="bibr" target="#b24">[25]</ref>. Therefore, the systolic array need to read 1,323 times fr gn used simple adder trees within their computation engine, which can benefit from our ART. Eyeriss <ref type="bibr" target="#b24">[25]</ref> analyzed data flow patterns in existing CNN accelerators a /label><figDesc>Figure 12. Total latency and compute unit utilization of systolic array(SA), Eyeriss<ref type="bibr" target="#b24">[25]</ref> style row-stationary accelerator, and Maeri with 64 PEs (m
a hierarchy of buses <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref> and/or trees <ref type="bibr" target="#b33">[34,</ref><ref </ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref> and/or trees <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. For instance, Eye
="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref> has been explored
<p>NoCs for DNN accelerators: Most of NoC studies for DNNs <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" t
r, a pointwise non-linearity, and either an invariant or equivariant linear output layer. Recently, <ref type="bibr" target="#b19">Maron et al. (2019b)</ref> showed that by allowing higherorder tensor ="bibr" target="#b13">Hornik et al., 1989;</ref><ref type="bibr" target="#b20">Pinkus, 1999)</ref>. <ref type="bibr" target="#b19">Maron et al. (2019b)</ref> recently proved that certain invariant GNN aces of continuous invariant (resp. equivariant) functions.</p><p>2 The case of invariant functions <ref type="bibr" target="#b19">Maron et al. (2019b)</ref> recently proved that invariant GNNs simila n the whole set G inv. , that is, for all numbers of nodes n n max simultaneously. On the contrary, <ref type="bibr" target="#b19">Maron et al. (2019b)</ref> work with a fixed n, and it does not seem on the order of tensorization k s . Indeed, through Noether's theorem on polynomials, the proof of <ref type="bibr" target="#b19">Maron et al. (2019b)</ref> shows that k s n d (n d − 1)/2 is sufficie previous invariant case could be easily extended to invariance to subgroups of O n , as is done by <ref type="bibr" target="#b19">Maron et al. (2019b)</ref>, for the equivariant case our theorem only ions in the rest of the introduction, in Section 2 we provide an alternative proof of the result of <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref> for invariant GNNs (Theorem 1), which wil t. Theorem 1. For any ρ ∈ F MLP , N inv. (ρ) is dense in C(G inv. , d edit ).</p><p>Comparison with <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <re th <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref>. The two proofs are however different: th See the next subsection for details.</p><p>One improvement of our result with respect to the one of <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref> is that it can handle graphs of varying s bibr" target="#b0">(Battaglia et al., 2016)</ref>. Another outstanding open question, formulated in <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref>, is the characterization of the approxima ing a single hidden layer of such equivariant operators followed by an invariant layer is proved in <ref type="bibr" target="#b19">(Maron et al., 2019b</ref>) (see also <ref type="bibr">(Kondor et al.
ivariant functions (Theorem 4), obtained with a non-trivial adaptation of the constructive proof by <ref type="bibr" target="#b2">Brosowski and Deutsch (1981)</ref>.</p><p>Like the invariant case, our x B.</p><p>Our proof is inspired by the one for the classical Stone-Weierstrass theorem (Thm. 2) of <ref type="bibr" target="#b2">Brosowski and Deutsch (1981)</ref>. Let us first give a bit of intuiti
ake use of non-linear functions of the adjacency matrix, for instance using spectral decompositions <ref type="bibr" target="#b3">(Bruna et al., 2014)</ref> or polynomials <ref type="bibr" target="#b8
"#b22">(Ravanbakhsh et al., 2017;</ref><ref type="bibr" target="#b10">Gens and Domingos, 2014;</ref><ref type="bibr" target="#b5">Cohen and Welling, 2016)</ref> for finite groups and <ref type="bibr" ariance to translation and its relation to convolutions <ref type="bibr">(Kondor et al., 2018;</ref><ref type="bibr" target="#b5">Cohen and Welling, 2016)</ref>, which are ubiquitous in image processi
<ref type="bibr" target="#b32">(Ying et al., 2018)</ref>, interaction networks of physical systems <ref type="bibr" target="#b0">(Battaglia et al., 2016)</ref>, state prediction <ref type="bibr" targ e graph-valued, which are useful for instance to model dynamic graphs using recurrent architectures <ref type="bibr" target="#b0">(Battaglia et al., 2016)</ref>. Another outstanding open question, for
use of fully general versions of Stone-Weierstrass theorem, for which some questions are still open <ref type="bibr" target="#b11">(Glimm, 1960)</ref>. Hence we prove a new, specialized Stone-Weierstr
ake use of non-linear functions of the adjacency matrix, for instance using spectral decompositions <ref type="bibr" target="#b3">(Bruna et al., 2014)</ref> or polynomials <ref type="bibr" target="#b8
e="bibr" target="#b6">(Cybenko, 1989;</ref><ref type="bibr" target="#b13">Hornik et al., 1989;</ref><ref type="bibr" target="#b20">Pinkus, 1999)</ref>. <ref type="bibr" target="#b19">Maron et al. (201 note their set F MLP . This includes in particular any continuous function that is not a polynomial <ref type="bibr" target="#b20">(Pinkus, 1999)</ref>. Among these, we denote the sigmoid ρ sig (x) =
(Maron et al., 2019a)</ref>. This generalizes several previous constructions, for instance for sets <ref type="bibr" target="#b33">(Zaheer et al., 2017;</ref><ref type="bibr" target="#b12">Hartford et ing their proof to high-order input tensors like graphs. Indeed, it strongly relies on Theorem 2 of <ref type="bibr" target="#b33">(Zaheer et al., 2017)</ref> that characterizes invariant functions R
et al., 2017)</ref>, among many others. See <ref type="bibr" target="#b34">(Zhou et al., 2018;</ref><ref type="bibr" target="#b1">Bronstein et al., 2017)</ref> for thorough reviews. It is therefore of 14)</ref> or polynomials <ref type="bibr" target="#b8">(Defferrard et al., 2016)</ref>. We refer to <ref type="bibr" target="#b1">(Bronstein et al., 2017;</ref><ref type="bibr" target="#b30">Xu et al.
nly translations) requires the use of a small sub-space of linear operators, which is identified in <ref type="bibr" target="#b18">(Maron et al., 2019a)</ref>. This generalizes several previous constr rsality. A benchmarking of deep GNNs that use invariant and equivariant linear operators is done in <ref type="bibr" target="#b18">(Maron et al., 2019a)</ref>.</p></div> <div xmlns="http://www.tei-c.o es between linear operators that are invariant or equivariant to permutations, and non-linearities. <ref type="bibr" target="#b18">Maron et al. (2019a)</ref> elegantly characterize all such linear fun s terms B s ∈ R n ks are equivariant, so that B s = σ B s for all σ. They are also characterized by <ref type="bibr" target="#b18">Maron et al. (2019a)</ref> and belong to a linear space of dimension s simple architecture in Fig. <ref type="figure">1</ref>.</p><p>In light of the characterization by <ref type="bibr" target="#b18">Maron et al. (2019a)</ref> of linear invariant and equivariant operat
ulers still difficult to implement.</p><p>Academic researchers have proposed dataflow prescheduling <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target so been academic work in attempting to reduce use of the picker. Select-free instruction scheduling <ref type="bibr" target="#b5">[5]</ref> and grandchild scheduling <ref type="bibr" target="#b30">[30
ready instructions for execution, forms a tight loop which is well known as critical to performance <ref type="bibr" target="#b2">[2]</ref>.</p><p>There are two conventional methods to implement the m
metadata storage. An interesting approach applicable only to CAMbased schedulers is tag elimination <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16]</ref>. These researchers
get="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref>, though the most common in industry is by execution resourc
that age is not the optimal conflict-resolution heuristic <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b31">31]</ref>. Criticality predic
cting scheduler, disclosed by the DEC Alpha 21264 architects <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b19">19]</ref>, which provides an interesting alternative to expansive pic
that age is not the optimal conflict-resolution heuristic <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b31">31]</ref>. Criticality predic
ch applicable only to CAMbased schedulers is tag elimination <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16]</ref>. These researchers  observe that the worst-case design of t
-way partitioned entries. Multilevel partitioning techniques <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b18">18]</ref> are a related approach which resemble cache hierarchies by

different heuristics <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" tar
><ref type="bibr" target="#b5">5]</ref>,and trusted measurement is a key problem of this technology <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref>. Trusted computing tre
<ref type="bibr" target="#b10">[10]</ref> and Control Flow Checking by Software Signatures (CFCSS) <ref type="bibr" target="#b11">[11]</ref> and On-line Control Flow Error Detection Using Relationshi is a software-only solution and does not require any special hardware. This solution is inspired by <ref type="bibr" target="#b11">[11]</ref> and <ref type="bibr" target="#b12">[12,</ref><ref type="bi sions for each benchmark:</p><p>The original code.</p><p>A safe one, obtained by applying the CFCSS <ref type="bibr" target="#b11">[11]</ref> technique to the original code.</p><p>A safe one, obtained

11">[11]</ref> and <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref> and absorbs their advantages. CFCBS embed measurement instr ranode CFEs occur if the program control before and after the illegal jump resides in the same node <ref type="bibr" target="#b14">[14]</ref>.</p><p>Considering a Program Flow Graph { P ,V}, we define not constant and is determined by the length of the first two fields and consists of a series of 0s <ref type="bibr" target="#b14">[14]</ref>.(see Fig. <ref type="figure" target="#fig_1">1</ref>).</p>
are control flow checking is to partition the program into basic blocks (branch-free parts of code) <ref type="bibr" target="#b9">[9]</ref>. For each block a deterministic signature is calculated and f> and On-line Control Flow Error Detection Using Relationship Signatures Among Basic Blocks(RSCFC) <ref type="bibr" target="#b9">[9]</ref>. ECCA, firstly, assigns a unique prime number identifier (BI es, depending on the number of checks inserted and the aliasing allowed in the particular technique <ref type="bibr" target="#b9">[9]</ref>. Fundamentally, there are three critical aspects of any sign ="#b11">[11]</ref> technique to the original code.</p><p>A safe one, obtained by applying the RSCFC <ref type="bibr" target="#b9">[9]</ref> technique to the original code.</p><p>A safe one, obtained b
ernet, increasingly demand for secure communication and secure operation due to rising online fraud <ref type="bibr" target="#b0">[1]</ref> and software attacks <ref type="bibr" target="#b2">[2]</ref>
r cycles to run between the terminal of the illegal branching and the line detecting the error here <ref type="bibr" target="#b15">[15]</ref>. In CFCSS and CFCBS, the checking instruction is located a
attestation. The cores of trusted computing technology are trusted computing base and trusted chain <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>,and trusted measuremen
attestation. The cores of trusted computing technology are trusted computing base and trusted chain <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>,and trusted measuremen
inspired by <ref type="bibr" target="#b11">[11]</ref> and <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref> and absorbs their a ks, is chosen depending on the outcome of the control instructions that are encountered at run-time <ref type="bibr" target="#b13">[13]</ref>.We called the sequence of basic blocks which will be execu
ernet, increasingly demand for secure communication and secure operation due to rising online fraud <ref type="bibr" target="#b0">[1]</ref> and software attacks <ref type="bibr" target="#b2">[2]</ref>
ng and testing phases. Recent studies have investigated the significant redundancy in deep networks <ref type="bibr" target="#b5">[6]</ref> and reduced the number of neurons and filters <ref type="bib recent interest in reducing the redundancy of deep CNNs to achieve acceleration and compression. In <ref type="bibr" target="#b5">[6]</ref> the redundancy in the parameterization of deep learning mode >, the redundancy in the parameterization of deep learning models has been studied and demonstrated <ref type="bibr" target="#b5">[6]</ref>. We present NISP to efficiently propagate the importance sco
get="#b40">41,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" tar
the exact solution is very hard to obtain given the complexity of nonlinearity. Some previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target
get="#b47">48,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" tar
n, and Huffman coding. Sparsity regularization terms have been use to learn sparse CNN structure in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" ta
neurons and filters <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref> by pruning the unim ters consider only the statistics of one layer (e.g., prune neurons with small magnitude of weights <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref>), or two consecuti " neurons layer-by-layer either independently <ref type="bibr" target="#b13">[14]</ref> or greedily <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>, without consideri tance is an open problem. Besides using feature ranking to measure neuron importance, other methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" ta p-1 accuracy loss, which shows superior performance when compared with the state-of-the-art methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div> <div xm 56, we achieve a 43.61% FLOP reduction, significantly higher than the 27.60% reduction by Li et al. <ref type="bibr" target="#b24">[25]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head ased on the Lipschitz continuity of a neural network.</p><p>Most similar to our approach, Li et al. <ref type="bibr" target="#b24">[25]</ref> pruned filters by their weight magnitude. Luo et al. <ref reduction (58.34% vs. 51.50%) Using ResNet on Cifar10 dataset, with top-1 accuracy loss similar to <ref type="bibr" target="#b24">[25]</ref> (56-A, 56-B. 110-A and 110-B), our method reduces more FLO et="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n ghts as described in Algorithm 1.</p><p>The propagation matrices used in algorithm 1 are defined in <ref type="bibr" target="#b24">(25)</ref> and ( <ref type="formula" target="#formula_32">26</ref>)</
.g., our approach loses 1.43% accuracy on Alexnet and reduces FLOPs by 67.85% while Figurnov et al. <ref type="bibr" target="#b10">[11]</ref> loses more (2%) and reduces FLOPs less (50%). With almost a scaling factor in the training process and facilitated one channel-level pruning. Figurnov et al. <ref type="bibr" target="#b10">[11]</ref> speeded up the convolutional layers by skipping operations multiplication and the number of parameters following <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b10">[11]</ref>, and denote them as [FLOPs?%] and [Params.?%] in the table and comparing the other.</p><p>On AlexNet, by achieving smaller accuracy loss (1.43% ours vs. 2.00% <ref type="bibr" target="#b10">[11]</ref>), our method NISP-A manages to reduce significantly more F >[11]</ref>), our method NISP-A manages to reduce significantly more FLOPs (67.85%) than the one in <ref type="bibr" target="#b10">[11]</ref> (50%), denoted as "Perforate" in the table; compare to the on, respectively. Finally, we benchmark the pruning results and compare to existing methods such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" ta
ation terms have been use to learn sparse CNN structure in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref>. Miao et al. <ref t
agation. Other studies focused on fixed point computation rather than exploiting the CNN redundancy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref>. Another work studie
y. Some previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> approximate it using 2nd-order Taylor expansion. Our work is
the exact solution is very hard to obtain given the complexity of nonlinearity. Some previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target
er could generate adversarial examples to fool classifiers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" targe sible to generate adversarial examples to fool classifiers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" targe ier with adversarial examples, called adversarial training <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5]</ref>; <ref type="bibr" target="#b1">(2)</ref> Training a classifie from normal examples <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b4">5]</ref>. Moosavi et al. showed that it was even possible to find one "2.3.1">Fast gradient sign method(FGSM). Given a normal image</head><p>x, fast gradient sign method <ref type="bibr" target="#b4">[5]</ref> looks for a similar image x ′ in the L ∞ neighborhood of x t "#b21">22]</ref>, or mix the adversarial objective with the classification objective as regularizer <ref type="bibr" target="#b4">[5]</ref>. Though this idea is promising, it is hard to reason about w
er, recent research showed that an attacker could generate adversarial examples to fool classifiers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" targ b0">(1)</ref> Training the target classifier with adversarial examples, called adversarial training <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5]</ref>; <ref type="bibr" ta cally, researchers showed that it was possible to generate adversarial examples to fool classifiers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" targ one may use a mixture of normal and adversarial examples in the training set for data augmentation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22]</ref>, or mix the advers ="2.3">Existing attacks</head><p>Since the discovery of adversarial examples for neural networks in <ref type="bibr" target="#b33">[34]</ref>, researchers have found adversarial examples on various ne h leveraged gradient based optimization from normal examples <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b4">5]</ref>. Moosavi et al. showe
uch as image classification <ref type="bibr" target="#b8">[9]</ref> and natural language processing <ref type="bibr" target="#b15">[16]</ref>. However, recent research showed that an attacker could ge
operties of the data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. We train an autoencoder with adequate normal examples for
uch as image classification <ref type="bibr" target="#b8">[9]</ref> and natural language processing <ref type="bibr" target="#b15">[16]</ref>. However, recent research showed that an attacker could ge
duce regularization to uncover useful properties of the data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. We train an autoen
Making target classifiers hard to attack by blocking gradient pathway, e.g., defensive distillation <ref type="bibr" target="#b24">[25]</ref>.</p><p>However, all these approaches have limitations. Bot therefore faster and more flexible. MagNet may benefit from a robust target classifier (section 5). <ref type="bibr" target="#b24">[25]</ref> trains the classifier in a certain way such that it is nea ows all the parameters of the classifier, it would be difficult for her to find adversarial example <ref type="bibr" target="#b24">[25]</ref>. However, <ref type="bibr" target="#b1">[2]</ref> showed t </ref> showed that it was actually easy to find adversarial examples for the classifier hardened in <ref type="bibr" target="#b24">[25]</ref>. We do not know how to find such robust classifiers, or ev rlini's attack is the most interesting because it is the most effective on the distillation defense <ref type="bibr" target="#b24">[25]</ref> and there is no known effective defense prior to our work.
uch as image classification <ref type="bibr" target="#b8">[9]</ref> and natural language processing <ref type="bibr" target="#b15">[16]</ref>. However, recent research showed that an attacker could ge
systems with carefully crafted input for real-world systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8]</ref>. More specifically, r
Making target classifiers hard to attack by blocking gradient pathway, e.g., defensive distillation <ref type="bibr" target="#b24">[25]</ref>.</p><p>However, all these approaches have limitations. Bot therefore faster and more flexible. MagNet may benefit from a robust target classifier (section 5). <ref type="bibr" target="#b24">[25]</ref> trains the classifier in a certain way such that it is nea ows all the parameters of the classifier, it would be difficult for her to find adversarial example <ref type="bibr" target="#b24">[25]</ref>. However, <ref type="bibr" target="#b1">[2]</ref> showed t </ref> showed that it was actually easy to find adversarial examples for the classifier hardened in <ref type="bibr" target="#b24">[25]</ref>. We do not know how to find such robust classifiers, or ev rlini's attack is the most interesting because it is the most effective on the distillation defense <ref type="bibr" target="#b24">[25]</ref> and there is no known effective defense prior to our work.
ears, deep learning demonstrated impressive performance on many tasks, such as image classification <ref type="bibr" target="#b8">[9]</ref> and natural language processing <ref type="bibr" target="#b1
b31">[32]</ref>, medical treatments <ref type="bibr" target="#b30">[31]</ref>, information security <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>, and human-compute
to fool classifiers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>. Their algorithms p to fool classifiers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>. Their algorithms p
target="#b1">(2)</ref> Training a classifier to distinguish between normal and adversarial examples <ref type="bibr" target="#b19">[20]</ref>; and (3) Making target classifiers hard to attack by block ed statistical features <ref type="bibr" target="#b6">[7]</ref> or separate classification networks <ref type="bibr" target="#b19">[20]</ref>. An representative work of this idea is <ref type="bibr" t fication networks <ref type="bibr" target="#b19">[20]</ref>. An representative work of this idea is <ref type="bibr" target="#b19">[20]</ref>. For each attack generating method considered, it construc approach, a recent work trained a classifier to distinguish between normal and adversarial examples <ref type="bibr" target="#b19">[20]</ref>. However, it has the fundamental limitation that it requir efore, it unlikely generalizes to other processes for generating adversarial examples. For example, <ref type="bibr" target="#b19">[20]</ref> used a basic iterative attack based on the L 2 norm. Its r
systems with carefully crafted input for real-world systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8]</ref>. More specifically, r
rget="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>. Their algorithms perturbed benign examples, which were cor rget="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>. Their algorithms perturbed normal examples by a small volu ck is easier to succeed, results in smaller perturbations, and transfers better to different models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. Since untargeted at
operties of the data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. We train an autoencoder with adequate normal examples for
target="#b1">(2)</ref> Training a classifier to distinguish between normal and adversarial examples <ref type="bibr" target="#b19">[20]</ref>; and (3) Making target classifiers hard to attack by block ed statistical features <ref type="bibr" target="#b6">[7]</ref> or separate classification networks <ref type="bibr" target="#b19">[20]</ref>. An representative work of this idea is <ref type="bibr" t fication networks <ref type="bibr" target="#b19">[20]</ref>. An representative work of this idea is <ref type="bibr" target="#b19">[20]</ref>. For each attack generating method considered, it construc approach, a recent work trained a classifier to distinguish between normal and adversarial examples <ref type="bibr" target="#b19">[20]</ref>. However, it has the fundamental limitation that it requir efore, it unlikely generalizes to other processes for generating adversarial examples. For example, <ref type="bibr" target="#b19">[20]</ref> used a basic iterative attack based on the L 2 norm. Its r
target="#b1">(2)</ref> Training a classifier to distinguish between normal and adversarial examples <ref type="bibr" target="#b19">[20]</ref>; and (3) Making target classifiers hard to attack by block ed statistical features <ref type="bibr" target="#b6">[7]</ref> or separate classification networks <ref type="bibr" target="#b19">[20]</ref>. An representative work of this idea is <ref type="bibr" t fication networks <ref type="bibr" target="#b19">[20]</ref>. An representative work of this idea is <ref type="bibr" target="#b19">[20]</ref>. For each attack generating method considered, it construc approach, a recent work trained a classifier to distinguish between normal and adversarial examples <ref type="bibr" target="#b19">[20]</ref>. However, it has the fundamental limitation that it requir efore, it unlikely generalizes to other processes for generating adversarial examples. For example, <ref type="bibr" target="#b19">[20]</ref> used a basic iterative attack based on the L 2 norm. Its r
duce regularization to uncover useful properties of the data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. We train an autoen
rget="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>. Their algorithms perturbed benign examples, which were cor rget="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>. Their algorithms perturbed normal examples by a small volu ck is easier to succeed, results in smaller perturbations, and transfers better to different models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. Since untargeted at
uch as image classification <ref type="bibr" target="#b8">[9]</ref> and natural language processing <ref type="bibr" target="#b15">[16]</ref>. However, recent research showed that an attacker could ge
ut regard to their distance in the input or output sequences <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. In all but a few cases <ref type="bibr" target="#b21">[22]
ll describe the Transformer, motivate self-attention and discuss its advantages over models such as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> and <ref type="bib
<p>We employ three types of regularization during training:</p><p>Residual Dropout We apply dropout <ref type="bibr" target="#b26">[27]</ref> to the output of each sub-layer, before it is added to the
div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optimizer</head><p>We used the Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with β 1 = 0.9, β 2 = 0.98 and = 10 −9 . We varied the lea
i-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks, long short-term memory <ref type="bibr" target="#b11">[12]</ref> and gated recurrent <ref type="bibr" target="#b6">[7]</ref
i-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks, long short-term memory <ref type="bibr" target="#b11">[12]</ref> and gated recurrent <ref type="bibr" target="#b6">[7]</ref
d have been shown to perform well on simple-language question answering and language modeling tasks <ref type="bibr" target="#b27">[28]</ref>.</p><p>To the best of our knowledge, however, the Transfor
es in sequence modeling and transduction problems such as language modeling and machine translation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" targ oder-decoder structure <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29]</ref>. Here, the encoder maps an input sequence of symbol represe
es in sequence modeling and transduction problems such as language modeling and machine translation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" targ oder-decoder structure <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29]</ref>. Here, the encoder maps an input sequence of symbol represe
ll describe the Transformer, motivate self-attention and discuss its advantages over models such as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> and <ref type="bib
type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. In all but a few cases <ref type="bibr" target="#b21">[22]</ref>, however, such attention mechanisms are used in conjunctio lment and learning task-independent sentence representations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar
rget="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref> or designing specific CNN-RNN networks <ref type="bibr" target="#b4">[5]</ref>. Such classifier that takes video sequences as input is beco erm Memory (LSTM) recurrent neural network is widely adopted <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref>. LSTM has memory abilit ion recognition using CNN or RNN structures in recent papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref>. Such deep networks r 18">19]</ref>. Such deep networks reach top competitive results in the history of EmotiW challenges <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref>. Therefore we take s ork shows that either CNN-RNN or C3D model alone can achieve good performance in action recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target
atures with a linear classifier can achieve good performance on different video analysis benchmarks <ref type="bibr" target="#b5">[6]</ref>.</p><p>As for task of video-based emotion recognition, few w 3D ConvNets, the operations are performed spatio-temporally by adding an additional time dimension <ref type="bibr" target="#b5">[6]</ref>. Hence such C3D networks preserve the temporal information o , and 2 fully connected layers, followed by a softmax output layer. Other parameters are similar to <ref type="bibr" target="#b5">[6]</ref>. The specific C3D structure used in our implementation is sh del alone can achieve good performance in action recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>. And we found that the
tures in recent papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref>. Such deep networks reach top competitive results in the hi riginally used for a face verification task. We also evaluate DCN network architecture described in <ref type="bibr" target="#b18">[19]</ref> without any pre-training. The faces are resized to a fixed
rea. Previous winners usually focus on facial graph analysis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref> or designing specific CNN-RNN networks <ref type="bibr" tar
wn to be efficient to deal with various sequences versus sequences problems, such as audio analysis <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, video captioning

b11">[12]</ref>, which perform well for general objects recognition. Deep-face VGG model (VGG-FACE) <ref type="bibr" target="#b13">[14]</ref> is tested for comparisons with VGG model trained on ImageN
b11">[12]</ref>, which perform well for general objects recognition. Deep-face VGG model (VGG-FACE) <ref type="bibr" target="#b13">[14]</ref> is tested for comparisons with VGG model trained on ImageN
equences versus sequences problems, such as audio analysis <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, video captioning <ref type="bibr" target="#b23">[24,</ref>
<ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, video captioning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, video action reco
equences versus sequences problems, such as audio analysis <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, video captioning <ref type="bibr" target="#b23">[24,</ref>
also trained a SVM with the linear kernel using audio features extracted with the OpenSmile toolkit <ref type="bibr" target="#b6">[7]</ref>.</p><p>The CNN-RNN, C3D and audio SVM model were trained sep
<ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, video captioning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, video action reco
rea. Previous winners usually focus on facial graph analysis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref> or designing specific CNN-RNN networks <ref type="bibr" tar
equences versus sequences problems, such as audio analysis <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, video captioning <ref type="bibr" target="#b23">[24,</ref>
work is widely adopted <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref>. LSTM has memory ability and suits for processing sequences w in action recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>. And we found that the CNN-RNN and C3D hybrid network can fur ember or forget the value, and when the unit should output the value. We use LSTM unit described in <ref type="bibr" target="#b7">[8]</ref> in our method (Figure <ref type="figure" target="#fig_3">2</ nd  is the hyperbolic tangent function.</p><p>We use a similar framework with the one described in <ref type="bibr" target="#b7">[8]</ref>, which combines LSTMs with deep convolutional networks to tr
m is applied to preprocess video frames. Faces of video frames are detected by Viola-Jones cascades <ref type="bibr" target="#b27">[28]</ref> and falsely detected non-face frames are filtered out by a
dden state.</p><p>Although RNNs have been widely used in many tasks such as handwriting recognition <ref type="bibr" target="#b16">[17]</ref> or speech recognition <ref type="bibr" target="#b17">[18]<
models, several mainstream CNN architectures trained with ImageNet are examined, including CaffeNet <ref type="bibr" target="#b9">[10]</ref>, GoogLeNet <ref type="bibr" target="#b10">[11]</ref>, VGG <
l models on optical flow images and perform late fusion akin to the two-stream hypothesis following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The performance o ligned faces as inputs, give a competitive result. Also, optical flow of faces are tested following <ref type="bibr" target="#b20">[21]</ref>, but no improvements are obtained for emotion classificati
rea. Previous winners usually focus on facial graph analysis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref> or designing specific CNN-RNN networks <ref type="bibr" tar
l models on optical flow images and perform late fusion akin to the two-stream hypothesis following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The performance o ligned faces as inputs, give a competitive result. Also, optical flow of faces are tested following <ref type="bibr" target="#b20">[21]</ref>, but no improvements are obtained for emotion classificati
wn to be efficient to deal with various sequences versus sequences problems, such as audio analysis <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, video captioning

l models on optical flow images and perform late fusion akin to the two-stream hypothesis following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The performance o ligned faces as inputs, give a competitive result. Also, optical flow of faces are tested following <ref type="bibr" target="#b20">[21]</ref>, but no improvements are obtained for emotion classificati
l models on optical flow images and perform late fusion akin to the two-stream hypothesis following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The performance o ligned faces as inputs, give a competitive result. Also, optical flow of faces are tested following <ref type="bibr" target="#b20">[21]</ref>, but no improvements are obtained for emotion classificati
with ImageNet are examined, including CaffeNet <ref type="bibr" target="#b9">[10]</ref>, GoogLeNet <ref type="bibr" target="#b10">[11]</ref>, VGG <ref type="bibr" target="#b12">[13]</ref>, and Residu
also trained a SVM with the linear kernel using audio features extracted with the OpenSmile toolkit <ref type="bibr" target="#b6">[7]</ref>.</p><p>The CNN-RNN, C3D and audio SVM model were trained sep
dden state.</p><p>Although RNNs have been widely used in many tasks such as handwriting recognition <ref type="bibr" target="#b16">[17]</ref> or speech recognition <ref type="bibr" target="#b17">[18]<
models, several mainstream CNN architectures trained with ImageNet are examined, including CaffeNet <ref type="bibr" target="#b9">[10]</ref>, GoogLeNet <ref type="bibr" target="#b10">[11]</ref>, VGG <
dden state.</p><p>Although RNNs have been widely used in many tasks such as handwriting recognition <ref type="bibr" target="#b16">[17]</ref> or speech recognition <ref type="bibr" target="#b17">[18]<
dden state.</p><p>Although RNNs have been widely used in many tasks such as handwriting recognition <ref type="bibr" target="#b16">[17]</ref> or speech recognition <ref type="bibr" target="#b17">[18]<
l models on optical flow images and perform late fusion akin to the two-stream hypothesis following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The performance o ligned faces as inputs, give a competitive result. Also, optical flow of faces are tested following <ref type="bibr" target="#b20">[21]</ref>, but no improvements are obtained for emotion classificati
rg/ns/1.0"><head>III. RU-NET AND R2U-NET ARCHITECTURES</head><p>Inspired by the deep residual model <ref type="bibr" target="#b6">[7]</ref>, RCNN <ref type="bibr" target="#b40">[41]</ref>, and U-Net < type="bibr" target="#b4">[5]</ref>, GoogleNet <ref type="bibr" target="#b5">[6]</ref>, Residual Net <ref type="bibr" target="#b6">[7]</ref>, DenseNet <ref type="bibr" target="#b7">[8]</ref>, and Capsu
TECTURES</head><p>Inspired by the deep residual model <ref type="bibr" target="#b6">[7]</ref>, RCNN <ref type="bibr" target="#b40">[41]</ref>, and U-Net <ref type="bibr" target="#b11">[12]</ref>, we p RCL) are performed with respect to the discrete time steps that are expressed according to the RCNN <ref type="bibr" target="#b40">[41]</ref>. Let's consider the 𝑥 𝑙 input sample in the 𝑙 𝑡ℎ layer of
. Furthermore, many models struggle to define the class boundary properly during segmentation tasks <ref type="bibr" target="#b61">[64]</ref>. However, if we observe the experimental outputs shown in
la><p>The DC is expressed as in Eq. ( <ref type="formula" target="#formula_5">7</ref>) according to <ref type="bibr" target="#b48">[51]</ref>. Here GT refers to the ground truth and SR refers the segm
etwork (FCN) also provides state-of-the-art results for image segmentation tasks in computer vision <ref type="bibr" target="#b1">[2]</ref>. Another variant of FCN was also proposed which is called Se based segmentation methods based on FCN provide superior performance for natural image segmentation <ref type="bibr" target="#b1">[2]</ref>. One of the image patch-based architectures is called Random -the-art performance for image classification <ref type="bibr" target="#b0">[1]</ref>, segmentation <ref type="bibr" target="#b1">[2]</ref>, detection and tracking <ref type="bibr" target="#b2">[3]</r
FCN has improved with recurrent neural networks (RNN), which are fine-tuned on very large datasets <ref type="bibr" target="#b26">[27]</ref>. Semantic image segmentation with DeepLab is one of the st
etwork (FCN) also provides state-of-the-art results for image segmentation tasks in computer vision <ref type="bibr" target="#b1">[2]</ref>. Another variant of FCN was also proposed which is called Se based segmentation methods based on FCN provide superior performance for natural image segmentation <ref type="bibr" target="#b1">[2]</ref>. One of the image patch-based architectures is called Random -the-art performance for image classification <ref type="bibr" target="#b0">[1]</ref>, segmentation <ref type="bibr" target="#b1">[2]</ref>, detection and tracking <ref type="bibr" target="#b2">[3]</r

e-out" method, in which each image is tested, and training is conducted on the remaining 19 samples <ref type="bibr" target="#b46">[47]</ref>. Therefore, there is no overlap between training and testi RE and CHASE_DB1 datasets, we generated FOV masks using a similar technique to the one described in <ref type="bibr" target="#b46">[47]</ref>. One advantage of the patch-based approach is that the pat

d dice similarity are introduced for efficient training of classification and segmentation tasks in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Furthermore e 3D-Unet architecture for volumetric segmentation learns from sparsely annotated volumetric images <ref type="bibr" target="#b12">[13]</ref>. A powerful end-toend 3D medical image segmentation system y be applied deep learning models based on SegNet <ref type="bibr" target="#b9">[10]</ref>, 3D-UNet <ref type="bibr" target="#b12">[13]</ref>, and V-Net <ref type="bibr" target="#b13">[14]</ref> with



l model that overcomes the problem utilizing an identity mapping to facilitate the training process <ref type="bibr" target="#b25">[26]</ref>.</p><p>In addition, CNNs based segmentation methods based
color images, and each image has a size of 700×605 pixels <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. Due to the smaller number of samples, two approaches are a

d dice similarity are introduced for efficient training of classification and segmentation tasks in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Furthermore e 3D-Unet architecture for volumetric segmentation learns from sparsely annotated volumetric images <ref type="bibr" target="#b12">[13]</ref>. A powerful end-toend 3D medical image segmentation system y be applied deep learning models based on SegNet <ref type="bibr" target="#b9">[10]</ref>, 3D-UNet <ref type="bibr" target="#b12">[13]</ref>, and V-Net <ref type="bibr" target="#b13">[14]</ref> with
histogram features <ref type="bibr" target="#b16">[17]</ref>, the region based segmentation method <ref type="bibr" target="#b17">[18]</ref>, and the graph-cut approach <ref type="bibr" target="#b18"
color images, and each image has a size of 700×605 pixels <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. Due to the smaller number of samples, two approaches are a
ng and remaining 20 samples are used for testing. The size of each original image is 565×584 pixels <ref type="bibr" target="#b43">[44]</ref>. To develop a square dataset, the images are cropped to on
ng="en"> 		<body> <div xmlns="http://www.tei-c.org/ns/1.0"><p>are available for training CNN models <ref type="bibr" target="#b0">[1]</ref>. However, in most cases, models are explored and evaluated u ls are explored and evaluated using classification tasks on very large-scale datasets like ImageNet <ref type="bibr" target="#b0">[1]</ref>, where the outputs of the classification tasks are single la /www.tei-c.org/ns/1.0"><p>OWADAYS DL provides state-of-the-art performance for image classification <ref type="bibr" target="#b0">[1]</ref>, segmentation <ref type="bibr" target="#b1">[2]</ref>, detec ce 2012, several Deep Convolutional Neural Network (DCNN) models have been proposed such as AlexNet <ref type="bibr" target="#b0">[1]</ref>, VGG <ref type="bibr" target="#b4">[5]</ref>, GoogleNet <ref
istance in terms of memory references between two accesses to the same memory location. Fang et al. <ref type="bibr" target="#b9">[9]</ref> define memory distance to include reuse distance, access dis he first store in a sequence of stores writing to the same address with the same value. Fang et al. <ref type="bibr" target="#b9">[9]</ref> propose a feedback-directed mechanism based upon access dist d method performs much better than the previous access distance based mem-ory disambiguation scheme <ref type="bibr" target="#b9">[9]</ref>, and yields performance very close to perfect memory disambi ace is small.</p><p>While the above schemes are based on memory dependence predictions, Fang et al. <ref type="bibr" target="#b9">[9]</ref> have proposed a feedback-directed memory scheme which use me e, the load/store queue size, and machine state. Taking all these factors into account, Fang et al. <ref type="bibr" target="#b9">[9]</ref> define the access dis-tance of a load as the number of memor filed one. Our method assumes that small store distances are independent of input size. Fang et al. <ref type="bibr" target="#b9">[9]</ref> have observed that over 80% of the memory instructions in SP n store distances is not negligible, the memory distance analysis framework proposed by Fang et al. <ref type="bibr" target="#b9">[9]</ref> can be applied to enhance our mechanism by predicting store listed in Table <ref type="table" target="#tab_2">1</ref>(b). For the access distance based scheme <ref type="bibr" target="#b9">[9]</ref> as described in Section 2.3.2, we use the test and train inp This last phenomenon can be overcome using the memory distance prediction developed by Fang et al. <ref type="bibr" target="#b9">[9]</ref>.</p><p>When comparing SD and SS16K, SD suffers from issues r ref type="bibr" target="#b28">28]</ref> and instructionbased <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b13">13]</ref> reuse distances have ir performance models to calculate cache misses. Fang et al. <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref> introduce the notion of memory distance to encompass reuse di
> and instructionbased <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b13">13]</ref> reuse distances have been predicted accurately across all p nt hints for an Itanium processor <ref type="bibr" target="#b3">[3]</ref>. Marin and Mellor-Crummey <ref type="bibr" target="#b13">[13]</ref> incorporate instruction-based reuse distance analysis in t n previous memory distance analysis <ref type="bibr">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b29">29</ref>]. An analysis of the
whole program miss rates <ref type="bibr" target="#b28">[28]</ref>, to perform data transformations <ref type="bibr" target="#b29">[29]</ref> and to predict the locality phases of a program <ref type= type="bibr">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b29">29</ref>]. An analysis of the benchmarks in our suite shows that 99.1
</ref> and to evaluate the effect of program transformations <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b6">6]</ref>. Ding et al. <ref type=
cient reuse distance analysis tools to estimate cache misses <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" targe
y disambiguation, we use the FAST micro-architectural simulator based upon the MIPS instruction set <ref type="bibr" target="#b18">[18]</ref>. The simulator models the superscalar pipeline and is cycl
get="#b11">11,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref>. While many hardware techniques achieve good performance, m ave shown that memory order violation detection can be based on values, instead of addresses. ?nder <ref type="bibr" target="#b17">[17]</ref> has proposed a light-weight memory dependence predictor wh
s static-analysis and hard-ware techniques to disambiguate memory references to improve performance <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" targ predictor space and achieves performance comparable to that of a 16K-entry store set implementation <ref type="bibr" target="#b5">[5]</ref> for both floating point and integer programs. In both cases, se memory order violations and synchronizing them when they are encountered.</p><p>Chrysos and Emer <ref type="bibr" target="#b5">[5]</ref> introduce the store set concept which uses direct mapped str e pairs precisely. With sufficient resources, the store set scheme provides near oracle performance <ref type="bibr" target="#b5">[5]</ref> for a set of SPEC95 benchmarks. We choose this scheme as one e those load instructions always because blind speculation outperforms no speculation in most cases <ref type="bibr" target="#b5">[5]</ref>. Rule 1 makes it likely that a dominant number of load insta tore table, and apply cyclic clearing every one million instructions to invalidate all SSIT entries <ref type="bibr" target="#b5">[5]</ref>. Perfect memory disambiguation never mis-speculates with the 3.applu exhibits the second phenomenon mentioned above. As reported in the original store set paper <ref type="bibr" target="#b5">[5]</ref>, some load instructions in 173.applu only depend on neighbor et="#b11">[11,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b5">5]</ref>. The problem of memory disambiguation and communication throu
y disambiguation, we use the FAST micro-architectural simulator based upon the MIPS instruction set <ref type="bibr" target="#b18">[18]</ref>. The simulator models the superscalar pipeline and is cycl
ole-program <ref type="bibr">[7,</ref><ref type="bibr" target="#b28">28]</ref> and instructionbased <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target on-based reuse distance analysis in their performance models to calculate cache misses. Fang et al. <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref> introduce the notion o than the multiple training runs used in previous memory distance analysis <ref type="bibr">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" targe
get="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref>. While many hardwar Work in this area has produced increasingly better results <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" tar
ediction accuracy. Similar attention mechanisms have been proposed for natural image classification <ref type="bibr" target="#b10">[11]</ref> and captioning <ref type="bibr" target="#b0">[1]</ref> to ntributions of this work can be summarised as follows: • We take the attention approach proposed in <ref type="bibr" target="#b10">[11]</ref> a step further by proposing grid-based gating that allows intermediate space. In image captioning <ref type="bibr" target="#b0">[1]</ref> and classification <ref type="bibr" target="#b10">[11]</ref> tasks, the  softmax activation function is used to normali n. This results experimentally in better training convergence for the AG parameters. In contrast to <ref type="bibr" target="#b10">[11]</ref> we propose a grid-attention technique. In this case, gatin the feature-maps and map them to lower dimensional space for the gating operation. As suggested in <ref type="bibr" target="#b10">[11]</ref>, low-level feature-maps, i.e. the first skip connections, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>, and classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" ta [2,</ref><ref type="bibr" target="#b28">29]</ref> and more recently applied to image classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>. In <ref type="bib was the top-performer in the ILSVRC 2017 image classification challenge. Self-attention techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref> have been proposed tention is used in <ref type="bibr" target="#b32">[33]</ref> to capture long range dependencies. In <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref> self-attention is
target="#b12">[13]</ref> and holistically nested networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> have been shown to achieve robust and accurate performance
3">[24]</ref>, DeepMedic <ref type="bibr" target="#b12">[13]</ref> and holistically nested networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> have been shown to ion since they do not represent the input data in a high dimensional space. We use deep-supervision <ref type="bibr" target="#b15">[16]</ref> to force the intermediate feature-maps to be semantically the Adam optimiser <ref type="bibr" target="#b14">[15]</ref>, batch-normalisation, deep-supervision <ref type="bibr" target="#b15">[16]</ref>, and standard data-augmentation techniques (affine transfo
3">[24]</ref>, DeepMedic <ref type="bibr" target="#b12">[13]</ref> and holistically nested networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> have been shown to ion since they do not represent the input data in a high dimensional space. We use deep-supervision <ref type="bibr" target="#b15">[16]</ref> to force the intermediate feature-maps to be semantically the Adam optimiser <ref type="bibr" target="#b14">[15]</ref>, batch-normalisation, deep-supervision <ref type="bibr" target="#b15">[16]</ref>, and standard data-augmentation techniques (affine transfo
3">[24]</ref>, DeepMedic <ref type="bibr" target="#b12">[13]</ref> and holistically nested networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> have been shown to ion since they do not represent the input data in a high dimensional space. We use deep-supervision <ref type="bibr" target="#b15">[16]</ref> to force the intermediate feature-maps to be semantically the Adam optimiser <ref type="bibr" target="#b14">[15]</ref>, batch-normalisation, deep-supervision <ref type="bibr" target="#b15">[16]</ref>, and standard data-augmentation techniques (affine transfo
ormance can be achieved in automated medical image analysis tasks including cardiac MR segmentation <ref type="bibr" target="#b2">[3]</ref> and cancerous lung nodule detection <ref type="bibr" target= f> have been shown to achieve robust and accurate performance in various tasks including cardiac MR <ref type="bibr" target="#b2">[3]</ref>, brain tumours <ref type="bibr" target="#b11">[12]</ref> and
<ref type="bibr" target="#b0">[1]</ref>, machine translation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>, and classification <ref type="bibr" target="#b10">[11,</re n multi-class abdominal CT segmentation. We use CT -150 dataset for both training (120) and testing <ref type="bibr" target="#b29">(30)</ref>. The corresponding Dice scores (DSC) and surface distances "#fig_3">3b</ref>. In the second experiment, the same models are trained with fewer training images <ref type="bibr" target="#b29">(30)</ref> to show that the performance improvement is consistent and
on the other hand, is enforced by design and categorised as hard-and soft-attention. Hard attention <ref type="bibr" target="#b20">[21]</ref>, e.g. iterative region proposal and cropping, is often non rd back-propagation updates without a need for sampling based update methods used in hard-attention <ref type="bibr" target="#b20">[21]</ref>.</p><p>Attention Gates in U-Net Model: The proposed AGs ar 4">[25]</ref>. The dataset contains in total 82 scans which are split into training (61) and testing<ref type="bibr" target="#b20">(21)</ref> sets. The corresponding results are obtained before (BFT)
segmentation from abdominal CT used statistical shape models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref> or multi-atlas techniques <ref type="bibr" target="#b21">[2
rmance in various tasks including cardiac MR <ref type="bibr" target="#b2">[3]</ref>, brain tumours <ref type="bibr" target="#b11">[12]</ref> and abdominal CT <ref type="bibr" target="#b25">[26,</ref>
"bibr" target="#b13">[14]</ref>, cardiac CT <ref type="bibr" target="#b22">[23]</ref>, abdominal CT <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> segmentation, and to remove the dependency of atlas to image registration. Recently, cascaded multi-stage CNN models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" ta r" target="#b2">[3]</ref>, brain tumours <ref type="bibr" target="#b11">[12]</ref> and abdominal CT <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> image segmentation ntation frameworks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>• An extensi r to improve the accuracy, current segmentation frameworks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> rely on additional rformance compared to concatenated multi-model CNN approaches<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref> due to additional t contrast to the state-of-the-art CNN segmentation frameworks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, we propose a 3D-model to capture sufficient semantic conte
e normalized maximum load ? for Fennel, the previously bestknown heuristic (linear weighted degrees <ref type="bibr" target="#b28">[29]</ref>) and hash partitioning of vertices for the Twitter graph w balanced graph partitioning problem in the dynamic setting is known as streaming graph partitioning <ref type="bibr" target="#b28">[29]</ref>. Vertices (or edges) arrive and the decision of the placem ich places a vertex to the cluster with the fewest non-neighbors, and the degree-based heuristic of <ref type="bibr" target="#b28">[29]</ref>, which serves as the current state-of-the-art method with n the non-neighbors heuristic <ref type="bibr" target="#b25">[26]</ref> and the neighbors heuristic <ref type="bibr" target="#b28">[29]</ref>. This provides improved performance for the balanced parti formance of Fennel versus the best previously-known heuristic, which is the linear weighted degrees <ref type="bibr" target="#b28">[29]</ref>, and the baseline Hash Partition of vertices. We observe t in <ref type="bibr">[1]</ref>.</p><p>Online graph partitioning was introduced by Stanton and Kliot <ref type="bibr" target="#b28">[29]</ref>. The online setting is also well adapted to dynamic graphs stributed system. Currently the most advanced online partitioning algorithm is by Stanton and Kliot <ref type="bibr" target="#b28">[29]</ref>, against which we extensively compare our approach.</p></d ome order, each one with the set of its neighbors. We consider three different stream orders, as in <ref type="bibr" target="#b28">[29]</ref>.</p><p>? Random: Vertices arrive according to a random per rtex balanced partitions in Section 5, Fennel also works for edge balanced parititions as well, see <ref type="bibr" target="#b28">[29]</ref>.</p><p>Application to Classical Balanced Partitioning. Cla f neighbours in Si, i.e |N (v)?Si|. This is one of the greedy rules considered by Stanton and Kliot <ref type="bibr" target="#b28">[29]</ref>, and is a greedy rule that may result in highly imbalanced most ?? n k . This algorithm for 1 ? ? ? 2 amounts to interpolating between the basic heuristics of <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b25">[26]</ref>. The overall te-of-the-art heuristics. Specifically, in our evaluation we consider the following heuristics from <ref type="bibr" target="#b28">[29]</ref>, which we briefly describe here for completeness. Let v be (v)(1 -exp |Si| -n/k)). ? Non-Neighbors (NN): minumum |Si \ N (v)|.</formula><p>In accordance with <ref type="bibr" target="#b28">[29]</ref>, we observed that LDG is the best performing heuristic. Ev e used in practice <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Our work contributes towards bridging the gap between theo e provide a novel perspective on a recent line of research <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> for the balanced graph partitioning problem, which results interpolation between the two state-of-the-art heuristics <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref> for streaming partitioning. Specifically, we evaluate our p
#b19">[20]</ref>, and heuristics that are used in practice <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Our work contribut e most popular heuristics used for streaming balanced graph partitioning: the folklore heuristic of <ref type="bibr" target="#b25">[26]</ref> which places a vertex to the cluster with the fewest non-n llows us to define formally the notion of interpolation between between the non-neighbors heuristic <ref type="bibr" target="#b25">[26]</ref> and the neighbors heuristic <ref type="bibr" target="#b28" elds the following heuristic: place a vertex to the partition with the least number of nonneighbors <ref type="bibr" target="#b25">[26]</ref>. This assignment accounts for both the cost of cut edges a unts to interpolating between the basic heuristics of <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b25">[26]</ref>. The overall complexity of our algorithm is O(n + m).</p>< o, it allows us to quantify the notion of interpolation between the two state-of-the-art heuristics <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref> for streaming part
are exchanged between different partitions. Many popular graph processing platforms such as Pregel <ref type="bibr" target="#b22">[23]</ref> that builds on MapReduce <ref type="bibr" target="#b6">[7] In practice, systems aim at providing good partitions in order to enhance their performance, e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>. It is worth empha
h partitioning that relaxes the hard cardinality constraints on the number of vertices in a cluster <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. Our formulation pro m significantly hard. Currently, the stateof-the-art work depends on the Arora-Rao-Vazirani barrier <ref type="bibr" target="#b2">[3]</ref> which results in a O( ? log n) approximation factor. The typ
tems, e.g. <ref type="bibr" target="#b13">[14]</ref>. Recently, it was shown that label propagation <ref type="bibr" target="#b30">[31]</ref> is efficient and effective. An extensive summary of existi
d application since data placement typically affects significantly the execution efficiency of jobs <ref type="bibr" target="#b16">[17]</ref>. The goal of balanced graph partitioning is to minimize an
Due to the practical importance of k-paritioning there exist several heuristics, among which METIS <ref type="bibr" target="#b14">[15]</ref> and its parallel version <ref type="bibr" target="#b27">[2
learly outperforms Metis.</p><p>Power Law: It is well known that power law graphs have no good cuts <ref type="bibr" target="#b11">[12]</ref>, but power-law graphs are commonly observed in practice so
a rich history in the context of theoretical computer science. This problem is known to be NP-hard <ref type="bibr" target="#b18">[19]</ref> and several approximation algorithms have been derived in = 1 + ? for any desired but fixed ? &gt; 0 there exists a O(? -2 log 1.5 n) approximation algorithm <ref type="bibr" target="#b18">[19]</ref>. When ? = 2 there exists an O( ? log k log n) approximatio se, and how to derive appropriate cost functions.</p><p>The starting point in the literature, e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, is to impose hard
ts open source cousin Apache Giraph, PEGASUS <ref type="bibr" target="#b12">[13]</ref> and GraphLab <ref type="bibr" target="#b21">[22]</ref> use as a default partitioner Hash Partition of vertices, w
of research. Rigorous mathematically work and algorithms that do not scale to massive graphs, e.g., <ref type="bibr" target="#b19">[20]</ref>, and heuristics that are used in practice <ref type="bibr" 2 there exists an O( ? log k log n) approximation algorithm based on semidefinite programming (SDP) <ref type="bibr" target="#b19">[20]</ref>. Due to the practical importance of k-paritioning there ex rdinality constraints on the number of vertices in a cluster <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. Our formulation provides a unifying framework that subsume nctions.</p><p>The starting point in the literature, e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, is to impose hard cardinality constraints, namely |S * i |
od partitions in order to enhance their performance, e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>. It is worth emphasizing that the balanced graph partitioni
stands out for its good performance. For this reason, METIS is used in many existing systems, e.g. <ref type="bibr" target="#b13">[14]</ref>. Recently, it was shown that label propagation <ref type="
formulation as a maximization problem which makes a connection with the concept of graph modularity <ref type="bibr" target="#b23">[24]</ref>. For a graph G = (V, E) and S ? V , we define the function power law graphs CL(20 000,2.5), since this value matches the typical slope of real-world networks <ref type="bibr" target="#b23">[24]</ref>. Figure <ref type="figure" target="#fig_1">1</ref> shows t
apReduce <ref type="bibr" target="#b6">[7]</ref>, and its open source cousin Apache Giraph, PEGASUS <ref type="bibr" target="#b12">[13]</ref> and GraphLab <ref type="bibr" target="#b21">[22]</ref> use

d application since data placement typically affects significantly the execution efficiency of jobs <ref type="bibr" target="#b16">[17]</ref>. The goal of balanced graph partitioning is to minimize an
n="7.">CONCLUSION</head><p>In this work we provide a novel perspective on a recent line of research <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> for the balanced g Fennel performs a single pass over the graph, it achieves performance comparable to Metis, see also <ref type="bibr" target="#b24">[25]</ref>. Furthermore, our general framework is particularly suitab ://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc><ref type="bibr" target="#b24">25</ref>, over five randomly generated power law graphs with slope 2.
al heuristics, among which METIS <ref type="bibr" target="#b14">[15]</ref> and its parallel version <ref type="bibr" target="#b27">[28]</ref> stands out for its good performance. For this reason, METI
#b9">[10]</ref>. Several approximation algorithms and heuristics exist for this problem, see, e.g., <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b17">[18]</ref> respectively.
cessing platforms such as Pregel <ref type="bibr" target="#b22">[23]</ref> that builds on MapReduce <ref type="bibr" target="#b6">[7]</ref>, and its open source cousin Apache Giraph, PEGASUS <ref type

sion reduction occurs in the recurrent convolutional neural networks used for semantic segmentation <ref type="bibr" target="#b21">[22]</ref>. As SR methods predict full-sized images, dimension reduct
se performances than a single convolution due to overfitting. To overcome overfitting, Liang and Hu <ref type="bibr" target="#b16">[17]</ref> uses a recurrent layer that takes feed-forward inputs into is in accordance with the limited success of previous methods using at most three recursions so far <ref type="bibr" target="#b16">[17]</ref>. Among many reasons, two severe problems are vanishing and
s, two severe problems are vanishing and exploding gradients <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Exploding gradients refer to the large increase in t
ef>, sparse coding <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, convolutional neur ibr" target="#b31">[32]</ref> are often used for benchmark <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5]</ref>. Dataset B100 consist y small number of locations have non-zero values. For this reason, several super-resolution methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" ta
puter vision tasks often use very large receptive fields (224x224 common in ImageNet classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>). Among many appro
uential data, have seen limited use on algorithms operating on a single static image. Socher et al. <ref type="bibr" target="#b24">[25]</ref> used a convolutional network in a separate stage to first
ead><p>We apply DRCN to single-image super-resolution (SR) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Many SR methods have b
the more powerful methods utilize statistical image priors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref> or internal patch recurrence <ref type="bibr" target="#b7">
the more powerful methods utilize statistical image priors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref> or internal patch recurrence <ref type="bibr" target="#b7">
uper-resolution (SR) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Many SR methods have been proposed in the computer vision co bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref> or internal patch recurrence <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Recently, sophistica
ession functions from LR to HR images. This is achieved with various techniques: neighbor embedding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, sparse coding <ref
ding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, sparse coding <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" ta org/ns/1.0"><head n="4.1.">Datasets</head><p>For training, we use 91 images proposed in Yang et al. <ref type="bibr" target="#b30">[31]</ref> for all experiments. For testing, we use four datasets. Da
the more powerful methods utilize statistical image priors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref> or internal patch recurrence <ref type="bibr" target="#b7">
stic gradient descent method does not easily converge. This is due to exploding/vanishing gradients <ref type="bibr" target="#b0">[1]</ref>. Learning long-range dependencies between pixels with a sing et="#b16">[17]</ref>. Among many reasons, two severe problems are vanishing and exploding gradients <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Exploding gra
s, two severe problems are vanishing and exploding gradients <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Exploding gradients refer to the large increase in t
ession functions from LR to HR images. This is achieved with various techniques: neighbor embedding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, sparse coding <ref
the more powerful methods utilize statistical image priors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref> or internal patch recurrence <ref type="bibr" target="#b7">
e set all weights to zero except self-connections (connection to the same neuron in the next layer) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14]</ref>. Biases are set to
e set all weights to zero except self-connections (connection to the same neuron in the next layer) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14]</ref>. Biases are set to
This is achieved with various techniques: neighbor embedding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, sparse coding <ref type="bibr" target="#b30">[31,</ref><re resolution methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2]</ref> predict image details ibr" target="#b30">[31]</ref> for all experiments. For testing, we use four datasets. Datasets Set5 <ref type="bibr" target="#b18">[19]</ref> and Set14 <ref type="bibr" target="#b31">[32]</ref> are of
et="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2]</ref> predict image details only. Similarly, we find that this doma
2 H D Filters W Conv / ReLU Filters W Conv / ReLU Filters W Conv / ReLU</formula><p>In Eigen et al. <ref type="bibr" target="#b5">[6]</ref>, recursive layers have the same input and output dimension,
n this work, we focus on three of these proposed schemes -the first based on working set signatures <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref>, the second based n lead to unpredictable, non-optimal results. Consequently, algorithms such as the ones proposed in <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref> do not perform tu ets, BBVs, and conditional branch counters. In addition to instruction working set based techniques <ref type="bibr" target="#b9">[10]</ref>[11], we evaluate branch and procedure working set based tec are defined as the set of branches/procedures touched over the sampling interval. In previous work <ref type="bibr" target="#b9">[10]</ref>[11], we defined a similarity metric called the relative wor and working sets are too large to be efficiently stored and compared in hardware. In previous work <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref>, we proposed a ha e</head><p>A working set signature is a lossy-compressed representation of the complete working set <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref>. The signature is 512 bits) and accumulator tables (1024, 128, 32 entries) are similar to those used in previous work <ref type="bibr" target="#b9">[10]</ref>[11] <ref type="bibr" target="#b16">[19]</ref>. Procedure si be used in tuning algorithms to reuse previously found optimal configurations for recurring phases <ref type="bibr" target="#b9">[10]</ref>[11] <ref type="bibr">[12] [19]</ref>. This eliminates a sig
n may enable reuse of configuration information for recurring phases, thereby improving performance <ref type="bibr" target="#b10">[11]</ref>[12] <ref type="bibr" target="#b16">[19]</ref>.</p><p>There ">[13]</ref> or explicitly detect program phase changes <ref type="bibr" target="#b1">[2]</ref>[10] <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b11">[12]</ref> [19] <ref type=" phase changes, identifying phases and predicting phases <ref type="bibr" target="#b1">[2]</ref>[10] <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b11">[12]</ref> <ref type="bibr" roposed schemes -the first based on working set signatures <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref>, the second based on basic block vectors <ref type="bibr" hm dynamically varies the threshold throughout the execution of the program.</p><p>In previous work <ref type="bibr" target="#b10">[11]</ref>, we defined a program phase to be the instruction working been used in power/performance optimization algorithms <ref type="bibr" target="#b1">[2]</ref>[10] <ref type="bibr" target="#b10">[11]</ref> [12] <ref type="bibr" target="#b16">[19]</ref> and to redu lts. Consequently, algorithms such as the ones proposed in <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref> do not perform tuning while in unstable regions. Tuning al re more efficient at detecting major phase changes, a result in agreement with our previous results <ref type="bibr" target="#b10">[11]</ref>. </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head iciently stored and compared in hardware. In previous work <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref>, we proposed a hardware structure called the working set s ossy-compressed representation of the complete working set <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref>. The signature is formed by sampling the working set i.e. significant fraction of reconfigurations, leading to performance improvements. In our previous work <ref type="bibr" target="#b10">[11]</ref>, we show that phase-identification based algorithms can re signatures have an added advantage that they can be used to estimate the working set size directly <ref type="bibr" target="#b10">[11]</ref>. The working set size k can be estimated from the fill fac to reduce the number of reconfigurations by 74% in a particular instruction cache tuning algorithm <ref type="bibr" target="#b10">[11]</ref>.</p><p>However, it should be noted that to make use of thi

e="table" target="#tab_0">1</ref>. The results presented are averaged over all SPEC 2000 benchmarks <ref type="bibr" target="#b22">[25]</ref> with the exception of sixtrack and facerec. The latter two
[11]</ref> <ref type="bibr" target="#b11">[12]</ref> [19] <ref type="bibr" target="#b17">[20]</ref> <ref type="bibr" target="#b18">[21]</ref>. Recently, several researchers have proposed hardware tech ose performance is representative of the entire benchmark <ref type="bibr" target="#b17">[20]</ref> <ref type="bibr" target="#b18">[21]</ref>. Thus, one of the desirable properties in a phase detectio
[11]</ref> <ref type="bibr" target="#b11">[12]</ref> [19] <ref type="bibr" target="#b17">[20]</ref> <ref type="bibr" target="#b18">[21]</ref>. Recently, several researchers have proposed hardware tech ose performance is representative of the entire benchmark <ref type="bibr" target="#b17">[20]</ref> <ref type="bibr" target="#b18">[21]</ref>. Thus, one of the desirable properties in a phase detectio

dapts to changing program requirements in order to achieve better power/performance characteristics <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b11">[12]</ref>. Similarly, on th

dapts to changing program requirements in order to achieve better power/performance characteristics <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b11">[12]</ref>. Similarly, on th

tative CNN architectures, which we chose because of their nice tradeoffs between speed and accuracy <ref type="bibr" target="#b2">[2]</ref>:</p><p>1. The ILSVRC-2012 <ref type="bibr" target="#b19">[19 ng entry of <ref type="bibr" target="#b12">[12]</ref> (AlexNet).</p><p>2. The CNN-M-2048 model from <ref type="bibr" target="#b2">[2]</ref> (VGG-CNN-M-2048), which is a variant of the model introduced nt of training data without overfitting. Our approach to tackling this problem follows recent works <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target ch smaller datasets, we set an initial learning rate of 0.001, which is lower than the typical 0.01 <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12]</ref>, so as not to drasti
n Units (AU) are activated <ref type="bibr" target="#b24">[24]</ref> or to estimate their intensity <ref type="bibr" target="#b18">[18]</ref>. Fewer works follow the dimensional approach, according to
for HOG, dense SIFT, and deep convolutional features was learned based on a Riemannian manifold. In <ref type="bibr" target="#b23">[23]</ref> audio, LPQ-TOP, LBP-TOP, PHOG and SIFT features were used
were learned for LPQ-TOP, audio, gist and SIFT features, and were combined in an SVM classifier. In <ref type="bibr" target="#b14">[14]</ref>, the optimal fusion of classifiers for HOG, dense SIFT, an >[10]</ref>, particularly previous years' EmotiW challenge <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>However, due to the small dataset size for the Emoti
ing the target dataset <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b26">26]</ref>. This approach has
for HOG, dense SIFT, and deep convolutional features was learned based on a Riemannian manifold. In <ref type="bibr" target="#b23">[23]</ref> audio, LPQ-TOP, LBP-TOP, PHOG and SIFT features were used
ing the target dataset <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b26">26]</ref>. This approach has
P, LBP-TOP, PHOG and SIFT features were used along with a hierarchical classifier fusion method. In <ref type="bibr" target="#b3">[3]</ref> HOG-TOP and audio features were fused using multiple kernel
space <ref type="bibr" target="#b29">[29]</ref>. A very detailed and recent review can be found in <ref type="bibr" target="#b21">[21]</ref>.</p><p>The Emotion Recognition in the Wild (EmotiW) contes
ImageNet dataset, a first-stage fine-tuning is applied using the FER-2013 facial expression dataset <ref type="bibr" target="#b10">[10]</ref>, which comprises 28K/32K low resolution images of facial e e in a number of state-of-the-art algorithms for this task, as well as winning related competitions <ref type="bibr" target="#b10">[10]</ref>, particularly previous years' EmotiW challenge <ref type=" e expression dataset introduced in the ICML 2013 workshop's facial expression recognition challenge <ref type="bibr" target="#b10">[10]</ref> as auxiliary data to fine-tune the respective CNNs that we



iption documents using CNN or Word2Vec (H. <ref type="bibr" target="#b55">Wang et al., 2015)</ref>, <ref type="bibr" target="#b20">(Kim et al., 2016)</ref>, <ref type="bibr" target="#b41">(Seo et al., el review texts in predicting user preferences. The first baseline we considered was the ConvMF+ by <ref type="bibr" target="#b20">Kim et al., (2016)</ref> which incorporates CNN into PMF to capture r nsidered baseline models have stated to beat similar alternative methods including CDL as stated by <ref type="bibr" target="#b20">Kim et al., (2016)</ref>, CNN as verified by <ref type="bibr" target=
rget="#b9">(Deerwester, Dumais, Furnas, Landauer, &amp; Harshman, 1990)</ref>, random indexing (RI) <ref type="bibr" target="#b38">(Sahlgren, 2005)</ref> and Word2Vec. <ref type="bibr" target="#b0">Al



ts to learn and represent item topics, features and description documents using CNN or Word2Vec (H. <ref type="bibr" target="#b55">Wang et al., 2015)</ref>, <ref type="bibr" target="#b20">(Kim et al.,



pe="bibr" target="#b8">[9]</ref>. The gesture annotation is performed using the MUMIN coding scheme <ref type="bibr" target="#b2">[3]</ref>, which is a standard multimodal annotation scheme for interp
While there is research work that has used court trial transcripts to identify deceptive statements <ref type="bibr" target="#b13">[14]</ref>, we are not aware of any previous work that took into cons cusing on real-life high-stake data. The work closest to ours is presented by Fornaciari and Poesio <ref type="bibr" target="#b13">[14]</ref>, which targets the identification of deception in statemen
r" target="#b34">[35]</ref>, which was then used to develop a multimodal deception detection system <ref type="bibr" target="#b1">[2]</ref>. An extensive review of approaches for evaluating human cred sent useful clues for deception, their performance is often similar to that of the n-grams features <ref type="bibr" target="#b1">[2]</ref>. Since in our current work we are not focusing on the insigh
k using a number of modalities, including text <ref type="bibr" target="#b12">[13]</ref> and speech <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29]</ref>. Unlike the polygr
human contributors, in a lab setting or via crowdsourcing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref>, for instance by asking subjects to narrate stories in dece b27">[28]</ref>, by performing one-on-one interviews, or by participating in "Mock crime" scenarios <ref type="bibr" target="#b32">[33]</ref>. However, an important drawback identified in this datadri
>[32]</ref> extracted geometric-based features from facial expressions, and Pfister and Pietikainen <ref type="bibr" target="#b35">[36]</ref> developed a micro-expression dataset to identify expressio
>[32]</ref> extracted geometric-based features from facial expressions, and Pfister and Pietikainen <ref type="bibr" target="#b35">[36]</ref> developed a micro-expression dataset to identify expressio
isplays and hand movements, as they have been previously found to correlate with deceptive behavior <ref type="bibr" target="#b8">[9]</ref>. The gesture annotation is performed using the MUMIN coding
text <ref type="bibr" target="#b12">[13]</ref> and speech <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29]</ref>. Unlike the polygraph method, learning-based methods for de
several researchers have also studied the relation between text syntactic complexity and deception <ref type="bibr" target="#b43">[44]</ref>.</p><p>While most of the data used in related research was
ures that can be related to the act of deception using data from simulated interviews. Cohen et al. <ref type="bibr" target="#b7">[8]</ref> found that fewer iconic hand gestures were a sign of a decep
ures that can be related to the act of deception using data from simulated interviews. Cohen et al. <ref type="bibr" target="#b7">[8]</ref> found that fewer iconic hand gestures were a sign of a decep
e. Several studies <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10]</ref> indicated that relying solely on such physiological measurem
ies were integrated in order to find a combination of multimodal features with superior performance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>. A multimodal decept
websites <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b17">18]</ref>, forums <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b22">23]</ref>, social networks <
relying solely on such physiological measurements can be biased and misleading. Chittaranjan et al. <ref type="bibr" target="#b6">[7]</ref> created an audio-visual recordings of the "Are you a Werewol
ies previous findings where human ability to spot liars was found to be slightly better than chance <ref type="bibr" target="#b0">[1]</ref>. Moreover, the performance of the human annotators appears t
l deception dataset consisting of linguistic, thermal, and physiological features was introduced in <ref type="bibr" target="#b34">[35]</ref>, which was then used to develop a multimodal deception det
ype="bibr" target="#b39">40]</ref>, or using geometric features related to the hand and head motion <ref type="bibr" target="#b26">[27]</ref>. Caso et al. <ref type="bibr" target="#b5">[6]</ref> ident
ng human credibility using physiological, visual, acoustic, and linguistic features is available in <ref type="bibr" target="#b29">[30]</ref>.</p><p>To our knowledge, no work to date has considered th
s derived from syntactic CFG trees and part of speech tags <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>. Some studies have also incorporated the analysis of psycho
ies previous findings where human ability to spot liars was found to be slightly better than chance <ref type="bibr" target="#b0">[1]</ref>. Moreover, the performance of the human annotators appears t
d the identification of deceptive content in a variety of domains, including online dating websites <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b17">18]</ref>, forums <ref type=
human contributors, in a lab setting or via crowdsourcing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref>, for instance by asking subjects to narrate stories in dece b27">[28]</ref>, by performing one-on-one interviews, or by participating in "Mock crime" scenarios <ref type="bibr" target="#b32">[33]</ref>. However, an important drawback identified in this datadri
isplays and hand movements, as they have been previously found to correlate with deceptive behavior <ref type="bibr" target="#b8">[9]</ref>. The gesture annotation is performed using the MUMIN coding
d the identification of deceptive content in a variety of domains, including online dating websites <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b17">18]</ref>, forums <ref type=
several researchers have also studied the relation between text syntactic complexity and deception <ref type="bibr" target="#b43">[44]</ref>.</p><p>While most of the data used in related research was
ref> found that fewer iconic hand gestures were a sign of a deceptive narration, and Hillman et al. <ref type="bibr" target="#b18">[19]</ref> determined that increased speech prompting gestures were a
>[32]</ref> extracted geometric-based features from facial expressions, and Pfister and Pietikainen <ref type="bibr" target="#b35">[36]</ref> developed a micro-expression dataset to identify expressio
l deception dataset consisting of linguistic, thermal, and physiological features was introduced in <ref type="bibr" target="#b34">[35]</ref>, which was then used to develop a multimodal deception det
s derived from syntactic CFG trees and part of speech tags <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>. Some studies have also incorporated the analysis of psycho
ures that can be related to the act of deception using data from simulated interviews. Cohen et al. <ref type="bibr" target="#b7">[8]</ref> found that fewer iconic hand gestures were a sign of a decep
websites <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b17">18]</ref>, forums <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b22">23]</ref>, social networks <
vides tolerance against incorrect labels.</p><p>The recently introduced transform/stability loss of <ref type="bibr" target="#b20">Sajjadi et al. (2016b)</ref> is based on the same principle as our wo rements is ∼0.5 percentage points better than independent flips.</p><p>A principled comparison with <ref type="bibr" target="#b20">Sajjadi et al. (2016b)</ref> is difficult due to several reasons. The paths, and comparing the outputs of the network instead of pre-activation data of the final layer. <ref type="bibr" target="#b20">Sajjadi et al. (2016b)</ref> recently introduced a new loss function
t="#b21">(Salimans &amp; Kingma, 2016)</ref> with momentum 0.999 to all of them. We used leaky ReLU <ref type="bibr" target="#b13">(Maas et al., 2013)</ref> with α = 0.1 as the non-linearity, and chos

ase of it. The Π-model can also be seen as a simplification of the Γ-model of the ladder network by <ref type="bibr" target="#b17">Rasmus et al. (2015)</ref>, a previously presented network architectu he data is obtained.</p><p>Our approach is somewhat similar to the Γ-model of the ladder network by <ref type="bibr" target="#b17">Rasmus et al. (2015)</ref>, but conceptually simpler. In the Π-model, ed the issue by shuffling the input sequences in such a way that stratification is guaranteed, e.g. <ref type="bibr" target="#b17">Rasmus et al. (2015)</ref> (confirmed from the authors). This kind of he ones that are most directly connected to our work.</p><p>Γ-model is a subset of a ladder network <ref type="bibr" target="#b17">(Rasmus et al., 2015)</ref> that introduces lateral connections into

upervised learning with promising results <ref type="bibr" target="#b12">(Maaløe et al., 2016;</ref><ref type="bibr" target="#b25">Springenberg, 2016;</ref><ref type="bibr" target="#b15">Odena, 2016;<
r" target="#b12">(Maaløe et al., 2016;</ref><ref type="bibr" target="#b25">Springenberg, 2016;</ref><ref type="bibr" target="#b15">Odena, 2016;</ref><ref type="bibr">Salimans et al., 2016)</ref>. It A
upervised learning with promising results <ref type="bibr" target="#b12">(Maaløe et al., 2016;</ref><ref type="bibr" target="#b25">Springenberg, 2016;</ref><ref type="bibr" target="#b15">Odena, 2016;<
r" target="#b12">(Maaløe et al., 2016;</ref><ref type="bibr" target="#b25">Springenberg, 2016;</ref><ref type="bibr" target="#b15">Odena, 2016;</ref><ref type="bibr">Salimans et al., 2016)</ref>. It A

ch, their technique follows the general pseudo-ensemble agreement (PEA) regularization framework of <ref type="bibr" target="#b0">Bachman et al. (2014)</ref>. In addition, they employ a mutual exclusi
n the MDP state, which consists of the query, the preceding documents, and the remaining candidates <ref type="bibr" target="#b32">[33]</ref>.</p><p>The greedy sequential document selection simpli es 28]</ref> for the Game of Go, in this paper we propose to enhance the MDP model for diverse ranking <ref type="bibr" target="#b32">[33]</ref> with the Monte Carlo tree search (MCTS), for alleviating t anism <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Xia et al. <ref type="bibr" target="#b32">[33]</ref> proposed to model the dynamics of the document utility wit <ref type="bibr" target="#b37">[38]</ref>, the log-based document re-ranking is modeled as a POMDP. <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b29">[30]</ref> propose to m <p>In our experiments, for e ective training of the model parameters and following the practices in <ref type="bibr" target="#b32">[33]</ref>, we combined four TREC datasets and constructed a new data approach which automatically learns novelty features based on neural tensor networks.</p><p>MDP-DIV <ref type="bibr" target="#b32">[33]</ref>: a state-of-the-art learning approach which uses an MDP fo ning approach which uses an MDP for modeling the diverse ranking process. Following the practice in <ref type="bibr" target="#b32">[33]</ref>, we con gured the reward function in MDP-DIV as α-DCG and minary representations of the queries and the documents as their inputs. Following the practices in <ref type="bibr" target="#b32">[33]</ref>, in the experiments we used the query vector and document





N ! di erent rankings for N documents.</p><p>Inspired by the success and methodology of the AlphaGo <ref type="bibr" target="#b26">[27]</ref> and AlphaGo Zero <ref type="bibr" target="#b27">[28]</ref> could lead to suboptimal rankings.</p><p>To alleviate the issue, following the practices in AlphaGo <ref type="bibr" target="#b26">[27]</ref> and AlphaGo Zero <ref type="bibr" target="#b27">[28]</ref>

In the model, the state of the search users are encoded as a four hidden decision making states. In <ref type="bibr" target="#b37">[38]</ref>, the log-based document re-ranking is modeled as a POMDP.

document can provide. For example, the representative approach of maximal marginal relevance (MMR) <ref type="bibr" target="#b2">[3]</ref> uses the sum of the query-document relevance and the maximal designing e ective criteria to estimate the utility a document can provide. Carbonell and Goldstein <ref type="bibr" target="#b2">[3]</ref> proposed the maximal marginal relevance criterion, which is compared M 2 Div with several state-of-the-art baselines in search result diversi cation:</p><p>MMR <ref type="bibr" target="#b2">[3]</ref>: a heuristic approach in which the document is selected acco


ronous techniques can play an important role in globally asynchronous, locally synchronous systems. <ref type="bibr" target="#b1">2</ref> Such systems reduce clock power and help with the growing prob
st-known example of speculation. If the predictors are accurate, it can increase the MIPS/W figure. <ref type="bibr" target="#b2">3</ref> New architectural ideas can contribute most profitably to redu
ache refills, are often sequential, and counting in Gray code switches the least number of signals. <ref type="bibr" target="#b4">5</ref> Adapting other ideas to this problem is straightforward. Trans

em stores the program in compressed form and decompresses it on the fly, typically on a cache miss. <ref type="bibr" target="#b6">7</ref> Reducing memory size translates to power savings. It also redu
em stores the program in compressed form and decompresses it on the fly, typically on a cache miss. <ref type="bibr" target="#b6">7</ref> Reducing memory size translates to power savings. It also redu
em stores the program in compressed form and decompresses it on the fly, typically on a cache miss. <ref type="bibr" target="#b6">7</ref> Reducing memory size translates to power savings. It also redu
ache refills, are often sequential, and counting in Gray code switches the least number of signals. <ref type="bibr" target="#b4">5</ref> Adapting other ideas to this problem is straightforward. Trans
st-known example of speculation. If the predictors are accurate, it can increase the MIPS/W figure. <ref type="bibr" target="#b2">3</ref> New architectural ideas can contribute most profitably to redu
be more efficient <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39]</ref>.</p><p>In this work ing sure that the search index fits in memory, eliminating page faults and minimizing disk activity <ref type="bibr" target="#b36">[38]</ref>. We simulate clients using the Faban driver. The clients a ng in inefficiency through waste of resources. At the same time, our results corroborate prior work <ref type="bibr" target="#b36">[38]</ref>, indicating that niche processors offer excessively simple Microsoft and showed the implications of such workloads on data-center server design. Reddi et al. <ref type="bibr" target="#b36">[38]</ref> characterized the Bing search engine, showing that the com nt research activity on characterizing particular scale-out workloads, either micro-architecturally <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b40">42]</ref>, or at the system
performed precisely due to overlapped work in the pipeline <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">28]</ref>. We present execution-time breakdown results based on the p cations when running on modern hardware, using real machines <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">28]</ref> and simulation environments <ref type="bibr" target="#b20">
L2 data cache hits in the computation because they are effectively hidden by the out-of-order core <ref type="bibr" target="#b25">[27]</ref>.</p><p>We perform a cache sensitivity analysis by dedicati
ation environments <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b34">36]</ref>. We include traditi
used for processing cores rather than caches. A similar observation has been made in the GPU domain <ref type="bibr" target="#b17">[18]</ref>. Our results corroborate these findings, showing that, for
Yahoo! Cloud Serving Benchmark (YCSB) dataset. Server load is generated using the YCSB 0.1.3 client <ref type="bibr" target="#b6">[7]</ref> that sends requests following a Zipfian distribution with a on benchmarking the cloud and datacenter infrastructure. The Yahoo! Cloud Serving Benchmark (YCSB) <ref type="bibr" target="#b6">[7]</ref> is a framework to benchmark large-scale distributed data ser ype="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b40">42]</ref>, or at the system level <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" targ
get="#b28">30,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b39">41]</ref>. To the best of our knowledge, our study is the first work
Yahoo! Cloud Serving Benchmark (YCSB) dataset. Server load is generated using the YCSB 0.1.3 client <ref type="bibr" target="#b6">[7]</ref> that sends requests following a Zipfian distribution with a on benchmarking the cloud and datacenter infrastructure. The Yahoo! Cloud Serving Benchmark (YCSB) <ref type="bibr" target="#b6">[7]</ref> is a framework to benchmark large-scale distributed data ser ype="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b40">42]</ref>, or at the system level <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" targ
rticular scale-out workloads, either micro-architecturally <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b40">42]</ref>, or at the system level <ref type="bibr" target="#b6">[7,</
ed analytical processing to cluster, classify, and filter this information. The map-reduce paradigm <ref type="bibr" target="#b8">[9]</ref> has emerged as a popular approach to large-scale analysis, f
ndustry has reached the physical limits of voltage scaling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">24]</ref>, no longer able to reduce power consumption or increase pow

correction <ref type="bibr" target="#b3">[Agrawal et al. 2005]</ref>, reproducing photographic look <ref type="bibr" target="#b4">[Bae et al. 2006]</ref>, and alpha matting <ref type="bibr" target="#b es <ref type="bibr" target="#b3">[Agrawal et al. 2005]</ref>, and reproduction of photographic look <ref type="bibr" target="#b4">[Bae et al. 2006</ref>], since in these cases the desired gradient fie
region quadtree that represents a 2D array whose elements can be values in a small, discrete range. <ref type="bibr" target="#b6">Dyer [1982]</ref> showed that the number of quadtree nodes in a region
aptively vary resolution when solving linear systems or discretized partial differential equations. <ref type="bibr" target="#b16">Losasso et al. [2004]</ref> performed large-scale fluid simulations b
st matches a specified gradient-field; recent examples include high dynamic range (HDR) compression <ref type="bibr" target="#b7">[Fattal et al. 2002]</ref>, intrinsic image recovery <ref type="bibr"
copy a region from one image into another, as well as a variety of other image editing operations. <ref type="bibr" target="#b9">Georgiev [2004]</ref> revealed that the Adobe Photoshop Healing Brush
nly way to solve Poisson equations; Fourier transforms can be used to directly calculate a solution <ref type="bibr" target="#b23">[Simchony et al. 1990</ref>]. However, this approach requires O(n lg
removing any artifacts remaining at the seams. This approach has been used to fill holes in images <ref type="bibr" target="#b11">[Hays and Efros 2007]</ref> and to compute multi-viewpoint <ref type=
Georgiev [2004]</ref> revealed that the Adobe Photoshop Healing Brush uses a similar technique, and <ref type="bibr" target="#b12">Jia et al. [2006]</ref> improved on this basic approach by first opti . When compositing a region from image I A into image I B <ref type="bibr">[Pérez et al. 2003;</ref><ref type="bibr" target="#b12">Jia et al. 2006</ref>], the gradients inside this region are ∇I A and
a et al. 2005]</ref>. Others have confirmed <ref type="bibr" target="#b15">[Levin et al. 2004;</ref><ref type="bibr" target="#b31">Zomet et al. 2006;</ref><ref type="bibr" target="#b10">Goldman and Ch
pute multi-viewpoint <ref type="bibr" target="#b2">[Agarwala et al. 2006</ref>] and video panoramas <ref type="bibr" target="#b1">[Agarwala et al. 2005]</ref>. Others have confirmed <ref type="bibr" t 2</ref>). Most of our results are panoramas whose seams were computed using hierarchical graph cuts <ref type="bibr" target="#b1">[Agarwala et al. 2005]</ref>, though the first result in Table <ref ty perform gradientdomain compositing for video <ref type="bibr" target="#b29">[Wang et al. 2004;</ref><ref type="bibr" target="#b1">Agarwala et al. 2005]</ref>, where scalability concerns are even great
al., 2015)</ref> firstly model sentences by RNN, and then use CNN to get the final representation. <ref type="bibr" target="#b21">Shi et al. (2016)</ref> replace convolution filters with deep LSTM, w of recurrent units. We find that using GRU as recurrent units outperforms LSTM which is utilized by <ref type="bibr" target="#b21">Shi et al. (2016)</ref>.</p><formula xml:id="formula_0">w 1 w 2 w 3 w



.org/ns/1.0"><head>Implementation Details</head><p>We tokenize all the corpus with NLTK's tokenizer <ref type="bibr" target="#b1">(Bird and Loper, 2004)</ref>. We limit the vocabulary size of each dat
ze 1D CNN in part of speech (POS), named entity recognition (NER) and semantic role labeling (SRL). <ref type="bibr" target="#b14">Kim (2014)</ref> proposes to classify sentence by encoding a sentence n a pooling operation can be applied after the convolutional layer and generate a fixed size vector <ref type="bibr" target="#b14">(Kim, 2014)</ref>. Similarly to RNN and DRNN, we can also represent t


entiment analysis <ref type="bibr" target="#b23">(Tang et al., 2015)</ref>, question classification <ref type="bibr" target="#b35">(Zhang and Lee, 2003)</ref> and topic classification <ref type="bibr"
target="#b23">Tang et al. (2015)</ref> utilize LSTM to model the relation of sentences. Similarly, <ref type="bibr" target="#b30">Yang et al. (2016)</ref> propose hierarchical attention model which i >(Yogatama et al., 2017)</ref> is a discriminative LSTM model. Hierarchical attention network (HAN) <ref type="bibr" target="#b30">(Yang et al., 2016)</ref> is a hierarchical GRU model with attentive
ierarchical GRU model so that the model can better capture the important information of a document. <ref type="bibr" target="#b27">Wang and Tian (2016)</ref> incorporate the residual networks <ref typ
true. In particular we demonstrate that the recently proposed AutoAugment data augmentation policy <ref type="bibr" target="#b5">[6]</ref> achieves state-of-the-art results on the CIFAR-10-C benchmar mentation strategies. Towards this end, we investigated the learned augmentation policy AutoAugment <ref type="bibr" target="#b5">[6]</ref>. AutoAugment applies a learned mixture of image transformati
e. Naively augmenting on different corruptions often will not transfer well to held out corruptions <ref type="bibr" target="#b11">[12]</ref>. However, the impressive robustness of AutoAugment gives u
on. This difficulty has led to many false claims of methods for detecting adversarial perturbations <ref type="bibr" target="#b4">[5]</ref>. Thus the analysis presented here is to better understand co
e. Naively augmenting on different corruptions often will not transfer well to held out corruptions <ref type="bibr" target="#b11">[12]</ref>. However, the impressive robustness of AutoAugment gives u
models lack the robustness of the human vision system when the train and test distributions differ <ref type="bibr" target="#b23">[24]</ref>. For example, it has been observed that commonly occurring
st time.</p><p>As a simple example of this principle, consider Figure <ref type="figure">8</ref> in <ref type="bibr" target="#b18">[19]</ref>. The authors experimented with training models on a "cheat
model <ref type="bibr" target="#b12">[13]</ref>. This observation was also made in concurrent work <ref type="bibr" target="#b25">[26]</ref>.</p><p>Finally, we extend our frequency analysis to obtain
oposed several times <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref> as a method for improving robustness to small perturbations.<
st time.</p><p>As a simple example of this principle, consider Figure <ref type="figure">8</ref> in <ref type="bibr" target="#b18">[19]</ref>. The authors experimented with training models on a "cheat
ed on the input in the frequency domain. While modest filtering has been used for model compression <ref type="bibr" target="#b8">[9]</ref>, we experiment with extreme filtering in order to test the l
y at random from [0, σ]. For our experiments on CIFAR-10, we use the Wide ResNet-28-10 architecture <ref type="bibr" target="#b26">[27]</ref>, and for our experiment on ImageNet, we use the ResNet-50
arget="#b36">37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar
et="#b39">[40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" tar
arget="#b6">7,</ref><ref type="bibr" target="#b78">80,</ref><ref type="bibr" target="#b82">84,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b86">88,</ref><ref type="bibr" tar
get="#b73">75,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b87">89,</ref><ref type="bibr" target="#b84">86,</ref><ref type="bibr" target="#b83">85,</ref><ref type="bibr" tar
arget="#b2">3,</ref><ref type="bibr" target="#b76">78,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" targe
get="#b86">88,</ref><ref type="bibr" target="#b73">75,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b87">89,</ref><ref type="bibr" target="#b84">86,</ref><ref type="bibr" tar
deadlock issues when the occupancy exceed an unknown bound <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n
), which is widely used in graph visualization application <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b52">53]</ref>, iteratively deletes the vertices whose degree is less than
get="#b86">88,</ref><ref type="bibr" target="#b73">75,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b87">89,</ref><ref type="bibr" target="#b84">86,</ref><ref type="bibr" tar
er to use GPUs efficiently, a programmer needs to possess an in-depth knowledge of GPU architecture <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref>, e.g., Gunrock requi
. Other algorithms, such as, weakly connected component and strongly connected component algorithms <ref type="bibr" target="#b66">[67]</ref> also fall into this category.</p></div> <div xmlns="http:/
cted dependence violations -squashing those threads where parallelism was not found. Helper threads <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
lying architecture implements a form of speculative multithreading. We modified a version of SMTSIM <ref type="bibr" target="#b45">[46]</ref> to imple- Our framework simulates SpMT running on a parall
get="#b51">52,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> execute short threads in support of the original thread of
/ref><ref type="bibr" target="#b6">7]</ref> and those that work even in the presence of dependences <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>. It includes those f type="bibr" target="#b20">21]</ref>. Some models rely primarily on hardware prediction of liveins <ref type="bibr" target="#b23">[24]</ref> and others use software to compute register and memory liv
hose threads where parallelism was not found. Helper threads <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" targe
rporate individual instruction results of a violating thread <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>. Some models rely primarily on hardware prediction of livei
hose threads where parallelism was not found. Helper threads <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" targe
d also include the bi-mode predictor <ref type="bibr" target="#b38">[39]</ref>, the agree predictor <ref type="bibr" target="#b40">[41]</ref>, the YAGS predictor <ref type="bibr" target="#b10">[11]</r
w loads, in others it seeks to capture all the iterations of a loop instance.</p><p>Chapparo et al. <ref type="bibr" target="#b2">[3]</ref> propose core-hopping to eliminate or react to thermal emerge
effects are seen in the heterogeneous multi-core proposals <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref>, where heavy sampling is used to find the best mapping of t
ranch prediction on simultaneous multithreading processors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>. In general, branch prediction was not shown to be a signif
he transferability <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b8">7]</ref>, an adversarial example is usually generated for a single inp box attacks than FGSM at the cost of worse transferability <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b8">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) ="#b20">[19]</ref> of adversarial examples can be used to attack a black-box model. Several methods <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b38">37]</ref> have been proposed t rg/ns/1.0"><head n="3.2.">Translation-Invariant Attack Method</head><p>Although many attack methods <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b38">37]</ref> can generate adversa ref type="bibr" target="#b8">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type="bibr" target="#b8">[7]</ref> proposes to improve the transferability of adversarial examp formula_13">)</formula><p>The translation-invariant method can be similarly integrated into MI-FGSM <ref type="bibr" target="#b8">[7]</ref> and DIM <ref type="bibr" target="#b38">[37]</ref> as TI-MI-F ) <ref type="bibr" target="#b11">[10]</ref>, momentum iterative fast gradient sign method (MI-FGSM) <ref type="bibr" target="#b8">[7]</ref>, and diverse inputs method (DIM) <ref type="bibr" target="#b target="#b6">[5]</ref> since that they are not good at generating transferable adversarial examples <ref type="bibr" target="#b8">[7]</ref>. We denote the attacks combined with our translation-invaria more likely to transfer to another black-box model.</p><p>We adopt the ensemble method proposed in <ref type="bibr" target="#b8">[7]</ref>, which fuses the logit activations of different models. We a
</ref><ref type="bibr" target="#b36">35]</ref>, and others <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b29">28]</ref>. Although the nonce
Recent methods use queries to estimate the gradient or the decision boundary of the black-box model <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b5">4]</ref> to generate adversaria
" target="#b37">36,</ref><ref type="bibr" target="#b12">11]</ref>, theoretically-certified defenses <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b36">35]</ref>, and others <ref t
normally trained models-Inception v3 <ref type="bibr" target="#b32">[31]</ref>, Inception ResNet v2 <ref type="bibr" target="#b31">[30]</ref>, ResNet 152 <ref type="bibr" target="#b13">[12]</ref> and ) <ref type="bibr" target="#b32">[31]</ref>, Inception v4 (Inc-v4), Inception ResNet v2 (IncRes-v2) <ref type="bibr" target="#b31">[30]</ref>, and ResNet v2-152 (Res-v2-152) <ref type="bibr" target="#

l examples with similar computation complexity.</p><p>Extensive experiments on the ImageNet dataset <ref type="bibr" target="#b28">[27]</ref> demonstrate that the proposed translation-invariant attack
normally trained models-Inception v3 <ref type="bibr" target="#b32">[31]</ref>, Inception ResNet v2 <ref type="bibr" target="#b31">[30]</ref>, ResNet 152 <ref type="bibr" target="#b13">[12]</ref> and ) <ref type="bibr" target="#b32">[31]</ref>, Inception v4 (Inc-v4), Inception ResNet v2 (IncRes-v2) <ref type="bibr" target="#b31">[30]</ref>, and ResNet v2-152 (Res-v2-152) <ref type="bibr" target="#

</ref><ref type="bibr" target="#b36">35]</ref>, and others <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b29">28]</ref>. Although the nonce
ions of the defense models compared with normally trained models. We adopt class activation mapping <ref type="bibr" target="#b39">[38]</ref> to visualize the attention maps of three normally trained
ost will be increased.</p><p>This paper surveys the-state-of-the-art FTL algorithms. Gal and Toledo <ref type="bibr" target="#b6">[7]</ref> also provided algorithms and data structures for flash memor
blocks is updated.</p><p>Recently, some variations of the log scheme, including those known as FAST <ref type="bibr" target="#b10">[11]</ref> and STAFF <ref type="bibr" target="#b2">[3]</ref> have bee
file system issues <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8]</ref>. Here, the problem is defined, FTL algorithms are discussed,
f the log scheme, including those known as FAST <ref type="bibr" target="#b10">[11]</ref> and STAFF <ref type="bibr" target="#b2">[3]</ref> have been proposed. In the FAST scheme, more than one logica
file system issues <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8]</ref>. Here, the problem is defined, FTL algorithms are discussed,
to their work, the present study focuses on FTL algorithms and does not discuss file system issues <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta
" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targe advantages, as mentioned in the previous two subsections, hybrid mapping approaches were introduced <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe </div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Log block scheme</head><p>Kim et al. <ref type="bibr" target="#b8">[9]</ref> proposed a log block based FTL scheme. The main objective of
" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targe advantages, as mentioned in the previous two subsections, hybrid mapping approaches were introduced <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe </div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Log block scheme</head><p>Kim et al. <ref type="bibr" target="#b8">[9]</ref> proposed a log block based FTL scheme. The main objective of
to their work, the present study focuses on FTL algorithms and does not discuss file system issues <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta
s two subsections, hybrid mapping approaches were introduced <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>. A hybrid technique, ck number 1(=4/4) and the logical page offset 0(=4%4) are calculated, and the physical block number <ref type="bibr" target="#b9">(10)</ref> is obtained using the block-level mapping table. Given that ock number 1(=4/4) and the logical page offset 0(=4%4) are calculated and the physical block number <ref type="bibr" target="#b9">(10)</ref> is obtained using the block-level mapping table. As the phy
s two subsections, hybrid mapping approaches were introduced <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>. A hybrid technique, ck number 1(=4/4) and the logical page offset 0(=4%4) are calculated, and the physical block number <ref type="bibr" target="#b9">(10)</ref> is obtained using the block-level mapping table. Given that ock number 1(=4/4) and the logical page offset 0(=4%4) are calculated and the physical block number <ref type="bibr" target="#b9">(10)</ref> is obtained using the block-level mapping table. As the phy
has not interacted before. We use the widely-used protocols <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>: Precision@K, Recall@K, and NDCG@K to evaluate the performa
can be summarized as the paths connecting the target user and item based on historical interactions <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b41">41]</ref>. For example, give al behaviors of users reflect personal interests; meanwhile, the user groups can also profile items <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b41">41]</ref>. Hence, in each mo e d ′ m is the transformation size; and we select LeakyReLU(•) as the nonlinear activation function <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b41">41]</ref>. Such aggregation y based on CF models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref><ref type="bibr" t
the target user and item based on historical interactions <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b41">41]</ref>. For example, given two paths p 1 u 1 → i 1 → u 2 → i 2 and terests; meanwhile, the user groups can also profile items <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b41">41]</ref>. Hence, in each modality (e.g., visual), we aggregate signa e select LeakyReLU(•) as the nonlinear activation function <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b41">41]</ref>. Such aggregation method assumes that different neighbors w ntegrate multi-modal features as the node features to learn the representation of each node. • NGCF <ref type="bibr" target="#b41">[41]</ref>. This method represent a novel recommendation framework to
et="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" t
dality but coordinate them with constraints. To represent the multi-modal information, Frome et al. <ref type="bibr" target="#b11">[12]</ref> proposed a deep visual-semantic embedding model which proj
multiple modalities of visual, acoustic, and textual ones <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Incorporatin
.</p><p>However, existing works on multimedia recommendation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref> mainly treat multi-modal information as a whole and incorpo derived from CF framework, such as MF <ref type="bibr" target="#b29">[30]</ref>. For instance, VBPR <ref type="bibr" target="#b16">[17]</ref> leverages visual features to enrich ID embeddings of items to two categories: CF-based (VBPR and ACF) and GCN-based (NGCF and GraphSAGE) methods.</p><p>• VBPR <ref type="bibr" target="#b16">[17]</ref>. Such model integrates the content features and ID embeddi
D, remaining invariant and serves as the connection across modalities.</p><p>Inspired by prior work <ref type="bibr" target="#b2">[3]</ref> on multi-modal representation, we first apply the idea of co entations can be grouped into two categories: joint representations and coordinated representations <ref type="bibr" target="#b2">[3]</ref>. Joint representations usually combine the various single-mo
nd tracks, and descriptions -that involve multiple modalities of visual, acoustic, and textual ones <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" ta
ti-modal recommendation algorithms mainly based on CF models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" ta
odal features. Recently, with its success in computer vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref> and natural languag
, and textual ones <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Incorporating such multi-modal information into hist
ti-modal recommendation algorithms mainly based on CF models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" ta
ti-modal recommendation algorithms mainly based on CF models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" ta
dal representations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref><ref type="bibr" t
mmendation systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Towards video reco
get="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref><ref type="bibr" target="#b40">[40]</ref>. CF-based models
<ref type="bibr" target="#b21">[22]</ref>, GraphSage <ref type="bibr" target="#b13">[14]</ref>, GAT <ref type="bibr" target="#b33">[33]</ref>) only consider homogeneous features from one data source.
d items in multiple modalities. Inspired by the recent success of graph convolution networks (GCNs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, we use the inform ties. However, existing GNN efforts (e.g., GCN <ref type="bibr" target="#b21">[22]</ref>, GraphSage <ref type="bibr" target="#b13">[14]</ref>, GAT <ref type="bibr" target="#b33">[33]</ref>) only consi as one component of the micro-video, which is consistent with the idea of standard ACF. • GraphSAGE <ref type="bibr" target="#b13">[14]</ref>. Such model is based on the general inductive framework th 5">26,</ref><ref type="bibr" target="#b28">29]</ref>. Towards video recommendation, Hamilton et al. <ref type="bibr" target="#b13">[14]</ref> proposed a general inductive framework which leverages the
s and random walks are combined to generate the representations of nodes. Concurrently, Berg et al. <ref type="bibr" target="#b4">[5]</ref> treated the recommender systems as the view of link predicti
mmendation systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Towards video reco
tent information to generate node representation for unseen data. Based on this method, Ying et al. <ref type="bibr" target="#b42">[42]</ref> developed and deployed a large-scale deep recommendation e
ponding trailers instead of the full-length videos from Youtube 5 . We use the pre-trained ResNet50 <ref type="bibr" target="#b15">[16]</ref> models to extract the visual features from key frames extr
s in computer vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref> and natural language processing <ref type="bibr" target="#b
ponding trailers instead of the full-length videos from Youtube 5 . We use the pre-trained ResNet50 <ref type="bibr" target="#b15">[16]</ref> models to extract the visual features from key frames extr
<ref type="bibr" target="#b21">[22]</ref>, GraphSage <ref type="bibr" target="#b13">[14]</ref>, GAT <ref type="bibr" target="#b33">[33]</ref>) only consider homogeneous features from one data source.
ibr" target="#b22">23]</ref> and natural language processing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">32]</ref>, neural networks are increasingly used in the multi-modal d
0]</ref> to learn the acoustic deep learning features. For textual modality, we use Sentence2Vector <ref type="bibr" target="#b0">[1]</ref> to derive the textual features from microvideos' description
N techniques to represent the users and micro-videos, which is widespread in recommendation systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta
information into a joint representation can be learned. Besides, the probabilistic graphical models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref> are another way to c
by the recent success of graph convolution networks (GCNs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, we use the information-propagation mechanism to encode hig wn feature and the interaction among different modalities. However, existing GNN efforts (e.g., GCN <ref type="bibr" target="#b21">[22]</ref>, GraphSage <ref type="bibr" target="#b13">[14]</ref>, GAT icro-videos, which is widespread in recommendation systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar
tent information to generate node representation for unseen data. Based on this method, Ying et al. <ref type="bibr" target="#b42">[42]</ref> developed and deployed a large-scale deep recommendation e
micro-video. In terms of acoustic modality, we separate audio tracks with FFmpeg 6 and adopt VGGish <ref type="bibr" target="#b19">[20]</ref> to learn the acoustic deep learning features. For textual
r's short-term search history becomes more important as the search session progresses. White et al. <ref type="bibr" target="#b50">[51]</ref> reported the use of users' on-task behavior yielded promis us clicks or queries <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>, or manually crafted rules are introduced to characterize t ssion to re-rank document for future queries. White et al. <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> develop a rich set of statistical features to quantify cont et="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b50">51]</ref>. However, as the users' information need and behavior patte
e recent success of neural network based retrieval solutions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar t <ref type="bibr" target="#b42">[43]</ref> 0.242 0.224 0.212 0.275 0.332 Single-task Learning DRMM <ref type="bibr" target="#b13">[14]</ref> 0.201 0.228 0.129 0.223 0.264 DSSM <ref type="bibr" target
<ref type="bibr" target="#b37">[38]</ref>. Then, a bidirectional recurrent neural network (BiLSTM) <ref type="bibr" target="#b41">[42]</ref> with an inner-attention mechanism <ref type="bibr" target=
aditional IR-models BM25 <ref type="bibr" target="#b39">[40]</ref> 0.230 0.206 0.206 0.269 0.319 QL <ref type="bibr" target="#b38">[39]</ref> 0.195 0.166 0.166 0.213 0.276 FixInt <ref type="bibr" targ consider BM25 <ref type="bibr" target="#b39">[40]</ref>, Query likelihood based Language model (QL) <ref type="bibr" target="#b38">[39]</ref>, and a context-sensitive ranking model FixInt <ref type="b
t t i ).</formula><p>This can be readily estimated by the decoder in a sequence to sequence network <ref type="bibr" target="#b47">[48]</ref>.</p><p>We use the search context attentive vector to initi
<ref type="bibr" target="#b37">[38]</ref>. Then, a bidirectional recurrent neural network (BiLSTM) <ref type="bibr" target="#b41">[42]</ref> with an inner-attention mechanism <ref type="bibr" target=
t="#b19">[20]</ref> and an average of 62% terms in a query are retained from their previous queries <ref type="bibr" target="#b44">[45]</ref>. Motivated by this, we predict the t-th word in the next q
g user search intent <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. A rich body of research has explored different forms of co rmation for systems to improve their retrieval performance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar "bibr" target="#b29">30]</ref>. This further introduces the compounding concepts of in-session task <ref type="bibr" target="#b25">[26]</ref> and across-session task <ref type="bibr" target="#b48">[49
n="4.1">Dataset and Experimental Setups</head><p>We conduct experiments on the AOL search log data <ref type="bibr" target="#b36">[37]</ref>. Following <ref type="bibr" target="#b45">[46]</ref>, we u
rs and a system.</p><p>Encouraged by the recent success of neural network based retrieval solutions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ ize of 3 or</p></note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p><ref type="bibr" target="#b4">5)</ref> reported in the respective papers.</p></note> 			<note xmlns=
CARS models the relatedness between these two tasks via a regularized multi-task learning approach <ref type="bibr" target="#b9">[10]</ref>. We evaluate CARS on the AOL search log, the largest public the search context representations, we adopt the regularization based multi-task learning technique <ref type="bibr" target="#b9">[10]</ref> and decompose W u 1 (defined in Eq (4)) and W ? 1 (defined
" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>. Hidasi et al. <ref type="bibr" target="#b4">[5]</ref> use d each item a fixed weight based on the relative distance with response to the target item. Li et al. <ref type="bibr" target="#b9">[10]</ref> propose an RNN based encoder-decoder model (NARM), which ta transaction data is used in this study.</p><p>Following <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b9">[10]</ref>, we filter out sessions of length 1 and items that appear l
arn both long-term interests and short-term interests of such implicit feedbacks. As Jannach et al. <ref type="bibr" target="#b6">[7]</ref> noted that both the users' short-term and long-term interest
of RNNs in session-based recommendation tasks has led to significant progress in the past few years <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>. Although RNN models s a sequence of items, without explicitly taking into account that users' interests drift with time <ref type="bibr" target="#b5">[6]</ref>, which could be problematic in practice. For example, if a s ls are introduced to capture relevance between items and more accurate general interests. Hu et al. <ref type="bibr" target="#b5">[6]</ref> propose a neural network with wide-in-wideout structure (SWI are only interested in viewing recommendations on the first page of real-world recommender systems <ref type="bibr" target="#b5">[6]</ref>. In order to verify the performance of our proposed STAMP mo in SRS research field <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>. Hidasi et al. <ref ty
t.</p><p>Deep neural networks have proven to be very effective in modeling sequential data recently <ref type="bibr" target="#b8">[9]</ref>. Inspired by recent advances in natural language processing
the uncertainty inherent in user behavior and the limited information provided by browser sessions <ref type="bibr" target="#b17">[18]</ref>.</p><p>Based on existing literature, almost all the RNN-ba
RNN models have been proven useful in capturing users' general interests from a sequence of actions <ref type="bibr" target="#b19">[20]</ref>, learning to predict from sessions is still a challenging tation technique to improve the performance of the RNNs for session-based recommendation. Yu et al. <ref type="bibr" target="#b19">[20]</ref>propose a dynamic recurrent model, which applies RNN to lea idental high similarities between rarely visited items as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
type="bibr" target="#b8">[9]</ref>. Inspired by recent advances in natural language processing area <ref type="bibr" target="#b15">[16]</ref>, some deep learning based solutions have been developed an
ns. The third approach is the Markov chain (MC) based models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>, which utilize sequential connections between the user acti
ad, only some positive observations (e.g. purchases or clicks) are available to the decision makers <ref type="bibr" target="#b3">[4]</ref>. In the past few years, an increasing amount of research att A constraint is included to avoid coincidental high similarities between rarely visited items as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. </p></div> <div xml
s based on users' whole purchase/click history. For example, the Matrix Factorization (MF) approach <ref type="bibr" target="#b7">[8]</ref> uses latent vectors to represent general interests, which ar
onsisting of the whole historical transaction data. Another approach is called neighborhood methods <ref type="bibr" target="#b13">[14]</ref>, which try to make recommendations based on item similarit RS model that always recommends items based on occurrence frequency in the training set. • Item-KNN <ref type="bibr" target="#b13">[14]</ref>: An item-to-item model which recommends items similar to t
t.</p><p>Deep neural networks have proven to be very effective in modeling sequential data recently <ref type="bibr" target="#b8">[9]</ref>. Inspired by recent advances in natural language processing
s based on users' whole purchase/click history. For example, the Matrix Factorization (MF) approach <ref type="bibr" target="#b7">[8]</ref> uses latent vectors to represent general interests, which ar
chitectures are not designed to distinguish and exploit these two types of interests simultaneously <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this study, we consider to solve this problem by
w items at once, the relevant item should be amongst the first few items in the recommendation list <ref type="bibr" target="#b11">[12]</ref>. We therefore evaluate the recommendation quality in terms
based on the sequence of the actions in the current session <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>.Recent studies have highlighted the importance of using rec n purpose(general interests) in a given session. Another recent related work is the Time-LSTM model <ref type="bibr" target="#b20">[21]</ref> which is a variant of the LSTM. Time-LSTM considers both s
users' next actions (click on an item) based on the sequence of the actions in the current session <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>.Recent studies have f which represent the state-of-the-art in SRS research field <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target= ,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>. Hidasi et al. <ref type="bibr" target="#b4">[5]</ref> use deep recurrent neural networks with a gated recurrent un om the CIKM Cup 2016 2 , for which only the transaction data is used in this study.</p><p>Following <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b9">[10]</ref>, we filter out
ision, then the user is very likely to visit another digital camera brand catalog in the next move. <ref type="bibr" target="#b1">(2)</ref> If the current action is to add a camera into the shopping c d our model can effectively utilize such information through the temporal interests representation. <ref type="bibr" target="#b1">(2)</ref> The proposed attention mechanism can effectively capture lon olutions have been developed and some of which represent the state-of-the-art in SRS research field <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target
t.</p><p>Deep neural networks have proven to be very effective in modeling sequential data recently <ref type="bibr" target="#b8">[9]</ref>. Inspired by recent advances in natural language processing
ad, only some positive observations (e.g. purchases or clicks) are available to the decision makers <ref type="bibr" target="#b3">[4]</ref>. In the past few years, an increasing amount of research att A constraint is included to avoid coincidental high similarities between rarely visited items as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. </p></div> <div xml
o improve the SRS models by taking into consideration of both type of user interests. Rendle et al. <ref type="bibr" target="#b12">[13]</ref> proposed a hybrid model FPMC, which combined the power of
based on the sequence of the actions in the current session <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>.Recent studies have highlighted the importance of using rec n purpose(general interests) in a given session. Another recent related work is the Time-LSTM model <ref type="bibr" target="#b20">[21]</ref> which is a variant of the LSTM. Time-LSTM considers both s
olutional kernels such as a fixed window <ref type="bibr" target="#b5">(Collobert et al. 2011;</ref><ref type="bibr" target="#b12">Kalchbrenner and Blunsom 2013)</ref>. When using such kernels, it is e="bibr" target="#b17">Mikolov (2012)</ref> uses recurrent neural network to build language models. <ref type="bibr" target="#b12">Kalchbrenner and Blunsom (2013)</ref> proposed a novel recurrent netw
a text word by word and stores the semantics of all the previous text in a fixed-sized hidden layer <ref type="bibr" target="#b7">(Elman 1990</ref>). The advantage of RecurrentNN is the ability to bet
dding, some composition-based methods are proposed to capture the semantic representation of texts. <ref type="bibr" target="#b23">Socher et al. (2011a;</ref><ref type="bibr" target="#b24">2011b;</ref l. (2011b)</ref> use semi-supervised recursive autoencoders to predict the sentiment of a sentence. <ref type="bibr" target="#b23">Socher et al. (2011a)</ref> proposed a method for paraphrase detectio
(Cover and Thomas 2012)</ref>, pLSA <ref type="bibr" target="#b5">(Cai and Hofmann 2003)</ref>, LDA <ref type="bibr" target="#b9">(Hingmire et al. 2013)</ref>, are applied to select more discriminativ n several classification tasks. We select two methods as the methods for comparison: ClassifyLDA-EM <ref type="bibr" target="#b9">(Hingmire et al. 2013</ref>) and Labeled-LDA (Li, Sun, and Zhang 2008) the bydate version and select four major categories (comp, politics, rec, and religion) followed by<ref type="bibr" target="#b9">Hingmire et al. (2013)</ref>.ACL Anthology Network 3 This dataset cont
dding for w i . We use the embedding e because some speed-up approaches (e.g., hierarchical softmax <ref type="bibr" target="#b19">(Morin and Bengio 2005)</ref>) will be used here, and e is not calcul
e="bibr" target="#b17">Mikolov 2012;</ref><ref type="bibr" target="#b5">Collobert et al. 2011;</ref><ref type="bibr" target="#b11">Huang et al. 2012;</ref><ref type="bibr">Mikolov et al. 2013)</ref>. d embeddings and subsequently applies a softmax layer. The weight for each word is its tfidf value. <ref type="bibr" target="#b11">Huang et al. (2012)</ref> also used this strategy as the global conte
cument set and class D is the correct class of document D.</p><p>We use stochastic gradient descent <ref type="bibr" target="#b4">(Bottou 1991)</ref> to optimize the training target. In each step, we
(Cover and Thomas 2012)</ref>, pLSA <ref type="bibr" target="#b5">(Cai and Hofmann 2003)</ref>, LDA <ref type="bibr" target="#b9">(Hingmire et al. 2013)</ref>, are applied to select more discriminativ n several classification tasks. We select two methods as the methods for comparison: ClassifyLDA-EM <ref type="bibr" target="#b9">(Hingmire et al. 2013</ref>) and Labeled-LDA (Li, Sun, and Zhang 2008) the bydate version and select four major categories (comp, politics, rec, and religion) followed by<ref type="bibr" target="#b9">Hingmire et al. (2013)</ref>.ACL Anthology Network 3 This dataset cont
ure the semantic representation of texts. <ref type="bibr" target="#b23">Socher et al. (2011a;</ref><ref type="bibr" target="#b24">2011b;</ref><ref type="bibr">2013)</ref> proposed the Recursive Neura pre-trained word embeddings, neural networks demonstrate their great performance in many NLP tasks. <ref type="bibr" target="#b24">Socher et al. (2011b)</ref> use semi-supervised recursive autoencoder
arget="#b11">Huang et al. (2012)</ref> also used this strategy as the global context in their task. <ref type="bibr" target="#b13">Klementiev, Titov, and Bhattarai (2012)</ref> used this in crosslingu

uniform distribution. The magnitude of the maximum or minimum equals the square root of the "fanin" <ref type="bibr" target="#b21">(Plaut and Hinton 1987</ref>). The number is the network node of the
omponent in many applications, such as web searching, information filtering, and sentiment analysis <ref type="bibr" target="#b0">(Aggarwal and Zhai 2012)</ref>. Therefore, it has attracted considerab
ip-gram model to pre-train the word embedding. this model is the state-of-the-art in many NLP tasks <ref type="bibr" target="#b1">(Baroni, Dinu, and Kruszewski 2014)</ref>. The Skipgram model trains t
cument set and class D is the correct class of document D.</p><p>We use stochastic gradient descent <ref type="bibr" target="#b4">(Bottou 1991)</ref> to optimize the training target. In each step, we
arget="#b11">Huang et al. (2012)</ref> also used this strategy as the global context in their task. <ref type="bibr" target="#b13">Klementiev, Titov, and Bhattarai (2012)</ref> used this in crosslingu

uniform distribution. The magnitude of the maximum or minimum equals the square root of the "fanin" <ref type="bibr" target="#b21">(Plaut and Hinton 1987</ref>). The number is the network node of the
distinguish the meaning. Although high-order ngrams and more complex features (such as tree kernels <ref type="bibr" target="#b22">(Post and Bergsma 2013)</ref>) are designed to capture more contextua eatures have been designed, such as part-of-speech tags, noun phrases (Lewis 1992) and tree kernels <ref type="bibr" target="#b22">(Post and Bergsma 2013)</ref>. Feature selection aims at deleting noi et="#b9">(Hingmire et al. 2013</ref>) and Labeled-LDA (Li, Sun, and Zhang 2008).</p><p>Tree Kernels <ref type="bibr" target="#b22">Post and Bergsma (2013)</ref> used various tree kernels as features. contains scientific documents published by the ACL and by related organizations. It is annotated by<ref type="bibr" target="#b22">Post and Bergsma (2013)</ref> with the five most common native langua
omponent in many applications, such as web searching, information filtering, and sentiment analysis <ref type="bibr" target="#b0">(Aggarwal and Zhai 2012)</ref>. Therefore, it has attracted considerab
l. 2003)</ref>. Recent research <ref type="bibr" target="#b10">(Hinton and Salakhutdinov 2006;</ref><ref type="bibr" target="#b8">Erhan et al. 2010)</ref> shows that neural networks can converge to a
ing word representations have been proposed <ref type="bibr" target="#b2">(Bengio et al. 2003;</ref><ref type="bibr" target="#b18">Mnih and Hinton 2007;</ref><ref type="bibr" target="#b17">Mikolov 201
</p><p>Finally, a recent trend in computer graphics has been the use of rendered images as textures <ref type="bibr" target="#b2">[3]</ref>. As a result, it has become desirable to unify the framebuff accessed in parallel is possible if the texels are stored in a morton order within the cache lines <ref type="bibr" target="#b2">[3]</ref>. Morton order implies that the texels are stored in 2x2 bloc
rency <ref type="bibr" target="#b6">[7]</ref>, and shadows <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>One characteristic of texture mapping is that textur
ta that is actively in use at a particular time. Most applications have a hierarchy of working sets <ref type="bibr" target="#b5">[6]</ref>. In a graph of miss rate versus cache size, the different le
pe="bibr" target="#b8">[9]</ref>, transparency <ref type="bibr" target="#b6">[7]</ref>, and shadows <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>One charact
Examples of recent developments in this area are the Visual Instruction Set (VIS) in UltraSPARC TM <ref type="bibr" target="#b15">[16]</ref> and FBRAM <ref type="bibr" target="#b17">[18]</ref> from S ditions of visualization instructions such as MMX <ref type="bibr" target="#b18">[19]</ref> and VIS <ref type="bibr" target="#b15">[16]</ref> and deep pipelines have shifted the bottleneck away from c
its ability to substantially enhance the realism and visual complexity of computergenerated imagery <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>. Examples of this
ration, Magic Carpet <ref type="bibr" target="#b11">[12]</ref> from MIPS Technologies, and Talisman <ref type="bibr" target="#b12">[13]</ref> from Microsoft Corporation. In this paper, we focus on one en and the memory bandwidth is met.</p><p>One solution for hiding the memory latency is proposed in <ref type="bibr" target="#b12">[13]</ref>. The basic idea is to compute the texel addresses far in a guaranteed performance under worst-case conditions, although this philosophy is beginning to change <ref type="bibr" target="#b12">[13]</ref>. By using techniques such as (i) block-based representatio
its ability to substantially enhance the realism and visual complexity of computergenerated imagery <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>. Examples of this
f type="bibr" target="#b20">[21]</ref>, bumps <ref type="bibr" target="#b8">[9]</ref>, transparency <ref type="bibr" target="#b6">[7]</ref>, and shadows <ref type="bibr" target="#b19">[20,</ref><ref t
parameters including reflection of the environment <ref type="bibr" target="#b20">[21]</ref>, bumps <ref type="bibr" target="#b8">[9]</ref>, transparency <ref type="bibr" target="#b6">[7]</ref>, and s
ration, Magic Carpet <ref type="bibr" target="#b11">[12]</ref> from MIPS Technologies, and Talisman <ref type="bibr" target="#b12">[13]</ref> from Microsoft Corporation. In this paper, we focus on one en and the memory bandwidth is met.</p><p>One solution for hiding the memory latency is proposed in <ref type="bibr" target="#b12">[13]</ref>. The basic idea is to compute the texel addresses far in a guaranteed performance under worst-case conditions, although this philosophy is beginning to change <ref type="bibr" target="#b12">[13]</ref>. By using techniques such as (i) block-based representatio
ent works have focused on learning deep embeddings that can be used as universal object descriptors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ effect of incorporating the CF into the fully-convolutional Siamese framework of Bertinetto et al. <ref type="bibr" target="#b2">[3]</ref>. We find that the CF does not improve results for networks t e performance. For our method, we prefer to build upon the fully-convolutional Siamese architecture <ref type="bibr" target="#b2">[3]</ref>, as it enforces the prior that the appearance similarity fun .">Fully-convolutional Siamese networks</head><p>Our starting point is a network similar to that of <ref type="bibr" target="#b2">[3]</ref>, which we later modify in order to allow the model to be int t is necessary to combine this with a procedure that describes the logic of the tracker. Similar to <ref type="bibr" target="#b2">[3]</ref>, we employ a simplistic tracking algorithm to assess the uti r during training. We first compare against the symmetric Siamese architecture of Bertinetto et al. <ref type="bibr" target="#b2">[3]</ref>. We then compare the endto-end trained CFNet to a variant wh random seeds, this would require significantly more resources. Our baseline diverges slightly from <ref type="bibr" target="#b2">[3]</ref> in two ways. Firstly, we reduce the total stride of the netw ). We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s <ref type="bibr" target="#b2">[3]</ref>, Staple <ref type="bibr" target="#b1">[2]</ref> and LCT <ref s="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>We follow the procedure of <ref type="bibr" target="#b2">[3]</ref> to minimize the loss (equation 2) through SGD, with the Xavi the following ReLU but not the following pooling layer (if any).Our baseline diverges slightly from<ref type="bibr" target="#b2">[3]</ref> in two ways. Firstly, we reduce the total stride of the netw e xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that this differs from<ref type="bibr" target="#b2">[3]</ref>, in which the target object and search area were instead den ve been introduced <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3]</ref>, raising interest in the tracking community for their simplic rget="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b2">3]</ref> with CNN features, as proposed in previous work <ref type="bi
atch from the surrounding patches by solving a large ridge regression problem extremely efficiently <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>. It has proved to be r to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div> <div xmln p://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Since the seminal work of Bolme et al. <ref type="bibr" target="#b3">[4]</ref>, the Correlation Filter has enjoyed great popularity within To reduce the effect of circular boundaries, the feature map x is pre-multiplied by a cosine window <ref type="bibr" target="#b3">[4]</ref> and the final template is cropped <ref type="bibr" target="#
rget="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2]</ref>), where its efficiency enables a tracker to adapt its interna ibr" target="#b24">[25]</ref>, ensembling of multiple cues <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>, optical flow <ref type="bibr" target="#b27">[28]</ref>).</p> rt trackers that can operate in realtime: SiamFC-3s <ref type="bibr" target="#b2">[3]</ref>, Staple <ref type="bibr" target="#b1">[2]</ref> and LCT <ref type="bibr" target="#b21">[22]</ref>. We also i
olution to an optimization problem during training has been previously investigated. Ionescu et al. <ref type="bibr" target="#b13">[14]</ref> and Murray <ref type="bibr" target="#b23">[24]</ref> have s the linear map which is the adjoint of the differential. This property was used by Ionescu et al. <ref type="bibr" target="#b13">[14]</ref> to compute backpropagation maps using matrix differential
1,</ref><ref type="bibr" target="#b8">9]</ref> and augmenting the objective with a more robust loss <ref type="bibr" target="#b25">[26]</ref>. For the sake of simplicity, in this work we adopt the bas
ariant employs features taken from a network trained to solve the ImageNet classification challenge <ref type="bibr" target="#b26">[27]</ref>. The results show that these features, which are often the tialization and using mini-batches of size 8. We use all the 3862 training videos of ImageNet Video <ref type="bibr" target="#b26">[27]</ref>, containing more than 1 million annotated frames, with mul
orithm invites a comparison to meta-learning. Recent works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b0">1]</ref> have proposed feed-forward architectures that can be interpre
descent (SGD), the workhorse of deep network optimization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>. The extremely limi target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar Furthermore, SGD is quite expensive for online adaptation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>A possible answer to these shortcomings is to have n onal improvements which can often be found in the tracking literature (e.g. bounding box regression <ref type="bibr" target="#b24">[25]</ref>, ensembling of multiple cues <ref type="bibr" target="#b21
the discriminative power of CNN features trained offline. This has been done in several works (e.g. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" targ "#b14">15,</ref><ref type="bibr" target="#b7">8]</ref>, incorporating multi-resolution feature maps <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9]</ref> and augmenting the o 21">22,</ref><ref type="bibr" target="#b2">3]</ref> with CNN features, as proposed in previous work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref>. However, these earl ombining CFs with CNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar
l object descriptors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar Correlation Filter.</p><p>Recently, several methods based on Siamese networks have been introduced <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta cues <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>, optical flow <ref type="bibr" target="#b27">[28]</ref>).</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
descent (SGD), the workhorse of deep network optimization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>. The extremely limi target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar Furthermore, SGD is quite expensive for online adaptation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>A possible answer to these shortcomings is to have n onal improvements which can often be found in the tracking literature (e.g. bounding box regression <ref type="bibr" target="#b24">[25]</ref>, ensembling of multiple cues <ref type="bibr" target="#b21
lving a large ridge regression problem extremely efficiently <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>. It has proved to be highly successful in object tracking ( ared to the conventional choice of a fixed Gaussian response <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n the training feature map x = f ρ (x ′ ) by solving a ridge regression problem in the Fourier domain <ref type="bibr" target="#b12">[13]</ref>. Its effect can be understood as crafting a discriminative t with each circular shift of the image x * δ −u is as close as possible to a desired response y[u] <ref type="bibr" target="#b12">[13]</ref>, minimizing u∈U</p><formula xml:id="formula_3">( x * δ −u hat is equivalent to eq. 5. The solution to eq. 6 can be computed efficiently in the Fourier domain <ref type="bibr" target="#b12">[13]</ref>,</p><formula xml:id="formula_9">     k = 1 n ( x * • in Figure <ref type="figure">1</ref> corresponds exactly to the operation of a standard CF tracker <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" targ
for example by mitigating the effect of periodic boundaries <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8]</ref>, incorporating multi- tent with the findings of Danelljan et al. <ref type="bibr" target="#b7">[8]</ref> and Kiani et al. <ref type="bibr" target="#b14">[15]</ref>. To reduce the effect of circular boundaries, the feature
periodic boundaries <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8]</ref>, incorporating multi-resolution feature maps <ref type="bibr" region of context in the training image, which is consistent with the findings of Danelljan et al. <ref type="bibr" target="#b7">[8]</ref> and Kiani et al. <ref type="bibr" target="#b14">[15]</ref>.
l object descriptors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar Correlation Filter.</p><p>Recently, several methods based on Siamese networks have been introduced <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta cues <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>, optical flow <ref type="bibr" target="#b27">[28]</ref>).</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
lving a large ridge regression problem extremely efficiently <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>. It has proved to be highly successful in object tracking ( ared to the conventional choice of a fixed Gaussian response <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n the training feature map x = f ρ (x ′ ) by solving a ridge regression problem in the Fourier domain <ref type="bibr" target="#b12">[13]</ref>. Its effect can be understood as crafting a discriminative t with each circular shift of the image x * δ −u is as close as possible to a desired response y[u] <ref type="bibr" target="#b12">[13]</ref>, minimizing u∈U</p><formula xml:id="formula_3">( x * δ −u hat is equivalent to eq. 5. The solution to eq. 6 can be computed efficiently in the Fourier domain <ref type="bibr" target="#b12">[13]</ref>,</p><formula xml:id="formula_9">     k = 1 n ( x * • in Figure <ref type="figure">1</ref> corresponds exactly to the operation of a standard CF tracker <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" targ
descent (SGD), the workhorse of deep network optimization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>. The extremely limi target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar Furthermore, SGD is quite expensive for online adaptation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>A possible answer to these shortcomings is to have n onal improvements which can often be found in the tracking literature (e.g. bounding box regression <ref type="bibr" target="#b24">[25]</ref>, ensembling of multiple cues <ref type="bibr" target="#b21
the discriminative power of CNN features trained offline. This has been done in several works (e.g. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" targ "#b14">15,</ref><ref type="bibr" target="#b7">8]</ref>, incorporating multi-resolution feature maps <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9]</ref> and augmenting the o 21">22,</ref><ref type="bibr" target="#b2">3]</ref> with CNN features, as proposed in previous work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref>. However, these earl ombining CFs with CNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar
rget="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5]</ref>. These methods use a
orithm invites a comparison to meta-learning. Recent works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b0">1]</ref> have proposed feed-forward architectures that can be interpre
olution to an optimization problem during training has been previously investigated. Ionescu et al. <ref type="bibr" target="#b13">[14]</ref> and Murray <ref type="bibr" target="#b23">[24]</ref> have s the linear map which is the adjoint of the differential. This property was used by Ionescu et al. <ref type="bibr" target="#b13">[14]</ref> to compute backpropagation maps using matrix differential
rget="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2]</ref>), where its efficiency enables a tracker to adapt its interna ibr" target="#b24">[25]</ref>, ensembling of multiple cues <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>, optical flow <ref type="bibr" target="#b27">[28]</ref>).</p> rt trackers that can operate in realtime: SiamFC-3s <ref type="bibr" target="#b2">[3]</ref>, Staple <ref type="bibr" target="#b1">[2]</ref> and LCT <ref type="bibr" target="#b21">[22]</ref>. We also i
, and those have been very promising <ref type="bibr" target="#b17">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b6">Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Gilmer et al b28">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b17">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b6">Hamilton et al., 2017)</ref>.</p><p>Yet, such aggregation schemes some • u∈ N (v) (deg(v)deg(u)) −1/2 h (l−1) u (2)</formula><p>where deg(v) is the degree of node v in G. <ref type="bibr" target="#b6">Hamilton et al. (2017)</ref> derived a variant of GCN that also works and can be viewed as a form of a "skip connection" between different layers.For COMBINE, GraphSAGE <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref> uses concatenation after a feature trans o select the important neighbors via an attention mechanism. The max-pooling operation in GraphSAGE <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref> implicitly selects the important nodes. ords features for each document (node) and citation links (edges) between documents. (II) On Reddit <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>, the task is to predict the community to ataset contains word vectors as node features. (III) For protein-protein interaction networks (PPI) <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref> We compare against three baselines: Grap lutional Networks (GCN) <ref type="bibr" target="#b17">(Kipf &amp; Welling, 2017)</ref>, Graph-SAGE <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref> and Graph Attention Networks (GAT) <ref r gives 6 JK-Net variants. We follow exactly the same setting of GraphSAGE as in the original paper <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>, where the model consists of 2 hidden la the well-behaved middle-sized communities to avoid the noisy cores and tree-like small communities <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>. As a result, this graph is more regular

xtremely useful for many applications, such as node classification, clustering, and link prediction <ref type="bibr" target="#b25">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b24">Monti et a
on of the ReLU activations similar to that in <ref type="bibr" target="#b14">(Kawaguchi, 2016;</ref><ref type="bibr" target="#b2">Choromanska et al., 2015)</ref>, we can draw connections between GCNs
>We evaluate JK-Nets on four benchmark datasets. (I) The task on citation networks (Citeseer, Cora) <ref type="bibr" target="#b27">(Sen et al., 2008)</ref> is to classify academic papers into differen

motivated by spectral graph convolutions <ref type="bibr" target="#b7">(Hammond et al., 2011;</ref><ref type="bibr" target="#b3">Defferrard et al., 2016)</ref>, are a specific instantiation of this f br" target="#b0">(Bruna et al., 2014;</ref><ref type="bibr" target="#b29">Shuman et al., 2013;</ref><ref type="bibr" target="#b3">Defferrard et al., 2016)</ref>. A major drawback of the spectral metho

motivated by spectral graph convolutions <ref type="bibr" target="#b7">(Hammond et al., 2011;</ref><ref type="bibr" target="#b3">Defferrard et al., 2016)</ref>, are a specific instantiation of this f br" target="#b0">(Bruna et al., 2014;</ref><ref type="bibr" target="#b29">Shuman et al., 2013;</ref><ref type="bibr" target="#b3">Defferrard et al., 2016)</ref>. A major drawback of the spectral metho
ated GNN <ref type="bibr" target="#b21">(Li et al., 2016)</ref> uses the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b1">(Cho et al., 2014)</ref>. Another wellknown variant of skip connection
and the small communities respectively <ref type="bibr" target="#b20">(Leskovec et al., 2009;</ref><ref type="bibr" target="#b23">Maehara et al., 2014;</ref><ref type="bibr" target="#b31">Tsonis et a
">(Hamilton et al., 2017)</ref> uses concatenation after a feature transform.</p><p>Column Networks <ref type="bibr" target="#b26">(Pham et al., 2017)</ref> interpolate the neighborhood representation
at a given node, a well-understood phenomenon as a function of the graph structure and eigenvalues <ref type="bibr" target="#b22">(Lovász, 1993)</ref>. For instance, in some cases and applications, a by the random walk's mixing time, which changes dramatically on subgraphs with different structures <ref type="bibr" target="#b22">(Lovász, 1993)</ref>. Thus, the same number of iterations (layers) ca and can be bounded by the spectral gap (or the conductance) of the random walk's transition matrix <ref type="bibr" target="#b22">(Lovász, 1993)</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1
r" target="#b6">Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Gilmer et al., 2017;</ref><ref type="bibr" target="#b32">Veličković et al., 2018;</ref><ref type="bibr" target="#b15">Kearnes fluenced by some directions of expansion more than the others.</p><p>Graph Attention Networks (GAT) <ref type="bibr" target="#b32">(Veličković et al., 2018)</ref> and VAIN <ref type="bibr" target="#b1 SAGE <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref> and Graph Attention Networks (GAT) <ref type="bibr" target="#b32">(Veličković et al., 2018)</ref>.</p></div> <div xmlns="http://www.tei
t we will discuss.</p><p>With a randomization assumption of the ReLU activations similar to that in <ref type="bibr" target="#b14">(Kawaguchi, 2016;</ref><ref type="bibr" target="#b2">Choromanska et a
motivated by spectral graph convolutions <ref type="bibr" target="#b7">(Hammond et al., 2011;</ref><ref type="bibr" target="#b3">Defferrard et al., 2016)</ref>, are a specific instantiation of this f br" target="#b0">(Bruna et al., 2014;</ref><ref type="bibr" target="#b29">Shuman et al., 2013;</ref><ref type="bibr" target="#b3">Defferrard et al., 2016)</ref>. A major drawback of the spectral metho
target="#b17">(Kipf &amp; Welling, 2017)</ref>, initially motivated by spectral graph convolutions <ref type="bibr" target="#b7">(Hammond et al., 2011;</ref><ref type="bibr" target="#b3">Defferrard e
yright 2018 by the author(s). 2017; <ref type="bibr" target="#b5">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b30">Tang et al., 2015)</ref>.</p><p>Recent works focus on deep learning a
target="#b4">Gilmer et al., 2017;</ref><ref type="bibr" target="#b32">Veličković et al., 2018;</ref><ref type="bibr" target="#b15">Kearnes et al., 2016)</ref>. These models learn to iteratively aggreg
> interpolate the neighborhood representation and the node's previous representation, and Gated GNN <ref type="bibr" target="#b21">(Li et al., 2016)</ref> uses the Gated Recurrent Unit (GRU) <ref type
xtremely useful for many applications, such as node classification, clustering, and link prediction <ref type="bibr" target="#b25">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b24">Monti et a
et="#b17">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b6">Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Gilmer et al., 2017;</ref><ref type="bibr" target="#b32">Veličković et ="bibr" target="#b3">Defferrard et al., 2016)</ref>, are a specific instantiation of this framework <ref type="bibr" target="#b4">(Gilmer et al., 2017)</ref>, of the form</p><formula xml:id="formula_3
d effects are modeled correctly and bandwidth limitations are enforced in our model as described in <ref type="bibr" target="#b15">[16]</ref>. The memory bus has a bandwidth of 4.5 GB/s.</p><p>The bas
d Section 2.1.</p><p>3 Similar results were reported by <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b17">[18]</ref>. All average IPC results in this paper are computed as geo
aggressiveness is Dahlgren et al.'s paper that proposed adaptive sequential (next-line) prefetching <ref type="bibr" target="#b3">[4]</ref> for multiprocessors. This mechanism implemented two counters ide prefetchers. Previous adaptive mechanisms were applicable to only simple sequential prefetchers <ref type="bibr" target="#b3">[4]</ref>.</p><p>Future work can incorporate other important metrics,
<ref type="table" target="#tab_1">1</ref> and Section 2.1.</p><p>3 Similar results were reported by <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b17">[18]</ref>. All average I
aggressiveness is Dahlgren et al.'s paper that proposed adaptive sequential (next-line) prefetching <ref type="bibr" target="#b3">[4]</ref> for multiprocessors. This mechanism implemented two counters ide prefetchers. Previous adaptive mechanisms were applicable to only simple sequential prefetchers <ref type="bibr" target="#b3">[4]</ref>.</p><p>Future work can incorporate other important metrics,
t incurs a heavy overhead in terms of both hardware and complexity. We use the Bloom filter concept <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> to provide a simple
chanism does not need to keep history information for evicted L2 cache blocks.</p><p>Zhuang and Lee <ref type="bibr" target="#b24">[25]</ref> proposed to filter prefetcher-generated cache pollution by
namic program behavior even if it differs from the behavior of the compile-time profile. Lin et al. <ref type="bibr" target="#b14">[15]</ref> proposed using density vectors to determine what to prefet
namic program behavior even if it differs from the behavior of the compile-time profile. Lin et al. <ref type="bibr" target="#b14">[15]</ref> proposed using density vectors to determine what to prefet
rocessor caches rather than into separate prefetch buffers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref>. In many current processors (e.g. Intel Pentium 4 <ref type by bringing prefetched data into separate prefetch buffers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref> rather than inserting prefetched data into the L2 cache. Fi #b23">[24]</ref> and more details on the implementation of stream-based prefetching can be found in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta
ically adjust the aggressiveness of a global-historybuffer (GHB) based delta correlation prefetcher <ref type="bibr" target="#b9">[10]</ref> or a PC-based stride prefetcher <ref type="bibr" target="#b ed FDP on the C/DC (C-Zone Delta Correlation) variant of the Global History Buffer (GHB) prefetcher <ref type="bibr" target="#b9">[10]</ref>. In order to vary the aggressiveness of this prefetcher dyn se of execution, the prefetcher is tuned based on the characteristics of the phase in Nesbit et al. <ref type="bibr" target="#b9">[10]</ref>. In order to perform phase detection/prediction and identif
esigner's productivity in developing hardware accelerators <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref>. While such hardware accelerators can often deliver signific UKU <ref type="bibr" target="#b8">[8]</ref>, or QuickDough <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref> have demonstrated the benefits of overlay by improving desig hard processors <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b2">[2]</ref> are used and the integration between the processor and accel
omise of using FPGA overlays to improve designer's productivity in developing hardware accelerators <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref>. While such hardwar ref type="bibr" target="#b7">[7]</ref>, QUKU <ref type="bibr" target="#b8">[8]</ref>, or QuickDough <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref> have demonstrated t
ing overlay works, diverse choice of soft/ hard processors <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b2">[2]</ref> are used and the
" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, multi-thread soft processors <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref> and application
e and hardware acceleration for computationally intensive applications. Research works such as ZUMA <ref type="bibr" target="#b7">[7]</ref>, QUKU <ref type="bibr" target="#b8">[8]</ref>, or QuickDough
b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref> and application-specific soft processors <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref> are also extens
y intensive applications. Research works such as ZUMA <ref type="bibr" target="#b7">[7]</ref>, QUKU <ref type="bibr" target="#b8">[8]</ref>, or QuickDough <ref type="bibr" target="#b0">[1]</ref>, <ref
xists some lightweight RV32I designs such as zscale <ref type="bibr" target="#b28">[28]</ref>, GRVI <ref type="bibr" target="#b29">[29]</ref> or ORCA <ref type="bibr" target="#b6">[6]</ref> which are cessor implementation and therefore zscale is less desirable for FPGA overlay framework.</p><p>GRVI <ref type="bibr" target="#b29">[29]</ref> core, on the other hand, is an efficient, FPGAoptimized 3-
specified. Therefore in many of the existing overlay works, diverse choice of soft/ hard processors <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr"
" target="#b20">[20]</ref>, multi-thread soft processors <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref> and application-specific soft processors <ref type="bibr"
to another.</p><p>The closest work that is designed to resolve the above coupling problem is ADRES <ref type="bibr" target="#b11">[11]</ref>. Mei et al. proposed an architecture that contains a VLIW
the massive multi-level pathological information on nidus and its surrounding tissues.</p><p>WSISA <ref type="bibr" target="#b21">[21]</ref> was the first trial of moving survival prediction onto who n network. Different from previous DL-based survival models that basically act as feature extractor <ref type="bibr" target="#b21">[21]</ref>, DeepGraphSurv directly generates predicted risks. We inte e compared DeepGraphSurv with the state-of-theart deep learning based survival models on WSI. WSISA <ref type="bibr" target="#b21">[21]</ref> worked on clustered patches from WSIs, however, they simpl n little from topology. The previous GCN <ref type="bibr" target="#b4">[5]</ref> outperformed WSISA <ref type="bibr" target="#b21">[21]</ref> on most of datasets because it can aggregate node features e it can aggregate node features as graph representation of WSI according to graph structure, while <ref type="bibr" target="#b21">[21]</ref> cannot. However, <ref type="bibr" target="#b4">[5]</ref> s erns. This may explain the lift by DeepGraphSurv compared to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">21]</ref> who learn little from topology. The previous GCN <ref type=
present topological structures. However, modeling a WSI as graph is not straightforward. Cell-graph <ref type="bibr" target="#b5">[6]</ref> is infeasible for WSIs due to its huge number of cells and t
es from a generic cancer patient dataset TCGA, publicly released by The Cancer Genome Atlas project <ref type="bibr" target="#b7">[8]</ref>. The research studied what and how errors in DNA trigger the
spectral graph theory <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">20]</ref> is more applicable to irregular WSI graph. It was proved th
seline survival methods include: LASSO-Cox model <ref type="bibr" target="#b18">[18]</ref>, BoostCI <ref type="bibr" target="#b17">[17]</ref> and Multi-Task Learning model for Survival Analysis (MTLSA
I is irregular with ?(G) ?(G). A spectral convolutional filter built based on spectral graph theory <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target
r survival functions. Although it was showed that neural networks outperformed the linear Cox model <ref type="bibr" target="#b3">[4]</ref>, it cannot directly learn from pathological images. Along wi
ecome ideal data sources for training DL-based survival models. Among them, whole slide image (WSI) <ref type="bibr" target="#b11">[12]</ref> is one of the most valuable data formats due to the massiv

f convolutional neural networks (CNNs) on generic images, pathological image, as well as CT and MRI <ref type="bibr" target="#b13">[14]</ref>, have become ideal data sources for training DL-based surv
rovides invaluable information for clinical interventions.</p><p>The Cox proportional hazards model <ref type="bibr" target="#b2">[3]</ref> is most popular in survival analysis. However, the classical
tion problem.</p><p>Learning upscaling filters was briefly suggested in the footnote of Dong et.al. <ref type="bibr" target="#b5">[6]</ref>. However, the importance of integrating it into the CNN as p eration was not fully recognised and the option not explored. Additionally, as noted by Dong et al. <ref type="bibr" target="#b5">[6]</ref>, there are no efficient implementations of a convolution lay uate the power of the sub-pixel convolution layer by comparing against SRCNN's standard 9-1-5 model <ref type="bibr" target="#b5">[6]</ref>. Here, we follow the approach in <ref type="bibr" target="#b CNN's standard 9-1-5 model <ref type="bibr" target="#b5">[6]</ref>. Here, we follow the approach in <ref type="bibr" target="#b5">[6]</ref>, using relu as the activation function for our models in thi ard comparison with results from previous published results<ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b5">6]</ref>.</note> 		</body> 		<back> 			<div type="references">  				<l
g unseen data. This approach is proposed in the methods of <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b7">8]</ref>. A drawback of sparsity-based techniques is that introducing
signed features including the log-Gabor filters <ref type="bibr" target="#b47">[48]</ref>, wavelets <ref type="bibr" target="#b19">[20]</ref> and Haar features <ref type="bibr" target="#b41">[42]</ref
get="#b32">33]</ref>, satellite imaging <ref type="bibr" target="#b37">[38]</ref>, face recognition <ref type="bibr" target="#b16">[17]</ref> and surveillance <ref type="bibr" target="#b52">[53]</ref>
, i.e. with unique prior affine transformations. These can be categorised as multi-image SR methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref> and exploit explicit
of the result. An alternative family of methods are single image super-resolution (SISR) techniques <ref type="bibr" target="#b44">[45]</ref>. These techniques seek to learn implicit redundancy that i bibr" target="#b4">5]</ref> methods. A detailed review of more generic SISR methods can be found in <ref type="bibr" target="#b44">[45]</ref>. One family of approaches that has recently thrived in tac
get="#b12">13,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5]</ref> methods. A detailed review of more generic SISR methods can b /ref> BSD300 and BSD500 which provides 100 and 200 images for testing and the super texture dataset <ref type="bibr" target="#b4">[5]</ref> which provides 136 texture images. For our final models, we
get="#b42">43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" tar
icly available Xiph database 1 , which has been used to report video SR results in previous methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b22">23]</ref>. The database cont
rget="#b1">[2,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" tar
s been used to report video SR results in previous methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b22">23]</ref>. The database contains a collection of 8 HD videos approxim e exploited for video superresolution as has been shown in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b22">23]</ref>. Spatio-temporal networks are popular as they fully utilise
ly related to our approach are the Defense-GAN <ref type="bibr" target="#b36">[37]</ref> and MagNet <ref type="bibr" target="#b24">[25]</ref>, which first estimate the manifold of clean data to detect
type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, semantic segmentation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and visual question ch are then attacked using I-FGSM as:</p><p>x m+1 = clip (x m + α.sign(∇(L(τ (x m ; p), y c ; θ))). <ref type="bibr" target="#b7">(8)</ref> Here p is the ratio of transformed images to total number of
type="bibr" target="#b6">[7]</ref>, semantic segmentation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and visual question answering <ref type="bibr" target="#b9">
. S = 2 is selected due to its superior performance.CAMs Visualization: Class Activation Maps (CAMs)<ref type="bibr" target="#b57">[58]</ref> are weakly supervised localization techniques, which are h
br" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, including image classification <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, object detection < f>. Our experiments show that EDSR consistently performs better. EDSR builds on a residual learning <ref type="bibr" target="#b2">[3]</ref> scheme that specifically focuses on high-frequency patterns
type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, semantic segmentation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and visual question ch are then attacked using I-FGSM as:</p><p>x m+1 = clip (x m + α.sign(∇(L(τ (x m ; p), y c ; θ))). <ref type="bibr" target="#b7">(8)</ref> Here p is the ratio of transformed images to total number of

er-resolved image. The network considered in this work is the Enhanced Deep Super-Resolution (EDSR) <ref type="bibr" target="#b46">[47]</ref> network (trained on the DIVerse 2K resolution image (DIV2K sNet) <ref type="bibr" target="#b55">[56]</ref>, ii) Enhanced Deep Residual Network for SISR (EDSR) <ref type="bibr" target="#b46">[47]</ref> and iii) Super Resolution using Generative Adversarial Net ge super-resolution techniques-SR-ResNet, SR-GAN <ref type="bibr" target="#b55">[56]</ref> and EDSR <ref type="bibr" target="#b46">[47]</ref>. Specifically, attacked images are super-resolved to 2×, w

ignificant research has been carried out to devise defense mechanisms against these vulnerabilities <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b27">[28]</ref>. We can broadly aim to regularize a specific model's parameters through adversarial training or parameter smoothing <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bib ing the decision boundaries, thereby encompassing nearby adversarial images. Defensive distillation <ref type="bibr" target="#b16">[17]</ref> improves the model robustness in an essentially similar fa
"bibr" target="#b46">[47]</ref> network (trained on the DIVerse 2K resolution image (DIV2K) dataset <ref type="bibr" target="#b47">[48]</ref>), which uses a hierarchy of such residual blocks. While ou
p>Existing literature on training with noisy labels focuses primarily on loss correction approaches <ref type="bibr" target="#b22">(Reed et al., 2015;</ref><ref type="bibr" target="#b9">Hendrycks et a ling using the network predictions to predict hard or soft labels.</p><p>Loss correction approaches <ref type="bibr" target="#b22">(Reed et al., 2015;</ref><ref type="bibr" target="#b11">Jiang et al., pe="bibr" target="#b11">Jiang et al., 2018b)</ref>. A well-known approach is the bootstrapping loss <ref type="bibr" target="#b22">(Reed et al., 2015)</ref>, which introduces a perceptual consistency ilities used to compute it, to compensate for the incorrect guidance provided by the noisy samples. <ref type="bibr" target="#b22">(Reed et al., 2015)</ref> extend the loss with a perceptual term that is unsupervised model to implement a loss correction approach that benefits both from bootstrapping <ref type="bibr" target="#b22">(Reed et al., 2015)</ref> and mixup data augmentation <ref type="bibr ibr" target="#b33">(Zhang et al., 2017)</ref>.</p><p>The static hard bootstrapping loss proposed in <ref type="bibr" target="#b22">(Reed et al., 2015)</ref> provides a mechanism to deal with label noi ) ,<label>(10)</label></formula><p>where w i weights the model prediction z i in the loss function. <ref type="bibr" target="#b22">(Reed et al., 2015)</ref> use w i = 0.2, ∀i. We refer to this approac Reed et al., 2015)</ref> use w i = 0.2, ∀i. We refer to this approach as static hard bootstrapping. <ref type="bibr" target="#b22">(Reed et al., 2015)</ref> also proposed a static soft bootstrapping l beling. We also run our proposed approach under these conditions in Subsection 4.5 for comparison.  <ref type="bibr" target="#b22">(Reed et al., 2015)</ref>. The overall results demonstrate that apply sed dynamic hard bootstrapping exhibits better performance than the state-of-the-art static version <ref type="bibr" target="#b22">(Reed et al., 2015)</ref>. It is, however, not better than the perfor d the 300 epochs training scheme (see Subsection 4.1) . We introduce bootstrapping in epoch 105 for <ref type="bibr" target="#b22">(Reed et al., 2015)</ref> for the proposed methods, estimate the T ma
that pulls representations of noisy samples away from clean ones. Finally, mixup data augmentation <ref type="bibr" target="#b34">(Zhang et al., 2018)</ref> has recently demonstrated outstanding robu ushing the state-of-the-art one step forward by combining our approach with mixup data augmentation <ref type="bibr" target="#b34">(Zhang et al., 2018)</ref>.</p><p>4. Guiding mixup data augmentation bootstrapping <ref type="bibr" target="#b22">(Reed et al., 2015)</ref> and mixup data augmentation <ref type="bibr" target="#b34">(Zhang et al., 2018)</ref> to deal with the closed-set label noise sc i-c.org/ns/1.0"><head n="3.3.">Joint label correction and mixup data augmentation</head><p>Recently <ref type="bibr" target="#b34">(Zhang et al., 2018)</ref> proposed a data augmentation technique nam ref type="bibr" target="#b9">(Hendrycks et al., 2018)</ref>), and use the configuration reported in <ref type="bibr" target="#b34">(Zhang et al., 2018)</ref> for mixup. We outperform the related work tab_8">6</ref> shows the results of the proposed approaches M-DYR-H and MD-DYR-SH compared to mixup <ref type="bibr" target="#b34">(Zhang et al., 2018)</ref> on TinyImageNet to demonstrate that our ap onstrate that our approach is useful far from CIFAR data. The proposed approach clearly outperforms <ref type="bibr" target="#b34">(Zhang et al., 2018)</ref> for different levels of label noise, obtai ref type="bibr" target="#b11">Jiang et al., 2018b;</ref><ref type="bibr">Patrini et al., 2017;</ref><ref type="bibr" target="#b34">Zhang et al., 2018)</ref> modify either the loss directly, or the pro
he true label of a noisy sample x i may be outside S; i.e. x i may be an out-of-distribution sample <ref type="bibr" target="#b13">(Liang et al., 2018)</ref>. The remainder of this section briefly rev
ibr" target="#b21">Redmon et al., 2016;</ref><ref type="bibr" target="#b35">Zhao et al., 2017;</ref><ref type="bibr" target="#b12">Krishna et al., 2017)</ref>. Their widespread use is attributable to
raphical models <ref type="bibr" target="#b32">(Xiao et al., 2015)</ref>, Conditional Random Fields <ref type="bibr" target="#b26">(Vahdat, 2017)</ref>, or CNNs <ref type="bibr" target="#b27">(Veit et

l., 2017)</ref>. Their widespread use is attributable to their capability to model complex patterns <ref type="bibr" target="#b23">(Ren et al., 2018)</ref> when vast amounts of labeled data are availa its distribution in a feature space that benefits from training with both clean and noisy samples. <ref type="bibr" target="#b23">(Ren et al., 2018)</ref> weights each sample in the loss based on the ets of clean data during training in <ref type="bibr" target="#b11">(Jiang et al., 2018b)</ref> and <ref type="bibr" target="#b23">(Ren et al., 2018)</ref>.</p></div> <div xmlns="http://www.tei-c.org/ to formulate a robust learning procedure <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b23">Ren et al., 2018)</ref>. Curriculum learning <ref type="bibr" target= ontrary to most successful recent approaches that assume the existence of a known set of clean data <ref type="bibr" target="#b23">(Ren et al., 2018;</ref><ref type="bibr" target="#b9">Hendrycks et al bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b14">Ma et al., 2018;</ref><ref type="bibr" target="#b23">Ren et al., 2018;</ref><ref type="bibr">Wang et al., 2018b)</ref>. We
ses primarily on loss correction approaches <ref type="bibr" target="#b22">(Reed et al., 2015;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018;</ref><ref type="bibr" target="#b11">Jiang et a y samples. Other approaches modify class probabilities <ref type="bibr">(Patrini et al., 2017;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018)</ref> by estimating the noise associated with e the existence of a known set of clean data <ref type="bibr" target="#b23">(Ren et al., 2018;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018)</ref>, we propose an unsupervised model of lab ass-conditional label noise, on the other hand, has different flipping probabilities for each class <ref type="bibr" target="#b9">(Hendrycks et al., 2018)</ref>. Previous research <ref type="bibr">(Pa the loss, goes back to the predicted probabilities to correct them by multiplying by the T matrix. <ref type="bibr" target="#b9">(Hendrycks et al., 2018)</ref> corrects the predicted probabilities us ods, estimate the T matrix of <ref type="bibr">(Patrini et al., 2017)</ref> in epoch 75 (as done in <ref type="bibr" target="#b9">(Hendrycks et al., 2018)</ref>), and use the configuration reported in
mmon adverse scenario that requires attention to ensure useful visual representations can be learnt <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b29">Wang et al. ct classes. Still other approaches use curriculum learning to formulate a robust learning procedure <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b23">Ren et al., true label could be randomly maintained). Note that there is another popular label noise criterion <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr">Wang et al., 2018b)</ref> (i.e. the criterion followed in previous experiments), a criterion adopted by several other authors <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b14">Ma et al., " target="#b22">(Reed et al., 2015;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018;</ref><ref type="bibr" target="#b11">Jiang et al., 2018b)</ref>. A well-known approach is the bootstrappin ft labels.</p><p>Loss correction approaches <ref type="bibr" target="#b22">(Reed et al., 2015;</ref><ref type="bibr" target="#b11">Jiang et al., 2018b;</ref><ref type="bibr">Patrini et al., 2017;</ref rrupted data. Other approaches focus on re-weighting the contribution of noisy samples on the loss. <ref type="bibr" target="#b11">(Jiang et al., 2018b)</ref> proposes an alternating minimization fram in CIFAR-10 and CIFAR-100 with MD-DYR-SH, while the results for M-DYR-H are slightly below those of <ref type="bibr" target="#b11">(Jiang et al., 2018b)</ref> for low label noise levels in CIFAR-100. re due to the different architectures employed and the use of sets of clean data during training in <ref type="bibr" target="#b11">(Jiang et al., 2018b)</ref> and <ref type="bibr" target="#b23">(Ren e
seful for learning visual representations <ref type="bibr" target="#b18">(Pathak et al., 2017;</ref><ref type="bibr" target="#b6">Gidaris et al., 2018)</ref>; however, a recent study on the generaliza
="bibr" target="#b29">Wang et al., 2018a;</ref><ref type="bibr" target="#b31">Wu et al., 2018;</ref><ref type="bibr" target="#b10">Jiang et al., 2018a;</ref><ref type="bibr" target="#b36">Zlateski et
t a small set of clean samples is always available, which limits their applicability. Tanaka et al. <ref type="bibr" target="#b25">(Tanaka et al., 2018)</ref> have, however, recently demonstrated that he network to predict the same class to minimize the loss. We apply the regularization term used in <ref type="bibr" target="#b25">(Tanaka et al., 2018)</ref>, which seeks preventing the assignment of for the prior probabilities (i.e. p c = 1/C), while approximating h c using mini-batches as done in <ref type="bibr" target="#b25">(Tanaka et al., 2018)</ref>. We add the term ηR to * (Eq. ( <ref type sharing similar visual patterns with the true class. We followed a similar network and procedure as <ref type="bibr" target="#b25">(Tanaka et al., 2018)</ref> with ImageNet pre-trained weights and Res and ResNet-50, obtaining over 71% test accuracy, which falls short of the state-of-the-art (72.23% <ref type="bibr" target="#b25">(Tanaka et al., 2018)</ref>). We found that finetuning a pre-trained naka et al., 2018)</ref>). We found that finetuning a pre-trained network for one epoch, as done in <ref type="bibr" target="#b25">(Tanaka et al., 2018)</ref>, easily fits label noise limiting our uns ef>  We follow <ref type="bibr" target="#b33">(Zhang et al., 2017;</ref><ref type="bibr">2018;</ref><ref type="bibr" target="#b25">Tanaka et al., 2018)</ref> criterion for label noise addition, which
mmon adverse scenario that requires attention to ensure useful visual representations can be learnt <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b29">Wang et al. ct classes. Still other approaches use curriculum learning to formulate a robust learning procedure <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b23">Ren et al., true label could be randomly maintained). Note that there is another popular label noise criterion <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr">Wang et al., 2018b)</ref> (i.e. the criterion followed in previous experiments), a criterion adopted by several other authors <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b14">Ma et al., " target="#b22">(Reed et al., 2015;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018;</ref><ref type="bibr" target="#b11">Jiang et al., 2018b)</ref>. A well-known approach is the bootstrappin ft labels.</p><p>Loss correction approaches <ref type="bibr" target="#b22">(Reed et al., 2015;</ref><ref type="bibr" target="#b11">Jiang et al., 2018b;</ref><ref type="bibr">Patrini et al., 2017;</ref rrupted data. Other approaches focus on re-weighting the contribution of noisy samples on the loss. <ref type="bibr" target="#b11">(Jiang et al., 2018b)</ref> proposes an alternating minimization fram in CIFAR-10 and CIFAR-100 with MD-DYR-SH, while the results for M-DYR-H are slightly below those of <ref type="bibr" target="#b11">(Jiang et al., 2018b)</ref> for low label noise levels in CIFAR-100. re due to the different architectures employed and the use of sets of clean data during training in <ref type="bibr" target="#b11">(Jiang et al., 2018b)</ref> and <ref type="bibr" target="#b23">(Ren e
ibr" target="#b21">Redmon et al., 2016;</ref><ref type="bibr" target="#b35">Zhao et al., 2017;</ref><ref type="bibr" target="#b12">Krishna et al., 2017)</ref>. Their widespread use is attributable to
however, is not trivial and usually involves an error prone automatic or a manual labeling process <ref type="bibr" target="#b29">(Wang et al., 2018a;</ref><ref type="bibr" target="#b36">Zlateski et eful visual representations can be learnt <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b29">Wang et al., 2018a;</ref><ref type="bibr" target="#b31">Wu et al., 20
r" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b29">Wang et al., 2018a;</ref><ref type="bibr" target="#b31">Wu et al., 2018;</ref><ref type="bibr" target="#b10">Jiang et al., 20
one automatic or a manual labeling process <ref type="bibr" target="#b29">(Wang et al., 2018a;</ref><ref type="bibr" target="#b36">Zlateski et al., 2018)</ref>. These errors lead to noisy samples: sam "bibr" target="#b31">Wu et al., 2018;</ref><ref type="bibr" target="#b10">Jiang et al., 2018a;</ref><ref type="bibr" target="#b36">Zlateski et al., 2018)</ref>. Automatically obtained noisy labels hav
re <ref type="figure">3</ref> shows uniform manifold approximation and projection (UMAP) embeddings <ref type="bibr" target="#b16">(McInnes et al., 2018)</ref> of the 512 features in the penultimate f
however, is not trivial and usually involves an error prone automatic or a manual labeling process <ref type="bibr" target="#b29">(Wang et al., 2018a;</ref><ref type="bibr" target="#b36">Zlateski et eful visual representations can be learnt <ref type="bibr" target="#b11">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b29">Wang et al., 2018a;</ref><ref type="bibr" target="#b31">Wu et al., 20
ses primarily on loss correction approaches <ref type="bibr" target="#b22">(Reed et al., 2015;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018;</ref><ref type="bibr" target="#b11">Jiang et a y samples. Other approaches modify class probabilities <ref type="bibr">(Patrini et al., 2017;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018)</ref> by estimating the noise associated with e the existence of a known set of clean data <ref type="bibr" target="#b23">(Ren et al., 2018;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018)</ref>, we propose an unsupervised model of lab ass-conditional label noise, on the other hand, has different flipping probabilities for each class <ref type="bibr" target="#b9">(Hendrycks et al., 2018)</ref>. Previous research <ref type="bibr">(Pa the loss, goes back to the predicted probabilities to correct them by multiplying by the T matrix. <ref type="bibr" target="#b9">(Hendrycks et al., 2018)</ref> corrects the predicted probabilities us ods, estimate the T matrix of <ref type="bibr">(Patrini et al., 2017)</ref> in epoch 75 (as done in <ref type="bibr" target="#b9">(Hendrycks et al., 2018)</ref>), and use the configuration reported in
raphical models <ref type="bibr" target="#b32">(Xiao et al., 2015)</ref>, Conditional Random Fields <ref type="bibr" target="#b26">(Vahdat, 2017)</ref>, or CNNs <ref type="bibr" target="#b27">(Veit et
unsupervised modeling technique <ref type="bibr" target="#b24">(Stauffer &amp; Grimson, 1999;</ref><ref type="bibr" target="#b20">Permuter et al., 2006;</ref><ref type="bibr" target="#b15">Ma &amp; L ><ref type="bibr" target="#b15">Ma &amp; Leijon, 2011)</ref>, with the Gaussian Mixture Model (GMM) <ref type="bibr" target="#b20">(Permuter et al., 2006)</ref> being the most popular. The probability
ibr" target="#b19">[19]</ref>- <ref type="bibr" target="#b21">[21]</ref>, architecture optimization <ref type="bibr" target="#b22">[22]</ref>, and many more. A number of programable performance measur data centers and then they provide several insights for server architecture design in data centers <ref type="bibr" target="#b22">[22]</ref>. Chen et al. leveraged hardware-event sampling to generate
>[46]</ref> and other cloud computing researches such as <ref type="bibr" target="#b47">[47]</ref>- <ref type="bibr" target="#b49">[49]</ref>.   </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><he
COE becomes inefficient with more than one hundred and up to fourteen hundreds of measurable events <ref type="bibr" target="#b29">[29]</ref>.</p><p>This motivates the multiplexing (MLPX) approach, wh sic Ivy-Bridge model defines only 338 events whereas the high-end IvyTown chips support 1423 events <ref type="bibr" target="#b29">[29]</ref>. In summary, the number of events greatly outnumbers that ge number of measurable events compared to the available hardware counters (e.g., 207? for HaswellX <ref type="bibr" target="#b29">[29]</ref>), MLPX is therefore mandatory, but it comes at a high cost can only leverage a small number of events to provide feedback to a runtime system for optimization <ref type="bibr" target="#b29">[29]</ref>.</p><p>Therefore, it is crucial to identify a subset of ev along with X axis) measured simultaneously. strongly relevant for a given architecture or workload <ref type="bibr" target="#b29">[29]</ref>. It can reduce both measurement and optimization overhead. Reduction</head><p>The measurement errors caused by MLPX have been observed for nearly two decades <ref type="bibr" target="#b29">[29]</ref>- <ref type="bibr" target="#b31">[31]</ref>, <ref type="bib
A number of programable performance measurement tools have therefore been developed, including PAPI <ref type="bibr" target="#b23">[23]</ref>, VTune <ref type="bibr" target="#b24">[24]</ref>, Perfmon
ployed hardware counters to characterize a group of scale-out workloads and released the CloudSuite <ref type="bibr" target="#b9">[9]</ref>. Later on, Yasin et al. performed a deep characterization by
re hundreds of crucial events such as cache and TLB misses <ref type="bibr" target="#b1">[1]</ref>- <ref type="bibr" target="#b4">[4]</ref>. These events can generally reveal root causes and key insig nt from its samples. However, MLPX incurs large measurement errors due to time-sharing and sampling <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b30">[30]</ref>- <ref type="bibr" o improve the measurement efficiency by letting multiple events timeshare a single hardware counter <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b38">[38]</ref>. In MLPX, events
e="bibr" target="#b18">[18]</ref>, compiler optimization <ref type="bibr" target="#b19">[19]</ref>- <ref type="bibr" target="#b21">[21]</ref>, architecture optimization <ref type="bibr" target="#b22">
]</ref>. Jia et al. use performance counters to characterize data analysis workloads in datacenters <ref type="bibr" target="#b8">[8]</ref>. Wang et al. characterized big data workloads for internet s
compilers and show speedups between 32.5% and 893% on selected regions of SPEC CPU 2006 benchmarks <ref type="bibr" target="#b20">[20]</ref>.</p><p>3) Application Optimization: Chen et al. used hardw
bibr" target="#b25">[25]</ref>, Oprofile <ref type="bibr" target="#b26">[26]</ref>, and many others <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>.</p><p>However,
bibr" target="#b25">[25]</ref>, Oprofile <ref type="bibr" target="#b26">[26]</ref>, and many others <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>.</p><p>However,
ere has been a surge of interest in learning graph representations from data. For example, DeepWalk <ref type="bibr" target="#b19">[20]</ref>, one recent model, transforms a graph structure into a sam thod, which used stochastic gradient descent to optimize matrices from large graphs. Perozzi et al. <ref type="bibr" target="#b19">[20]</ref> presented an approach, which transformed graph structure i ep relational information and tuning the threshold of maximum number of vertices.</p><p>2. DeepWalk <ref type="bibr" target="#b19">[20]</ref>. DeepWalk is a method that learns the representation of so uction strategy for vertices with small degrees to achieve the optimal performance. As mentioned in <ref type="bibr" target="#b19">[20]</ref>, for DeepWalk and E-SGNS, we set window size as 10, walk l lti-label classification task by regarding the learned representations as features.</p><p>Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>, we use the LibLin
p-Gram model with Negative Sampling (SGNS) can be regarded as a model that implicitly such a matrix <ref type="bibr" target="#b15">[16]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head et="#b10">[11]</ref>, to define our objective function. Following a similar discussion presented in <ref type="bibr" target="#b15">[16]</ref>, we first introduce our k-step loss function defined over ula_9">L k (w, c) = A k w,c • log σ( w • c) + λ N w A k w ,c • log σ(− w • c)</formula><p>Following <ref type="bibr" target="#b15">[16]</ref>, we define e = w • c, and setting ∂L k ∂e = 0. This yields .0"><head n="3.3">Optimization with Matrix Factorization</head><p>Following the work of Levy et al. <ref type="bibr" target="#b15">[16]</ref>, to reduce noise, we replace all negative entries in Y k w as one of the important methods that can be used for dimensionality reduction. It was also used in <ref type="bibr" target="#b15">[16]</ref>.</p><p>For the matrix X k , SVD factorizes it as:</p><form rity introduced in this dimensionality reduction step. To maintain the consistency with Levy et al. <ref type="bibr" target="#b15">[16]</ref>, we only employed SVD in this work.</p></div> <div xmlns=" , that is, |D| = γK. This matrix Y E−SGN S becomes exactly the same as that of SGNS as described in <ref type="bibr" target="#b15">[16]</ref>. This shows SGNS is essentially a special version of our G
ype="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>, and deep neural networks <ref type="bibr" target="#b11">[12]</ref>. Our focus in this work is on the novel model for learning
) method due to its simplicity. SVD has been shown successful in several matrix factorization tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>, and is regarded as
vision. We visualize the learned representations from all systems using a visualisation tool t-SNE <ref type="bibr" target="#b30">[31]</ref>, which provides both qualitative and quantitative results real citation network -DBLP. We feed the learned graph representations into the standard t-SNE tool <ref type="bibr" target="#b30">[31]</ref> to lay out the graph, where the authors from the same rese The graphs are visualized on a 2-dimensional space and the Kullback-Leibler divergence is reported <ref type="bibr" target="#b30">[31]</ref>, which captures the errors between the input pairwise simi
vision. We visualize the learned representations from all systems using a visualisation tool t-SNE <ref type="bibr" target="#b30">[31]</ref>, which provides both qualitative and quantitative results real citation network -DBLP. We feed the learned graph representations into the standard t-SNE tool <ref type="bibr" target="#b30">[31]</ref> to lay out the graph, where the authors from the same rese The graphs are visualized on a 2-dimensional space and the Kullback-Leibler divergence is reported <ref type="bibr" target="#b30">[31]</ref>, which captures the errors between the input pairwise simi
>[21]</ref>, and Laplacian Eigenmaps <ref type="bibr" target="#b2">[3]</ref>. Recently, Tang et al. <ref type="bibr" target="#b26">[27]</ref> presented methods for learning latent representational vec g the learned representations as features.</p><p>Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>, we use the LibLinear package <ref type="bibr" target="#b9"
Based on these similarity scores of each pair of documents, a language network is built. Following <ref type="bibr" target="#b28">[29]</ref>, in order to show the robustness of our model, we also con comp.sys.ibm.pc.hardware</p><p>Besides randomly sampling 200 documents from a topic as described in <ref type="bibr" target="#b28">[29]</ref>, we also conduct experiment on all documents as comparison d in <ref type="bibr" target="#b24">[25]</ref> and is set as 64 for 20-NewsGroup network as used in <ref type="bibr" target="#b28">[29]</ref>. For GraRep, we set β = 1 N and maximum matrix transition
>[21]</ref>, and Laplacian Eigenmaps <ref type="bibr" target="#b2">[3]</ref>. Recently, Tang et al. <ref type="bibr" target="#b26">[27]</ref> presented methods for learning latent representational vec g the learned representations as features.</p><p>Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>, we use the LibLinear package <ref type="bibr" target="#b9"
, factorizing a word-word co-occurrence counts matrix to generate word representations. Levy et al. <ref type="bibr" target="#b3">[4]</ref> presented matrix factorization over shifted positive Pointwi
del of Perozzi et al. [20]  as well as the skip-gram model with negative sampling of Mikolov et al. <ref type="bibr" target="#b17">[18]</ref> We conduct experiments on a language network, a social net f vertices using uniform sampling (which is also called truncated random walk). The skip-gram model <ref type="bibr" target="#b17">[18]</ref>, originally designed for learning word representations fro g methods employ a fixed slide window capturing context words of current word. Models like skipgram <ref type="bibr" target="#b17">[18]</ref> are proposed, which provide an efficient approach to learn ll other pairs do not come from the graph.</p><p>Motivated by the skip-gram model by Mikolov et al. <ref type="bibr" target="#b17">[18]</ref>, we employ noise contrastive estimation (NCE), which is pr E-SGNS. Skip-gram is an efficient model that learns the representation of each word in large corpus <ref type="bibr" target="#b17">[18]</ref>. For this enhanced version, we first utilize uniform sampl
abulous semantic segmentation models such as <ref type="bibr">FCN [Long et al., 2015]</ref>, SegNet <ref type="bibr" target="#b0">[Badrinarayanan et al., 2017]</ref>, <ref type="bibr">DeepLab-v3 [Chen work is based on the classic encoderdecoder network architecture without the fully-connected layers <ref type="bibr" target="#b0">[Badrinarayanan et al., 2017]</ref> and we improve it by adding the re semantic segmentation networks -FCN <ref type="bibr" target="#b7">[Long et al., 2015]</ref>, SegNet <ref type="bibr" target="#b0">[Badrinarayanan et al., 2017]</ref>, and DeepLab-V3 <ref type="bibr" t r network with that of <ref type="bibr">FCN-32s, FCN-16s, FCN-8s [Long et al., 2015]</ref>, Seg-Net <ref type="bibr" target="#b0">[Badrinarayanan et al., 2017], and</ref><ref type="bibr">DeepLab-v3 [C
" target="#b7">[Mottaghi et al., 2014;</ref><ref type="bibr" target="#b2">Cordts et al., 2016;</ref><ref type="bibr" target="#b1">Caesar et al., 2018;</ref><ref type="bibr" target="#b9">Ros et al., 20 owards furthering botanical taxonomy, as illustrated by the wealth of research regarding this topic <ref type="bibr" target="#b1">[Cerutti et al., 2011;</ref><ref type="bibr" target="#b5">Kebapci et a
" target="#b7">[Mottaghi et al., 2014;</ref><ref type="bibr" target="#b2">Cordts et al., 2016;</ref><ref type="bibr" target="#b1">Caesar et al., 2018;</ref><ref type="bibr" target="#b9">Ros et al., 20 owards furthering botanical taxonomy, as illustrated by the wealth of research regarding this topic <ref type="bibr" target="#b1">[Cerutti et al., 2011;</ref><ref type="bibr" target="#b5">Kebapci et a
e wealth of research regarding this topic <ref type="bibr" target="#b1">[Cerutti et al., 2011;</ref><ref type="bibr" target="#b5">Kebapci et al., 2011;</ref><ref type="bibr">Goëau et al., 2016]</ref>.
anned vehicles <ref type="bibr" target="#b7">[Menze and Geiger, 2015]</ref>, medical image analysis <ref type="bibr" target="#b11">[Zhang et al., 2017]</ref>, robots and etc. Endowed with the power of
" target="#b7">[Mottaghi et al., 2014;</ref><ref type="bibr" target="#b2">Cordts et al., 2016;</ref><ref type="bibr" target="#b1">Caesar et al., 2018;</ref><ref type="bibr" target="#b9">Ros et al., 20 owards furthering botanical taxonomy, as illustrated by the wealth of research regarding this topic <ref type="bibr" target="#b1">[Cerutti et al., 2011;</ref><ref type="bibr" target="#b5">Kebapci et a
er restores the target details and spatial dimensions step by step. Among such architectures, U-Net <ref type="bibr" target="#b8">[Ronneberger et al., 2015]</ref> is a very efficient one, whose semant
anned vehicles <ref type="bibr" target="#b7">[Menze and Geiger, 2015]</ref>, medical image analysis <ref type="bibr" target="#b11">[Zhang et al., 2017]</ref>, robots and etc. Endowed with the power of
ce and easy sample overwhelming, we substitute the classic Cross Entropy loss with a new Focal Loss <ref type="bibr" target="#b6">[Lin et al., 2017]</ref> to reduce the weight of easy samples. In this problem of class imbalance, we combine the advantages of reducing the class imbalance in Focal Loss<ref type="bibr" target="#b6">[Lin et al., 2017]</ref> and increasing class distance in smoothed Hin
" target="#b7">[Mottaghi et al., 2014;</ref><ref type="bibr" target="#b2">Cordts et al., 2016;</ref><ref type="bibr" target="#b1">Caesar et al., 2018;</ref><ref type="bibr" target="#b9">Ros et al., 20 owards furthering botanical taxonomy, as illustrated by the wealth of research regarding this topic <ref type="bibr" target="#b1">[Cerutti et al., 2011;</ref><ref type="bibr" target="#b5">Kebapci et a
ce and easy sample overwhelming, we substitute the classic Cross Entropy loss with a new Focal Loss <ref type="bibr" target="#b6">[Lin et al., 2017]</ref> to reduce the weight of easy samples. In this problem of class imbalance, we combine the advantages of reducing the class imbalance in Focal Loss<ref type="bibr" target="#b6">[Lin et al., 2017]</ref> and increasing class distance in smoothed Hin
s.</p><p>The idea of extracting features for NLP using convolutional DNN was previously explored by <ref type="bibr" target="#b3">Collobert et al. (2011)</ref>, in the context of POS tagging, chunking Recognition (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of <ref type="bibr" target="#b3">Collobert et al. (2011)</ref>. In <ref type="bibr" target="#b3">(Collo tation component, each input word token is transformed into a vector by looking up word embeddings. <ref type="bibr" target="#b3">Collobert et al. (2011)</ref> reported that word embeddings learned fr ural network, the convolution approach is a natural method to merge all of the features. Similar to <ref type="bibr" target="#b3">Collobert et al. (2011)</ref>, we first process the output of Window P , we heuristically choose d e = 5. Finally, the word dimension and learning rate are the same as in <ref type="bibr" target="#b3">Collobert et al. (2011)</ref>. Table <ref type="table">2 reports</ref> ons.org/licenses/by/4.0/ 1 A word embedding is a distributed representation for a word. For example,<ref type="bibr" target="#b3">Collobert et al. (2011)</ref> use a 50-dimensional vector to represent -words model</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><ref type="bibr" target="#b3">Collobert et al. (2011)</ref> proposed a pairwise ranking approach to ares similar intuition with that of <ref type="bibr" target="#b3">Collobert et al. (2011)</ref>. In <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref>, all of the tasks are considered as the


f type="bibr">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b19">Zhou et al., 2005;</ref><ref type="bibr" target="#b10">Mintz et al., 2009)</ref>. Supervised approaches are further divided
loited to convert the classification clues (such as sequences and parse trees) into feature vectors <ref type="bibr" target="#b9">(Kambhatla, 2004;</ref><ref type="bibr" target="#b15">Suchanek et al.,
pe="bibr" target="#b18">(Zelenko et al., 2003;</ref><ref type="bibr">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b19">Zhou et al., 2005;</ref><ref type="bibr" target="#b10">Mintz et al.,

pe="bibr" target="#b18">(Zelenko et al., 2003;</ref><ref type="bibr">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b19">Zhou et al., 2005;</ref><ref type="bibr" target="#b10">Mintz et al.,
ased on learning a distributed representation for each word, which is also called a word embeddings <ref type="bibr" target="#b17">(Turian et al., 2010)</ref>. <ref type="bibr" target="#b14">Socher et arget="#foot_1">3</ref> . However, there are many trained word embeddings that are freely available <ref type="bibr" target="#b17">(Turian et al., 2010)</ref>. A comparison of the available word embed beyond the scope of this paper. Our experiments directly utilize the trained embeddings provided by <ref type="bibr" target="#b17">Turian et al.(2010)</ref>.</p></div> <div xmlns="http://www.tei-c.org
rvised paradigm; such methods have been shown to be effective and yield relatively high performance <ref type="bibr" target="#b18">(Zelenko et al., 2003;</ref><ref type="bibr">Bunescu and Mooney, 2005
equences and parse trees) into feature vectors <ref type="bibr" target="#b9">(Kambhatla, 2004;</ref><ref type="bibr" target="#b15">Suchanek et al., 2006)</ref>. Feature-based methods suffer from the p
cases. In recent years, unsupervised learning has received increasing attention from the community <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Our novel appro metric in an unsupervised fashion, without any human annotations.</p><p>Exemplar CNN. Exemplar CNN <ref type="bibr" target="#b4">[5]</ref> appears similar to our work. The fundamental difference is t tecture <ref type="bibr" target="#b17">[18]</ref> in their original papers, except for exemplar CNN <ref type="bibr" target="#b4">[5]</ref>, whose results are reported with ResNet-101 <ref type="bibr"
is challenge by approximating the full softmax distribution with noise-contrastive estimation (NCE) <ref type="bibr" target="#b8">[9]</ref>, and by resorting to a proximal regularization method <ref t hierarchical softmax <ref type="bibr" target="#b25">[26]</ref>, noise-contrastive estimation (NCE) <ref type="bibr" target="#b8">[9]</ref>, and negative sampling <ref type="bibr" target="#b23">[24]</ target="#b8">[9]</ref>, and negative sampling <ref type="bibr" target="#b23">[24]</ref>. We use NCE <ref type="bibr" target="#b8">[9]</ref> to approximate the full softmax.</p><p>We adapt NCE to our p
rative models include Restricted Bolztmann Machines (RBMs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21]</ref>, and Autoencoders <
-CNN <ref type="bibr" target="#b6">[7]</ref> with AlexNet and VGG16 architectures, and Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> with ResNet-50. When fine-tuning Fast R-CNN, the learning
subsequent linear classifier. Metric learning has been shown to be effective for few-shot learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" ta
rlying CNN on ImageNet and fine-tune it for the detection task.</p><p>We experiment with Fast R-CNN <ref type="bibr" target="#b6">[7]</ref> with AlexNet and VGG16 architectures, and Faster R-CNN <ref
rs from grayscale images <ref type="bibr" target="#b46">[47]</ref>, or even solving a jigsaw puzzle <ref type="bibr" target="#b26">[27]</ref>. For videos, self-supervision strategies include: leveragi -supervised learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48]</ref>, adversarial learni

rning has been shown to be effective for few-shot learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37]</ref>. An important techn
as faithfully as possible. Classical generative models include Restricted Bolztmann Machines (RBMs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta
its own right, and each could differ significantly from other images in the same semantic category <ref type="bibr" target="#b22">[23]</ref>. If we learn to discriminate between individual instances,
end to sustain only a small fraction of the peak bandwidth <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b37">37]</ref>. The end result is either a significant performance hit, or ed descriptions in <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b37">37]</ref> on DRAM systems and in <ref type="bibr" target="#b6">[6,</r r, the FR-FCFS (first-ready first-come first-serve) policy <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> provides the best average performance. Among all ready comm hich was shown to be the best-performing policy on average <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>, (2) a conventional inorder memory controller <ref type="bi average) in our more aggressive setup.</p><p>Rixner et al. <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> examine various DRAM command scheduling policies and propos ]</ref>) with both a realistic, contemporary controller design (using the FR-FCFS scheduling policy <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b49">49]</ref>), and an optimisti erings and interleavings of DRAM commands result in different levels of DRAM throughput and latency <ref type="bibr" target="#b37">[37]</ref>. Finding a good schedule is not an easy task as scheduling >Current memory controllers use relatively simple policies to schedule DRAM accesses. Rixner et al. <ref type="bibr" target="#b37">[37]</ref> show that none of the fixed policies studied provide the b 6">[36,</ref><ref type="bibr" target="#b37">37]</ref>, (2) a conventional inorder memory controller <ref type="bibr" target="#b37">[37]</ref>, and (3) an optimistic (i.e., ideally efficient) scheduler significantly underperforms the baseline FR-FCFS controller, in line with previous research results <ref type="bibr" target="#b37">[37]</ref>.</p><p>Figure <ref type="figure" target="#fig_13">8</ref> ta bus utilization in our FR-FCFS baseline is consistent with what is reported in previous research <ref type="bibr" target="#b37">[37]</ref> and by DRAM manufacturers <ref type="bibr" target="#b27">[ arget="#b37">[37]</ref> and by DRAM manufacturers <ref type="bibr" target="#b27">[27]</ref>. Rixner <ref type="bibr" target="#b37">[37]</ref> reported an average utilization of approximately 35% for a get="#b25">25,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" tar
troller (which is the industry trend as seen in IBM POWER5 <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b39">39]</ref>, Sun Niagara <ref type="bibr" target="#b22">[22]</ref>, AMD
Other machine learning techniques have been used in the context of branch prediction. Calder et al. <ref type="bibr" target="#b7">[7]</ref> use neural networks and decision trees to predict conditiona
speedups over a variety of existing queueing-theoretic allocation policies.</p><p>McGovern and Moss <ref type="bibr" target="#b24">[24]</ref> apply reinforcement learning and Monte Carlo roll-outs to
networks <ref type="bibr" target="#b32">[32]</ref>, processor and memory allocation in data centers <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b45">45]</ref>, routing in ad-hoc decisions in shopbots. Its application to microarchitecture has been limited.</p><p>Tesauro et al. <ref type="bibr" target="#b43">[43]</ref> explore a reinforcement learning approach to make autonomi
n over-provisioned (and therefore expensive) memory system <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>. Figure <ref type=" WER5 <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b39">39]</ref>, Sun Niagara <ref type="bibr" target="#b22">[22]</ref>, AMD Athlon/Opteron <ref type="bibr" target="#b1">[1]</ref ak DRAM bandwidth for largerscale CMP platforms is to integrate multiple memory controllers on chip <ref type="bibr" target="#b22">[22]</ref>, where each controller serves a different set of physical
The leftmost graph shows the sustained DRAM bandwidth of an example parallel application (SCALPARC <ref type="bibr" target="#b20">[20]</ref>) with both a realistic, contemporary controller design (us
c outcomes (e.g., backgammon), elevator scheduling, dynamic channel assignment in cellular networks <ref type="bibr" target="#b32">[32]</ref>, processor and memory allocation in data centers <ref type
loads and under all circumstances. However, the FR-FCFS (first-ready first-come first-serve) policy <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> provides the best ner et al.'s FR-FCFS scheduling policy, which was shown to be the best-performing policy on average <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>, (2) a conventiona ss than 0.3% improvement over FR-FCFS on average) in our more aggressive setup.</p><p>Rixner et al. <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> examine various DR et="#b17">[17,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" tar
"bibr" target="#b43">[43,</ref><ref type="bibr" target="#b45">45]</ref>, routing in ad-hoc networks <ref type="bibr" target="#b10">[10]</ref>, and the control of pricing decisions in shopbots. Its app
iming constraints that must be obeyed when scheduling commands (e.g., over 50 timing constraints in <ref type="bibr" target="#b26">[26]</ref>).</p><p>Second, the controller must intelligently prioriti ameters of the shared L2 cache and the SDRAM memory subsystem modeled after Micron's DDR2-800 SDRAM <ref type="bibr" target="#b26">[26]</ref>.</p><p>Our parallel workloads represent a mix of scalable
ef type="bibr" target="#b36">[36]</ref> (e) EDSR <ref type="bibr" target="#b36">[36]</ref> (f) DBPN <ref type="bibr" target="#b20">[20]</ref> (g) RDN <ref type="bibr" target="#b6">[6]</ref> (h) Ours</ with more than 16 layers based on residual learning. To further improve the performance, Lim et al. <ref type="bibr" target="#b20">[20]</ref> proposed a very deep and wide network EDSR by stacking mod <ref type="bibr" target="#b14">[14]</ref>, Mem-Net <ref type="bibr" target="#b30">[30]</ref>, EDSR <ref type="bibr" target="#b20">[20]</ref>, SRMD <ref type="bibr" target="#b36">[36]</ref>, NLRN <ref module, and reconstruction part. Given I LR and I SR as the input and output of SAN. As explored in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref>, we apply only one ndencies.</p><p>It has been verified that stacking residual blocks is helpful to form a deep CNN in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref>. However, very dee filter are set as 3 × 3 and C =6 4 , respectively. For upscale part H ↑ (•), we follow the works in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref> and apply ESPCNN < ments</head></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Following <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" targ <ref type="bibr" target="#b39">[39]</ref> and RCAN <ref type="bibr" target="#b38">[38]</ref>. As in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" ta 30">30]</ref>, L 1 <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b39">39]</ref>, perceptual losses
to improve the performance of CNNs for various tasks, such as image and video classification tasks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b33">33]</ref>. Wang et al. <ref ty ks. The second one is the way of enhancing discriminative ability of the network. Channel attention <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b38">38]</ref> has been shown to be rate non-local operations for spatial attention in video classification. On the contrary, Hu et al. <ref type="bibr" target="#b9">[ 9]</ref> proposed SENet to exploit channel-wise relationships to ach N-based SR models do not consider the feature interdependencies. To utilize such information, SENet <ref type="bibr" target="#b9">[9]</ref> was introduced in CNNs to rescale the channelwise features f he aggregated information by global covariance pooling, we apply a gating mechanism. As explored in <ref type="bibr" target="#b9">[9]</ref>, the simple sigmoid function can serve as a proper gating fu
get="#b14">14,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>. Although considerable progress has been achieved in image get="#b17">17,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>. Due to space limitation, we here briefly review works rela arget="#b13">13,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>. For example, Dong et al. <ref type="bibr" target="#b2">[ 2 ing discriminative ability of the network. Channel attention <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b38">38]</ref> has been shown to be effective for better discriminative re rget="#b20">[20,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref >[38]</ref>. As in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>, we also adopt self-ensemble method to further improve our f the network, some other networks, such as NLRN <ref type="bibr" target="#b22">[22]</ref> and RCAN <ref type="bibr" target="#b38">[38]</ref>, improve the performance by considering feature correlatio classification.</p><p>Recently, SENet was introduced to deep CNNs to further improve SR performance <ref type="bibr" target="#b38">[38]</ref>. However, SENet only explores first-order statistics (e.g. se feature interdependencies. Difference to Residual Channel Attention Network (RCAN). Zhang et al. <ref type="bibr" target="#b38">[38]</ref> proposed a residual in residual structure to form a very d sidual blocks in each LSRAG, thus resulting in deep network with over 400 convolution layers. As in <ref type="bibr" target="#b38">[38]</ref>, we also add long and short skip connections in Base model BPN <ref type="bibr" target="#b6">[6]</ref>, RDN <ref type="bibr" target="#b39">[39]</ref> and RCAN <ref type="bibr" target="#b38">[38]</ref>. As in <ref type="bibr" target="#b20">[20,</ref><ref type= <ref type="bibr" target="#b36">[36]</ref>, RDN <ref type="bibr" target="#b39">[39]</ref>, and RCAN <ref type="bibr" target="#b38">[38]</ref>. All the results on 3× are shown in Table <ref type="table
type="bibr" target="#b34">[34]</ref>, and CNN-based methods <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" tar get="#b29">29,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" tar ly used, such as L 2 <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, L 1 <ref type="bib
37]</ref> and model-based <ref type="bibr" target="#b4">[4]</ref>, to recent learning-based methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>The early d
-art results in SISR <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" tar nd CNN-based methods <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" tar eature expression and feature correlation learning. Specifically, we 1 (a) HR (b) FSRCNN (c) LapSRN <ref type="bibr" target="#b14">[14]</ref> (d) SRMD <ref type="bibr" target="#b36">[36]</ref> (e) EDS CNN <ref type="bibr" target="#b3">[3]</ref>, VDSR <ref type="bibr" target="#b12">[12]</ref>, LapSRN <ref type="bibr" target="#b14">[14]</ref>, Mem-Net <ref type="bibr" target="#b30">[30]</ref>, EDSR < b12">12,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, L 1 <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" ta
/ref><ref type="bibr" target="#b38">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref type="bibr" target="#b31">[31]</ref> as training set. For testing, we adopt 5 standard benchmar
st-order, thus hindering the discriminative ability of the network. On the other hand, recent works <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b21">21]</ref> have shown that se and matrix of all ones, respectively.</p><p>It is shown in <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b19">19]</ref> that covariance normalization plays a critical role for mor onlinearly shrinks the eigenvalues larger than 1.0 and streches those less than 1.0. As explored in <ref type="bibr" target="#b19">[19]</ref>, α =1/2 works well for more discriminative representations
/ref><ref type="bibr" target="#b38">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref type="bibr" target="#b31">[31]</ref> as training set. For testing, we adopt 5 standard benchmar
ttp://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SISR) <ref type="bibr" target="#b5">[5]</ref> has recently received much attention. In general, the purpos
various tasks, such as image and video classification tasks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b33">33]</ref>. Wang et al. <ref type="bibr" target="#b33">[ 33]</ref> pro tasks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b33">33]</ref>. Wang et al. <ref type="bibr" target="#b33">[ 33]</ref> proposed non-local neural network to incorporate non-loca in HR nature scenes by RL-NL modules plugged before and after the SSRG. The nonlocal neural network <ref type="bibr" target="#b33">[33]</ref> is proposed to capture the computation of long-range depen
37]</ref> and model-based <ref type="bibr" target="#b4">[4]</ref>, to recent learning-based methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>The early d
ve ability of the network. On the other hand, recent works <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b21">21]</ref> have shown that second-order statistics in deep CNNs are mo
feature respectively. There are some choices to act as upscale part, such as transposed convolution <ref type="bibr" target="#b3">[3]</ref>, ESPCN <ref type="bibr" target="#b28">[28]</ref>.</p><p>The ss of our SAN, we compare our SAN with 11 state-of-the-art CNN-based SR methods: SR-CNN [1], FSRCNN <ref type="bibr" target="#b3">[3]</ref>, VDSR <ref type="bibr" target="#b12">[12]</ref>, LapSRN <ref SR <ref type="bibr" target="#b24">[24]</ref>, SRCNN <ref type="bibr" target="#b2">[2]</ref>, FSRCNN <ref type="bibr" target="#b3">[3]</ref>, VDSR <ref type="bibr" target="#b12">[12]</ref>, IR-CNN <ref mputational burden and performance, and thus is preferable to be used in recent CNN-based SR models <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target

Therefore, a great number of SR methods have been proposed, ranging from early interpolation-based <ref type="bibr" target="#b37">[37]</ref> and model-based <ref type="bibr" target="#b4">[4]</ref>, t age SISR methods have been proposed in the computer vision community, including interpolation-based <ref type="bibr" target="#b37">[37]</ref>, model-based <ref type="bibr" target="#b34">[34]</ref>, an
-art results in SISR <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" tar nd CNN-based methods <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" tar eature expression and feature correlation learning. Specifically, we 1 (a) HR (b) FSRCNN (c) LapSRN <ref type="bibr" target="#b14">[14]</ref> (d) SRMD <ref type="bibr" target="#b36">[36]</ref> (e) EDS CNN <ref type="bibr" target="#b3">[3]</ref>, VDSR <ref type="bibr" target="#b12">[12]</ref>, LapSRN <ref type="bibr" target="#b14">[14]</ref>, Mem-Net <ref type="bibr" target="#b30">[30]</ref>, EDSR < b12">12,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, L 1 <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" ta
-art results in SISR <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" tar nd CNN-based methods <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" tar eature expression and feature correlation learning. Specifically, we 1 (a) HR (b) FSRCNN (c) LapSRN <ref type="bibr" target="#b14">[14]</ref> (d) SRMD <ref type="bibr" target="#b36">[36]</ref> (e) EDS CNN <ref type="bibr" target="#b3">[3]</ref>, VDSR <ref type="bibr" target="#b12">[12]</ref>, LapSRN <ref type="bibr" target="#b14">[14]</ref>, Mem-Net <ref type="bibr" target="#b30">[30]</ref>, EDSR < b12">12,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, L 1 <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" ta

ref type="bibr" target="#b39">39]</ref>, perceptual losses <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b26">26]</ref>. To verify the effectiveness of our SAN, we adopt the same
act as upscale part, such as transposed convolution <ref type="bibr" target="#b3">[3]</ref>, ESPCN <ref type="bibr" target="#b28">[28]</ref>.</p><p>The way of embedding upscaling feature in the last <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref> and apply ESPCNN <ref type="bibr" target="#b28">[28]</ref> to upscale the deep features, followed by one final convol
ttp://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SISR) <ref type="bibr" target="#b5">[5]</ref> has recently received much attention. In general, the purpos
CNN <ref type="bibr" target="#b3">[3]</ref>, VDSR <ref type="bibr" target="#b12">[12]</ref>, IR-CNN <ref type="bibr" target="#b35">[35]</ref>, SRMD <ref type="bibr" target="#b36">[36]</ref>, RDN <ref
ttp://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SISR) <ref type="bibr" target="#b5">[5]</ref> has recently received much attention. In general, the purpos
/ref><ref type="bibr" target="#b38">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref type="bibr" target="#b31">[31]</ref> as training set. For testing, we adopt 5 standard benchmar
" target="#b4">[4]</ref>, to recent learning-based methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>The early developed interpolated-based methods (e.g. LR and I SR as the input and output of SAN. As explored in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref>, we apply only one convolutional layer to extract the shall nt CNN-based SR models <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b39">39]</ref>. The upscaled feature is then mapped into SR image via one et="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b39">39]</ref>, perceptual losses <ref type="bibr" target="#b11">[11,</ref stacking residual blocks is helpful to form a deep CNN in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref>. However, very deep network built in such way would suffer ectively. For upscale part H ↑ (•), we follow the works in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref> and apply ESPCNN <ref type="bibr" target="#b28">[28]</ref> ts with Blur-downscale Degradation (BD)</head><p>Following <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b39">39]</ref>, we also compare various SR methods on image with blur-down get="#b12">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>. Although considera get="#b29">29,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>. Due to space limit arget="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>. For example, Dong p</head><p>Following <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>, we use 800 high-re and RCAN <ref type="bibr" target="#b38">[38]</ref>. As in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>, we also adopt self role in image SR. Other recent works like MemNet <ref type="bibr" target="#b30">[30]</ref> and RDN <ref type="bibr" target="#b39">[39]</ref>, are based on dense blocks <ref type="bibr" target="#b10"> ability. Difference to Residual Dense Network (RDN). We summarize the main differences between RDN <ref type="bibr" target="#b39">[39]</ref> and our SAN. The first one is the design of basic block. R , NLRN <ref type="bibr" target="#b22">[22]</ref>, DBPN <ref type="bibr" target="#b6">[6]</ref>, RDN <ref type="bibr" target="#b39">[39]</ref> and RCAN <ref type="bibr" target="#b38">[38]</ref>. As in -CNN <ref type="bibr" target="#b35">[35]</ref>, SRMD <ref type="bibr" target="#b36">[36]</ref>, RDN <ref type="bibr" target="#b39">[39]</ref>, and RCAN <ref type="bibr" target="#b38">[38]</ref>. All t
st-order, thus hindering the discriminative ability of the network. On the other hand, recent works <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b21">21]</ref> have shown that se and matrix of all ones, respectively.</p><p>It is shown in <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b19">19]</ref> that covariance normalization plays a critical role for mor onlinearly shrinks the eigenvalues larger than 1.0 and streches those less than 1.0. As explored in <ref type="bibr" target="#b19">[19]</ref>, α =1/2 works well for more discriminative representations
various tasks, such as image and video classification tasks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b33">33]</ref>. Wang et al. <ref type="bibr" target="#b33">[ 33]</ref> pro tasks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b33">33]</ref>. Wang et al. <ref type="bibr" target="#b33">[ 33]</ref> proposed non-local neural network to incorporate non-loca in HR nature scenes by RL-NL modules plugged before and after the SSRG. The nonlocal neural network <ref type="bibr" target="#b33">[33]</ref> is proposed to capture the computation of long-range depen
-art results in SISR <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" tar nd CNN-based methods <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" tar eature expression and feature correlation learning. Specifically, we 1 (a) HR (b) FSRCNN (c) LapSRN <ref type="bibr" target="#b14">[14]</ref> (d) SRMD <ref type="bibr" target="#b36">[36]</ref> (e) EDS CNN <ref type="bibr" target="#b3">[3]</ref>, VDSR <ref type="bibr" target="#b12">[12]</ref>, LapSRN <ref type="bibr" target="#b14">[14]</ref>, Mem-Net <ref type="bibr" target="#b30">[30]</ref>, EDSR < b12">12,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, L 1 <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" ta
ve ability of the network. On the other hand, recent works <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b21">21]</ref> have shown that second-order statistics in deep CNNs are mo
d, ranging from early interpolation-based <ref type="bibr" target="#b37">[37]</ref> and model-based <ref type="bibr" target="#b4">[4]</ref>, to recent learning-based methods <ref type="bibr" target="# rs, such as non-local similarity prior <ref type="bibr" target="#b34">[34]</ref> and sparsity prior <ref type="bibr" target="#b4">[4]</ref>. Although such model-based methods are flexible to produce r
/ref><ref type="bibr" target="#b38">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref type="bibr" target="#b31">[31]</ref> as training set. For testing, we adopt 5 standard benchmar
CNN <ref type="bibr" target="#b3">[3]</ref>, VDSR <ref type="bibr" target="#b12">[12]</ref>, IR-CNN <ref type="bibr" target="#b35">[35]</ref>, SRMD <ref type="bibr" target="#b36">[36]</ref>, RDN <ref
ore, the left side of expression 3 becomes equal to AE ? ?. Thus AE ? ? AE ?</formula><p>? ? ? ? ?? <ref type="bibr" target="#b4">(5)</ref> Consider the sum in this expression. The number of terms wit <ref type="bibr" target="#b7">[8]</ref> and set sampling <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b4">[5]</ref>.</p><p>Hardware monitoring tools collect statistics from har
on 2.3.</p><p>The proof-of-concept implementation is based on code instrumentation. We use the SAIT <ref type="bibr" target="#b8">[9]</ref> SPARC assembly code instrumentation tool to insert a small p
which uses an advanced hardware support to collect detailed information to the programmer, and PAPI <ref type="bibr" target="#b2">[3]</ref> which is a common programming interface to access hardware m
<ref type="bibr" target="#b5">[6]</ref>, CPROF <ref type="bibr" target="#b13">[14]</ref> and MemSpy <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b18">[19]</ref>. These tools are
IOTA <ref type="bibr" target="#b15">[16]</ref>, ATOM <ref type="bibr" target="#b6">[7]</ref> or EEL <ref type="bibr" target="#b12">[13]</ref>. Examples are SIGMA <ref type="bibr" target="#b5">[6]</ref
which uses an advanced hardware support to collect detailed information to the programmer, and PAPI <ref type="bibr" target="#b2">[3]</ref> which is a common programming interface to access hardware m
tect for example cache conflicts <ref type="bibr" target="#b20">[21]</ref> and locate problem areas <ref type="bibr" target="#b3">[4]</ref>. The execution time overhead is very small, but they can onl
operating system interaction. Source instrumentation have also been explored, for example in MHSIM <ref type="bibr" target="#b9">[10]</ref>.</p><p>Trace sampling is used to speed up cache hierarchy s
common methods to reduce simulation time is time sampling and set sampling, but both have drawbacks <ref type="bibr" target="#b10">[11]</ref>. Time sampling often requires very long warm-up periods, a mpling <ref type="bibr">[22][11]</ref> [5] <ref type="bibr" target="#b7">[8]</ref> and set sampling <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b4">[5]</ref>.</p><p>Hardware m
rt.</p><p>Full system simulators include Simics <ref type="bibr" target="#b16">[17]</ref> and SimOS <ref type="bibr" target="#b14">[15]</ref>. They allow very detailed cache simulations, but suffer fr
using binary code instrumentation tools like DIOTA <ref type="bibr" target="#b15">[16]</ref>, ATOM <ref type="bibr" target="#b6">[7]</ref> or EEL <ref type="bibr" target="#b12">[13]</ref>. Examples a
><p>Speech animation for rigged models. Several related methods produce animation curves for speech <ref type="bibr" target="#b12">[Edwards et al. 2016;</ref><ref type="bibr" target="#b59">Taylor et a
ork on reconstructing facial geometry and appearance from a single image using optimization methods <ref type="bibr" target="#b14">[Fyffe et al. 2014;</ref><ref type="bibr" target="#b18">Garrido et al
"bibr" target="#b11">[Dou et al. 2017;</ref><ref type="bibr" target="#b20">Genova et al. 2018;</ref><ref type="bibr" target="#b48">Richardson et al. 2016;</ref><ref type="bibr" target="#b60">Tewari et
al. 2016]</ref>, provide audio descriptions of the video content for segmentation of B-roll footage <ref type="bibr" target="#b65">[Truong et al. 2016]</ref> and generate structured summaries of lectu
n learning-based image-to-image translation <ref type="bibr" target="#b26">[Isola et al. 2017;</ref><ref type="bibr" target="#b57">Sun et al. 2018</ref>].</p></div> <div xmlns="http://www.tei-c.org/ns
dubbing methods, such as VDub <ref type="bibr" target="#b17">[Garrido et al. 2015]</ref>, Face2Face <ref type="bibr" target="#b63">[Thies et al. 2016]</ref> and Deep Video Portraits <ref type="bibr" t e refer to the supplemental video for more results.</p><p>We also compare our approach to Face2Face <ref type="bibr" target="#b63">[Thies et al. 2016]</ref>, see Figure <ref type="figure" target="#fig get="#b58">Suwajanakorn et al. 2017;</ref><ref type="bibr" target="#b67">Vlasic et al. 2005]</ref>. <ref type="bibr" target="#b63">Thies et al. [2016]</ref> recently demonstrated real-time video reena nimization, please see the papers of <ref type="bibr" target="#b18">Garrido et al. [2016]</ref> and <ref type="bibr" target="#b63">Thies et al. [2016]</ref>. In total, we obtain a 257 parameter vector " target="#b55">Shi et al. 2014;</ref><ref type="bibr" target="#b58">Suwajanakorn et al. 2017;</ref><ref type="bibr" target="#b63">Thies et al. 2016</ref>]. Many of these techniques employ a parametri monocular model-based face reconstruction <ref type="bibr" target="#b18">[Garrido et al. 2016;</ref><ref type="bibr" target="#b63">Thies et al. 2016</ref>]. These techniques parameterize the rigid hea f type="figure" target="#fig_9">15</ref>. Our neural face rendering approach can better handle the  <ref type="bibr" target="#b63">[Thies et al. 2016</ref>] facial reenactment approach. Our approach p xml:id="fig_9"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Comparison to the Face2Face<ref type="bibr" target="#b63">[Thies et al. 2016</ref>] facial reenactment approach. Our approach p
r" target="#b12">[Edwards et al. 2016;</ref><ref type="bibr" target="#b59">Taylor et al. 2017;</ref><ref type="bibr" target="#b73">Zhou et al. 2018]</ref>. They are specifically designed for animated
approaches regress detailed depth maps <ref type="bibr" target="#b49">[Richardson et al. 2017;</ref><ref type="bibr" target="#b53">Sela et al. 2017]</ref>, or 3D displacements <ref type="bibr" target=

ally improved. In addition, we also show the result obtained with the Deep Video Portraits (DVP) of <ref type="bibr" target="#b31">Kim et al. [2018a]</ref>. We do not investigate alternatives to the R ith all new components (Full). In addition, we show a result from the Deep Video Portraits (DVP) of <ref type="bibr" target="#b31">Kim et al. [2018a]</ref>. All components of our approach positively c
">Garrido et al. 2014;</ref><ref type="bibr" target="#b30">Kemelmacher-Shlizerman et al. 2010;</ref><ref type="bibr" target="#b35">Li et al. 2014;</ref><ref type="bibr" target="#b38">Liu et al. 2001;<
f> which needs a CTC trained model to conduct pre-partition before the attention decoding.</p><p>In <ref type="bibr" target="#b13">[14]</ref>, Li al. present the important Adaptive Computation Steps (
attention to decide when to stop and then performing soft attention to calculate, also rather than <ref type="bibr" target="#b12">[13]</ref> which needs a CTC trained model to conduct pre-partition b shows a clear performance advantage than other soft and monotonic models (e.g. triggered attention <ref type="bibr" target="#b12">[13]</ref>), but also matches or surpasses most of the published resu int CTC-attention model / ESPNet <ref type="bibr" target="#b15">[16]</ref> 27.4 Triggered Attention <ref type="bibr" target="#b12">[13]</ref> 30 where the membrane potential Um is constantly simulated
ISHELL-2 <ref type="bibr" target="#b17">[18]</ref>) and the Mandarin telephone ASR benchmark (HKUST <ref type="bibr" target="#b18">[19]</ref>). For Librispeech, we use all the train data (960 hours) f

end-to-end ASR models. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>  to be a forward-moving window that fits gaussian distributi ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> or even heuristic rule <ref type="bibr" target="#b9">[10]</ref>, where the center and width of the window are predicted by

on but also locates acoustic boundaries. And we find inspirations from the integrate-and-fire model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Integrate-and-f
periment on three public ASR datasets including the popular English read-speech corpus (Librispeech <ref type="bibr" target="#b16">[17]</ref>), current largest Mandarin read-speech corpus (AISHELL-2 <
l networks (SNNs), which are more bio-plausible and known as the next generation of neural networks <ref type="bibr" target="#b6">[7]</ref>. The integrate-and-fire neuron operates using spikes, which
ous end-to-end models. Among them, the attention-based model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, which builds a soft alignment between each decoder step and
l networks (SNNs), which are more bio-plausible and known as the next generation of neural networks <ref type="bibr" target="#b6">[7]</ref>. The integrate-and-fire neuron operates using spikes, which
. And we find inspirations from the integrate-and-fire model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Integrate-and-fire is one of the earliest models in sp
l networks (SNNs), which are more bio-plausible and known as the next generation of neural networks <ref type="bibr" target="#b6">[7]</ref>. The integrate-and-fire neuron operates using spikes, which
d the soft and monotonic alignment in end-to-end ASR models. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>  to be a forward-movin o be a forward-moving window that fits gaussian distribution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> or even heuristic rule <ref type="bibr" target="#b9">[10]</re
and accurate with the spring up of various end-to-end models. Among them, the attention-based model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, which builds a soft a
on but also locates acoustic boundaries. And we find inspirations from the integrate-and-fire model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Integrate-and-f
l networks (SNNs), which are more bio-plausible and known as the next generation of neural networks <ref type="bibr" target="#b6">[7]</ref>. The integrate-and-fire neuron operates using spikes, which
ng the same setup as <ref type="bibr" target="#b14">[15]</ref> for all datasets. Speed perturbation <ref type="bibr" target="#b19">[20]</ref> with fixed ± 10% is applied for all training datasets. The
end-to-end ASR models. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>  to be a forward-moving window that fits gaussian distributi ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> or even heuristic rule <ref type="bibr" target="#b9">[10]</ref>, where the center and width of the window are predicted by
on but also locates acoustic boundaries. And we find inspirations from the integrate-and-fire model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Integrate-and-f
ISHELL-2 <ref type="bibr" target="#b17">[18]</ref>) and the Mandarin telephone ASR benchmark (HKUST <ref type="bibr" target="#b18">[19]</ref>). For Librispeech, we use all the train data (960 hours) f
ISHELL-2 <ref type="bibr" target="#b17">[18]</ref>) and the Mandarin telephone ASR benchmark (HKUST <ref type="bibr" target="#b18">[19]</ref>). For Librispeech, we use all the train data (960 hours) f
ng to a single point, triplet loss enables documents with the same identity to reside on a manifold <ref type="bibr" target="#b28">[20]</ref>, and at the same time maintain a distance from other docum
19">11]</ref>, name identi cation <ref type="bibr" target="#b24">[16]</ref>, and object distinction <ref type="bibr" target="#b39">[31]</ref> from a broader viewpoint, and has been extensively studied
re) than several state-of-the-art methods including GHOST [5], Zhang et al. [33], and Louppe et al. <ref type="bibr" target="#b25">[17]</ref>.</p><p>Finally, the algorithm has been deployed in AMiner target="#b41">[33]</ref>, Zhang et al. <ref type="bibr" target="#b41">[33]</ref>, and Louppe et al. <ref type="bibr" target="#b25">[17]</ref> (+7-35% in terms of F1-score). The automatically estimated ef> present supervised disambiguation methods based on SVM and Naïve Bayes. Moreover, Louppe et al. <ref type="bibr" target="#b25">[17]</ref> use a classi er to learn pairwise similarity and perform s paths. The nal clustering result is generated by a nity propagation algorithm.</p><p>Louppe et al. <ref type="bibr" target="#b25">[17]</ref>: This method rst trains a pairwise distance function base
get="#b18">10,</ref><ref type="bibr" target="#b20">12,</ref><ref type="bibr" target="#b30">22,</ref><ref type="bibr" target="#b40">32]</ref>. As many other clustering tasks, there are two main challen get="#b16">[8,</ref><ref type="bibr" target="#b18">10,</ref><ref type="bibr" target="#b30">22,</ref><ref type="bibr" target="#b40">32]</ref> leverage supervised learning method to learn a pairwise dis e cient Support Vector Machines (SVM) and nally employs DBSCAN to cluster documents. Yoshida et al. <ref type="bibr" target="#b40">[32]</ref> propose a two-stage clustering method to learn better feat
to as name disambiguation, also named as entity resolution <ref type="bibr" target="#b11">[3,</ref><ref type="bibr" target="#b12">4]</ref>, web appearance disambiguation <ref type="bibr">[1,</ref><re real applications, for example, matching records between enterprise databases with di erent schema <ref type="bibr" target="#b12">[4]</ref>, aligning protein-protein interaction networks to transfer
f type="bibr" target="#b14">[6]</ref>, and identifying users across multiple online social networks <ref type="bibr" target="#b42">[34]</ref>. However, despite much work that has been done, the proble
ns that if you happen to have a common rst and last names, according to the birthday paradox theory <ref type="bibr" target="#b35">[27]</ref>, the probability that some other researcher in your univer
sambiguation <ref type="bibr">[1,</ref><ref type="bibr" target="#b19">11]</ref>, name identi cation <ref type="bibr" target="#b24">[16]</ref>, and object distinction <ref type="bibr" target="#b39">[31
archical model can also address the scalability issue. Furthermore, human edits can be incorporated <ref type="bibr" target="#b36">[28]</ref> by running MCMC inference on the hierarchical model.</p></
sambiguation <ref type="bibr">[1,</ref><ref type="bibr" target="#b19">11]</ref>, name identi cation <ref type="bibr" target="#b24">[16]</ref>, and object distinction <ref type="bibr" target="#b39">[31
than 130,000,000 researcher pro les and over 200,000,000 papers from multiple publication databases <ref type="bibr" target="#b33">[25]</ref>.</p><p>In this paper, we present the implementation and de lem with large data in an online fashion. AMiner is a free online academic search and mining system <ref type="bibr" target="#b33">[25]</ref>. The system extracts researchers' pro les automatically fr ner is a free online academic search and mining system and also the second generation of ArnetMiner <ref type="bibr" target="#b33">[25]</ref>, with the emphasis to o er approaches to gain a deeper und

the GS-SVM classifier matches or outperforms all but the two most recent methods, i.e., S2S-N2N-PP <ref type="bibr" target="#b33">(Taheri et al., 2018)</ref> and GIN <ref type="bibr" target="#b36">(X 6">(Xu et al., 2019)</ref>. With regards to these two approaches, the GS-SVM outperforms S2S-N2N-PP <ref type="bibr" target="#b33">(Taheri et al., 2018</ref>) on 3 /6 datasets. Finally, while GIN <ref br" target="#b35">Verma &amp; Zhang, 2018)</ref>, recurrent neural network autoencoders (S2S-N2N-PP,<ref type="bibr" target="#b33">Taheri et al., 2018)</ref>, and the graph isomorphism network (GIN,<r
the GS-SVM classifier matches or outperforms all but the two most recent methods, i.e., S2S-N2N-PP <ref type="bibr" target="#b33">(Taheri et al., 2018)</ref> and GIN <ref type="bibr" target="#b36">(X 6">(Xu et al., 2019)</ref>. With regards to these two approaches, the GS-SVM outperforms S2S-N2N-PP <ref type="bibr" target="#b33">(Taheri et al., 2018</ref>) on 3 /6 datasets. Finally, while GIN <ref br" target="#b35">Verma &amp; Zhang, 2018)</ref>, recurrent neural network autoencoders (S2S-N2N-PP,<ref type="bibr" target="#b33">Taheri et al., 2018)</ref>, and the graph isomorphism network (GIN,<r
specific datasets or applications (e.g., <ref type="bibr" target="#b38">Yosinski et al., 2014;</ref><ref type="bibr" target="#b26">Oquab et al., 2014)</ref>. Such transfer learning approaches provide
commission (EC) exchange preferences in enzyme evolution, validated with established knowledge from <ref type="bibr" target="#b12">Cuesta et al. (2015)</ref>. Taken together, these results illustrate get="#fig_5">4</ref> and validated with respect to established preferences observed and reported in <ref type="bibr" target="#b12">Cuesta et al. (2015)</ref>. We note that the result there is observed ce, correlates well with the proportion of evolutionary exchanges generally observed for each EC in <ref type="bibr" target="#b12">Cuesta et al. (2015)</ref>, and therefore we use these as EC weights
commission (EC) exchange preferences in enzyme evolution, validated with established knowledge from <ref type="bibr" target="#b12">Cuesta et al. (2015)</ref>. Taken together, these results illustrate get="#fig_5">4</ref> and validated with respect to established preferences observed and reported in <ref type="bibr" target="#b12">Cuesta et al. (2015)</ref>. We note that the result there is observed ce, correlates well with the proportion of evolutionary exchanges generally observed for each EC in <ref type="bibr" target="#b12">Cuesta et al. (2015)</ref>, and therefore we use these as EC weights



descriptive qualities, in Sec. 4.4 we use geometric scattering features extracted from enzyme data <ref type="bibr" target="#b4">(Borgwardt et al., 2005)</ref> to infer emergent patterns of enzyme co eractions between the six top level enzyme classes, labeled by their Enzyme Commission (EC) numbers <ref type="bibr" target="#b4">(Borgwardt et al., 2005)</ref>. In order to emphasize the properties o tion of graph data. As a representative example, we consider here the ENZYMES dataset introduced in <ref type="bibr" target="#b4">Borgwardt et al. (2005)</ref>, which contains 600 enzymes evenly split n particular, the portion of enzymes considered from each EC is different between these data, since <ref type="bibr" target="#b4">Borgwardt et al. (2005)</ref> took special care to ensure each EC clas

d Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type="bibr" target="#b13">[14]</ref> . It could obtain transformation rules automatically durin
="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> , Condition Random Field (CRF) <ref type="bibr" target="#b5">[6]</ref> , Decision Tree <ref type="bibr" target="#b6">[7,</ref><ref
ype="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> , Hidden Markov Model (HMM) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> , Condition Random Fie

CRF) <ref type="bibr" target="#b5">[6]</ref> , Decision Tree <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> , Support Vector Machine (SVM) <ref type="bibr">[9~12]</ref>




the article. Named Entity was proposed by Message Understanding Conference (MUC) for the first time <ref type="bibr" target="#b0">[1]</ref> . Named Entity Recognition (NER) is a difficult and challeng
the article. Named Entity was proposed by Message Understanding Conference (MUC) for the first time <ref type="bibr" target="#b0">[1]</ref> . Named Entity Recognition (NER) is a difficult and challeng
ccess, to generate a speculative value that does not necessarily exhibit value locality (e.g., DLVP <ref type="bibr" target="#b2">[3]</ref>). While value predictors can generate speculative results fo rk has shown that load-only predictors are most efficient with a modest hardware budget (e.g., 8KB) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this stud r" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> Context Address Prediction (CAP) <ref type="bibr" target="#b2">[3]</ref> one another. We found that no individual predictor is strict e focus only on predicting load values since that is most effective with limited hardware resources <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p></div> <div xml sm needed to communicate the predicted values from the value-predicted producers to their consumers <ref type="bibr" target="#b2">[3]</ref>. Consumers of the load can use the prediction by reading the s practical implementations of value prediction, we encourage the readers to visit prior art papers <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" t about the baseline ISA, microarchitecture, and storage constraints (Sheikh reports similar findings <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b26">[27]</ref>).</p><p>In all of "#b7">[8]</ref>, and subsequent work confirmed the same is true for load instructions in particular <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Our implemen an directly generating values from We use the state-of-the-art DLVP predictor as a reference design <ref type="bibr" target="#b2">[3]</ref>. The predictor consists of one tagged table indexed by a has
iolating ARM's memory consistency model, we employ a technique similar to the work of Martin et al. <ref type="bibr" target="#b25">[26]</ref>. Also, address/value prediction is not used with memory or
ception: dependent loads are not allowed to be reordered <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Value prediction can violate this rule. To avoid violatin
">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Table <ref type="table" target="#tab_2">II</ref> sh
ception: dependent loads are not allowed to be reordered <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Value prediction can violate this rule. To avoid violatin
core uses state-of-art TAGE and ITTAGE branch predictors <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and a memory dependence predictor similar to Alpha 21264
exhibit value locality, meaning that the same static instruction often produces a predictable value <ref type="bibr" target="#b0">[1]</ref>. In the case of load instructions, it is also possible to pr type="table" target="#tab_0">I</ref>, to determine how they complement Last Value Prediction (LVP) <ref type="bibr" target="#b0">[1]</ref> Stride Address Prediction (SAP) <ref type="bibr" target="#b5
">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Table <ref type="table" target="#tab_2">II</ref> sh
exhibit value locality, meaning that the same static instruction often produces a predictable value <ref type="bibr" target="#b0">[1]</ref>. In the case of load instructions, it is also possible to pr type="table" target="#tab_0">I</ref>, to determine how they complement Last Value Prediction (LVP) <ref type="bibr" target="#b0">[1]</ref> Stride Address Prediction (SAP) <ref type="bibr" target="#b5
">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Table <ref type="table" target="#tab_2">II</ref> sh
e most efficient with a modest hardware budget (e.g., 8KB) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this study, we investigated techniques to increase mposite predictor significantly outperforms the winner of first championship value prediction (EVES <ref type="bibr" target="#b3">[4]</ref>) in terms of coverage (more than doubles) and speedup (incre nce that is most effective with limited hardware resources <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ted value is verified. Similar to the enhancements described for the stride value predictor in EVES <ref type="bibr" target="#b3">[4]</ref>, SAP takes into account the number of inflight occurrences o irmed the same is true for load instructions in particular <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Our implementation of CVP is similar to VTAGE value p d by employing optimizations similar to the ones described for the enhanced VTAGE implementation in <ref type="bibr" target="#b3">[4]</ref> (e.g., decoupling the value/address arrays and then sharing mpionship value prediction (CVP-1) <ref type="bibr" target="#b32">[33]</ref>, called EVES predictor <ref type="bibr" target="#b3">[4]</ref>, into our framework. EVES tunes VTAGE and augments it with a delivers more than twice the coverage of first championship value prediction winner predictor (EVES <ref type="bibr" target="#b3">[4]</ref>), and substantially increases the delivered speedup by more
cycle-bycycle schedule repeatedly. Caching these schedules is the motivation for Execution Caching <ref type="bibr" target="#b46">[47]</ref>, to allow the scheduler to be shut down (which they argue dynamic schedules should be generated. The major motivation in this regard has been energy savings, <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref>, generally at the via the dynamic scheduler (upwards of 70%, 45% and 20% for <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref> respectively) rathe
particularly in the context of attempting to match the OOO's performance with less complex hardware <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target
get="#b4">[5]</ref> to more modest compiler-inserted prefetch threads which use an idle SMT context <ref type="bibr" target="#b25">[26]</ref>. An excellent overview of runahead techniques that occupy
particularly in the context of attempting to match the OOO's performance with less complex hardware <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target
the data cache similar to the mechanisms proposed in <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b0">[1]</ref>. This obviously has little advantage if the schedule cannot
ined with specialized functional units can facilitate performance comparable to a narrow-window OOO <ref type="bibr" target="#b7">[8]</ref>. Though we do not address it in this work, quasi-dynamic and
et="#b24">[25]</ref>.</p><p>Some form of transactional memory or HW atomicity is typically required <ref type="bibr" target="#b32">[33]</ref> for quasi-dynamic scheduling but even a limited form combi
r processor model is configured to approximate the resources and latencies in modern OOO processors <ref type="bibr" target="#b33">[34]</ref>, as shown in Table <ref type="table" target="#tab_0">1</re at the cost of performance degradation though off-thecritical-path HW rescheduling of ROB traces in <ref type="bibr" target="#b33">[34]</ref> yields a performance speedup over the baseline OOO. It mus ificant percentage of their instructions via the dynamic scheduler (upwards of 70%, 45% and 20% for <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" ta
ll as concerns over object compatibility with the proposed simpler hardware.</p><p>Industry R&amp;D <ref type="bibr" target="#b11">[12]</ref> and academia <ref type="bibr" target="#b12">[13]</ref> wer
aggressive hardware prefetchers, software prefetching support and runahead execution of the Power6 <ref type="bibr" target="#b4">[5]</ref> to more modest compiler-inserted prefetch threads which use
r processor model is configured to approximate the resources and latencies in modern OOO processors <ref type="bibr" target="#b33">[34]</ref>, as shown in Table <ref type="table" target="#tab_0">1</re at the cost of performance degradation though off-thecritical-path HW rescheduling of ROB traces in <ref type="bibr" target="#b33">[34]</ref> yields a performance speedup over the baseline OOO. It mus ificant percentage of their instructions via the dynamic scheduler (upwards of 70%, 45% and 20% for <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" ta
get="#b60">[61,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" targe ting <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b9">10]</ref>, sketch2image <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>, and other image-to-
of the convonlutional layers is to generate the missing details in (x n+1 ) ↑ r (residual learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b56">57]</ref>). Namely, G n perf
have been explored only in the context of texture generation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. These models do no n map. As opposed to single-image GANs for textures (e.g., <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3]</ref>), here we define the
,</ref><ref type="bibr" target="#b9">10]</ref>, sketch2image <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>, and other image-to-image translation tasks <ref type="bibr
arget="#b20">21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b49">50]</ref>. The most closley related work in this context is <ref type
(AMT) "Real/Fake" user study, and (ii) a new single-image version of the Fréchet Inception Distance <ref type="bibr" target="#b22">[23]</ref>.</p><p>AMT perceptual study We followed the protocol of <r l statistics of x.</p><p>A common metric for GAN evaluation is the Fréchet Inception Distance (FID) <ref type="bibr" target="#b22">[23]</ref>, which measures the deviation between the distribution of
,</ref><ref type="bibr" target="#b9">10]</ref>, sketch2image <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>, and other image-to-image translation tasks <ref type="bibr
pe="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26]</ref>. We use the WGAN-GP loss <ref type="bibr" target="#b19">[20]</ref>, which we found to increase training stability, where the
manipulation tasks <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" targe "#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>, and other image-to-image translation tasks <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" ta the GAN does not disregard the noise, as often happens in conditional schemes involving randomness <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" ta
ibr" target="#b38">[39]</ref>, super resolution <ref type="bibr" target="#b17">[18]</ref>, dehazing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>, and image editing <
el for a specific task (e.g. super-resolution <ref type="bibr" target="#b29">[30]</ref>, inpainting <ref type="bibr" target="#b40">[41]</ref>, retargeting <ref type="bibr" target="#b44">[45]</ref>).</
., few hundreds/thousands of nodes/edges) and uses such counts as features for graph classification <ref type="bibr" target="#b37">(Vishwanathan, Schraudolph, Kondor and Borgwardt, 2010)</ref>. Previo role discovery <ref type="bibr" target="#b30">(Rossi and Ahmed, 2015b)</ref>, graph classification <ref type="bibr" target="#b37">(Vishwanathan et al., 2010)</ref>, and relational learning <ref type= et kernel) is that they scale poorly to large graphs with more than few hundreds/thousands of nodes <ref type="bibr" target="#b37">(Vishwanathan et al., 2010)</ref>. Thus, our fast algorithms would sp 178 protein graphs) and chemical compound graphs (MUTAG collection of 188 chemical compound graphs) <ref type="bibr" target="#b37">(Vishwanathan et al., 2010)</ref>. We extract the graphlet features u h kernels have been proposed in machine learning (e.g., graphlet, subtree, and random walk kernels) <ref type="bibr" target="#b37">(Vishwanathan et al., 2010;</ref><ref type="bibr" target="#b3">Costa
ation <ref type="bibr" target="#b41">(Zhang, Song, Liu, Liu, Bu and Chen, 2013)</ref>, among others <ref type="bibr" target="#b40">(Zhang, Han, Yang, Song, Yan and Tian, 2013)</ref>.</p><p>While graph
type="figure" target="#fig_20">8</ref> shows how we can find large cliques in the terroristRel data <ref type="bibr" target="#b42">(Zhao, Sen and Getoor, 2006)</ref>.</p></div> <div xmlns="http://www. e largest clique in the network. Figure8shows how we can find large cliques in the terroristRel data<ref type="bibr" target="#b42">(Zhao, Sen and Getoor, 2006)</ref>.</figDesc></figure> <figure xmlns=
pe="bibr" target="#b27">(Pr?ulj, Corneil and Jurisica, 2004)</ref>. Graphlets (also known as motifs <ref type="bibr" target="#b25">(Milo, Shen-Orr, Itzkovitz, Kashtan, Chklovskii and Alon, 2002)</ref> graphlets have found numerous applications and were used as the building blocks of network analysis <ref type="bibr" target="#b25">(Milo et al., 2002)</ref>. In social science, graphlet analysis (typi 004)</ref>. Previous work showed that graphlets can be used to define universal classes of networks <ref type="bibr" target="#b25">(Milo et al., 2002)</ref>. Moreover, graphlets are at the heart and f g in real-world networks at frequencies that are significantly higher than those in random networks <ref type="bibr" target="#b25">(Milo et al., 2002</ref>; </p><formula xml:id="formula_0">g4 8 4-node
ation <ref type="bibr" target="#b41">(Zhang, Song, Liu, Liu, Bu and Chen, 2013)</ref>, among others <ref type="bibr" target="#b40">(Zhang, Han, Yang, Song, Yan and Tian, 2013)</ref>.</p><p>While graph
on counting certain types of graphlets (e.g., only connected graphlets such as cliques and cycles) <ref type="bibr" target="#b18">(Kloks, Kratsch and M?ller, 2000;</ref><ref type="bibr" target="#b39"
pe="bibr" target="#b27">(Pr?ulj, Corneil and Jurisica, 2004)</ref>. Graphlets (also known as motifs <ref type="bibr" target="#b25">(Milo, Shen-Orr, Itzkovitz, Kashtan, Chklovskii and Alon, 2002)</ref> graphlets have found numerous applications and were used as the building blocks of network analysis <ref type="bibr" target="#b25">(Milo et al., 2002)</ref>. In social science, graphlet analysis (typi 004)</ref>. Previous work showed that graphlets can be used to define universal classes of networks <ref type="bibr" target="#b25">(Milo et al., 2002)</ref>. Moreover, graphlets are at the heart and f g in real-world networks at frequencies that are significantly higher than those in random networks <ref type="bibr" target="#b25">(Milo et al., 2002</ref>; </p><formula xml:id="formula_0">g4 8 4-node
lel for e = (u, v) ? E do 6:</p><p>60 networks categorized in 8 broad classes from social, facebook <ref type="bibr" target="#b35">(Traud, Mucha and Porter, 2012)</ref>, biological, web, technological f size k = 4, Caltech is noticeably different than others, which is consistent with the findings in <ref type="bibr" target="#b35">(Traud et al., 2012)</ref>.</p><p>thus, they are useful for numerous ata set of Facebook100, which contains 100 Facebook networks that represent a variety of US schools <ref type="bibr" target="#b35">(Traud et al., 2012)</ref>. We plot the GFD (i.e., graphlet frequency se k = 4. The figure shows Caltech noticeably different than others, consistent with the results in <ref type="bibr" target="#b35">(Traud et al., 2012)</ref> which shows how Caltech is well-known to b
mproving community detection <ref type="bibr" target="#b31">(Schaeffer, 2007)</ref>, role discovery <ref type="bibr" target="#b30">(Rossi and Ahmed, 2015b)</ref>, graph classification <ref type="bibr"
0">(Granovetter, 1983)</ref>). In biology <ref type="bibr" target="#b27">(Pr?ulj et al., 2004;</ref><ref type="bibr" target="#b23">Milenkoviae and Pr?ulj, 2008)</ref>, graphlets were widely used for p classification, network alignment, etc.) <ref type="bibr" target="#b27">(Pr?ulj et al., 2004;</ref><ref type="bibr" target="#b23">Milenkoviae and Pr?ulj, 2008;</ref><ref type="bibr" target="#b13">Hay
tion prediction <ref type="bibr" target="#b32">(Shervashidze et al., 2009)</ref>, network alignment <ref type="bibr" target="#b24">(Milenkovi?, Ng, Hayes and Pr?ulj, 2010)</ref>, and phylogeny <ref ty
r" target="#b19">Maclaurin et al., 2015;</ref><ref type="bibr" target="#b4">Chen et al., 2015;</ref><ref type="bibr" target="#b0">Abadi et al., 2016;</ref><ref type="bibr" target="#b23">Paszke et al., ribution, and code generation (see, e.g., <ref type="bibr" target="#b2">Bergstra et al., 2010;</ref><ref type="bibr" target="#b0">Abadi et al., 2016)</ref>. But, because declarative DSLs prevent users bed in §4.6. TensorFlow graphs come with their own set of design principles, which are presented in <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. The following terminology will be used in devices and parallelizes operations when possible. Readers interested in the runtime should consult <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>.</p><p>The function decorator supports code
ese software packages in fact more closely resemble domain-specific languages (DSLs) than libraries <ref type="bibr" target="#b12">(Innes et al., 2018)</ref>. Indeed, models written using automatic di
rentiate compositions thereof (see, e.g., <ref type="bibr" target="#b2">Bergstra et al., 2010;</ref><ref type="bibr" target="#b32">Tokui et al., 2015;</ref><ref type="bibr" target="#b19">Maclaurin et when embedded in an interpreted language like Pythonwhich is the case for popular DSLs like Chainer <ref type="bibr" target="#b32">(Tokui et al., 2015)</ref> and PyTorch <ref type="bibr" target="#b23" pport partially staged computation. Our implementation is similar to the implementations of Chainer <ref type="bibr" target="#b32">(Tokui et al., 2015)</ref>, Autograd <ref type="bibr" target="#b19">(
/ns/1.0"><head>L2HMC.</head><p>In Figure <ref type="figure">4</ref> we show performance of an L2HMC <ref type="bibr" target="#b18">(Levy et al., 2018)</ref>  Eager, TensorFlow Eager with function, and


y makes it difficult for DSLs embedded in it to use such an approach. Some projects, like AutoGraph <ref type="bibr" target="#b20">(Moldovan et al., 2019)</ref> do operate on Python abstract syntax tr raph that operate on abstract syntax trees and rewrite Python control flow to dataflow control flow <ref type="bibr" target="#b20">(Moldovan et al., 2019)</ref>.</p></div> <div xmlns="http://www.tei-c low can require nontrivial programmer intervention. We hope to decrease this friction via Autograph <ref type="bibr" target="#b20">(Moldovan et al., 2019)</ref>.</p><p>Finally, TensorFlow Eager has in

ref type="bibr">The Gluon Team, 2017;</ref><ref type="bibr" target="#b21">Neubig et al., 2017;</ref><ref type="bibr" target="#b10">Innes, 2018;</ref><ref type="bibr">Frostig et al., 2018)</ref>. These


two major end-to-end ASR implementations based on both connectionist temporal classification (CTC) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" tar
-c.org/ns/1.0"><head n="3.2.2.">Attention</head><p>ESPnet uses a location-aware attention mechanism <ref type="bibr" target="#b34">[35]</ref>, as a default attention. A dot-product attention <ref type conditions (e.g., <ref type="bibr" target="#b32">[33]</ref> does not use any language models, while <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b10">[11]</ref> use a word-b
f VGG layer (VGG2()) <ref type="bibr" target="#b31">[32]</ref> followed by BLSTM layers inspired by <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, that is</p><formu ion of character-based LSTMLM, and joint CTC/attention decoding steadily improved the performance.  <ref type="bibr" target="#b32">[33]</ref> 120 hours 10</p><p>Table 2 also compares the result of ESP sult of ESPnet with the other reports. Since these reports are based on different conditions (e.g., <ref type="bibr" target="#b32">[33]</ref> does not use any language models, while <ref type="bibr" t fast training especially for the PyTorch backend even with a single GPU (gtx1080ti), compared with <ref type="bibr" target="#b32">[33]</ref> for the same WSJ task. However, one of the issues of these
solve sequential problems by dynamic programming. ESPnet adopts hybrid CTC/attention end-to-end ASR <ref type="bibr" target="#b16">[17]</ref>, which effectively utilizes the advantages of both archite of end-to-end ASR techniques including a fusion of recurrent neural network language model (RNNLM) <ref type="bibr" target="#b16">[17]</ref>, fast CTC computation by using the warp CTC library <ref t 1.0"><head n="3.3.">Hybrid CTC/attention</head><p>ESPnet adopts hybrid CTC/attention end-to-end ASR <ref type="bibr" target="#b16">[17]</ref>, which effectively utilizes the advantages of both archite ture (multiobjective learning during training and joint decoding during recognition) is proposed in <ref type="bibr" target="#b16">[17]</ref>, and a unique function compared with the other end-to-end
luding Google voice search, Amazon Alexa, and Apple Siri and open source activities including Kaldi <ref type="bibr" target="#b0">[1]</ref>, HTK <ref type="bibr" target="#b1">[2]</ref>, Sphinx <ref ty "#b8">[9]</ref>, as a main deep learning engine. ESPnet also follows the style of Kaldi ASR toolkit <ref type="bibr" target="#b0">[1]</ref> for data processing, feature extraction/format, and recipes
>[6]</ref> network <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Attention-based me represented by bidirectional long short-term memory (BLSTM) with subsampling (called pyramid BLSTM <ref type="bibr" target="#b14">[15]</ref>) given T -length speech feature sequence o1:T to extract h
as follows:</p><p>• CTC-based:</p><p>EESEN <ref type="bibr" target="#b10">[11]</ref>, Stanford CTC <ref type="bibr" target="#b27">[28]</ref>, Baidu Deepsppech <ref type="bibr" target="#b11">[12]</ref
="bibr" target="#b20">[21]</ref>, AMI <ref type="bibr" target="#b21">[22]</ref>, HKUST Mandarin CTS <ref type="bibr" target="#b22">[23]</ref>, VoxForge <ref type="bibr" target="#b23">[24]</ref>, CHiME
y-used dynamic neural network toolkits, Chainer <ref type="bibr" target="#b7">[8]</ref> and PyTorch <ref type="bibr" target="#b8">[9]</ref>, as a main deep learning engine. ESPnet also follows the sty
y-used dynamic neural network toolkits, Chainer <ref type="bibr" target="#b7">[8]</ref> and PyTorch <ref type="bibr" target="#b8">[9]</ref>, as a main deep learning engine. ESPnet also follows the sty
gnition in an end-to-end manner. ESPnet adopts widely-used dynamic neural network toolkits, Chainer <ref type="bibr" target="#b7">[8]</ref> and PyTorch <ref type="bibr" target="#b8">[9]</ref>, as a ma
solve sequential problems by dynamic programming. ESPnet adopts hybrid CTC/attention end-to-end ASR <ref type="bibr" target="#b16">[17]</ref>, which effectively utilizes the advantages of both archite of end-to-end ASR techniques including a fusion of recurrent neural network language model (RNNLM) <ref type="bibr" target="#b16">[17]</ref>, fast CTC computation by using the warp CTC library <ref t 1.0"><head n="3.3.">Hybrid CTC/attention</head><p>ESPnet adopts hybrid CTC/attention end-to-end ASR <ref type="bibr" target="#b16">[17]</ref>, which effectively utilizes the advantages of both archite ture (multiobjective learning during training and joint decoding during recognition) is proposed in <ref type="bibr" target="#b16">[17]</ref>, and a unique function compared with the other end-to-end
phinx <ref type="bibr" target="#b2">[3]</ref>, Julius <ref type="bibr" target="#b3">[4]</ref>, RASR <ref type="bibr" target="#b4">[5]</ref> in addition to general research activities. These open sourc
HTK <ref type="bibr" target="#b1">[2]</ref>, Sphinx <ref type="bibr" target="#b2">[3]</ref>, Julius <ref type="bibr" target="#b3">[4]</ref>, RASR <ref type="bibr" target="#b4">[5]</ref> in addition to
>[6]</ref> network <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Attention-based me represented by bidirectional long short-term memory (BLSTM) with subsampling (called pyramid BLSTM <ref type="bibr" target="#b14">[15]</ref>) given T -length speech feature sequence o1:T to extract h
mple SRILM <ref type="bibr" target="#b5">[6]</ref> network <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar
>[37]</ref>, coverage mechanism <ref type="bibr" target="#b37">[38]</ref>, and multi-head attention <ref type="bibr" target="#b38">[39]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
Pnet also provides a number of recipes for major ASR benchmarks including Wall Street Journal (WSJ) <ref type="bibr" target="#b17">[18]</ref>, Librispeech <ref type="bibr" target="#b18">[19]</ref>, TE
nchmarks including Wall Street Journal (WSJ) <ref type="bibr" target="#b17">[18]</ref>, Librispeech <ref type="bibr" target="#b18">[19]</ref>, TED-LIUM <ref type="bibr" target="#b19">[20]</ref>, Corpu
Pnet also provides a number of recipes for major ASR benchmarks including Wall Street Journal (WSJ) <ref type="bibr" target="#b17">[18]</ref>, Librispeech <ref type="bibr" target="#b18">[19]</ref>, TE
19">[20]</ref>, Corpus of Spontaneous Japanese (CSJ) <ref type="bibr" target="#b20">[21]</ref>, AMI <ref type="bibr" target="#b21">[22]</ref>, HKUST Mandarin CTS <ref type="bibr" target="#b22">[23]</r tion, the ESPnet recipes also include noise robust/far-field speech recognition tasks including AMI <ref type="bibr" target="#b21">[22]</ref>, CHiME-4 <ref type="bibr" target="#b24">[25]</ref>, and CH
aidu Deepsppech <ref type="bibr" target="#b11">[12]</ref>,</p><p>• Attention-based: Attention-LVCSR <ref type="bibr" target="#b28">[29]</ref>, OpenNMT speech to text <ref type="bibr" target="#b29">[30
ctures (e.g., NASNet, AmoebaNet) are not efficient for inference. Recent hardware-aware NAS methods <ref type="bibr" target="#b3">(Cai et al., 2019;</ref><ref type="bibr" target="#b32">Tan et al., 201 yers and skip the last N -D layers, rather than keeping any D layers as done in current NAS methods <ref type="bibr" target="#b3">(Cai et al., 2019;</ref><ref type="bibr" target="#b35">Wu et al., 2019 nd input image size<ref type="foot" target="#foot_1">2</ref> . We also build a latency lookup table <ref type="bibr" target="#b3">(Cai et al., 2019)</ref> on each target hardware platform to predict t rms (Figure <ref type="figure" target="#fig_10">11</ref>) using the ProxylessNAS architecture space <ref type="bibr" target="#b3">(Cai et al., 2019)</ref>. OFA consistently improves the trade-off betw e). It is impossible for previous NAS methods<ref type="bibr" target="#b32">(Tan et al., 2019;</ref><ref type="bibr" target="#b3">Cai et al., 2019)</ref> due to the prohibitive training cost.</figDesc
Han et al., 2015)</ref> or redundant channels <ref type="bibr" target="#b12">(He et al., 2018;</ref><ref type="bibr" target="#b26">Liu et al., 2017)</ref>, and quantization that reduces the bit width



ype="bibr" target="#b40">Zhang et al., 2018)</ref> or accelerate the existing models by compression <ref type="bibr" target="#b9">(Han et al., 2016;</ref><ref type="bibr" target="#b12">He et al., 2018 iu et al., 2017)</ref>, and quantization that reduces the bit width for the weights and activations <ref type="bibr" target="#b9">(Han et al., 2016;</ref><ref type="bibr" target="#b5">Courbariaux et a g et al., 2018)</ref>, etc. Orthogonal to architecting efficient neural networks, model compression <ref type="bibr" target="#b9">(Han et al., 2016)</ref> is another very effective technique for effic
e="bibr" target="#b3">(Cai et al., 2019;</ref><ref type="bibr" target="#b32">Tan et al., 2019;</ref><ref type="bibr" target="#b35">Wu et al., 2019)</ref> directly incorporate the hardware feedback int g any D layers as done in current NAS methods <ref type="bibr" target="#b3">(Cai et al., 2019;</ref><ref type="bibr" target="#b35">Wu et al., 2019)</ref>. As such, one depth setting only corresponds t


ibr" target="#b11">(He et al., 2016;</ref><ref type="bibr" target="#b30">Sandler et al., 2018;</ref><ref type="bibr" target="#b16">Huang et al., 2017)</ref>, we divide a CNN model into a sequence of u

DIP) <ref type="bibr" target="#b14">[15]</ref> and extends it with unified prefetching into the BTB <ref type="bibr" target="#b12">[13]</ref>. The scheme, called Boomerang, discovers BTB misses on the k has addressed this limitation by adding a BTB prefetch capability in a technique called Boomerang <ref type="bibr" target="#b12">[13]</ref>. Boomerang uses a basic-block-oriented BTB to detect BTB m guide local control flow tend to have very short displacements, typically within a few cache blocks <ref type="bibr" target="#b12">[13]</ref>, as shown by dashed arrows in Figure <ref type="figure" ta filling the BTBs, Shotgun takes a hybrid approach by incorporating the features from both Boomerang <ref type="bibr" target="#b12">[13]</ref> and Confluence <ref type="bibr" target="#b9">[10]</ref>. S depending on branch type. The rest of the predecoded branches are stored in the BTB Prefetch Buffer <ref type="bibr" target="#b12">[13]</ref>. On a hit to the BTB Prefetch Buffer, the accessed branch d the associated system software support, making it an expensive proposition as shown in prior work <ref type="bibr" target="#b12">[13]</ref>. The LLC tag array extension, for storing index table, cos dditional coverage. Confluence performs poorly on these applications, as also noted by Kumar et al. <ref type="bibr" target="#b12">[13]</ref>, owing to frequent LLC accesses for loading history metada
l simulator with out-of-order(OoO) cores, memory hierarchy, and on-chip interconnect. We use SMARTS <ref type="bibr" target="#b18">[19]</ref> multiprocessor sampling methodology for sampled execution.
</ref>. Over the years, the problem has persisted; in fact, according to a recent study from Google <ref type="bibr" target="#b7">[8]</ref>, it is getting worse due to continuing expansion in instruct footprint had been expanding at an annualized rate of 27%, doubling over the course of their study <ref type="bibr" target="#b7">[8]</ref>.</p><p>Microarchitecture researchers have proposed a number /p><p>Given that software trends point in the direction of larger code bases and deeper call stacks <ref type="bibr" target="#b7">[8]</ref>, there is a need for a better control flow delivery architec
" target="#b7">[8]</ref>.</p><p>Microarchitecture researchers have proposed a number of instruction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target
">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> and BTB <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> prefetchers over the y
target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> and BTB <ref type=" t work in relieving the frontend bottleneck leverages fetch-directed instruction prefetching (FDIP) <ref type="bibr" target="#b14">[15]</ref> and extends it with unified prefetching into the BTB <ref research has revived the idea of BTB-directed (also called fetch-directed) instruction prefetching <ref type="bibr" target="#b14">[15]</ref>. The basic idea is to leverage the BTB to discover future BTB-directed prefetchers handle BTB misses in one of two ways:</p><p>? The original FDIP technique <ref type="bibr" target="#b14">[15]</ref> speculates through the misses, effectively fetching straig lns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Prefetching with Shotgun</head><p>Similar to FDIP <ref type="bibr" target="#b14">[15]</ref>, Shotgun also employs a Fetch Target Queue(FTQ), as shown
streaming research has focused on lowering the storage costs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>; however, even with B. By exploiting prior observations on control flow commonality in instruction and BTB working sets <ref type="bibr" target="#b9">[10]</ref>, Shotgun prefetches into the conditional branch BTB by pred efetching, thus avoiding the cost and complexity of maintaining two separate control flow histories <ref type="bibr" target="#b9">[10]</ref>.</p><p>The key insight behind unified front-end prefetching in temporal streaming combines the two ideas into a unified front-end prefetcher called Confluence <ref type="bibr" target="#b9">[10]</ref>. Confluence maintains only the L1-I history metadata for bo rporating the features from both Boomerang <ref type="bibr" target="#b12">[13]</ref> and Confluence <ref type="bibr" target="#b9">[10]</ref>. Specifically, while prefetching instruction blocks from LL spatial footprint to prefetch instructions into the L1-I, Shotgun exploits control flow commonality <ref type="bibr" target="#b9">[10]</ref> to prefetch into the C-BTB as well. Thus, when the prefetch the-art temporal streaming prefetcher that uses unified metadata to prefetch into both L1-I and BTB <ref type="bibr" target="#b9">[10]</ref>. To further reduce metadata storage costs, Confluence virtu with a 16K-entry BTB, which was shown to provide a generous upper bound on Confluence's performance <ref type="bibr" target="#b9">[10]</ref>. To provide high L1-I and BTB miss coverage, Confluence req
hem to prefetch the necessary state. The general concept has been applied to both instruction cache <ref type="bibr" target="#b4">[5]</ref> and BTB <ref type="bibr" target="#b2">[3]</ref> prefetching, ds of kilobytes per core) for capturing control flow history <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. To mitigate the cost, two complementary techniques have been
ell-established problem, first characterized in the late 90s <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. Over the years, th


<head n="2.">RELATED WORK</head><p>Simultaneous multithreading (SMT) was proposed by Tullsen et al. <ref type="bibr" target="#b22">[22]</ref> as a way to improve the utilization and throughput of a si
weight perfect matching problem, which can be solved in polynomial time using the blossom algorithm <ref type="bibr" target="#b3">[3]</ref>.</p><p>In summary, the scheduler does the following at the b e. 2. Update the correction factor for the combinations that were executed the previous time slice. <ref type="bibr" target="#b3">3</ref>. Use the inverted model to get an estimate of the CPI stacks i
f>, or performs an offline analysis to predict the interference between applications on an SMT core <ref type="bibr" target="#b23">[23]</ref>. In contrast, we perform online modelbased scheduling, wit requirements across the cores in order to reduce interference and improve throughput. Zhang et al. <ref type="bibr" target="#b23">[23]</ref> propose a methodology to predict the interference among th
applications should run together on one core. Selecting the optimal schedule is an NP-hard problem <ref type="bibr" target="#b10">[10]</ref>, and predicting the performance of a schedule is a non-tri e than 2 million possible schedules. Evaluating each of them would take too much time. Jiang et al. <ref type="bibr" target="#b10">[10]</ref> prove this problem to be NP-complete as soon as n c &gt; 2 ently cope with the large number of possible schedules, we use a technique proposed by Jiang et al. <ref type="bibr" target="#b10">[10]</ref>. They model the scheduling problem as a minimum-weight per
s work on symbiotic job scheduling for SMT uses sampling to explore the space of possible schedules <ref type="bibr" target="#b19">[19]</ref>, relies on novel hardware support <ref type="bibr" target= put, for example due to cache trashing <ref type="bibr" target="#b9">[9]</ref>. Snavely and Tullsen <ref type="bibr" target="#b19">[19]</ref> were the first to propose a mechanism to decide which appl good performance. Note that this sampling phase is different from SMT schedulers that use sampling <ref type="bibr" target="#b19">[19]</ref>: we sample single-threaded execution, while they sample th
ounter measurements to estimate the speedup of SMT when coexecuting two applications. Porter et al. <ref type="bibr" target="#b15">[15]</ref> estimate the speedup of a multithreaded application when e
ounter measurements to estimate the speedup of SMT when coexecuting two applications. Porter et al. <ref type="bibr" target="#b15">[15]</ref> estimate the speedup of a multithreaded application when e
les. We also use a much sparser sampling than used in per-thread cycle accounting proposals for SMT <ref type="bibr" target="#b12">[12]</ref>. We find that the model has a certain bias for some applic rior proposals require extra hardware <ref type="bibr" target="#b5">[5]</ref> or extensive sampling <ref type="bibr" target="#b12">[12]</ref>. Note that the extreme errors for both the forward and the
ctures with SMT cores that provide a similar cycle accounting mechanism, e.g., an Intel Xeon server <ref type="bibr" target="#b14">[14]</ref>. This only requires a one-time training step. The schedule
ughput of a single core. Enabling SMT increases the area and power consumption of a core (5% to 20% <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b11">11]</ref>), mainly due to repl
ounter measurements to estimate the speedup of SMT when coexecuting two applications. Porter et al. <ref type="bibr" target="#b15">[15]</ref> estimate the speedup of a multithreaded application when e
educes the effectiveness of prefetching.</p><p>Recent solutions use the Global History Buffer (GHB) <ref type="bibr" target="#b27">[28]</ref>, which organizes correlation information by storing recent uce the GHB as a general structure for prefetching streams of temporally correlated memory requests <ref type="bibr" target="#b27">[28]</ref>. However, when used to record address correlation <ref typ /DC prefetcher, which which learns the deltas, or differences, between consecutive memory addresses <ref type="bibr" target="#b27">[28]</ref>. Delta correlation allows PC/DC to store all meta-data on http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Using Nesbit and Smith's terminology<ref type="bibr" target="#b27">[28]</ref>, in which the name before the slash describes the referenc streams based on the PC of the loading instruction, which is known to improve coverage and accuracy <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta fetchers, SMS <ref type="bibr" target="#b38">[39]</ref>, which exploits spatial locality, and PC/DC <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13]</ref>, which uses delta long streams. Rather than use address correlation, other GHBbased prefetchers use delta correlation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>, whose space requi -based prefetchers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" tar ype="bibr" target="#b42">[43]</ref> or address correlation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12]</ref>, sacrificing signif
ff-chip meta-data access by fetching long temporal streams <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b8">9]</ref>. Unfortunately, temporal organizations cannot effectively hid
s and prefetch look-ahead distance for off-chip table access <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Nesbit and Smith introduce the GHB as a general stru 3]</ref>. These techniques reduce the memory traffic from 3× <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref> to 1.05-1.75× <ref
pe="bibr" target="#b28">[29]</ref> and by predicting strides <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. Ishii, et al., introduce a clever data structure that comp
d to prefetch these workloads effectively. While some designs reduce this on-chip table requirement <ref type="bibr" target="#b16">[17]</ref>, the table size still grows in proportion to the applicati
erage and accuracy <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b24">25]</ref>. In particular, the get="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref>, but until now, the combination of PC localization and addr poral Prefetching.</head><p>The best known irregular prefetcher, Somogyi, et al.'s STeMS prefetcher <ref type="bibr" target="#b37">[38]</ref>, exploits temporal correlation at a coarse granularity and he temporal component of spatial-temporal prefetchers similar to Somogyi, et al.'s STeMS prefetcher <ref type="bibr" target="#b37">[38]</ref>. More broadly, we believe that the use of a linearized str hes for coarse-grained temporal streams, relying on a complex spatial prefetcher to fill in the gaps<ref type="bibr" target="#b37">[38]</ref>.</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place
ream prefetchers by adding a small histogram of the stream lengths of recently seen memory accesses <ref type="bibr" target="#b18">[19]</ref>. These histograms allow stream buffers to accurately prefe
ltering mechanisms to inform a CDP prefetcher about the pointers that are most likely to be fetched <ref type="bibr" target="#b13">[14]</ref>.</p><p>There are two key differences between the ISB and p tcher whose accuracy is below 40% needs to be throttled down to preserve overall system performance <ref type="bibr" target="#b13">[14]</ref>.</p><p>Figure <ref type="figure" target="#fig_17">13</ref>
seph and Grunwald introduce the notion of correlationbased prefetching with their Markov Prefetcher <ref type="bibr" target="#b22">[23]</ref>, which uses a table to record possible successors of a giv s organize correlated address pairs spatially in a Markov table, which is indexed by memory address <ref type="bibr" target="#b22">[23]</ref>. Unfortunately, Markov tables require multiple table looku
etween storage and effectiveness, with large storage required to achieve good coverage and accuracy <ref type="bibr" target="#b39">[40]</ref>.</p><p>For example, prefetchers based on address correlati liness of hardware prefetching <ref type="bibr" target="#b32">[33]</ref>. Guided Region Prefetching <ref type="bibr" target="#b39">[40]</ref> uses static analysis to annotate load instructions with hi
relation information <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>. Access to this off-chip meta-data increases prediction lat ed prefetchers can amortize the cost of off-chip meta-data access by fetching long temporal streams <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b8">9]</ref>. Unfortunately, temp d memory traffic optimizations for reading and updating the off-chip history buffer and index table <ref type="bibr" target="#b42">[43]</ref>. These techniques reduce the memory traffic from 3× <ref t /ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref> to 1.05-1.75× <ref type="bibr" target="#b42">[43]</ref> for long streams. Rather than use address correlation, oth erences to traverse the entire chain. As a result, GHB-based designs forsake either PC localization <ref type="bibr" target="#b42">[43]</ref> or address correlation <ref type="bibr" target="#b26">[27,
ph embedding methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> to learn the embedding of each item, dubbed Base Graph Embe . Experiments are conducted to compare four methods: BGE, LINE, GES, and EGES. LINE was proposed in <ref type="bibr" target="#b16">[17]</ref>, which captures the first-order and second-order proximity
tory and then apply the state-of-art graph embedding methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> to learn the embedd this section, we give an overview of graph embedding and one of the most popular methods, DeepWalk <ref type="bibr" target="#b14">[15]</ref>, based on which we propose our graph embedding methods in spired by word2vec, Perozzi et al. proposed DeepWalk to learn the embedding of each node in a graph <ref type="bibr" target="#b14">[15]</ref>. They first generate sequences of nodes by running random walk based techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref> use random walks on graphs to obtain node representations w
eep learning methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> enhance the model's ability of capturing non-linearity in g
tory and then apply the state-of-art graph embedding methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> to learn the embedd this section, we give an overview of graph embedding and one of the most popular methods, DeepWalk <ref type="bibr" target="#b14">[15]</ref>, based on which we propose our graph embedding methods in spired by word2vec, Perozzi et al. proposed DeepWalk to learn the embedding of each node in a graph <ref type="bibr" target="#b14">[15]</ref>. They first generate sequences of nodes by running random walk based techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref> use random walks on graphs to obtain node representations w
type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>, content-based methods <ref type="bibr" target="#b1">[2]</ref>, and deep learning based methods <ref type="bibr" target="#b
/ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Moreover, in <ref type="bibr" target="#b3">[4]</ref>, Chang et al.</p><p>proposed a deep learning framework to si
eep learning methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> enhance the model's ability of capturing non-linearity in g
an item graph from users' behavior history and then apply the state-of-art graph embedding methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targ ring non-linearity in graph; 3) Random walk based techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref> use random walks on g
tory and then apply the state-of-art graph embedding methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> to learn the embedd this section, we give an overview of graph embedding and one of the most popular methods, DeepWalk <ref type="bibr" target="#b14">[15]</ref>, based on which we propose our graph embedding methods in spired by word2vec, Perozzi et al. proposed DeepWalk to learn the embedding of each node in a graph <ref type="bibr" target="#b14">[15]</ref>. They first generate sequences of nodes by running random walk based techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref> use random walks on graphs to obtain node representations w
et="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Moreover, in <ref type="bibr" target="#b3">[4]</ref>, Chan
to graph embedding <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Moreover, in <ref
ding block for many efficient neural network architectures <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b19">20]</ref> and we use them in
ype="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and connectivity le
nce with other architectures such as Faster-RCNN <ref type="bibr" target="#b35">[36]</ref> and RFCN <ref type="bibr" target="#b36">[37]</ref> since our focus is on mobile/real-time models.</p><p>SSDLi
"#b38">[39]</ref> for the task of mobile semantic segmentation. DeepLabv3 adopts atrous convolution <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" ta
further analysis. We measure our performance on ImageNet [1] classification, COCO object detection <ref type="bibr" target="#b1">[2]</ref>, VOC image segmentation <ref type="bibr" target="#b2">[3]</r version of the Single Shot Detector (SSD) <ref type="bibr" target="#b33">[34]</ref> on COCO dataset <ref type="bibr" target="#b1">[2]</ref>. We also compare to YOLOv2 <ref type="bibr" target="#b34">[3
nce with other architectures such as Faster-RCNN <ref type="bibr" target="#b35">[36]</ref> and RFCN <ref type="bibr" target="#b36">[37]</ref> since our focus is on mobile/real-time models.</p><p>SSDLi
of network pruning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">15,</ref><ref type="bibr" target="#b15">16,
ure maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) <ref type="bibr" target="#b42">[43]</ref> containing three 3 × 3 convolutions with different atrous
ure maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) <ref type="bibr" target="#b42">[43]</ref> containing three 3 × 3 convolutions with different atrous
assification, COCO object detection <ref type="bibr" target="#b1">[2]</ref>, VOC image segmentation <ref type="bibr" target="#b2">[3]</ref>. We evaluate the trade-offs between accuracy, and number of stride = 16 or 8 for denser feature maps. We conduct the experiments on the PASCAL VOC 2012 dataset <ref type="bibr" target="#b2">[3]</ref>, with extra annotated images from <ref type="bibr" target="#
>.</p><p>Recently, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, opened up a new di
nship between features, we introduce explicit feature maps <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b20">13]</ref> to CCA. Finally, using the features that are projected to t use. In contrast, recent advances of explicit feature maps <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b20">13]</ref> can convert nonlinear problems to linear problems, which ca computation complexity, one can use explicit feature maps <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b20">13]</ref>. Let φ(x) denote an explicit feature mapping such that K i rnel. All other histogram-based features were mapped using the exact Bhattacharyya kernel map- ping <ref type="bibr" target="#b20">[13]</ref>. Finally, similar to <ref type="bibr" target="#b16">[9]</r
orms the sentiment view. Then, using a framework of multi-view canonical correlation analysis (CCA) <ref type="bibr" target="#b18">[11]</ref>, we calculate a latent embedding space in which correlatio s the linear relationship between random variables. Several nonlinear extensions such as kernel CCA <ref type="bibr" target="#b18">[11]</ref> and Deep CCA <ref type="bibr" target="#b29">[22]</ref> hav ions among multiple views using a framework of the generalization of canonical correlation analysis <ref type="bibr" target="#b18">[11]</ref>. Let X i (i ∈ {v, t, s}) denote the feature matrix of the at the distances in the resulting space between each pair of views for the same image are minimized <ref type="bibr" target="#b18">[11]</ref>. The objective function to learn the latent space is as fo ϕ j (X j ), and w ik represents the k-th column of the matrix W i . In the conventional kernel CCA <ref type="bibr" target="#b18">[11]</ref>, kernel trick is used in Eq. (1). To reduce the computatio
sentiment information in the text, we introduce an external sentiment knowledge base, Senti-WordNet <ref type="bibr" target="#b17">[10]</ref>, which forms the sentiment view. Then, using a framework o ment aspect of the associate text. For this, we use an external knowledge base, called SentiWordNet <ref type="bibr" target="#b17">[10]</ref>. It is based on the well-known English lexical dictionary ures with the mid-level features (denoted as Low&amp;SentiBank), and a textual feature-based method <ref type="bibr" target="#b17">[10]</ref> (denoted as SentiStrength<ref type="foot" target="#foot_3"
fically, to capture the nonlinear relationship between features, we introduce explicit feature maps <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b20">13]</ref> to CCA. Finally, u high computational complexity and memory use. In contrast, recent advances of explicit feature maps <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b20">13]</ref> can convert nonlin l trick is used in Eq. (1). To reduce the computation complexity, one can use explicit feature maps <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b20">13]</ref>. Let φ(x) denote a IST features, attribute features, and SentiBank features, we use the random Fourier feature mapping <ref type="bibr" target="#b19">[12]</ref> to approximate the Gaussian kernel. All other histogram-ba
get="#b15">[8,</ref><ref type="bibr" target="#b16">9,</ref><ref type="bibr" target="#b23">[16]</ref><ref type="bibr" target="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" t
and descriptions) can improve the image content recognition <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9]</ref>. Inspired from these studies, to bridge images and sentiment ny times a word appears in text around the image. Following <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9]</ref>, we use the linear kernel for the textual features, which co ved several image annotation and crossmodal retrieval tasks <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9,</ref><ref type="bibr" target="#b23">[16]</ref><ref type="bibr" tar r problems to linear problems, which can be solved by linear frameworks with a low computation cost <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b30">23]</ref>. Following these st w.</p><p>Visual features: Following the feature design used in recent visual classification methods <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b25">18,</ref><ref type="bibr" tar view formulation has recently proven to be effective for cross-modal retrieval and image annotation <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. In the following s alues of each dimension in the embedding space. p is a weighting parameter, which is set to 4 as in <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. Using Eq. ( <ref t exact Bhattacharyya kernel map- ping <ref type="bibr" target="#b20">[13]</ref>. Finally, similar to <ref type="bibr" target="#b16">[9]</ref>, we reduce each kernelmapped feature to 500 dimensions usin φ(x). Instead of using the kernel trick, the mapping φ(x) can substituted to the objective function <ref type="bibr" target="#b16">[9]</ref>. Solving the following generalized eigenvalue problem provi
art theory using relatively small and controlled datasets <ref type="bibr" target="#b21">[14,</ref><ref type="bibr" target="#b22">15]</ref>, while recent works have started to analyze the sentiments
been investigated based on psychology and art theory using relatively small and controlled datasets <ref type="bibr" target="#b21">[14,</ref><ref type="bibr" target="#b22">15]</ref>, while recent work
odal retrieval tasks <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9,</ref><ref type="bibr" target="#b23">[16]</ref><ref type="bibr" target="#b24">[17]</ref><ref type="bibr" t
be solved by linear frameworks with a low computation cost <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b30">23]</ref>. Following these studies, we introduce the explicit feature
="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" target="#b26">[19]</ref><ref type="bibr" target="#b27">[20]</ref>, but its effectiveness has not been fully demonstrated in
art theory using relatively small and controlled datasets <ref type="bibr" target="#b21">[14,</ref><ref type="bibr" target="#b22">15]</ref>, while recent works have started to analyze the sentiments
and descriptions) can improve the image content recognition <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9]</ref>. Inspired from these studies, to bridge images and sentiment ny times a word appears in text around the image. Following <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9]</ref>, we use the linear kernel for the textual features, which co ved several image annotation and crossmodal retrieval tasks <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9,</ref><ref type="bibr" target="#b23">[16]</ref><ref type="bibr" tar r problems to linear problems, which can be solved by linear frameworks with a low computation cost <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b30">23]</ref>. Following these st w.</p><p>Visual features: Following the feature design used in recent visual classification methods <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b25">18,</ref><ref type="bibr" tar view formulation has recently proven to be effective for cross-modal retrieval and image annotation <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. In the following s alues of each dimension in the embedding space. p is a weighting parameter, which is set to 4 as in <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. Using Eq. ( <ref t exact Bhattacharyya kernel map- ping <ref type="bibr" target="#b20">[13]</ref>. Finally, similar to <ref type="bibr" target="#b16">[9]</ref>, we reduce each kernelmapped feature to 500 dimensions usin φ(x). Instead of using the kernel trick, the mapping φ(x) can substituted to the objective function <ref type="bibr" target="#b16">[9]</ref>. Solving the following generalized eigenvalue problem provi
et="#b16">9,</ref><ref type="bibr" target="#b23">[16]</ref><ref type="bibr" target="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" target="#b26">[19]</ref><ref type="bibr" t the projection P v to the original feature, following the conventional cross-modal retrieval method <ref type="bibr" target="#b25">[18]</ref>. Based on the new feature representation of the training d feature design used in recent visual classification methods <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b25">18,</ref><ref type="bibr" target="#b26">19]</ref>, we represent image
et="#b16">9,</ref><ref type="bibr" target="#b23">[16]</ref><ref type="bibr" target="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" target="#b26">[19]</ref><ref type="bibr" t the projection P v to the original feature, following the conventional cross-modal retrieval method <ref type="bibr" target="#b25">[18]</ref>. Based on the new feature representation of the training d feature design used in recent visual classification methods <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b25">18,</ref><ref type="bibr" target="#b26">19]</ref>, we represent image
be solved by linear frameworks with a low computation cost <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b30">23]</ref>. Following these studies, we introduce the explicit feature
ype="bibr" target="#b13">[6]</ref>. In addition, we will introduce the deep learning-based features <ref type="bibr" target="#b37">[30,</ref><ref type="bibr" target="#b38">31]</ref>, which have signif
<ref type="bibr" target="#b33">[26]</ref>, and has been utilized in text-based opinion mining tasks <ref type="bibr" target="#b34">[27]</ref>.</p><p>In SentiWordNet, three types of sentiment scores, "
ding opinion mining about social events, product marketing, and affective human-machine interaction <ref type="bibr" target="#b10">[3]</ref>. Thus, automatic inference of the sentiment implied in the
<ref type="bibr" target="#b33">[26]</ref>, and has been utilized in text-based opinion mining tasks <ref type="bibr" target="#b34">[27]</ref>.</p><p>In SentiWordNet, three types of sentiment scores, "
has received increasing research attention in recent years <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" target="#b13">[6]</ref><ref type="bibr" tar visual features for training sentiment polarity classifiers <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" target="#b13">[6]</ref>. However, due to th ntiments of unconstrained real-world images on social media <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" target="#b13">[6]</ref><ref type="bibr" tar istogram and SIFT-based features of images are used in <ref type="bibr" target="#b11">[4]</ref>. In <ref type="bibr" target="#b12">[5]</ref>, emotion-related adjective-noun pairs were selected for ima ttribute features <ref type="bibr" target="#b31">[24]</ref> and 1,200-dimensional SentiBank outputs <ref type="bibr" target="#b12">[5]</ref>.</p><p>For GIST features, attribute features, and SentiBank <ref type="bibr" target="#b11">[4]</ref> (denoted as Low), a mid-level visual feature-based method <ref type="bibr" target="#b12">[5]</ref> (denoted as SentiBank), a method that concatenates low-leve 5% 50.06 ± 1.09% Low <ref type="bibr" target="#b11">[4]</ref> 69.44 ± 0.85% 67.16 ± 1.28% SentiBank <ref type="bibr" target="#b12">[5]</ref> 70.01 ± 0.63% 67.26 ± 1.12% Low&amp;SentiBank 70.54 ± 1.00% linear SVM, which is also used in the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>. Note that although this paper focuses on binary classificat n binary classification as well as the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, our method can be easily extended to multi-class sentiment binary classification problem following the previous works <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, we discarded the images labeled by "neutral" and the images
get="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" target="#b13">[6]</ref><ref type="bibr" target="#b14">[7]</ref>.</p><p>Conventional methods of image sentiment analysis hav get="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" target="#b13">[6]</ref><ref type="bibr" target="#b14">[7]</ref>. Typically, the goal is to determine the sentiment polarity ws of the data (e.g., tag concurrence) are discarded in training classifiers. Recently, Wang et al. <ref type="bibr" target="#b14">[7]</ref> exploited both visual content and textual information for s ment-based image clustering in a nonnegative matrix factorization framework. However, the method in <ref type="bibr" target="#b14">[7]</ref> has severe sensitivity to the initialization, and the exper 03 ± 1.36% SentiStrength <ref type="bibr" target="#b36">[29]</ref> 59.30 ± 0.87% 62.78 ± 0.91% USEA <ref type="bibr" target="#b14">[7]</ref> 51.87 ± visual feature set as those described in Sec. 3.1, thod with the state-of-the-art method that exploits visual and textual features of the testing data <ref type="bibr" target="#b14">[7]</ref> (denoted as USEA). For reference, the random classification sing the automatic annotation algorithm based on image tags <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b14">7]</ref>, but it is unreliable due to the noisy tags or lack of tags.
">[5]</ref> 70.01 ± 0.63% 67.26 ± 1.12% Low&amp;SentiBank 70.54 ± 1.00% 68.03 ± 1.36% SentiStrength <ref type="bibr" target="#b36">[29]</ref> 59.30 ± 0.87% 62.78 ± 0.91% USEA <ref type="bibr" target="
e of the sentiment implied in the images has received increasing research attention in recent years <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar analysis have aimed to design effective visual features for training sentiment polarity classifiers <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar ent works have started to analyze the sentiments of unconstrained real-world images on social media <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar ain a sen-timent polarity classifier, color histogram and SIFT-based features of images are used in <ref type="bibr" target="#b11">[4]</ref>. In <ref type="bibr" target="#b12">[5]</ref>, emotion-relat ional methods, which exploit either visual or textual view: a low-level visual feature-based method <ref type="bibr" target="#b11">[4]</ref> (denoted as Low), a mid-level visual feature-based method < >[10]</ref> (denoted as SentiStrength<ref type="foot" target="#foot_3">3</ref> ). Note that for Low <ref type="bibr" target="#b11">[4]</ref>, we use the same Table <ref type="table">2</ref>. Average a classification accuracy of image sentiment polarity for 10 runs in each dataset. Note that for Low <ref type="bibr" target="#b11">[4]</ref>, we use the same visual feature set as those described in S iBank outputs.</p><p>Method Flickr dataset Instagram dataset Random 49.78 ± 1.05% 50.06 ± 1.09% Low <ref type="bibr" target="#b11">[4]</ref> 69.44 ± 0.85% 67.16 ± 1.28% SentiBank <ref type="bibr" targ ifier.</p><p>In this paper, we exploit a linear SVM, which is also used in the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>. Note that although Note that although this paper focuses on binary classification as well as the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, our method can be e hods exploited pseudo sentiment labels using the automatic annotation algorithm based on image tags <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b14">7]</ref>, but it is unreliabl et. Since this experiment targets on the binary classification problem following the previous works <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, we discarded the im
="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" target="#b26">[19]</ref><ref type="bibr" target="#b27">[20]</ref>, but its effectiveness has not been fully demonstrated in
art theory using relatively small and controlled datasets <ref type="bibr" target="#b21">[14,</ref><ref type="bibr" target="#b22">15]</ref>, while recent works have started to analyze the sentiments
e of the sentiment implied in the images has received increasing research attention in recent years <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar analysis have aimed to design effective visual features for training sentiment polarity classifiers <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar ent works have started to analyze the sentiments of unconstrained real-world images on social media <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar ain a sen-timent polarity classifier, color histogram and SIFT-based features of images are used in <ref type="bibr" target="#b11">[4]</ref>. In <ref type="bibr" target="#b12">[5]</ref>, emotion-relat ional methods, which exploit either visual or textual view: a low-level visual feature-based method <ref type="bibr" target="#b11">[4]</ref> (denoted as Low), a mid-level visual feature-based method < >[10]</ref> (denoted as SentiStrength<ref type="foot" target="#foot_3">3</ref> ). Note that for Low <ref type="bibr" target="#b11">[4]</ref>, we use the same Table <ref type="table">2</ref>. Average a classification accuracy of image sentiment polarity for 10 runs in each dataset. Note that for Low <ref type="bibr" target="#b11">[4]</ref>, we use the same visual feature set as those described in S iBank outputs.</p><p>Method Flickr dataset Instagram dataset Random 49.78 ± 1.05% 50.06 ± 1.09% Low <ref type="bibr" target="#b11">[4]</ref> 69.44 ± 0.85% 67.16 ± 1.28% SentiBank <ref type="bibr" targ ifier.</p><p>In this paper, we exploit a linear SVM, which is also used in the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>. Note that although Note that although this paper focuses on binary classification as well as the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, our method can be e hods exploited pseudo sentiment labels using the automatic annotation algorithm based on image tags <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b14">7]</ref>, but it is unreliabl et. Since this experiment targets on the binary classification problem following the previous works <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, we discarded the im
="#b23">[16]</ref><ref type="bibr" target="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" target="#b26">[19]</ref><ref type="bibr" target="#b27">[20]</ref>, but its effectiv ssification methods <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b25">18,</ref><ref type="bibr" target="#b26">19]</ref>, we represent image appearance using a combination of diffe be effective for cross-modal retrieval and image annotation <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. In the following subsection, we describe how to use the la space. p is a weighting parameter, which is set to 4 as in <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. Using Eq. ( <ref type="formula" target="#formula_3">3</ref
get="#b15">[8,</ref><ref type="bibr" target="#b16">9,</ref><ref type="bibr" target="#b23">[16]</ref><ref type="bibr" target="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" t
rrelations among multiple views for better sentiment analysis. Canonical correlation analysis (CCA) <ref type="bibr" target="#b28">[21]</ref> is one of the techniques typically used to learn the align
e="bibr" target="#b17">[10]</ref>. It is based on the well-known English lexical dictionary WordNet <ref type="bibr" target="#b33">[26]</ref>, and has been utilized in text-based opinion mining tasks
="#b23">[16]</ref><ref type="bibr" target="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" target="#b26">[19]</ref><ref type="bibr" target="#b27">[20]</ref>, but its effectiv ssification methods <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b25">18,</ref><ref type="bibr" target="#b26">19]</ref>, we represent image appearance using a combination of diffe be effective for cross-modal retrieval and image annotation <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. In the following subsection, we describe how to use the la space. p is a weighting parameter, which is set to 4 as in <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. Using Eq. ( <ref type="formula" target="#formula_3">3</ref
e of the sentiment implied in the images has received increasing research attention in recent years <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar analysis have aimed to design effective visual features for training sentiment polarity classifiers <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar ent works have started to analyze the sentiments of unconstrained real-world images on social media <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar ain a sen-timent polarity classifier, color histogram and SIFT-based features of images are used in <ref type="bibr" target="#b11">[4]</ref>. In <ref type="bibr" target="#b12">[5]</ref>, emotion-relat ional methods, which exploit either visual or textual view: a low-level visual feature-based method <ref type="bibr" target="#b11">[4]</ref> (denoted as Low), a mid-level visual feature-based method < >[10]</ref> (denoted as SentiStrength<ref type="foot" target="#foot_3">3</ref> ). Note that for Low <ref type="bibr" target="#b11">[4]</ref>, we use the same Table <ref type="table">2</ref>. Average a classification accuracy of image sentiment polarity for 10 runs in each dataset. Note that for Low <ref type="bibr" target="#b11">[4]</ref>, we use the same visual feature set as those described in S iBank outputs.</p><p>Method Flickr dataset Instagram dataset Random 49.78 ± 1.05% 50.06 ± 1.09% Low <ref type="bibr" target="#b11">[4]</ref> 69.44 ± 0.85% 67.16 ± 1.28% SentiBank <ref type="bibr" targ ifier.</p><p>In this paper, we exploit a linear SVM, which is also used in the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>. Note that although Note that although this paper focuses on binary classification as well as the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, our method can be e hods exploited pseudo sentiment labels using the automatic annotation algorithm based on image tags <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b14">7]</ref>, but it is unreliabl et. Since this experiment targets on the binary classification problem following the previous works <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, we discarded the im
ures around training images (e.g., tags and descriptions) can improve the image content recognition <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9]</ref>. Inspired from these features associated to images has improved several image annotation and crossmodal retrieval tasks <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9,</ref><ref type="bibr" targ agof-words approach, which counts how many times a word appears in text around the image. Following <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9]</ref>, we use the linear k
">[5]</ref> 70.01 ± 0.63% 67.26 ± 1.12% Low&amp;SentiBank 70.54 ± 1.00% 68.03 ± 1.36% SentiStrength <ref type="bibr" target="#b36">[29]</ref> 59.30 ± 0.87% 62.78 ± 0.91% USEA <ref type="bibr" target="
="#b23">[16]</ref><ref type="bibr" target="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" target="#b26">[19]</ref><ref type="bibr" target="#b27">[20]</ref>, but its effectiv ssification methods <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b25">18,</ref><ref type="bibr" target="#b26">19]</ref>, we represent image appearance using a combination of diffe be effective for cross-modal retrieval and image annotation <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. In the following subsection, we describe how to use the la space. p is a weighting parameter, which is set to 4 as in <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. Using Eq. ( <ref type="formula" target="#formula_3">3</ref
e of the sentiment implied in the images has received increasing research attention in recent years <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar analysis have aimed to design effective visual features for training sentiment polarity classifiers <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar ent works have started to analyze the sentiments of unconstrained real-world images on social media <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" tar ain a sen-timent polarity classifier, color histogram and SIFT-based features of images are used in <ref type="bibr" target="#b11">[4]</ref>. In <ref type="bibr" target="#b12">[5]</ref>, emotion-relat ional methods, which exploit either visual or textual view: a low-level visual feature-based method <ref type="bibr" target="#b11">[4]</ref> (denoted as Low), a mid-level visual feature-based method < >[10]</ref> (denoted as SentiStrength<ref type="foot" target="#foot_3">3</ref> ). Note that for Low <ref type="bibr" target="#b11">[4]</ref>, we use the same Table <ref type="table">2</ref>. Average a classification accuracy of image sentiment polarity for 10 runs in each dataset. Note that for Low <ref type="bibr" target="#b11">[4]</ref>, we use the same visual feature set as those described in S iBank outputs.</p><p>Method Flickr dataset Instagram dataset Random 49.78 ± 1.05% 50.06 ± 1.09% Low <ref type="bibr" target="#b11">[4]</ref> 69.44 ± 0.85% 67.16 ± 1.28% SentiBank <ref type="bibr" targ ifier.</p><p>In this paper, we exploit a linear SVM, which is also used in the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>. Note that although Note that although this paper focuses on binary classification as well as the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, our method can be e hods exploited pseudo sentiment labels using the automatic annotation algorithm based on image tags <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b14">7]</ref>, but it is unreliabl et. Since this experiment targets on the binary classification problem following the previous works <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, we discarded the im
dition, we will introduce the deep learning-based features <ref type="bibr" target="#b37">[30,</ref><ref type="bibr" target="#b38">31]</ref>, which have significantly improved many computer vision tas
dition, we will introduce the deep learning-based features <ref type="bibr" target="#b37">[30,</ref><ref type="bibr" target="#b38">31]</ref>, which have significantly improved many computer vision tas
eral nonlinear extensions such as kernel CCA <ref type="bibr" target="#b18">[11]</ref> and Deep CCA <ref type="bibr" target="#b29">[22]</ref> have been proposed to reveal nonlinear relationship betwee
eral nonlinear extensions such as kernel CCA <ref type="bibr" target="#b18">[11]</ref> and Deep CCA <ref type="bibr" target="#b29">[22]</ref> have been proposed to reveal nonlinear relationship betwee
="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" target="#b26">[19]</ref><ref type="bibr" target="#b27">[20]</ref>, but its effectiveness has not been fully demonstrated in
ding opinion mining about social events, product marketing, and affective human-machine interaction <ref type="bibr" target="#b10">[3]</ref>. Thus, automatic inference of the sentiment implied in the
eral nonlinear extensions such as kernel CCA <ref type="bibr" target="#b18">[11]</ref> and Deep CCA <ref type="bibr" target="#b29">[22]</ref> have been proposed to reveal nonlinear relationship betwee
representation to implement optimizations, e.g., auto differentiation and dynamic memory management <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target tional Graphs</head><p>Computational graphs are a common way to represent programs in DL frameworks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target <div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Deep learning frameworks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target on graph DSLs are a typical way to represent and perform high-level optimizations. Tensorflow's XLA <ref type="bibr" target="#b2">[3]</ref> and the recently introduced DLVM <ref type="bibr" target="#b
erators. Our approach could potentially benefit existing systems that compile deep learning to FPGA <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref>, as well. This pap
rk on DSL for GPUs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> and polyhedralbased
get="#b36">37]</ref> and polyhedralbased loop transformation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43]</ref>. TACO <ref type="bibr" target="#b22">[23]</ref> introduces
versity of hardware characteristics, including embedded CPUs, GPUs, FPGAs, and ASICs (e.g., the TPU <ref type="bibr" target="#b20">[21]</ref>). These hardware targets diverge in  terms of memory organ tives to benefit from acceleration. Further, accelerator designs also commonly favor leaner control <ref type="bibr" target="#b20">[21]</ref> and offload most scheduling complexity to the compiler sta <ref type="bibr" target="#b43">[44]</ref>. In contrast, specialized DL accelerators such as the TPU <ref type="bibr" target="#b20">[21]</ref> usually favor leaner control with a decoupled access-execu r compute primitives <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref>, while GPUs and CPUs continuously improve their processing r compute primitives <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref>. These new primitives create both opportunities and challen the emerging popularity of accelerators for deep learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>, it remains unclear how a compilation stack can be built to stills characteristics from previous accelerator proposals <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref> into a minimalist h
ral types of models in our ML optimizer. We employ a gradient tree boosting model (based on XGBoost <ref type="bibr" target="#b7">[8]</ref>), which makes predictions based on features extracted from t
amic memory management <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9]</ref>. Graph-level optimizations, however, are often too high-level " target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. Figure <ref type="figure" target="#fig_2">3</ref> shows an e " target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> provide convenient interfaces for users to express DL workloa "bibr" target="#b30">[31]</ref>. We compare our approach to existing DL frameworks, including MxNet <ref type="bibr" target="#b8">[9]</ref> and Tensor-Flow <ref type="bibr" target="#b1">[2]</ref>, tha
ncy. Results are shown in Figure <ref type="figure" target="#fig_12">10</ref> as a roofline diagram <ref type="bibr" target="#b46">[47]</ref>; roofline performance diagrams provide insight into how we
at generate different versions of the program with various optimizations. This layer extends Halide <ref type="bibr" target="#b31">[32]</ref>'s compute/schedule separation concept by also separating t ilds on Halide's idea of decoupling descriptions from computation rules (or schedule optimizations) <ref type="bibr" target="#b31">[32]</ref> and extends it to support new optimizations (nested parall e optimizations for various backends. Adopting the decoupled compute/schedule principle from Halide <ref type="bibr" target="#b31">[32]</ref>, we use a schedule to denote a specific mapping from a ten erate low-level code for a given final schedule.</p><p>Our tensor expression takes cues from Halide <ref type="bibr" target="#b31">[32]</ref>, Darkroom <ref type="bibr" target="#b16">[17]</ref>, and T ficant engineering effort for each hardware back-end and operator-variant combination.</p><p>Halide <ref type="bibr" target="#b31">[32]</ref> introduced the idea of separating computing and scheduling
t="#b20">[21]</ref> usually favor leaner control with a decoupled access-execute (DAE) architecture <ref type="bibr" target="#b34">[35]</ref> and offload the problem of fine-grained synchronization to
ncy. Results are shown in Figure <ref type="figure" target="#fig_12">10</ref> as a roofline diagram <ref type="bibr" target="#b46">[47]</ref>; roofline performance diagrams provide insight into how we
get="#b11">12,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b40">42]</ref>.</p><p>Single image target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" tar tep. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images <ref type="bibr" target="#b22">[24]</ref>. (d) Iterative up and downsampling approach is proposed by can preserve HR components better.</p><p>(c) Progressive upsampling was recently proposed in LapSRN <ref type="bibr" target="#b22">[24]</ref>. It progressively reconstructs the multiple SR images with N <ref type="bibr" target="#b20">[22]</ref>, DRRN <ref type="bibr" target="#b40">[42]</ref>, LapSRN <ref type="bibr" target="#b22">[24]</ref>) on Set5 dataset for 4× enlargement.</p><p>the-art methods N <ref type="bibr" target="#b20">[22]</ref>, DRRN <ref type="bibr" target="#b40">[42]</ref>, LapSRN <ref type="bibr" target="#b22">[24]</ref>, and EDSR <ref type="bibr" target="#b28">[30]</ref>. We ca step. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images<ref type="bibr" target="#b22">[24]</ref>. (d) Iterative up and downsampling approach is proposed by
i-c.org/ns/1.0"><head n="1.">Introduction</head><p>Significant progress in deep learning for vision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" ta (4) Improvement with dense connection. We improve the accuracy of our network by densely connected <ref type="bibr" target="#b14">[15]</ref> each upand down-sampling stage to encourage feature reuse. ad n="3.2.">Dense projection units</head><p>The dense inter-layer connectivity pattern in DenseNets <ref type="bibr" target="#b14">[15]</ref> has been shown to alleviate the vanishinggradient problem,
inear LR-to-HR mapping, implemented as a deep neural network <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" targe icubic, to upscale LR input images before entering the network. (b) Single upsampling (e.g., FSRCNN <ref type="bibr" target="#b6">[7]</ref>, ESPCN <ref type="bibr" target="#b35">[37]</ref>) propagates s simple yet effective way to increase the spatial resolution. This approach was proposed by FSRCNN <ref type="bibr" target="#b6">[7]</ref> and ESPCN <ref type="bibr" target="#b35">[37]</ref>. These m A+ <ref type="bibr" target="#b42">[44]</ref>, SRCNN <ref type="bibr" target="#b5">[6]</ref>, FSRCNN <ref type="bibr" target="#b6">[7]</ref>, VDSR <ref type="bibr" target="#b19">[21]</ref>,    DRCN <re Bicubic, to upscale LR input images before entering the network. (b) Single upsampling (e.g., FSRCNN<ref type="bibr" target="#b6">[7]</ref>, ESPCN<ref type="bibr" target="#b35">[37]</ref>) propagates crop s pixels near image boundary before evaluation as in <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b6">7]</ref>. Some of the existing networks such as SRCNN, FSRCNN, VDSR, a
get="#b44">46,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b29">31]</ref>.</p><p>In the conte
earning for vision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" targe et="#b25">[27,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">19]</ref>. For the SR task, Joh
2 × 12 convolutional layer with eight striding and two padding.  We initialize the weights based on <ref type="bibr" target="#b13">[14]</ref>. Here, std is computed by ( 2/n l ) where n l = f<ref type
the relevant results <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b23">25]</ref>. Perhaps hampered by lack of such feedback, the current SR
l system is believed to use a feedback connection to simply guide the task for the relevant results <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" targ
tly been propagating to the field of super-resolution (SR) <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" targe
="bibr" target="#b15">17]</ref> has recently been propagating to the field of super-resolution (SR) <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" ta arget="#b34">36,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">19]</ref>. For the SR task, Johnson et al. <ref type="bibr" target="# ibr" target="#b4">5,</ref><ref type="bibr" target="#b17">19]</ref>. For the SR task, Johnson et al. <ref type="bibr" target="#b17">[19]</ref> introduced perceptual losses based on high-level features
rget="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" tar
ize the accuracy. Although it is not possible to obtain a universally good active learning strategy <ref type="bibr" target="#b4">(Dasgupta, 2004)</ref>, there exist many heuristics <ref type="bibr" t l side, it is shown that greedy active learning is not possible in algorithm and data agnostic case <ref type="bibr" target="#b4">(Dasgupta, 2005)</ref>. However, there are data dependent results show

992)</ref>, ensemble approaches <ref type="bibr" target="#b30">(McCallumzy &amp; Nigamy, 1998;</ref><ref type="bibr" target="#b9">Freund et al., 1997)</ref> and uncertainty based methods <ref type="bi
sarial methods which can learn a data distribution as a result of a two-player non-cooperative game <ref type="bibr" target="#b35">(Salimans et al., 2016;</ref><ref type="bibr" target="#b15">Goodfello
use a non-parametric model like Gaussian process to estimate the expected improvement by each query <ref type="bibr" target="#b26">(Kapoor et al., 2007)</ref> or the expected error after a set of quer
#b36">Settles (2010)</ref>. It covers acquisition functions such as information theoretical methods <ref type="bibr" target="#b29">(MacKay, 1992)</ref>, ensemble approaches <ref type="bibr" target="#b


992)</ref>, ensemble approaches <ref type="bibr" target="#b30">(McCallumzy &amp; Nigamy, 1998;</ref><ref type="bibr" target="#b9">Freund et al., 1997)</ref> and uncertainty based methods <ref type="bi

rmation. Our algorithm is also the first one which is applied to the CNNs. Most similar to ours are <ref type="bibr" target="#b25">(Joshiy et al., 2010)</ref> and <ref type="bibr" target="#b42">(Wang ="#b25">(Joshiy et al., 2010)</ref> and <ref type="bibr" target="#b42">(Wang &amp; Ye, 2015)</ref>. <ref type="bibr" target="#b25">Joshiy et al. (2010)</ref> uses a similar optimization problem. Howev
c.org/ns/1.0"><head n="4.2.2">Predicting Performance in Solo-Mode.</head><p>Referring to prior work <ref type="bibr" target="#b11">[12]</ref>, we design shadow solo-cycle accounting (SSCA) approach to e prediction method of QoSMT is inspired by PTA. PTA uses MLP correction to achieve higher accuracy <ref type="bibr" target="#b11">[12]</ref>. However, we can not get an application's MLP without offl SMT throughput and fairness, but they did not take performance control into account. Eyerman et al. <ref type="bibr" target="#b11">[12]</ref> proposed the per-thread cycle accounting (PTA) mechanism t
ze in-core resources, against the original motivation of SMT. Although there is previous literature <ref type="bibr" target="#b8">[9]</ref> [10]on hardware design for guarantee of quality-of-service ( ol based on user-defined target. There are many studies <ref type="bibr" target="#b6">[7]</ref>[30] <ref type="bibr" target="#b8">[9]</ref>[10] <ref type="bibr" target="#b24">[27]</ref> providing solu
tel and AMD. Thus, we focus on IBM's design. Generally, IBM introduced two-level control mechanisms <ref type="bibr" target="#b3">[4]</ref> since POWER5 to enable software to adjust instruction fetch
means cache static partitioning. Cazorla is a state-of-the-art mechanism presented by Cazorla,e.g., <ref type="bibr" target="#b5">[6]</ref>. QoSMT, our methodology, will dynamically adjust all resourc ><head n="6.4">Hardware based policies</head><p>The closest work to QoSMT is the design proposed in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. For their work, wheth
means cache static partitioning. Cazorla is a state-of-the-art mechanism presented by Cazorla,e.g., <ref type="bibr" target="#b5">[6]</ref>. QoSMT, our methodology, will dynamically adjust all resourc ><head n="6.4">Hardware based policies</head><p>The closest work to QoSMT is the design proposed in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. For their work, wheth
interference, We divide these shared resources into three categories according to previous studies <ref type="bibr" target="#b10">[11]</ref>.</p><p>Front-end resources: Instruction fetch unit is the nges. Transparent thread also aims to maximize HPT's performance with reasonable overall throughput <ref type="bibr" target="#b10">[11]</ref>. Unlike QoSMT, it does not support precise performance con
any studies <ref type="bibr" target="#b6">[7]</ref>[30] <ref type="bibr" target="#b8">[9]</ref>[10] <ref type="bibr" target="#b24">[27]</ref> providing solutions on improving overall SMT throughput an
t does not support precise performance control based on user-defined target. There are many studies <ref type="bibr" target="#b6">[7]</ref>[30] <ref type="bibr" target="#b8">[9]</ref>[10] <ref type="b

rors, another 5 benchmarks are not used. To extract typical behaviors of workloads, we use SimPoint <ref type="bibr" target="#b22">[25]</ref> to acquire checkpoints for each benchmark. We run differen
interference, We divide these shared resources into three categories according to previous studies <ref type="bibr" target="#b10">[11]</ref>.</p><p>Front-end resources: Instruction fetch unit is the nges. Transparent thread also aims to maximize HPT's performance with reasonable overall throughput <ref type="bibr" target="#b10">[11]</ref>. Unlike QoSMT, it does not support precise performance con
y information for character-based model. To integrate words information into character-based model, <ref type="bibr" target="#b40">Zhang and Yang (2018)</ref> propose a lattice-structured LSTM model t characterbased model. The character baseline denotes the original character-based BiLSTM-CRF model. <ref type="bibr" target="#b40">Zhang and Yang (2018)</ref> propose a lattice LSTM to exploit word in y information into Chinese NER task. Another way to obtain word boundary information is proposed by <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>, using a lattice LSTM to integrate word where b &lt; i and c b,i matches a word in lexicon D. The lexicon D is the same as the one used in <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>, which is built by using automatically s x c i with x − → ws i to utilize word information. And this is quite different from the way used in <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>, since they use extra shortcut paths to j , y j )}| N j=1 , we minimize the sentence-level negative loglikelihood loss to train the model:  <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>.</p><formula xml:id="formula_16">L = − j rget="#tab_0">1</ref>. Implementation Details. We utilize the character and word embeddings used in <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>, both of which are pre-trained on Chines ng, 2018)</ref>, both of which are pre-trained on Chinese Giga-Word using word2vec model. Following <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>, we use the word embedding dictionary as with other parameters.</p><p>For hyper-parameter configurations, we mostly refer to the settings in <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>. We set both character embedding size an > 91.28 90.62 90.95 <ref type="bibr" target="#b0">Cao et al. (2018)</ref> 91.73 89.58 90.64 Lattice <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>   approach to integrating word informati n Chinese Resume dataset. Consistent with the previous results, our models outperform lattice model <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>. The above experimental results strongly some comparative experiments on training time and convergence speed. The lattice model proposed in <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref> is our principal comparison object, sinc ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>are the most common methods<ref type="bibr" target="#b40">Zhang and Yang, 2018)</ref> 94.81 94.11 94.46 Character baseline 93.2
M(WC-LSTM)</head><p>Inspired by the way character bigram is integrated into sequence labeling model <ref type="bibr" target="#b2">(Chen et al., 2015;</ref><ref type="bibr" target="#b36">Yang et al., 2
acter-based models. Motivated by the success of multi-task learning for Natural Language Processing <ref type="bibr" target="#b19">(Liu et al., 2016</ref><ref type="bibr" target="#b20">(Liu et al., ,
for NER and achieves good performance. <ref type="bibr" target="#b21">Ma and Hovy (2016)</ref> and <ref type="bibr" target="#b4">Chiu and Nichols (2016)</ref> use CNN to capture spelling characterist
<ref type="table" target="#tab_7">6</ref>    <ref type="bibr" target="#b14">(Ju et al., 2018;</ref><ref type="bibr" target="#b28">Sohrab and Miwa, 2018)</ref>. Short word first is good at identifying
Dredze (2015)</ref> propose to add segmentation features for better recognition of entity boundary. <ref type="bibr" target="#b7">Dong et al. (2016)</ref> integrate radical-level features into charact )</ref> 92.20 90.18 91.18 <ref type="bibr" target="#b41">Zhou et al. (2013)</ref> 91.86 88.75 90.28 <ref type="bibr" target="#b7">Dong et al. (2016)</ref> 91.28 90.62 90.95 <ref type="bibr" target="#b " target="#b41">Zhou et al. (2013)</ref> use the statistical model with rich hand-crafted features. <ref type="bibr" target="#b7">Dong et al. (2016)</ref> exploit radical features in Chinese character
y Models(ME) <ref type="bibr" target="#b3">(Chieu and Ng, 2003)</ref>, Support Vector Machines(SVM) <ref type="bibr" target="#b8">(Ekbal and Bandyopadhyay, 2010)</ref> and Conditional Random Fields(CR
for NER and achieves good performance. <ref type="bibr" target="#b21">Ma and Hovy (2016)</ref> and <ref type="bibr" target="#b4">Chiu and Nichols (2016)</ref> use CNN to capture spelling characterist

results on Name Entity Recognition task <ref type="bibr" target="#b10">(Gregoric et al., 2018;</ref><ref type="bibr" target="#b17">Lin and Lu, 2018)</ref>. Based on the level of granularity, most of t
(2016)</ref> first proposed to jointly train Chinese NER with Chinese word segmentation(CWS) task. <ref type="bibr" target="#b0">Cao et al. (2018)</ref> apply adversarial transfer learning framework 3)</ref> 91.86 88.75 90.28 <ref type="bibr" target="#b7">Dong et al. (2016)</ref> 91.28 90.62 90.95 <ref type="bibr" target="#b0">Cao et al. (2018)</ref> 91.73 89.58 90.64 Lattice <ref type="bibr" tar ef type="bibr" target="#b7">Dong et al. (2016)</ref> exploit radical features in Chinese character. <ref type="bibr" target="#b0">Cao et al. (2018)</ref> joint train Chinese NER task with Chinese word ance. Multi-task learning <ref type="bibr">(Peng and</ref><ref type="bibr">Dredze, 2015, 2016;</ref><ref type="bibr" target="#b0">Cao et al., 2018)</ref> and semi-supervised learning <ref type="bibr"
characters and the gazetteers. Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) <ref type="bibr" target="#b10">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref ty N <ref type="bibr" target="#b6">(Kipf and Welling, 2017)</ref>.</p><p>However, the traditional GGNN <ref type="bibr" target="#b10">(Li et al., 2016</ref>) is unable to distinguish edges with different
(GGNN) <ref type="bibr" target="#b10">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref type="bibr" target="#b7">(Lample et al., 2016)</ref> (BiLSTM-CRF), our model learns a weighted are respec-tively 39.70%, 44.75%, 36.10% and 46.05%.</p><p>Models for Comparison. We use BiLSTM-CRF <ref type="bibr" target="#b7">(Lample et al., 2016)</ref> with character+bigram embedding without us
sting methods often rely on hand-crafted templates or predefined selection strategies. For example, <ref type="bibr" target="#b12">Qi et al. (2019)</ref> defined several n-gram templates to construct
nodes, a bigram embedding table W bi is used since it has been shown to be useful for the NER task <ref type="bibr" target="#b2">(Chen et al., 2015)</ref>.</p><p>The structural information of the gra
011)</ref>. Besides, gazetteers can also be easily constructed from knowledge bases (e.g., Freebase <ref type="bibr" target="#b0">(Bollacker et al., 2008)</ref>) or com- While such background knowledg
nodes, a bigram embedding table W bi is used since it has been shown to be useful for the NER task <ref type="bibr" target="#b2">(Chen et al., 2015)</ref>.</p><p>The structural information of the gra
l outputs or to learn node representations through neural networks with gated recurrent units (GRU) <ref type="bibr" target="#b4">(Cho et al., 2014)</ref>. While other neural architectures for graphs
sentence <ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules <ref type="bibr" target="#b13">(Sassano, 2014)</ref>. Though general, these strategies are unable to a sentence<ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules<ref type="bibr" target="#b13">(Sassano, 2014)</ref>. Though general, these strategies are unable to
ection strategies are proposed, such as maximizing the total number of matched tokens in a sentence <ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules <ref type= lection strategies are proposed, such as maximizing the total number of matched tokens in a sentence<ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules<ref type="
its better capability of capturing the local textual information compared to other GNNs such as GCN <ref type="bibr" target="#b6">(Kipf and Welling, 2017)</ref>.</p><p>However, the traditional GGNN <r
ection strategies are proposed, such as maximizing the total number of matched tokens in a sentence <ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules <ref type= lection strategies are proposed, such as maximizing the total number of matched tokens in a sentence<ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules<ref type="
ers may also contain irrelevant and even erroneous information which harms the system's performance <ref type="bibr" target="#b3">(Chiu and Nichols, 2016)</ref>. This is especially the case for Chines eers may also contain irrelevant and even erroneous information which harms the system's performance<ref type="bibr" target="#b3">(Chiu and Nichols, 2016)</ref>. This is especially the case for Chines
sentence <ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules <ref type="bibr" target="#b13">(Sassano, 2014)</ref>. Though general, these strategies are unable to a sentence<ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules<ref type="bibr" target="#b13">(Sassano, 2014)</ref>. Though general, these strategies are unable to
ischedel et al., 2010)</ref>, MSRA <ref type="bibr" target="#b8">(Levow, 2006)</ref>, and Weibo-NER <ref type="bibr" target="#b11">(Peng and Dredze, 2016)</ref>. OntoNotes and MSRA are two datasets co of social media. We use the same split as <ref type="bibr" target="#b1">Che et al. (2013)</ref> and <ref type="bibr" target="#b11">Peng and Dredze (2016)</ref> on OntoNotes and on Weibo-NER. To demons
experiments are OntoNotes 4.0 <ref type="bibr" target="#b17">(Weischedel et al., 2010)</ref>, MSRA <ref type="bibr" target="#b8">(Levow, 2006)</ref>, and Weibo-NER <ref type="bibr" target="#b11">(Pen

ers may also contain irrelevant and even erroneous information which harms the system's performance <ref type="bibr" target="#b3">(Chiu and Nichols, 2016)</ref>. This is especially the case for Chines eers may also contain irrelevant and even erroneous information which harms the system's performance<ref type="bibr" target="#b3">(Chiu and Nichols, 2016)</ref>. This is especially the case for Chines




sentence <ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules <ref type="bibr" target="#b13">(Sassano, 2014)</ref>. Though general, these strategies are unable to a sentence<ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules<ref type="bibr" target="#b13">(Sassano, 2014)</ref>. Though general, these strategies are unable to
e ensemble approaches <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> fused different text features and achieved promising result
xt features more effectively and optimize the algorithm for higher accuracy are the main challenges <ref type="bibr" target="#b0">[1]</ref>. Some conventional machine learning models are simple but ha s decision-level fusion to concatenate the complementary information of different kinds of features <ref type="bibr" target="#b0">[1]</ref>. The decision vector [x 1 , ⋯ , x k−1 , x k ] represents the
gories with the same number of documents. The 20Newsgroup<ref type="foot" target="#foot_4">3</ref>  <ref type="bibr" target="#b14">[15]</ref> data set is a standard database for machine learning evalu
osed a SVM categorization model based on n-gram approach and achieved good performance. Wang et al. <ref type="bibr" target="#b2">[3]</ref> developed the NBSVM model which combines SVM with Naive Baye odels</head><p>At last, we compare our ensemble model with state-of-the-art models. Since the NBSVM <ref type="bibr" target="#b2">[3]</ref> model is only suitable for binary categorization, we do not
gories with the same number of documents. The 20Newsgroup<ref type="foot" target="#foot_4">3</ref>  <ref type="bibr" target="#b14">[15]</ref> data set is a standard database for machine learning evalu
gories with the same number of documents. The 20Newsgroup<ref type="foot" target="#foot_4">3</ref>  <ref type="bibr" target="#b14">[15]</ref> data set is a standard database for machine learning evalu
osed a SVM categorization model based on n-gram approach and achieved good performance. Wang et al. <ref type="bibr" target="#b2">[3]</ref> developed the NBSVM model which combines SVM with Naive Baye odels</head><p>At last, we compare our ensemble model with state-of-the-art models. Since the NBSVM <ref type="bibr" target="#b2">[3]</ref> model is only suitable for binary categorization, we do not
gories with the same number of documents. The 20Newsgroup<ref type="foot" target="#foot_4">3</ref>  <ref type="bibr" target="#b14">[15]</ref> data set is a standard database for machine learning evalu
hose the version which including 18,828 documents. The AG<ref type="foot" target="#foot_5">4</ref>  <ref type="bibr" target="#b15">[16]</ref> dataset is a collection of more than 1 million news articl e choose the 4 largest classes from this corpus. The Yelp<ref type="foot" target="#foot_6">5</ref>  <ref type="bibr" target="#b15">[16]</ref> reviews dataset is obtained from the Yelp Dataset Challeng categorization. How good the learnt representations are for language modeling is a crucial question <ref type="bibr" target="#b15">[16]</ref>. In the future, we intend to apply these semi-supervised l
pe="table" target="#tab_0">1</ref> is a summary. The IMDB<ref type="foot" target="#foot_2">1</ref>  <ref type="bibr" target="#b12">[13]</ref> data set consists of numerous film movie reviews. It is co
xt features more effectively and optimize the algorithm for higher accuracy are the main challenges <ref type="bibr" target="#b0">[1]</ref>. Some conventional machine learning models are simple but ha s decision-level fusion to concatenate the complementary information of different kinds of features <ref type="bibr" target="#b0">[1]</ref>. The decision vector [x 1 , ⋯ , x k−1 , x k ] represents the
>43]</ref>. Hypergraph neural networks <ref type="bibr" target="#b16">[17]</ref> and their variants <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> use the clique exp
et="#b51">[52,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b16">17]</ref>. A popular learning paradigm is graphbased / hypergraph-bas ral network f (G, X) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17]</ref> (X contains the initial features on the vertices for exampl learning problem on the approximation. While the state-of-the-art hypergraph neural networks (HGNN) <ref type="bibr" target="#b16">[17]</ref> approximates each hyperedge by a clique and hence requires detailed experimentation, we demonstrate their effectiveness compared to the state-of-the art HGNN <ref type="bibr" target="#b16">[17]</ref> and other baselines (Sections 5, and 7). • We thoroughly d ="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43]</ref>. Hypergraph neural networks <ref type="bibr" target="#b16">[17]</ref> and their variants <ref type="bibr" target="#b22">[23,</re 2 . Our approach requires at most a linear number of edges (1 and 2|e| − 3 respectively) while HGNN <ref type="bibr" target="#b16">[17]</ref> requires a quadratic number of edges for each hyperedge. < yperGCN and FastHyperGCN against the following baselines:</p><p>• Hypergraph neural networks (HGNN) <ref type="bibr" target="#b16">[17]</ref> uses the clique expansion <ref type="bibr" target="#b51">[ [7].Our approach requires at most a linear number of edges (1 and 2|e| − 3 respectively) while HGNN<ref type="bibr" target="#b16">[17]</ref> requires a quadratic number of edges for each hyperedge.</
><ref type="bibr" target="#b30">31]</ref>, computer vision <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref>, natural language processing <ref type="bibr" target="#b43"
rget="#b51">[52]</ref> and has become a popular approach for learning on hypergraph-structured data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta
le yet effective way to overcome the limitations is to introduce hyperedge-dependent vertex weights <ref type="bibr" target="#b13">[14]</ref>.</p><p>Researchers have fully utilised the hypergraph stru
et="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43]</ref>. Hypergraph neural networks <ref type="bibr" target="#b16">
hs (idea is to consider supremum in tail, infimum in head) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>• submodular hypergraphs (different submodular weights
ph-structured data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43]</ref>. Hypergraph neural
ph-structured data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43]</ref>. Hypergraph neural
opular approach for learning on hypergraph-structured data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" tar
ing performance for clustering, SSL, active learning, etc. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13]</ref>. A simple yet effective way to overcome the limitations is
n in the objective <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b47">48]</ref>, the state-of-the-a
methods encode the graph / hypergraph structure G = (V, E) implicitly via a neural network f (G, X) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" targ g vertices in each edge / hyperedge, implicit regularisation of graph convolutional networks (GCNs) <ref type="bibr" target="#b24">[25]</ref> avoids this restriction and enables application to a broad ork models to non-Euclidean domains such as graphs and manifolds. Graph convolutional network (GCN) <ref type="bibr" target="#b24">[25]</ref> defines the convolution using a simple linear function of valued vector representations for each node v ∈ V.</p><p>The basic formulation of graph convolution <ref type="bibr" target="#b24">[25]</ref> stems from the convolution theorem <ref type="bibr" target , Ā = D− 1 2 Ã D− 1 2 , Ã = A + I, and Dii = N j=1 Ãij . The proof involves a renormalisation trick <ref type="bibr" target="#b24">[25]</ref> and is in the supplementary.</p><p>GCN <ref type="bibr" ta ormalisation trick <ref type="bibr" target="#b24">[25]</ref> and is in the supplementary.</p><p>GCN <ref type="bibr" target="#b24">[25]</ref> The forward model for a simple two-layer GCN takes the fol plementary. We trained all methods for 200 epochs and used the same hyperparameters of a prior work <ref type="bibr" target="#b24">[25]</ref>. We report the mean test error and standard deviation over
g with hypergraphs <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b16">17]</ref>. A popular learning aph-based learning <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar rent settings:</p><p>• directed hypergraphs (idea is to consider supremum in tail, infimum in head) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>• submodular that the hypernodes in the same hyperedge are similar and hence are likely to share the same label <ref type="bibr" target="#b48">[49]</ref>. Suppose we use {h v : v ∈ V } to denote some representati all the above models for this baseline to get an optimal λ. • Confidence Interval-based method (CI) <ref type="bibr" target="#b48">[49]</ref> uses a subgradient-based method <ref type="bibr" target="# nterval-based method (CI) <ref type="bibr" target="#b48">[49]</ref> uses a subgradient-based method <ref type="bibr" target="#b48">[49]</ref>. We note that this method has consistently been shown to b
le yet effective way to overcome the limitations is to introduce hyperedge-dependent vertex weights <ref type="bibr" target="#b13">[14]</ref>.</p><p>Researchers have fully utilised the hypergraph stru
lationships in many real-world networks naturaly motivates the problem of learning with hypergraphs <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta egorical data (results in supplementary) which are a standard practice in hypergraph-based learning <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta pergraph neural networks (HGNN) <ref type="bibr" target="#b16">[17]</ref> uses the clique expansion <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b0">1]</ref> to approximate the h ning on hypergraphs:</head><p>The clique expansion of a hypergraph was introduced in a seminal work <ref type="bibr" target="#b51">[52]</ref> and has become a popular approach for learning on hypergra ior to the primal dual hybrid gradient (PDHG) of <ref type="bibr" target="#b21">[22]</ref> and also <ref type="bibr" target="#b51">[52]</ref>. Hence, we did not use these other previous methods as bas
ucture G = (V, E) implicitly via a neural network f (G, X) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17]</ref> (X contains the initi
ve used explicit Laplacian regularisation in the objective <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" tar
ula_2">1</ref>), when applied to a hypernode v ∈ V in G S , in the neural message-passing framework <ref type="bibr" target="#b17">[18]</ref> is h</p><formula xml:id="formula_5">(τ +1) v = σ (Θ (τ ) )
et="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43]</ref>. Hypergraph neural networks <ref type="bibr" target="#b16">
raph structure also through non-linear Laplacian operators <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b7">8]</ref>. It has been shown th ty for hypergraphs, relating the second smallest eigenvalue of the operator to hypergraph expansion <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref>. One such Laplacian lying motivation as stated above was proposed in prior works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref>. We present this Laplacian first. Then we run GCN over the on L : R n → R n ).</p><p>Definition 1 (Hypergraph Laplacian <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref> 2 ) Given a real-valued signal S ∈ R n defined on the hyper define a Laplacian for hypergraphs. One such way <ref type="bibr" target="#b7">[8]</ref> (see also <ref type="bibr" target="#b31">[32]</ref>) is a non-linear function L : R n → R n (the Laplacian mat l problem as shown in<ref type="bibr" target="#b7">[8]</ref>. Breaking ties randomly was proposed in<ref type="bibr" target="#b31">[32]</ref>, but<ref type="bibr" target="#b7">[8]</ref> showed that th
e detailed derivation from the convolution theorem uses existing tools from graph signal processing <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta
targeted network, which corresponds to a more restrictive black box threat model.</p><p>Recent work <ref type="bibr" target="#b4">(Chen et al., 2017;</ref><ref type="bibr" target="#b1">Bhagoji et al., ="bibr" target="#b10">Ilyas et al., 2017)</ref> provides a number of attacks for this threat model. <ref type="bibr" target="#b4">Chen et al. (2017)</ref> show how to use a basic primitive of zeroth o (x, ) (x l−1 + η s l ) with s l = Π ∂Bp(0,1) ∇ x L(x l−1 , y)<label>(4)</label></formula><p>Indeed, <ref type="bibr" target="#b4">Chen et al. (2017)</ref> were the first to use finite differences meth ty d=268,203 and thus this method would require 268,204 queries. (It is worth noting, however, that <ref type="bibr" target="#b4">Chen et al. (2017)</ref> developed additional methods to, at least par
od builds on the framework of bandit optimization, a fundamental tool in online convex optimization <ref type="bibr" target="#b9">Hazan (2016)</ref>. In the bandit optimization framework, an agent pla or ∆ of the gradient −∇ v ∇L(x, y), v of the loss be the standard spherical gradient estimator (see <ref type="bibr" target="#b9">Hazan (2016)</ref>). We take a two-query estimate of the expectation, dating the latent vector v t . We will adapt here the canonical "reduction from bandit information" <ref type="bibr" target="#b9">(Hazan, 2016)</ref>. Specifically, our update procedure is parametrize
od builds on the framework of bandit optimization, a fundamental tool in online convex optimization <ref type="bibr" target="#b9">Hazan (2016)</ref>. In the bandit optimization framework, an agent pla or ∆ of the gradient −∇ v ∇L(x, y), v of the loss be the standard spherical gradient estimator (see <ref type="bibr" target="#b9">Hazan (2016)</ref>). We take a two-query estimate of the expectation, dating the latent vector v t . We will adapt here the canonical "reduction from bandit information" <ref type="bibr" target="#b9">(Hazan, 2016)</ref>. Specifically, our update procedure is parametrize
ance unbiased (MVU) estimator of the latent vector g. Theorem 3 (Least-squares optimality (Proof in <ref type="bibr" target="#b16">Meir (1994)</ref>)). In the underdetermined setting, i.e. when k &lt;
der the set {ĝ, δ1 , . . . , δk } of k + 1 corresponding normalized directions, we have (see, e.g., <ref type="bibr" target="#b8">(Gorban et al., 2016)</ref>) that the probability that any two of them

od builds on the framework of bandit optimization, a fundamental tool in online convex optimization <ref type="bibr" target="#b9">Hazan (2016)</ref>. In the bandit optimization framework, an agent pla or ∆ of the gradient −∇ v ∇L(x, y), v of the loss be the standard spherical gradient estimator (see <ref type="bibr" target="#b9">Hazan (2016)</ref>). We take a two-query estimate of the expectation, dating the latent vector v t . We will adapt here the canonical "reduction from bandit information" <ref type="bibr" target="#b9">(Hazan, 2016)</ref>. Specifically, our update procedure is parametrize
der the set {ĝ, δ1 , . . . , δk } of k + 1 corresponding normalized directions, we have (see, e.g., <ref type="bibr" target="#b8">(Gorban et al., 2016)</ref>) that the probability that any two of them
2)</ref> presented the first such iterative attack on a special class of binary classifiers. Later, <ref type="bibr" target="#b26">Xu et al. (2016)</ref>   <ref type="formula">2017</ref>) designed a b

ance unbiased (MVU) estimator of the latent vector g. Theorem 3 (Least-squares optimality (Proof in <ref type="bibr" target="#b16">Meir (1994)</ref>)). In the underdetermined setting, i.e. when k &lt;
urveillance systems to automatically detect suicidal behaviors and trigger an alarm. In this sense, <ref type="bibr" target="#b9">(Lee et al., 2014)</ref> presented a method for automatically analyzin ide by hanging attempts <ref type="bibr" target="#b1">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type="bibr" target="#b9">(Lee et al., 2014)</ref>, we performed our experiments on a large data
presented an intelligent video-based system for automated detection of suicide by hanging attempts <ref type="bibr" target="#b1">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type="bibr" target=
ier, 2015)</ref>, contour <ref type="bibr" target="#b3">(Cheema et al., 2011)</ref> or optical flow <ref type="bibr" target="#b5">(Fathi and Mori, 2008)</ref>. Most proposed methods in this category o
and labeling the individual body parts. Numerous type of features have been used such as silhouette <ref type="bibr" target="#b6">(Gouiaa and Meunier, 2015)</ref>, contour <ref type="bibr" target="#b3
training data, and rebalance the latter using the SMOTE: Synthetic Minority Over-sampling Technique <ref type="bibr" target="#b2">(Chawla et al., 2002)</ref> For each experiment, we applied the algori . To overcome this problem, we propose to use the SMOTE: Synthetic Minority Over-sampling Technique <ref type="bibr" target="#b2">(Chawla et al., 2002)</ref> in order to rebalance the training data se
and labeling the individual body parts. Numerous type of features have been used such as silhouette <ref type="bibr" target="#b6">(Gouiaa and Meunier, 2015)</ref>, contour <ref type="bibr" target="#b3
ier, 2015)</ref>, contour <ref type="bibr" target="#b3">(Cheema et al., 2011)</ref> or optical flow <ref type="bibr" target="#b5">(Fathi and Mori, 2008)</ref>. Most proposed methods in this category o
y joints localization. For instance, <ref type="bibr" target="#b12">(Parisi and Wermter, 2013;</ref><ref type="bibr" target="#b11">Parisi et al., 2015)</ref> proposed methods for human action recognit
figuration of human body structure. This representation is derived from the principle, published in <ref type="bibr" target="#b8">(Johansson, 1973)</ref>, explaining how humans observe actions. This w
ier, 2015)</ref>, contour <ref type="bibr" target="#b3">(Cheema et al., 2011)</ref> or optical flow <ref type="bibr" target="#b5">(Fathi and Mori, 2008)</ref>. Most proposed methods in this category o
tackle the problem of human action recognition using human body joints localization. For instance, <ref type="bibr" target="#b12">(Parisi and Wermter, 2013;</ref><ref type="bibr" target="#b11">Parisi
een used such as silhouette <ref type="bibr" target="#b6">(Gouiaa and Meunier, 2015)</ref>, contour <ref type="bibr" target="#b3">(Cheema et al., 2011)</ref> or optical flow <ref type="bibr" target="#
figuration of human body structure. This representation is derived from the principle, published in <ref type="bibr" target="#b8">(Johansson, 1973)</ref>, explaining how humans observe actions. This w
y joints localization. For instance, <ref type="bibr" target="#b12">(Parisi and Wermter, 2013;</ref><ref type="bibr" target="#b11">Parisi et al., 2015)</ref> proposed methods for human action recognit
tackle the problem of human action recognition using human body joints localization. For instance, <ref type="bibr" target="#b12">(Parisi and Wermter, 2013;</ref><ref type="bibr" target="#b11">Parisi
ier, 2015)</ref>, contour <ref type="bibr" target="#b3">(Cheema et al., 2011)</ref> or optical flow <ref type="bibr" target="#b5">(Fathi and Mori, 2008)</ref>. Most proposed methods in this category o
and labeling the individual body parts. Numerous type of features have been used such as silhouette <ref type="bibr" target="#b6">(Gouiaa and Meunier, 2015)</ref>, contour <ref type="bibr" target="#b3
tackle the problem of human action recognition using human body joints localization. For instance, <ref type="bibr" target="#b12">(Parisi and Wermter, 2013;</ref><ref type="bibr" target="#b11">Parisi
tackle the problem of human action recognition using human body joints localization. For instance, <ref type="bibr" target="#b12">(Parisi and Wermter, 2013;</ref><ref type="bibr" target="#b11">Parisi
training data, and rebalance the latter using the SMOTE: Synthetic Minority Over-sampling Technique <ref type="bibr" target="#b2">(Chawla et al., 2002)</ref> For each experiment, we applied the algori . To overcome this problem, we propose to use the SMOTE: Synthetic Minority Over-sampling Technique <ref type="bibr" target="#b2">(Chawla et al., 2002)</ref> in order to rebalance the training data se
een used such as silhouette <ref type="bibr" target="#b6">(Gouiaa and Meunier, 2015)</ref>, contour <ref type="bibr" target="#b3">(Cheema et al., 2011)</ref> or optical flow <ref type="bibr" target="#
ier, 2015)</ref>, contour <ref type="bibr" target="#b3">(Cheema et al., 2011)</ref> or optical flow <ref type="bibr" target="#b5">(Fathi and Mori, 2008)</ref>. Most proposed methods in this category o
e of the key design choices for a multilevel cache hierarchy is whether or not to enforce inclusion <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" targ le inclusion greatly simplifies the cache coherence protocol <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b4">6]</ref>, it limits performance when the size of the largest cache is f all the smaller caches of a multi-level cache hierarchy be a subset of the last-level cache (LLC) <ref type="bibr" target="#b4">[6]</ref>. When a line is evicted from the LLC, inclusion is enforced fit that an inclusive LLC provides, thus breaking the coherence benefits that come with inclusivity <ref type="bibr" target="#b4">[6]</ref>. While snoop filters <ref type="bibr" target="#b3">[5,</ref>
ef> and in the context of better cache management policies <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b10">12]</ref>. Zahran <ref type="bibr" target="#b25">[27,</ref><ref type= ent should have been measured in the presence of an inclusive cache hierarchy.</p><p>Fletcher et al <ref type="bibr" target="#b10">[12]</ref> observed inclusion victims in the context of direct mapped

<ref type="bibr" target="#b4">[6]</ref>. While snoop filters <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23]</ref> can be used in addi
ef> and in the context of better cache management policies <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b10">12]</ref>. Zahran <ref type="bibr" target="#b25">[27,</ref><ref type= ent should have been measured in the presence of an inclusive cache hierarchy.</p><p>Fletcher et al <ref type="bibr" target="#b10">[12]</ref> observed inclusion victims in the context of direct mapped
/ns/1.0"><head>A. Simulator</head><p>We use CMP$im <ref type="bibr" target="#b14">[16]</ref>, a Pin <ref type="bibr" target="#b17">[19]</ref> based trace-driven x86 simulator for our performance studi

ce benefits that come with inclusivity <ref type="bibr" target="#b4">[6]</ref>. While snoop filters <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" targ
mitted by each application. This methodology is similar to existing work on shared cache management <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" ta been extensive research on managing shared caches in CMPs <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b23">25]</ref>. Most of the prior s in the presence of intelligent cache management policies <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref> and find that we achieve similar performance improvements.<

on increases the effective capacity of the cache hierarchy <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b27">29]</ref>. Unfortunately, non-inclusion eliminates the natural snoop ive caching capacity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b27">29]</ref>. However, we show that the first order benefit of non-inclu "#b21">23]</ref> can be used in addition to the LLC, such structures increase the hardware overhead <ref type="bibr" target="#b27">[29]</ref> and verification complexity <ref type="bibr" target="#b7">
o transfer knowledge from data-rich head classes to data-poor tail classes. While transfer learning <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" ta ref><ref type="bibr" target="#b18">18]</ref> from a source to target task is a well studied problem <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b19">19]</ref>, by far the most c
" learning problem <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" tar

ally balanced datasets <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target=
et="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" tar tion module of pre-trained ResNet152 <ref type="bibr" target="#b4">[4]</ref> on long-tailed SUN-397 <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b55">55]</ref>.  Ablation analysi model space for long-tail recognition. Fig.1ashows the number of examples by scene class on SUN-397<ref type="bibr" target="#b13">[13]</ref>, a representative dataset that follows an intrinsic long-t
get="#b43">43,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" tar
9">19]</ref>, by far the most common approach is fine-tuning a model pre-trained on the source task <ref type="bibr" target="#b20">[20]</ref>. In the long-tailed setting, this fails to provide any not

get="#b41">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" tar
get="#b43">43,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" tar
get="#b39">39,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" tar
r" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> and natural language processing <ref type="bibr" target="#b11">[12]</ref>.</p><p>The novelty of Caser is to represent the previous L where d is the number of latent dimensions and the rows preserve the order of the items. Similar to <ref type="bibr" target="#b11">[12]</ref>, we regard this embedding matrix as the "image" of the L i r" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> and natural language processing <ref type="bibr" target="#b11">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref sing <ref type="bibr" target="#b11">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref type="bibr" target="#b11">[12]</ref>, our approach regards the L × d matrix E as the "image" of
d n="1.1">Top-N Sequential Recommendation</head><p>To model user's sequential patterns, the work in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref> considers top-N se ), in other words, its sequential signals are much weaker than the above data sets.</p><p>Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" ta target="#b3">4]</ref> nd explicit sequential association rules based on statistical co-occurrences <ref type="bibr" target="#b16">[17]</ref>. This approach depends on  the explicit representation of
s.</p><p>Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>, we hold the rst 70% of actions in each user's sequence as
#b31">[32]</ref> also produce a good recommendation performance. Convolutional neural network (CNN) <ref type="bibr" target="#b35">[36]</ref> has been used to extract users' preferences from their rev
#b31">[32]</ref> also produce a good recommendation performance. Convolutional neural network (CNN) <ref type="bibr" target="#b35">[36]</ref> has been used to extract users' preferences from their rev
hnique with 50% drop ratio is used on fully-connected layers. We implemented Caser with MatCon-vNet <ref type="bibr" target="#b27">[28]</ref>. The whole training time is proportional to the number of
applied to recommendation problems. Auto-encoder framework <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> and its variant denoising autoencoder <ref type="bibr" targ ion Metrics. As in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, we evaluate a mode
type="bibr" target="#b23">[24]</ref>, matrix factorization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>, and top-N recommendation <ref type="bibr" target="#b8">[9]
h aims to forecast repeated buyers. Following previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32]</ref>, we converted all n ber of interactions. • BPR. Combined with Matrix Factorization model, Bayesian personalized ranking <ref type="bibr" target="#b19">[20]</ref> is the state-of-the-art method for non-sequential item rec
target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> is an early approach to top-N sequential recommendation, wh
.tei-c.org/ns/1.0"><head n="1.2">Limitations of Previous Work</head><p>The Markov chain based model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target v chains (FPMC) <ref type="bibr" target="#b20">[21]</ref> proposed by Rendle et al. and its variant <ref type="bibr" target="#b1">[2]</ref> improved this method by factorizing this transition matrix i of E is latent for us, it is meaningless to interact with multiple successive columns at one time. <ref type="bibr" target="#b1">(2)</ref> There is no need to apply max pooling operation over the ver
conductances of memristors to exact values and thus the resistances are usually programmed instead <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. After software
ficiently, with the crossbar architecture shown in Fig. <ref type="figure" target="#fig_0">1</ref>  <ref type="bibr" target="#b3">[4]</ref>. The voltage vector V I = [V I 1 , V I 2 , ...V I m ] applie
range.</p><p>The aging problem of memristors is different from the drifting effect as described in <ref type="bibr" target="#b7">[8]</ref>, where the conductance of the memristor drifts away from its
ese signs, the polarities of constant amplitudes used to program the conductances can be determined <ref type="bibr" target="#b15">[16]</ref>, as</p><formula xml:id="formula_7">V i ? sign(- ?Cost ?W i
and thus the resistances are usually programmed instead <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. After software training, the weights usually satisfy a qu hed lines in Fig. <ref type="figure" target="#fig_2">3</ref>(b). For example, 32 levels are used in <ref type="bibr" target="#b13">[14]</ref> and 64 levels in <ref type="bibr" target="#b14">[15]</ref>
0 classes instead. The neural networks and the proposed algorithm were implemented using Tensorflow <ref type="bibr" target="#b21">[22]</ref> and tested with an Intel 3.6 GHz CPU and an NVIDA GeForce
0 classes instead. The neural networks and the proposed algorithm were implemented using Tensorflow <ref type="bibr" target="#b21">[22]</ref> and tested with an Intel 3.6 GHz CPU and an NVIDA GeForce
opted to implement neural networks, power consumption is still a limiting factor for such platforms <ref type="bibr" target="#b2">[3]</ref>. To meet the increasing computing demand in complex neural n
0 classes instead. The neural networks and the proposed algorithm were implemented using Tensorflow <ref type="bibr" target="#b21">[22]</ref> and tested with an Intel 3.6 GHz CPU and an NVIDA GeForce
purpose and specific hardware platforms, e.g., GPU <ref type="bibr" target="#b0">[1]</ref> and FPGA <ref type="bibr" target="#b1">[2]</ref>, have been adopted to implement neural networks, power consu
ficiently, with the crossbar architecture shown in Fig. <ref type="figure" target="#fig_0">1</ref>  <ref type="bibr" target="#b3">[4]</ref>. The voltage vector V I = [V I 1 , V I 2 , ...V I m ] applie
les <ref type="bibr" target="#b50">[51]</ref>. The related study is presented in our previous paper <ref type="bibr" target="#b49">[50]</ref>.</p><formula xml:id="formula_15">Hmatch 2 = 1/(norm(|P U − 6"><head></head><label></label><figDesc>) T U describes learning styles. Referring to other research<ref type="bibr" target="#b49">[50]</ref>, we design the elements of learning styles as: T U = {CL,
rs' attitude towards the quality of the target LOs, especially in the age of information overloaded <ref type="bibr" target="#b51">[52]</ref>.</p><p>• SE is the system entropy which reflects the stabi
odels in adaptive hypermedia systems such as the overlay model and the uncertainty-based user model <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. In the applicatio h some sequential pattern mining algorithms are applied to extracting learners' learning activities <ref type="bibr" target="#b20">[21]</ref>, it is not wise to predefine or utilize constant activity
to detect the diversity of users' preferences by analyzing users' history profiles. Project et al. <ref type="bibr" target="#b39">[40]</ref> proposed an adaptive recommender system by incorporating t
combined with learners' knowledge ststes to optimize the knowledge tracing algorithm. Pavlic et al. <ref type="bibr" target="#b46">[47]</ref> predicted learners' performance by using logistic regressi
he most important inputs. Besides ontology and concept map <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, metadata is often applied to the description of LOs <ref t
as resource utilization, learners' scores, and learning time <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b56">57]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n
tadata is extended for special educational recommendation systems. The metadata includes competence <ref type="bibr" target="#b32">[33]</ref>, reputation <ref type="bibr" target="#b33">[34]</ref>, etc
as the overlay model and the uncertainty-based user model <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. In the application of the overlay model for recommender sy
he most important inputs. Besides ontology and concept map <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, metadata is often applied to the description of LOs <ref t
mprove the quality of recommendations using influence propagation among individuals. Janssen et al. <ref type="bibr" target="#b47">[48]</ref> provided recommendations for active learners by feeding ba
an knowledge about the extraction. For a general introduction of first-order logic, please refer to <ref type="bibr" target="#b11">[12]</ref>.</p><p>Complete consistency describes the fact that the va
used a Naive Bayes classifier to incrementally learn profiles from user feedback on the Web sites. <ref type="bibr" target="#b15">[16]</ref> had developed a personalized web browser. It learned a use
d in this field to automate the process of profile extraction, the problem remains largely unsolved <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" t
. Related studies <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" t t helping user navigate the Web by searching for potentially interesting pages for recommendations. <ref type="bibr" target="#b2">[3]</ref> described an experimental work to study whether user interes
="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> can be traced back to 30 years ago. The general task of web r">Li et al.</ref> propose a weakly supervised method to extract user profiles from Twitter in 2014 <ref type="bibr" target="#b5">[6]</ref>. <ref type="bibr">Merler et al.</ref> propose a method to ex
an important and challenging problem in Web mining and natural language processing. Related studies <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" t
f profile extraction, the problem remains largely unsolved <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b4">[5]</ref>. To generate the pr
an important and challenging problem in Web mining and natural language processing. Related studies <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" t
opose a cascaded information extraction framework for identifying personal information from resumes <ref type="bibr" target="#b16">[17]</ref>. Tang et al. propose a conditional random field to extract
eb mining and natural language processing. Related studies <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t /head><p>Previously considerable efforts have been made for obtaining user profiles. Back in 1900s, <ref type="bibr" target="#b1">[2]</ref> discussed algorithms for learning and revising user profiles
cebook Generated Name List Predictor(FGNL).</p><p>For Gender inference, we use a method proposed by <ref type="bibr" target="#b9">[10]</ref> as the baseline (to hereafter refer to as: FGNL). Most stat of the art methods for inferring Gender depend on a list of common names for males and females. In <ref type="bibr" target="#b9">[10]</ref>, the authors proposed an approach which used data from Face
6">(Yao et al. 2015)</ref> and extracting the last hidden state of recurrent visual feature encoder <ref type="bibr" target="#b20">(Venugopalan et al. 2015)</ref>.</p><p>Those feature encoding methods Our basic video caption framework is extended from S2VT (sequence to sequence: video to text) model <ref type="bibr" target="#b20">(Venugopalan et al. 2015)</ref> and M 3 (multimodal memory modeling)
<p>The second category treat video caption as a retrieval problem. They tagged videos with metadata <ref type="bibr" target="#b0">(Aradhye, Toderici, and Yagnik 2009)</ref> and then clustered videos a
rget="#b16">Szegedy et al. 2015;</ref><ref type="bibr">2016)</ref>, recurrent neural networks (RNN) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber 1997)</ref> and large paired video languag
rget="#b16">Szegedy et al. 2015;</ref><ref type="bibr">2016)</ref>, recurrent neural networks (RNN) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber 1997)</ref> and large paired video languag

, many researchers have tried to incorporate audio information into their specific applications. In <ref type="bibr" target="#b7">(Owens et al. 2016)</ref>, Owens et al. adopted ambient sounds as a su ME-RNNs have been adopted for multi-tasks to model long temporal dependency across different tasks <ref type="bibr" target="#b7">(Liu, Qiu, and Huang 2016)</ref>.</p><p>To explore whether long visual ories After we obtain information from external memory r t , deep fusion strategy proposed in paper <ref type="bibr" target="#b7">(Liu, Qiu, and Huang 2016</ref>) is utilized to comprehensively integr
bach et al. 2015;</ref><ref type="bibr">2013)</ref>, sub-sampling on a fixed number of input frames <ref type="bibr" target="#b26">(Yao et al. 2015)</ref> and extracting the last hidden state of recur /ref>. Their split method can be found in <ref type="bibr" target="#b25">(Xu et al. 2016</ref>) and <ref type="bibr" target="#b26">(Yao et al. 2015)</ref> respectively. In addition, gradients are clip tion into specific provided sentences <ref type="bibr" target="#b19">(Venugopalan et al. 2014;</ref><ref type="bibr" target="#b26">Yao et al. 2015;</ref><ref type="bibr" target="#b8">Pan et al. 2016a;
ype="bibr" target="#b26">Yao et al. 2015;</ref><ref type="bibr" target="#b8">Pan et al. 2016a;</ref><ref type="bibr" target="#b9">2016b)</ref>, which take inspiration from image caption <ref type="bib
" target="#b19">(Venugopalan et al. 2014;</ref><ref type="bibr" target="#b26">Yao et al. 2015;</ref><ref type="bibr" target="#b8">Pan et al. 2016a;</ref><ref type="bibr" target="#b9">2016b)</ref>, whi
esides handling single task which needs long temporal dependency, such as visual question answering <ref type="bibr" target="#b24">(Xiong, Merity, and Socher 2016)</ref> and dialog systems <ref type="
rget="#b16">Szegedy et al. 2015;</ref><ref type="bibr">2016)</ref>, recurrent neural networks (RNN) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber 1997)</ref> and large paired video languag
it feedback, implicit feedback is more difficult to utilize because of the lack of negative samples <ref type="bibr" target="#b10">[Pan et al., 2008]</ref>. Secondly, generating top-k preferred items it operation. Finding approximate top-K items can be even finished in sublinear or logarithmic time <ref type="bibr" target="#b10">[Wang et al., 2012;</ref><ref type="bibr" target="#b10">Muja and Lowe n finished in sublinear or logarithmic time <ref type="bibr" target="#b10">[Wang et al., 2012;</ref><ref type="bibr" target="#b10">Muja and Lowe, 2009]</ref> by making use of index technique.</p><p>Se bibr">[Zhou and Zha, 2012]</ref>, PPH <ref type="bibr" target="#b12">[Zhang et al., 2014]</ref>, CH <ref type="bibr" target="#b10">[Liu et al., 2014]</ref> incur large quantization loss <ref type="bib rm for f (x) and β is its penalty coefficient. <ref type="bibr">[Giannessi and Tardella, 1998;</ref><ref type="bibr" target="#b10">Lucidi and Rinaldi, 2010]</ref> show that the above two problems are 13)</label></formula><p>In terms of the loss function, we employ the popular and effective BPR loss <ref type="bibr" target="#b10">[Rendle et al., 2009]</ref>. In particular, given a user matrix U and
ices but it may incur the loss of information during the training process. To this end, inspired by <ref type="bibr" target="#b3">[Dai et al., 2016]</ref>, we transform the binary optimization problem
items for each user is extremely time-consuming.</p><p>For the first problem, recently, Spec-tralCF <ref type="bibr" target="#b13">[Zheng et al., 2018]</ref> combined collaborative filtering model wit users and items from spectral domain, which showed enormous potential for implicit feedback problem <ref type="bibr" target="#b13">[Zheng et al., 2018]</ref>. However, SpectralCF ignores high-order fe bipartite graph is crucial for implicit feedback. Some work used GCN to solve it such as SpectralCF <ref type="bibr" target="#b13">[Zheng et al., 2018]</ref>, GCMC <ref type="bibr" target="#b0">[Berg used as the initialization of P ,Q in DGCN-BinCF. All parameters of SpectralCF are set according to <ref type="bibr" target="#b13">[Zheng et al., 2018]</ref>.</p><p>Besides, for DCF, BCCF and PPH, we
on. Some two-stage approximation methods like BCCF <ref type="bibr">[Zhou and Zha, 2012]</ref>, PPH <ref type="bibr" target="#b12">[Zhang et al., 2014]</ref>, CH <ref type="bibr" target="#b10">[Liu et 14]</ref>, CH <ref type="bibr" target="#b10">[Liu et al., 2014]</ref> incur large quantization loss <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref loss <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref> is easy to fall into a local optimum becau ref> which relaxed binary constraints at first and then quantified binary codes.</p><p>Nonetheless, <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref> proposed that those two-stage methods suff sentation. Later, following this, some two stage methods <ref type="bibr">[Zhou and Zha, 2012;</ref><ref type="bibr" target="#b12">Zhang et al., 2014]</ref> which relaxed binary constraints at first a
d n="1">Introduction</head><p>Nowadays, recommender systerms are widely used in people's daily life <ref type="bibr" target="#b9">[Liu et al., 2011;</ref><ref type="bibr" target="#b8">Lian et al., 201
on. Some two-stage approximation methods like BCCF <ref type="bibr">[Zhou and Zha, 2012]</ref>, PPH <ref type="bibr" target="#b12">[Zhang et al., 2014]</ref>, CH <ref type="bibr" target="#b10">[Liu et 14]</ref>, CH <ref type="bibr" target="#b10">[Liu et al., 2014]</ref> incur large quantization loss <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref loss <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref> is easy to fall into a local optimum becau ref> which relaxed binary constraints at first and then quantified binary codes.</p><p>Nonetheless, <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref> proposed that those two-stage methods suff sentation. Later, following this, some two stage methods <ref type="bibr">[Zhou and Zha, 2012;</ref><ref type="bibr" target="#b12">Zhang et al., 2014]</ref> which relaxed binary constraints at first a
lar to a teacher, and the role of the small model is similar to a student. Following this, DarkRank <ref type="bibr" target="#b2">[Chen et al., 2018]</ref> proposed a method combining deep metric lear Li et al., 2018b]</ref> points that setting a small temperature will harm the optimization process. <ref type="bibr" target="#b2">[Courbariaux et al., 2015]</ref> mentions that generating binary codes
d n="1">Introduction</head><p>Nowadays, recommender systerms are widely used in people's daily life <ref type="bibr" target="#b9">[Liu et al., 2011;</ref><ref type="bibr" target="#b8">Lian et al., 201
trained GCN-CF model into a binarized model (DGCN-BinCF) with knowledge distillation technique (KD <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref>). To be more specific, we introduce a nove e distance one. So it is easy to fall into local optima.</p><p>2.3 Distilling Knowledge for Ranking <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref> was the first one that proposed method "Kn utions via softmax function, and utilize cross entropy for penalizing the discrepancy. According to <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref>, combining L Bin−CF with L rank as a multi
d n="1">Introduction</head><p>Nowadays, recommender systerms are widely used in people's daily life <ref type="bibr" target="#b9">[Liu et al., 2011;</ref><ref type="bibr" target="#b8">Lian et al., 201
sterms are widely used in people's daily life <ref type="bibr" target="#b9">[Liu et al., 2011;</ref><ref type="bibr" target="#b8">Lian et al., 2016;</ref><ref type="bibr" target="#b8">Li et al., 2018a e="bibr" target="#b9">[Liu et al., 2011;</ref><ref type="bibr" target="#b8">Lian et al., 2016;</ref><ref type="bibr" target="#b8">Li et al., 2018a]</ref>, but a growing scale of users and products ren a direct method to use tanh(x/t) to approximate sign function, where t is a small temperature. But <ref type="bibr" target="#b8">[Li et al., 2018b]</ref> points that setting a small temperature will
trained GCN-CF model into a binarized model (DGCN-BinCF) with knowledge distillation technique (KD <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref>). To be more specific, we introduce a nove e distance one. So it is easy to fall into local optima.</p><p>2.3 Distilling Knowledge for Ranking <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref> was the first one that proposed method "Kn utions via softmax function, and utilize cross entropy for penalizing the discrepancy. According to <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref>, combining L Bin−CF with L rank as a multi
d n="1">Introduction</head><p>Nowadays, recommender systerms are widely used in people's daily life <ref type="bibr" target="#b9">[Liu et al., 2011;</ref><ref type="bibr" target="#b8">Lian et al., 201
trained GCN-CF model into a binarized model (DGCN-BinCF) with knowledge distillation technique (KD <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref>). To be more specific, we introduce a nove e distance one. So it is easy to fall into local optima.</p><p>2.3 Distilling Knowledge for Ranking <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref> was the first one that proposed method "Kn utions via softmax function, and utilize cross entropy for penalizing the discrepancy. According to <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref>, combining L Bin−CF with L rank as a multi
>[Zheng et al., 2018]</ref> combined collaborative filtering model with graph convolutional network <ref type="bibr" target="#b6">[Henaff et al., 2015]</ref> to mine hidden interactions between users tween distributions of sampled negative items. Noting that learning hash codes is generally NP-hard <ref type="bibr" target="#b6">[Håstad, 2001]</ref>, approximation methods are appropriate choices bu
>[Zheng et al., 2018]</ref> combined collaborative filtering model with graph convolutional network <ref type="bibr" target="#b6">[Henaff et al., 2015]</ref> to mine hidden interactions between users tween distributions of sampled negative items. Noting that learning hash codes is generally NP-hard <ref type="bibr" target="#b6">[Håstad, 2001]</ref>, approximation methods are appropriate choices bu
>[Zheng et al., 2018]</ref> combined collaborative filtering model with graph convolutional network <ref type="bibr" target="#b6">[Henaff et al., 2015]</ref> to mine hidden interactions between users tween distributions of sampled negative items. Noting that learning hash codes is generally NP-hard <ref type="bibr" target="#b6">[Håstad, 2001]</ref>, approximation methods are appropriate choices bu
on. Some two-stage approximation methods like BCCF <ref type="bibr">[Zhou and Zha, 2012]</ref>, PPH <ref type="bibr" target="#b12">[Zhang et al., 2014]</ref>, CH <ref type="bibr" target="#b10">[Liu et 14]</ref>, CH <ref type="bibr" target="#b10">[Liu et al., 2014]</ref> incur large quantization loss <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref loss <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref> is easy to fall into a local optimum becau ref> which relaxed binary constraints at first and then quantified binary codes.</p><p>Nonetheless, <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref> proposed that those two-stage methods suff sentation. Later, following this, some two stage methods <ref type="bibr">[Zhou and Zha, 2012;</ref><ref type="bibr" target="#b12">Zhang et al., 2014]</ref> which relaxed binary constraints at first a
target="#b0">[Berg et al., 2017]</ref>, <ref type="bibr">RMGCNN [Monti et al., 2017]</ref>, GCNWSRS <ref type="bibr" target="#b11">[Ying et al., 2018]</ref>, <ref type="bibr">LGCN [Gao et al., 2018]</
shing for Collaborative Filtering</head><p>A pioneer work was to exploit Locality-Sensitive Hashing <ref type="bibr" target="#b5">[Datar et al., 2004]</ref> to generate binary codes for Google News re
al., 2004]</ref> to generate binary codes for Google News readers according to their click history <ref type="bibr" target="#b4">[Das et al., 2007]</ref>. Then <ref type="bibr">[Karatzoglou et al., 2
on. Some two-stage approximation methods like BCCF <ref type="bibr">[Zhou and Zha, 2012]</ref>, PPH <ref type="bibr" target="#b12">[Zhang et al., 2014]</ref>, CH <ref type="bibr" target="#b10">[Liu et 14]</ref>, CH <ref type="bibr" target="#b10">[Liu et al., 2014]</ref> incur large quantization loss <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref loss <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref> is easy to fall into a local optimum becau ref> which relaxed binary constraints at first and then quantified binary codes.</p><p>Nonetheless, <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref> proposed that those two-stage methods suff sentation. Later, following this, some two stage methods <ref type="bibr">[Zhou and Zha, 2012;</ref><ref type="bibr" target="#b12">Zhang et al., 2014]</ref> which relaxed binary constraints at first a
refetcher, offset prefetchers, and the sandbox method for selecting the prefetch offset dynamically <ref type="bibr" target="#b25">[26]</ref>. Offset prefetching is a generalization of next-line prefe </ref> (this list is not exhaustive).</p><p>Recently, Pugsley et al. introduced Sandbox prefetching <ref type="bibr" target="#b25">[26]</ref>. The Sandbox prefetcher prefetches line X + D when line X dge, the first published full-fledged offset prefetcher is the Sandbox prefetcher by Pugsley et al. <ref type="bibr" target="#b25">[26]</ref>. However, the offset selection mechanism in the Sandbox pr owledge, the SBP prefetcher of Pugsley et al. is the first published full-fledged offset prefetcher <ref type="bibr" target="#b25">[26]</ref>. The SBP prefetcher is cost-effective and was shown to out with actual prefetches.</p><p>We implemented the SBP prefetcher as described in the original paper <ref type="bibr" target="#b25">[26]</ref>, but with a few modifications to make the comparison with
microarchitecture simulators implement L2 and L3 MSHRs. MSHRs hold information about pending misses <ref type="bibr" target="#b17">[18]</ref>. An MSHR entry is deallocated only after the corresponding
[33]</ref>. So and Rechtschaffen proposed to use cache replacement status instead of a prefetch bit <ref type="bibr" target="#b33">[34]</ref>.</p><p>Stride prefetchers try to identify, among load and IP3: MRU insertion if demand miss, otherwise LRU insertion <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref>).</p><p>• IP4: MRU
efetch accuracy, cache pollution, prefetch timeliness, which depend on applications characteristics <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target
that have not yet been requested by the program. Prefetching has been studied since the 1970's (see <ref type="bibr" target="#b32">[33]</ref> for early references).</p><p>We consider hardware-only pre hen line X is accessed. A prefetch bit may be added to each cache line to reduce useless prefetches <ref type="bibr" target="#b32">[33]</ref>. So and Rechtschaffen proposed to use cache replacement st n="3.">OFFSET PREFETCHING</head><p>Offset prefetching is a generalization of next-line prefetching <ref type="bibr" target="#b32">[33]</ref>. When a line of address X is requested by the core, an off bers unchanged.</p><p>Our default L2 prefetcher is a simple next-line prefetcher with prefetch bits <ref type="bibr" target="#b32">[33]</ref>. Each L2 cache entry contains a prefetch bit, which is set

get="#b37">38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29]</ref>) and core-aware.</p><p>Our L3 baseline replacement policy,
refetch non-unit stride accesses, each stream being identified by the memory region it is accessing <ref type="bibr" target="#b23">[24]</ref>.</p><p>Hagersten proposed ROT, a sophisticated stream pref
hit in the DL1, and preferably in program order.</p><p>Stream prefetching was introduced by Jouppi <ref type="bibr" target="#b14">[15]</ref>. It exploits sequential streams, like next-line prefetchin e stream buffer full. Several stream buffers are needed for good performance on interleaved streams <ref type="bibr" target="#b14">[15]</ref>. Palacharla and Kessler proposed an off-chip stream prefet
policy</head><p>Replacement policies such as DIP <ref type="bibr" target="#b26">[27]</ref> or DRRIP <ref type="bibr" target="#b12">[13]</ref> have been proposed for L2 and L3 caches. We experimented w
hit in the DL1, and preferably in program order.</p><p>Stream prefetching was introduced by Jouppi <ref type="bibr" target="#b14">[15]</ref>. It exploits sequential streams, like next-line prefetchin e stream buffer full. Several stream buffers are needed for good performance on interleaved streams <ref type="bibr" target="#b14">[15]</ref>. Palacharla and Kessler proposed an off-chip stream prefet
n the same train/validation/test splits of the same three datasets (CORA, CiteSeer and PubMed) from <ref type="bibr" target="#b15">Yang et al. [2016]</ref>. Such experimental setup favors the model th consider the problem of semi-supervised transductive node classification in a graph, as defined in <ref type="bibr" target="#b15">Yang et al. [2016]</ref>. In this paper we compare the four following raged over all datasets. See text for the definition. (b) Model accuracy on the Planetoid split from<ref type="bibr" target="#b15">Yang et al. [2016]</ref> and another split on the same datasets. Diff ute the following simple experiment. We run the 4 models on the datasets and respective splits from <ref type="bibr" target="#b15">[Yang et al., 2016]</ref>. As shown in Table <ref type="table" target
r, we use the same training procedure for all the models. That is, we used the same optimizer (Adam <ref type="bibr" target="#b4">[Kingma and Ba, 2015]</ref> with default parameters), same initializat
e of using a train/validation/test split -finding the model with the best generalization properties <ref type="bibr" target="#b2">[Friedman et al., 2001]</ref>. Second, when evaluating performance of
datasets: PubMed <ref type="bibr" target="#b12">[Namata et al., 2012]</ref>, CiteSeer and CORA from <ref type="bibr" target="#b13">Sen et al. [2008]</ref>, as well as the extended version of CORA from
e of using a train/validation/test split -finding the model with the best generalization properties <ref type="bibr" target="#b2">[Friedman et al., 2001]</ref>. Second, when evaluating performance of
datasets: PubMed <ref type="bibr" target="#b12">[Namata et al., 2012]</ref>, CiteSeer and CORA from <ref type="bibr" target="#b13">Sen et al. [2008]</ref>, as well as the extended version of CORA from
datasets: PubMed <ref type="bibr" target="#b12">[Namata et al., 2012]</ref>, CiteSeer and CORA from <ref type="bibr" target="#b13">Sen et al. [2008]</ref>, as well as the extended version of CORA from
statistics</head><p>Amazon Computers and Amazon Photo are segments of the Amazon co-purchase graph <ref type="bibr" target="#b9">[McAuley et al., 2015]</ref>, where nodes represent goods, edges indic
datasets: PubMed <ref type="bibr" target="#b12">[Namata et al., 2012]</ref>, CiteSeer and CORA from <ref type="bibr" target="#b13">Sen et al. [2008]</ref>, as well as the extended version of CORA from
statistics</head><p>Amazon Computers and Amazon Photo are segments of the Amazon co-purchase graph <ref type="bibr" target="#b9">[McAuley et al., 2015]</ref>, where nodes represent goods, edges indic
statistics</head><p>Amazon Computers and Amazon Photo are segments of the Amazon co-purchase graph <ref type="bibr" target="#b9">[McAuley et al., 2015]</ref>, where nodes represent goods, edges indic
esults. Our work is partly inspired by the works on generating and refining score maps. Yang et al. <ref type="bibr" target="#b42">[43]</ref> adopts pyramid features as inputs of the network in the pr
int locations based on hand-crafted features. Recent works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targe ]</ref> uses deep consensus voting to vote the most probable location of keypoints. Gkioxary et al. <ref type="bibr" target="#b13">[14]</ref> and Zisserman et al. <ref type="bibr" target="#b1">[2]</re
arget="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> mostly rely on the development of convolutional neural netw
et="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9]</ref> interpret the process of detecting keypoints as a twostage pi
imation mainly adopt the techniques of pictorial structures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref> or graphical models <ref type="bibr" target="#b6">[7]</ref>. or graphical models <ref type="bibr" target="#b6">[7]</ref>. More specifically, the classical works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" targ
Detection approaches are mainly guided by the RCNN family <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref>, the up-to-date det
et="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9]</ref> interpret the process of detecting keypoints as a twostage pi
arget="#b7">8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20]</ref> formulate the problem of human keypoints estimation as a tr
IA Titan X Pascal GPUs. Our models are all initialized with weights of the public-released ImageNet <ref type="bibr" target="#b31">[32]</ref>pretrained model.</p><p>Testing Details. In order to minimi
ally integrates the informations of different levels via upsam- pling and concatenating as HyperNet <ref type="bibr" target="#b20">[21]</ref>. Different from the refinement strategy like stacked hourg mid output from the GlobalNet: 1) Concatenate (Concat) operation is directly attached like HyperNet <ref type="bibr" target="#b20">[21]</ref>,</p><p>2) a bottleneck block is attached first in each lay
rget="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" targe
get="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" targe
estimation.</p><p>Top-Down Approaches. Top-down approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar
n pose estimation.</p><p>Human Detection. Detection approaches are mainly guided by the RCNN family <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" ta

terative error feedback to get pose estimation and refine the prediction gradually. Lifshitz et al. <ref type="bibr" target="#b22">[23]</ref> uses deep consensus voting to vote the most probable locat
by the RCNN family <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref>, the up-to-date detector of which are <ref type="bibr" targ
arget="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> mostly rely on the
ally integrates the informations of different levels via upsam- pling and concatenating as HyperNet <ref type="bibr" target="#b20">[21]</ref>. Different from the refinement strategy like stacked hourg mid output from the GlobalNet: 1) Concatenate (Concat) operation is directly attached like HyperNet <ref type="bibr" target="#b20">[21]</ref>,</p><p>2) a bottleneck block is attached first in each lay
Detection approaches are mainly guided by the RCNN family <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref>, the up-to-date det
arget="#b7">8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20]</ref> formulate the problem of human keypoints estimation as a tr
s tackling the problem of human pose estimation mainly adopt the techniques of pictorial structures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref> or graphical models <
rget="#b6">[7]</ref>. More specifically, the classical works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar
rget="#b6">[7]</ref>. More specifically, the classical works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar
and RefineNet. Our GlobalNet learns a good feature representation based on feature pyramid network <ref type="bibr" target="#b23">[24]</ref>. More importantly, the pyramid feature representation can n="3.1.">Human Detector</head><p>We adopt the state-of-art object detector algorithms based on FPN <ref type="bibr" target="#b23">[24]</ref>. ROIAlign from Mask RCNN <ref type="bibr" target="#b14">[1 ain both the spatial resolution and semantic information for the feature layers. More recently, FPN <ref type="bibr" target="#b23">[24]</ref> further improves the U-shape structure with deeply supervi rget="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref>, the up-to-date detector of which are <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15]</ref>. These detection a sals to get the final boxes via R-CNN network. The detector used in our methods are mostly based on <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15]</ref>. </p></div> <div x
arget="#b32">33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20]</ref> formulate the probl

estimation.</p><p>Top-Down Approaches. Top-down approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar
/ref>, which proposes a cascade of CNN pose regressors to deal with pose estimation. Tompson et al. <ref type="bibr" target="#b36">[37]</ref> attempt to solve the problem by predicting heatmaps of key
terative error feedback to get pose estimation and refine the prediction gradually. Lifshitz et al. <ref type="bibr" target="#b22">[23]</ref> uses deep consensus voting to vote the most probable locat
d by the involvement of deep convolutional neural networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16]</ref>. For example, in <ref type="bibr" target="#b4">[5]</ref>, c ly on the development of convolutional neural network(CNN) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16]</ref>, which largely improve the performance of pose estimation. et="#b18">[19]</ref> improves DeepCut <ref type="bibr" target="#b29">[30]</ref> using deeper ResNet <ref type="bibr" target="#b15">[16]</ref> and employs image-conditioned pairwise terms to get better e eight-stage stacked hourglass module. <ref type="bibr" target="#b27">[28]</ref> utilizes a ResNet <ref type="bibr" target="#b15">[16]</ref> network to estimate pose in the wild achieving promising p
graphical model problem and predict keypoint locations based on hand-crafted features. Recent works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" ta he wild achieving promising performance in the COCO 2016 keypoint challenge. Motivated by the works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> described above, w l models. Later works such as Wei et al. <ref type="bibr" target="#b39">[40]</ref>and Newell et al. <ref type="bibr" target="#b26">[27]</ref> show great performance via generating the score map of key generate coarse results, and continuously refine the result in the following stages. Newell et al. <ref type="bibr" target="#b26">[27]</ref> propose an U-shape network, i.e., hourglass module, and st ucture for the single person skeleton estimator based on each human bounding box. Stacked hourglass <ref type="bibr" target="#b26">[27]</ref>, which is a prevalent method for pose estimation, stacks e type="bibr" target="#b20">[21]</ref>. Different from the refinement strategy like stacked hourglass <ref type="bibr" target="#b26">[27]</ref>, our Re-fineNet concatenates all the pyramid features rath ion, we apply a 2D gaussian filter on the predicted heatmaps. Following the same techniques used in <ref type="bibr" target="#b26">[27]</ref>, we also predict the pose of the corresponding flipped ima //www.tei-c.org/ns/1.0"><head n="4.2.2">Cascaded Pyramid Network</head><p>8-stage hourglass network <ref type="bibr" target="#b26">[27]</ref> and ResNet-50 with dilation <ref type="bibr" target="#b27"
rget="#b26">[27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" tar rget="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19]</ref> directly predict all keypoints at first and assemble them i stimation results are obtained when person clusters are combined with labeled body parts. DeeperCut <ref type="bibr" target="#b18">[19]</ref> improves DeepCut <ref type="bibr" target="#b29">[30]</ref>
(amino acids). Specifically, we augment the autoregressive self-attention of recent sequence models <ref type="bibr" target="#b6">[7]</ref> with graph-based descriptions of the 3D structure. By compos dependent, contextual embeddings of each residue in the 3D structure with multi-head self-attention <ref type="bibr" target="#b6">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive d Structured Transformer model that draws inspiration from the selfattention based Transformer model <ref type="bibr" target="#b6">[7]</ref> and is augmented for scalable incorporation of relational in an attend to a separate subspace of the embeddings via learned query, key and value transformations <ref type="bibr" target="#b6">[7]</ref>.</p><p>The queries are derived from the current embedding at een these self-attention layers and position-wise feedforward layers as in the original Transformer <ref type="bibr" target="#b6">[7]</ref>. We stack multiple layers atop each other, and thereby obtai rained models using the learning rate schedule and initialization of the original Transformer paper <ref type="bibr" target="#b6">[7]</ref>, a dropout rate of 10% <ref type="bibr" target="#b41">[42]</ f their structure. Our model augments the traditional sequence-level self-attention of Transformers <ref type="bibr" target="#b6">[7]</ref> with relational 3D structural encodings and is able to lever ndependent, contextual embeddings of each residue in the 3D structure with multi-head self-attention<ref type="bibr" target="#b6">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive d
in folds, we collected a dataset based on the CATH hierarchical classification of protein structure <ref type="bibr" target="#b39">[40]</ref>. For all domains in the CATH 4.2 40% non-redundant set of
plementary line of research, several deep and differentiable parameterizations of protein structure <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" t
tein science, namely that long-range dependencies in sequence are generally short-range in 3D space <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta
m protein sequences under these null models are unlikely to be functional without further selection <ref type="bibr" target="#b43">[44]</ref>. First order profiles of protein sequences such as those f
to a prior benchmark from members of the Rosetta community <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> across 40 diverse proteins. For this set of proteins, we re
he past two decades <ref type="bibr" target="#b0">[1]</ref>, including the design of novel 3D folds <ref type="bibr" target="#b1">[2]</ref>, enzymes <ref type="bibr" target="#b2">[3]</ref>, and comple
to a prior benchmark from members of the Rosetta community <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> across 40 diverse proteins. For this set of proteins, we re
n design. More recently, there have been successes with non-parametric approaches to protein design <ref type="bibr" target="#b31">[32]</ref> that are based on finding substructural homologies between
he past two decades <ref type="bibr" target="#b0">[1]</ref>, including the design of novel 3D folds <ref type="bibr" target="#b1">[2]</ref>, enzymes <ref type="bibr" target="#b2">[3]</ref>, and comple
nd structure, in contrast to previous parameteric approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> that are limited to only the first-order effects.</p><p>The g d design <ref type="bibr" target="#b12">[13]</ref>. Recently <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref> proposed neural netwo
o the root causes of relatively low IPC using the Top-down Microarchitecture Analysis Method (TMAM) <ref type="bibr" target="#b62">[63]</ref> to categorize processor pipelines' execution stalls, as re f> comprises both latency-sensitive and throughput-oriented scale-out cloud workloads. Yasin et al. <ref type="bibr" target="#b62">[63]</ref> perform a microarchitectural characterization of several C mark suite studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" tar
rget="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b65">66]</ref> typically find current LLC sizes to be sufficient to encomp
ever, performing an exhaustive search is prohibitive; better search heuristics (e.g., hill climbing <ref type="bibr" target="#b85">[86]</ref>) may be required.</p><p>?SKU and co-location. Our producti
re negligible for others. For example, microsecondscale overheads that arise from accesses to Flash <ref type="bibr" target="#b39">[40]</ref>, emerging memory technologies like 3D XPoint by Intel and
0">[111]</ref><ref type="bibr" target="#b111">[112]</ref><ref type="bibr" target="#b112">[113]</ref><ref type="bibr" target="#b113">[114]</ref> or schedule them in a machine characteristics-aware mann
="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref> like Cache1 or Cache2. However, such microsecond-scale ove
rget="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b65">66]</ref> typically find current LLC sizes to be sufficient to encomp
ices could benefit from larger Icache and ITLB and other techniques that address instruction misses <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>. In contrast, micr
8">[109]</ref><ref type="bibr" target="#b109">[110]</ref><ref type="bibr" target="#b110">[111]</ref><ref type="bibr" target="#b111">[112]</ref><ref type="bibr" target="#b112">[113]</ref><ref type="bib
="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref> that reduce context switch latency or counts might conside
="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" t
out the learning process would be beneficial to enhance the OOD discriminative power of the system. <ref type="bibr" target="#b3">Hendrycks et al. (2019)</ref> demonstrate that utilizing auxiliary dat ing higher likelihood estimates on unseen OOD samples. The ominous observation is presented also by <ref type="bibr" target="#b3">Hendrycks et al. (2019)</ref>, but they concentrate on improving the O r" target="#b8">Nalisnick et al., 2019a;</ref><ref type="bibr" target="#b0">Choi et al., 2018;</ref><ref type="bibr" target="#b3">Hendrycks et al., 2019)</ref>. These works report that despite intuiti
ora of work demonstrates the effectiveness of deep generative models in this regard, recent work of <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> and <ref type="bibr" target="#b0">Choi y influences the phenomenon The results suggest that the intriguing phenomenon in VAEs discussed by <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> and <ref type="bibr" target="#b0">Choi noise model from Bernoulli to Gaussian (and otherwise remaining in the same experimental setting as <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>), the issue of assigning higher likelih s hardly feasible, with below-1/2 AUC scores. Meanwhile, with a Bernoulli noise model (also used in <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>) both the likelihoodestimates and the K the table (where models are trained on MNIST) confirm the asymmetric behaviour already described by <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>, that is, switching the roles of the in re> <figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> examine the phenomenon in detail, focus luation of generative models on OOD data <ref type="bibr" target="#b12">(Shafaei et al., 2018;</ref><ref type="bibr" target="#b8">Nalisnick et al., 2019a;</ref><ref type="bibr" target="#b0">Choi et al

ora of work demonstrates the effectiveness of deep generative models in this regard, recent work of <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> and <ref type="bibr" target="#b0">Choi y influences the phenomenon The results suggest that the intriguing phenomenon in VAEs discussed by <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> and <ref type="bibr" target="#b0">Choi noise model from Bernoulli to Gaussian (and otherwise remaining in the same experimental setting as <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>), the issue of assigning higher likelih s hardly feasible, with below-1/2 AUC scores. Meanwhile, with a Bernoulli noise model (also used in <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>) both the likelihoodestimates and the K the table (where models are trained on MNIST) confirm the asymmetric behaviour already described by <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>, that is, switching the roles of the in re> <figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> examine the phenomenon in detail, focus luation of generative models on OOD data <ref type="bibr" target="#b12">(Shafaei et al., 2018;</ref><ref type="bibr" target="#b8">Nalisnick et al., 2019a;</ref><ref type="bibr" target="#b0">Choi et al
ora of work demonstrates the effectiveness of deep generative models in this regard, recent work of <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> and <ref type="bibr" target="#b0">Choi y influences the phenomenon The results suggest that the intriguing phenomenon in VAEs discussed by <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> and <ref type="bibr" target="#b0">Choi noise model from Bernoulli to Gaussian (and otherwise remaining in the same experimental setting as <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>), the issue of assigning higher likelih s hardly feasible, with below-1/2 AUC scores. Meanwhile, with a Bernoulli noise model (also used in <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>) both the likelihoodestimates and the K the table (where models are trained on MNIST) confirm the asymmetric behaviour already described by <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>, that is, switching the roles of the in re> <figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> examine the phenomenon in detail, focus luation of generative models on OOD data <ref type="bibr" target="#b12">(Shafaei et al., 2018;</ref><ref type="bibr" target="#b8">Nalisnick et al., 2019a;</ref><ref type="bibr" target="#b0">Choi et al
btained negative samples are richer in features and semantically more meaningful for the task. (See <ref type="bibr" target="#b6">Lee et al. (2018)</ref> for an incarnation of this idea in the context asets carefully to obtain robust OOD detection.</p><p>Within the context of uncertainty estimation, <ref type="bibr" target="#b6">Lee et al. (2018)</ref> demonstrate that adversarially generated sampl samples of GANs is closest to our approach of using generated data points as negative samples, but <ref type="bibr" target="#b6">Lee et al. (2018)</ref> work within a classification setting. <ref typ

ora of work demonstrates the effectiveness of deep generative models in this regard, recent work of <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> and <ref type="bibr" target="#b0">Choi y influences the phenomenon The results suggest that the intriguing phenomenon in VAEs discussed by <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> and <ref type="bibr" target="#b0">Choi noise model from Bernoulli to Gaussian (and otherwise remaining in the same experimental setting as <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>), the issue of assigning higher likelih s hardly feasible, with below-1/2 AUC scores. Meanwhile, with a Bernoulli noise model (also used in <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>) both the likelihoodestimates and the K the table (where models are trained on MNIST) confirm the asymmetric behaviour already described by <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>, that is, switching the roles of the in re> <figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> examine the phenomenon in detail, focus luation of generative models on OOD data <ref type="bibr" target="#b12">(Shafaei et al., 2018;</ref><ref type="bibr" target="#b8">Nalisnick et al., 2019a;</ref><ref type="bibr" target="#b0">Choi et al



rt from communication, their shared access to the wireless channel enables environmental perception <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t xploited by various modalities, including RF-fluctuation, such as WiFi, FM-radio or software radios <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" t
ion from phase and time-domain signal strength fluctuation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Recognition of
="#b13">[14]</ref> 0.64 RSSI (Active) <ref type="bibr" target="#b17">[18]</ref> 0.72 RSSI (Passive) <ref type="bibr" target="#b16">[17]</ref> 0.78 Acceleration <ref type="bibr" target="#b18">[19]</ref type="bibr" target="#b13">[14]</ref>, custom SDR <ref type="bibr" target="#b15">[16]</ref> or RSSI <ref type="bibr" target="#b16">[17]</ref>.</p><p>Exploiting a prototype 5G OFDM system, developed at
t="#b17">[18]</ref> 0.72 RSSI (Passive) <ref type="bibr" target="#b16">[17]</ref> 0.78 Acceleration <ref type="bibr" target="#b18">[19]</ref> 0.85 This work (cellular 5G) 0.95</p></div> <div xmlns="ht
red from variance in FM radio signal strength <ref type="bibr" target="#b13">[14]</ref>, custom SDR <ref type="bibr" target="#b15">[16]</ref> or RSSI <ref type="bibr" target="#b16">[17]</ref>.</p><p>E
signal envelope <ref type="bibr" target="#b6">[7]</ref>, respiration rate exploiting Fresnel zones <ref type="bibr" target="#b7">[8]</ref> as well as emotion recognition from phase and time-domain si
as a reliable modality to detect movement disorders such as in Multiple sclerosis and elderly care <ref type="bibr" target="#b14">[15]</ref>. Environmental sensing (e.g. RF-based) of walking speed an
s to the wireless channel enables environmental perception <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However, unlike cl ominently studied in recent years, ranging from the recognition of gestures via Doppler fluctuation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> or CSI signal envel adio frequency fluctuation has become undemanding, as pre-installed infrastructure can be exploited <ref type="bibr" target="#b2">[3]</ref>. This situation will further improve with upcoming 5G commun
s to the wireless channel enables environmental perception <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However, unlike cl ominently studied in recent years, ranging from the recognition of gestures via Doppler fluctuation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> or CSI signal envel adio frequency fluctuation has become undemanding, as pre-installed infrastructure can be exploited <ref type="bibr" target="#b2">[3]</ref>. This situation will further improve with upcoming 5G commun
ion from phase and time-domain signal strength fluctuation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Recognition of
mental perception <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However, unlike classical RF-sensing approaches that exploi g from the recognition of gestures via Doppler fluctuation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> or CSI signal envelope <ref type="bibr" target="#b6">[7]</re RF-fluctuation, such as WiFi, FM-radio or software radios <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Walking speed is a nto environments and convenience for subjects as the sensing equipment need not be worn on the body <ref type="bibr" target="#b3">[4]</ref>.</p><p>The recognition of walking speed from RF-fluctuation
are control flow checking is to partition the program into basic blocks (branch-free parts of code) <ref type="bibr" target="#b14">[14]</ref>. For each block a deterministic signature is calculated an > and On-line control flow error detection using relationship signatures among basic blocks (RSCFC) <ref type="bibr" target="#b14">[14]</ref>.</p><p>ECCA, firstly, assigns a unique prime number identi rget="#b15">[15]</ref> technique to the original code,  a safe one, obtained by applying the RSCFC <ref type="bibr" target="#b14">[14]</ref> technique to the original code,  a safe one, obtained by


in the literature are the techniques called Enhanced Control Flow Checking Using Assertions (ECCA) <ref type="bibr" target="#b12">[13]</ref> and Control Flow Checking by Software Signatures (CFCSS) < from the signature associated with the current node, it means an error has occurred in the program <ref type="bibr" target="#b12">[13]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> ) in which the execution always enters at the first instruction and leaves via the last instruction <ref type="bibr" target="#b12">[13]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>
attestation. The cores of trusted computing technology are trusted computing base and trusted chain <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, and trusted measureme
ntal requirement and necessary consequence in order to ensure trust in the computing infrastructure <ref type="bibr" target="#b2">[3]</ref>.</p><p>Trusted Computing as proposed by the Trusted Computin

ntal requirement and necessary consequence in order to ensure trust in the computing infrastructure <ref type="bibr" target="#b2">[3]</ref>.</p><p>Trusted Computing as proposed by the Trusted Computin

eturn-into-libc attack by allowing the attacker arbitrary computation without calling any functions <ref type="bibr" target="#b7">[8]</ref>. In a traditional return-into libc attack, an attacker could
Table <ref type="table" target="#tab_0">2</ref> is calculated according to the equation 2 and 3 of <ref type="bibr" target="#b17">[17]</ref>. Table <ref type="table" target="#tab_0">2</ref> shows tha



Table <ref type="table" target="#tab_0">2</ref> is calculated according to the equation 2 and 3 of <ref type="bibr" target="#b17">[17]</ref>. Table <ref type="table" target="#tab_0">2</ref> shows tha
emand for secure communication and secure operation due to rising online fraud and software attacks <ref type="bibr" target="#b0">[1]</ref>. Some of these vulnerabilities are due to the complexity and a binary positive number to each BB, and the length of a signature Len can be obtained by equation <ref type="bibr" target="#b0">(1)</ref>, where N is the number of total BBs. </p></div> <div xmlns="
<ref type="bibr" target="#b12">[13]</ref> and Control Flow Checking by Software Signatures (CFCSS) <ref type="bibr" target="#b15">[15]</ref> and On-line control flow error detection using relationshi ersions for each benchmark:</p><p> the original code,  a safe one, obtained by applying the CFCSS <ref type="bibr" target="#b15">[15]</ref> technique to the original code,  a safe one, obtained by
ting technology are trusted computing base and trusted chain <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, and trusted measurement is a key problem of this technology
attestation. The cores of trusted computing technology are trusted computing base and trusted chain <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, and trusted measureme
, some are due to poor software development practices and lack of software security in applications <ref type="bibr" target="#b1">[2]</ref>. In this context, the integrity of system software and appli


ef><ref type="bibr" target="#b15">16]</ref> further extend the deep models for multimodal learning. <ref type="bibr" target="#b16">[17]</ref> design a cross-media learning method based on DNN, and lev network (CNN) with cross autoencoders to learn the latent high-level attributes on crossmodal units <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b17">[18]</ref>. Finally, we pro very tweets by utilizing a recently proposed cross-media model, namely the Cross Autoencoders (CAE) <ref type="bibr" target="#b16">[17]</ref>.</p><p>An auto encoder is a basic unit in deep neural netw s for comparison with previous work, due to the different goal, our results are not comparable with <ref type="bibr" target="#b16">[17]</ref>. Actually, the most related user-level prediction work is
earn the latent high-level attributes on crossmodal units <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b17">[18]</ref>. Finally, we propose a deep neural network (DNN) model to further model a user as a subject of series of tweets, we apply Convolutional Neural Networks (CNN) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. CNNs have large l
eases the risk of developing health problems such as insomnia, obesity, heart diseases, cancer etc. <ref type="bibr" target="#b0">[1]</ref>. Many studies have revealed a link between stress and mental ><head n="2.2.3">Observations on Behavioral Correlation</head><p>As revealed by psychology theories <ref type="bibr" target="#b0">[1]</ref>, there are many common symptoms may be related to stress, in
It tries to find a hyperplane that divides training samples into their classes with maximum margin <ref type="bibr" target="#b26">[27]</ref>. In our problem we use SVM with RBF kernel which can handl

et="#b8">[9]</ref> leverage Tweeter postings to identify the spread of flu symptoms. Paul M.J. etc. <ref type="bibr" target="#b9">[10]</ref> apply the Ailment Topic Aspect Model to over 1.5 million he
(DNN) in learning features from large scale unlabeled data <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr" ral networks for learning distinctive attributes from data <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. It is a shallow
/1.0"><head>2)</head><p>Visual Attributes: Based on previous work on affective image classification <ref type="bibr" target="#b21">[22]</ref> and color psychology theories <ref type="bibr" target="#b2 s been revealed to have important impact on human emotions according to psychology and art theories <ref type="bibr" target="#b21">[22]</ref>. Saturation (2 dimensions): the mean value of saturation a
ress, including psychological questionnaire based interviews <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and physiological signal based measures <ref type="bibr" targ
earn the latent high-level attributes on crossmodal units <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b17">[18]</ref>. Finally, we propose a deep neural network (DNN) model to further model a user as a subject of series of tweets, we apply Convolutional Neural Networks (CNN) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. CNNs have large l
any methods to measure psychological stress, including psychological questionnaire based interviews <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and physiological sign ed to instant emotions, indicating that the stressed stated can last for several days in psychology <ref type="bibr" target="#b2">[3]</ref>. It remains a challenge to make use of aggregated cross-medi
se access patterns <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar e requesting PC, and then adapt their policy to each class <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar [35]</ref> computes a statistical cost function from sampled reuse distance histograms. And Hawkeye <ref type="bibr" target="#b15">[16]</ref> emulates MIN's past decisions. Without a theoretical found ef type="bibr" target="#b20">[21]</ref>, PRP <ref type="bibr" target="#b10">[11]</ref>, and Hawkeye <ref type="bibr" target="#b15">[16]</ref> learn the behavior of different PCs. And PDP <ref type="bi
get="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar ment in practice: Many high-performance policies try to emulate MIN through various heuristics. DIP <ref type="bibr" target="#b29">[30]</ref> avoids thrashing by inserting most lines at low priority, ond, recent high-performance policies adapt themselves to the access stream to varying degrees. DIP <ref type="bibr" target="#b29">[30]</ref> detects thrashing with set dueling, and thereafter inserts a fraction of sets to monitoring alternative policies (cf. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref>), nor does it require auxiliary tags to monitor properties
target="#b38">[39]</ref>, 41% for DRRIP <ref type="bibr" target="#b16">[17]</ref>, and 42% for PDP <ref type="bibr" target="#b13">[14]</ref>.</p><p>Fewer misses translate into large area savings-EVA RDP <ref type="bibr" target="#b26">[27]</ref> try to predict candidates' times until reference. PDP <ref type="bibr" target="#b13">[14]</ref> protects lines from eviction for a fixed number of accesse ies that do not assume recency still base their policy on when a candidate was last referenced: PDP <ref type="bibr" target="#b13">[14]</ref> protects candidates until a certain age; and IRGD <ref typ and Hawkeye <ref type="bibr" target="#b15">[16]</ref> learn the behavior of different PCs. And PDP <ref type="bibr" target="#b13">[14]</ref> and IRGD <ref type="bibr" target="#b34">[35]</ref> use aux ance, maximizing upon the available information.</p><p>In contrast, no "protecting distance" in PDP <ref type="bibr" target="#b13">[14]</ref> can do so. Protecting the small array gives a hit rate of bandwidth.</p><p>A. Hardware operations Aging: Aging: Aging: Aging: We use per-set, coarsened ages <ref type="bibr" target="#b13">[14]</ref>. Each cache line has a k-bit age, and each set has a j-bit ally any ranking function of ages <ref type="bibr" target="#b8">[9]</ref>, such as random, LRU, PDP <ref type="bibr" target="#b13">[14]</ref>, IRGD <ref type="bibr" target="#b34">[35]</ref>, etc. We s stics based on observations of common-case access patterns <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar onitor properties independent of the replacement policy (cf. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>). As a result, our
d. RRIP <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39]</ref> and IbRDP <ref type="bibr" target="#b26">[27]</ref> try to predict candidates' times until reference. PDP <ref e candidate with the highest expected time until reference <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref> or the lowest expec til reference and evict the candidate with the longest one <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>. A simple counterex

, a large body of work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar 2">33]</ref> has studied cache behavior under more complex memory reference models. (Garetto et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">?VI]</ref> summarize recent work that mode
get="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>. We observe that, u e="bibr" target="#b13">[14]</ref> protects lines from eviction for a fixed number of accesses. IRGD <ref type="bibr" target="#b34">[35]</ref> computes a statistical cost function from sampled reuse di ed: PDP <ref type="bibr" target="#b13">[14]</ref> protects candidates until a certain age; and IRGD <ref type="bibr" target="#b34">[35]</ref> uses a heuristic function of ages. Another common way poli ef> learn the behavior of different PCs. And PDP <ref type="bibr" target="#b13">[14]</ref> and IRGD <ref type="bibr" target="#b34">[35]</ref> use auxiliary monitors to profile the access pattern and p r" target="#b8">[9]</ref>, such as random, LRU, PDP <ref type="bibr" target="#b13">[14]</ref>, IRGD <ref type="bibr" target="#b34">[35]</ref>, etc. We set priorities to implement EVA.</p><p>Periodical me until reference <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref> or the lowest expected hit probability <ref type="bibr" tar th the longest one <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>. A simple counterexample shows why predictions of time unti lacement policy (cf. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>). As a result, our implementation makes full use of the ent
="bibr" target="#b37">[38]</ref>), or on systems with dedicated OS cores (e.g., the Kalray MPPA-256 <ref type="bibr" target="#b11">[12]</ref> or Fujitsu Sparc64 XIfx <ref type="bibr" target="#b39">[40
target="#b38">[39]</ref>, 41% for DRRIP <ref type="bibr" target="#b16">[17]</ref>, and 42% for PDP <ref type="bibr" target="#b13">[14]</ref>.</p><p>Fewer misses translate into large area savings-EVA RDP <ref type="bibr" target="#b26">[27]</ref> try to predict candidates' times until reference. PDP <ref type="bibr" target="#b13">[14]</ref> protects lines from eviction for a fixed number of accesse ies that do not assume recency still base their policy on when a candidate was last referenced: PDP <ref type="bibr" target="#b13">[14]</ref> protects candidates until a certain age; and IRGD <ref typ and Hawkeye <ref type="bibr" target="#b15">[16]</ref> learn the behavior of different PCs. And PDP <ref type="bibr" target="#b13">[14]</ref> and IRGD <ref type="bibr" target="#b34">[35]</ref> use aux ance, maximizing upon the available information.</p><p>In contrast, no "protecting distance" in PDP <ref type="bibr" target="#b13">[14]</ref> can do so. Protecting the small array gives a hit rate of bandwidth.</p><p>A. Hardware operations Aging: Aging: Aging: Aging: We use per-set, coarsened ages <ref type="bibr" target="#b13">[14]</ref>. Each cache line has a k-bit age, and each set has a j-bit ally any ranking function of ages <ref type="bibr" target="#b8">[9]</ref>, such as random, LRU, PDP <ref type="bibr" target="#b13">[14]</ref>, IRGD <ref type="bibr" target="#b34">[35]</ref>, etc. We s stics based on observations of common-case access patterns <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar onitor properties independent of the replacement policy (cf. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>). As a result, our
etween theory and practice.</p><p>From a theoretical standpoint, the optimal policy is Belady's MIN <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>, which evicts the c
imple scheme because it has proven effective in prior work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. (In the above exam
sharer representation. We leverage recent prior work on efficient highly-associative caches (ZCache <ref type="bibr" target="#b24">[25]</ref> and Cuckoo Directory <ref type="bibr" target="#b9">[10]</r ractice as if replacement candidates were selected randomly, independently of the addresses tracked <ref type="bibr" target="#b24">[25]</ref>. We exploit this property to design and analyze SCD:</p><p array designs, such as skew-associative caches <ref type="bibr" target="#b26">[27]</ref> or zcaches <ref type="bibr" target="#b24">[25]</ref>. SPACE <ref type="bibr" target="#b35">[36]</ref> observes has proposed cache designs that provide high associativity with a small number of ways. Both ZCache <ref type="bibr" target="#b24">[25]</ref> and Cuckoo Directory <ref type="bibr" target="#b9">[10]</r se models to show that associativity depends only on the number of replacement candidates, not ways <ref type="bibr" target="#b24">[25]</ref>, and to implement scalable and efficient cache partitionin ized, and processed in FCFS order, to preserve atomicity and ensure fairness. Second, as in zcaches <ref type="bibr" target="#b24">[25]</ref>, the array is pipelined, and we allow concurrent non-confl having 16 cores, a directory and L3 bank, and a memory controller. Both L2 and L3 are 4-way zcaches <ref type="bibr" target="#b24">[25]</ref> with 16 and 52 replacement candidates, respectively. Cache ts, but replacements incur similar energy costs as a set-associative cache of similar associativity <ref type="bibr" target="#b24">[25]</ref>. In directories, the cost of a replacement is also much sm on over the cache array. We have shown that in practice, this is an accurate assumption for zcaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. We leverage this ons, which are simple to implement and work well in practice <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>. Tiles are connected with an 8?8 mesh network-on-chip (NoC)
1.">Introduction</head><p>As Moore's Law enables chip-multiprocessors (CMPs) with hundreds of cores <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" ta
xactly through coarse-grain bit-vectors <ref type="bibr" target="#b12">[13]</ref>, limited pointers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, Tagless directories < e one of the existing sharers to be invalidated, a broadcast on future invalidations and downgrades <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>, or trigger an inter
being set-associative, and would not work with other array designs, such as skew-associative caches <ref type="bibr" target="#b26">[27]</ref> or zcaches <ref type="bibr" target="#b24">[25]</ref>. SPAC /ref> and Cuckoo Directory <ref type="bibr" target="#b9">[10]</ref> build on skewassociative caches <ref type="bibr" target="#b26">[27]</ref> and Cuckoo hashing <ref type="bibr" target="#b23">[24]</re
oherence protocols are more complex than single-level protocols, and significantly harder to verify <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Motivated by th
<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, Tagless directories <ref type="bibr" target="#b34">[35]</ref> and SPACE <ref type="bibr" target="#b35">[36]</ref>. Unfor nal traffic in the form of spurious invalidations, and often increase coherence protocol complexity <ref type="bibr" target="#b34">[35]</ref>.</p><p>In this paper, we present the Scalable Coherence Di e directory organizations that improve scalability in a single-level directory. Tagless directories <ref type="bibr" target="#b34">[35]</ref> use Bloom filters to represent sharer sets. Tagless does n
hes and directories use H 3 hash functions, which are simple to implement and work well in practice <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>. Tiles are connected
per level). Workloads: We simulate 14 multithreaded workloads selected from multiple suites: PARSEC <ref type="bibr" target="#b2">[3]</ref> (blackscholes, canneal, fluidanimate), SPLASH-2 <ref type="b
ef type="bibr" target="#b24">[25]</ref>, and to implement scalable and efficient cache partitioning <ref type="bibr" target="#b25">[26]</ref>. In this paper, we extend these models   to characterize a at in practice, this is an accurate assumption for zcaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. We leverage this assumption in the derivations, and verify
1.">Introduction</head><p>As Moore's Law enables chip-multiprocessors (CMPs) with hundreds of cores <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" ta
lu, ocean, radix, water), SPECOMP (applu, equake, wupwise), SPECJBB2005 (specjbb), and BioParallel <ref type="bibr" target="#b15">[16]</ref> (svm). We have selected workloads that scale reasonably we
olution operations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. GC-MC <ref type="bibr" target="#b28">[29]</ref> applies th t is captured on the level of item relations, rather than the collective user behaviors. SpectralCF <ref type="bibr" target="#b41">[42]</ref> proposes a spectral convolution operation to discover all , is used as suggested in <ref type="bibr" target="#b28">[29]</ref>.</p><p>We also tried SpectralCF <ref type="bibr" target="#b41">[42]</ref> but found that the eigen-decomposition leads to high time
omplex and nonlinear relationships between users and items <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Towards this end, recent efforts <ref type="bibr" target=" nd, recent efforts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref> focus on exploiting
ensure data quality. Amazon-book: Amazon-review is a widely used dataset for product recommendation <ref type="bibr" target="#b8">[9]</ref>. We select Amazon-book from the collection. Similarly, we us
g procedure, which is commonly used to make graph convolution network runnable on large-scale graph <ref type="bibr" target="#b24">[25]</ref>. We will analyze the complexity in Section 2.5.2.</p></div
atures <ref type="bibr" target="#b39">[40]</ref> in context-aware and semantics-rich recommendation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, item knowledge gr
convolution networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref> that consider the c o prevent neural networks from overfitting. Following the prior work on graph convolutional network <ref type="bibr" target="#b28">[29]</ref>, we propose to adopt two dropout techniques in NGCF: messa ">[29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. GC-MC <ref type="bibr" target="#b28">[29]</ref> applies the graph convolution network (GCN) <ref type="bib ibr" target="#b40">[41]</ref>, and the hidden dimension is set equal to the embedding size. • GC-MC <ref type="bibr" target="#b28">[29]</ref>: This model adopts GCN <ref type="bibr" target="#b17">[18] convolution layer, where the hidden dimension is set as the embedding size, is used as suggested in <ref type="bibr" target="#b28">[29]</ref>.</p><p>We also tried SpectralCF <ref type="bibr" target="# <ref type="bibr" target="#b36">[37]</ref>.</p><p>4.4.3 Effect of Dropout. Following the prior work <ref type="bibr" target="#b28">[29]</ref>, we employ node dropout and message dropout techniques to opout is more effective than message dropout, which is consistent with the findings of prior effort <ref type="bibr" target="#b28">[29]</ref>. We believe this is an interesting finding, which means th ties. Here we discuss existing recommendation methods that also employ graph convolution operations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" ta
g procedure, which is commonly used to make graph convolution network runnable on large-scale graph <ref type="bibr" target="#b24">[25]</ref>. We will analyze the complexity in Section 2.5.2.</p></div
(MF) directly embeds user/item ID as an vector and models user-item interaction with inner product <ref type="bibr" target="#b19">[20]</ref>; collaborative deep learning extends the MF embedding func presentations and reconstruct user-item interaction data based on model parameters. For example, MF <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref> projects the ID of
f user u obtained after the first embedding propagation layer. The activation function of LeakyReLU <ref type="bibr" target="#b22">[23]</ref> allows messages to encode both positive and small negative
edding function by integrating the deep representations learned from rich side information of items <ref type="bibr" target="#b29">[30]</ref>; neural collaborative filtering models replace the MF inte en devoted to incorporate side information like item content <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>, social relations <ref type="bibr" target="#b32">[33]</ref>
ems for reconstructing historical interactions, and predict user preference based on the parameters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Generally spe ww.tei-c.org/ns/1.0"><head n="2.1">Embedding Layer</head><p>Following mainstream recommender models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ <ref type="formula" target="#formula_3">3</ref>)). Analogously, we can obtain the representation e <ref type="bibr" target="#b0">(1)</ref> i for item i by propagating information from its connected u able the transformation matrix and nonlinear activation function.</p><p>Thereafter, e</p><p>u and e <ref type="bibr" target="#b0">(1)</ref> i are treated as the final representations for user u and it
bibr" target="#b43">(Xiong et al., 2016;</ref><ref type="bibr" target="#b31">Seo et al., 2016;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>. The resulting representation is encoded l language.</p><p>In this work, we consider attention-based neural machine translation (NMT) models <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref>; <ref type="bibr" target="#b24">Luong et
type="bibr" target="#b19">(Kim, 2014;</ref><ref type="bibr" target="#b9">Gehring et al., 2017;</ref><ref type="bibr" target="#b36">Vaswani et al., 2017b;</ref><ref type="bibr" target="#b32">Shen et al
>) or instrinsic paraphrase evaluations <ref type="bibr" target="#b41">Wieting et al. (2017)</ref>; <ref type="bibr" target="#b26">Mallinson et al. (2017)</ref>. Our approach is a novel application of

by full convolution or full attention architectures <ref type="bibr" target="#b19">(Kim, 2014;</ref><ref type="bibr" target="#b9">Gehring et al., 2017;</ref><ref type="bibr" target="#b36">Vaswani et a


ata for down-stream tasks, in this case, the question answering (QA) task. It is worth to note that <ref type="bibr" target="#b8">(Dong et al., 2017)</ref> use paraphrasing techniques to improve QA; h
77.1).</p><p>Data augmentation has also been explored in natural language processing. For example, <ref type="bibr" target="#b47">Zhang et al. (2015)</ref> proposed to enhance the dataset by replacin


ave been made to replace the recurrent networks by full convolution or full attention architectures <ref type="bibr" target="#b19">(Kim, 2014;</ref><ref type="bibr" target="#b9">Gehring et al., 2017;<
bibr" target="#b31">(Seo et al., 2016;</ref><ref type="bibr" target="#b43">Xiong et al., 2016;</ref><ref type="bibr" target="#b38">Wang et al., 2017;</ref><ref type="bibr" target="#b3">Chen et al., 20 oposed to tackle these challenges, including <ref type="bibr">BiDAF (Seo et al., 2016)</ref>, r-net <ref type="bibr" target="#b38">(Wang et al., 2017)</ref>, DCN <ref type="bibr" target="#b43">(Xiong ref> improved the diversity of the SQuAD data by generating more questions. However, as reported by <ref type="bibr" target="#b38">Wang et al. (2017)</ref>, their method did not help improve the perfo
/ref>. For simple tasks such as text classification, with reinforcement learning techniques, models <ref type="bibr" target="#b44">(Yu et al., 2017)</ref> have been proposed to skip irrelevant tokens
neural machine translation (NMT) models <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref>; <ref type="bibr" target="#b24">Luong et al. (2015)</ref>, which have demonstrated excellent translat
neural machine translation (NMT) models <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref>; <ref type="bibr" target="#b24">Luong et al. (2015)</ref>, which have demonstrated excellent translat
ention Layer. This module is standard in almost every previous reading comprehension models such as <ref type="bibr" target="#b40">Weissenborn et al. (2017)</ref> and <ref type="bibr" target="#b3">Che model is still based on the RNNs and the accuracy is not competitive, with an EM 68.4 and F1 76.2. <ref type="bibr" target="#b40">Weissenborn et al. (2017)</ref> also tried to build a fast Q&amp;A mo n Networks <ref type="bibr" target="#b43">(Xiong et al., 2016)</ref> 66.2 / 75.9 66.2 / 75.9 FastQA <ref type="bibr" target="#b40">(Weissenborn et al., 2017)</ref> 68.4 / 77.1 68.4 / 77.1 BiDAF <ref t 8.0 RaSoR <ref type="bibr" target="#b21">(Lee et al., 2016)</ref> 70.8 / 78.7 69.6 / 77.7 FastQAExt <ref type="bibr" target="#b40">(Weissenborn et al., 2017)</ref> 70.8 / 78.9 70.8 / 78.9 ReasoNet <re
slation task Sennrich et al. ( <ref type="formula">2016</ref>) or instrinsic paraphrase evaluations <ref type="bibr" target="#b41">Wieting et al. (2017)</ref>; <ref type="bibr" target="#b26">Mallinson
017)</ref>, CNN/Daily News <ref type="bibr" target="#b11">(Hermann et al., 2015)</ref>, WikiReading <ref type="bibr" target="#b12">(Hewlett et al., 2016</ref><ref type="bibr">), Children Book Test (Hi
ata for down-stream tasks, in this case, the question answering (QA) task. It is worth to note that <ref type="bibr" target="#b8">(Dong et al., 2017)</ref> use paraphrasing techniques to improve QA; h
017)</ref>, CNN/Daily News <ref type="bibr" target="#b11">(Hermann et al., 2015)</ref>, WikiReading <ref type="bibr" target="#b12">(Hewlett et al., 2016</ref><ref type="bibr">), Children Book Test (Hi
/ref>. For simple tasks such as text classification, with reinforcement learning techniques, models <ref type="bibr" target="#b44">(Yu et al., 2017)</ref> have been proposed to skip irrelevant tokens
methods train image embeddings through the local relationships between images in the form of pairs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> or triplets <ref type=
rget="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. The goal of metric learning is that the learned embedding
bibr" target="#b26">27]</ref>, and achieve state-of-the-art performance in image retrieval datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta ommonly used in image retrieval tasks with the standard train/test split on four datasets: CARS-196 <ref type="bibr" target="#b11">[12]</ref>, CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref>, S
sampling method, the parametric nature may cause issue in fine-grained open-set recognition setting <ref type="bibr" target="#b27">[28]</ref>.</p><p>Ensembling Ensembling embeddings has been the most ts as also used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>. We follow the same derivation and notations as in <ref typ
<ref type="bibr" target="#b30">[31]</ref>, and visual search <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar storage and distance computation costs especially in large scale applications such as visual search <ref type="bibr" target="#b10">[11]</ref>. We observe however that as we increase dimensionality of
rget="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. The goal of metric learning is that the learned embedding
g face recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, fine-grained retri ion-based training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar osses are widely adopted in face verification applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> and achieve state-o
ford Online Products (SOP) <ref type="bibr" target="#b17">[18]</ref>, and In-shop Clothes Retrieval <ref type="bibr" target="#b33">[34]</ref>. We compare our method using Recall@K to measure retrieval
<ref type="bibr" target="#b30">[31]</ref>, and visual search <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar storage and distance computation costs especially in large scale applications such as visual search <ref type="bibr" target="#b10">[11]</ref>. We observe however that as we increase dimensionality of
em of a variety of applications including face recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar s sampling informative samples for training. As described in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref>, negatives that are >[1,</ref><ref type="bibr" target="#b1">2]</ref> or triplets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. A core challenge with metric learning is sampling informat mpling informative training samples plays an important role in metric learning as also suggested in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>. Semi-hard samplin ,</ref><ref type="bibr" target="#b26">27]</ref>. Semi-hard sampling in conjunction with tripletloss <ref type="bibr" target="#b15">[16]</ref> has been widely adopted for many tasks. Distanced-weighted
sampling method, the parametric nature may cause issue in fine-grained open-set recognition setting <ref type="bibr" target="#b27">[28]</ref>.</p><p>Ensembling Ensembling embeddings has been the most ts as also used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>. We follow the same derivation and notations as in <ref typ
loss from the image-space to a higher-level feature space of an object recognition system like VGG <ref type="bibr" target="#b48">[49]</ref>, resulting in sharper results despite lower PSNR values.</ etwork once to get the result. The exclusive use of 3×3 filters is inspired by the VGG architecture <ref type="bibr" target="#b48">[49]</ref> and allows for deeper models at a low number of parameters map φ, we use a pre-trained implementation of the popular VGG-19 network <ref type="bibr">[1,</ref><ref type="bibr" target="#b48">49]</ref>. It consists of stacked convolutions coupled with pooling l
produce sharp results in a number of image generation tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar
)×(αh).</p><p>We trained all models for a maximum of 24 hours on an Nvidia K40 GPU using TensorFlow <ref type="bibr" target="#b1">[2]</ref>, though convergence rates depend on the applied combination
dictionary atoms, as well as neural network-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target= get="#b18">[19,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>More specifically, Dong et al. <ref type="bibr" target="#b7">[8]</ref> apply shallow networks to the task of SISR by training a CNN mate. It may seem natural to simply feed the bicubic interpolation of the LR image into the network <ref type="bibr" target="#b7">[8]</ref>. However, this introduces redundancies to the input image an SR <ref type="bibr" target="#b59">[60]</ref> SelfEx <ref type="bibr" target="#b21">[22]</ref> SRCNN <ref type="bibr" target="#b7">[8]</ref> PSyCo <ref type="bibr" target="#b39">[40]</ref> VDSR <ref ty
ing to produce only a single texture and have so far not been applied to SISR. Adversarial networks <ref type="bibr" target="#b17">[18]</ref> have recently been shown to produce sharp results in a num ns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Adversarial training</head><p>Adversarial training <ref type="bibr" target="#b17">[18]</ref> is a recent technique that has proven to be a useful mecha
type="bibr" target="#b22">[23]</ref>.</p><p>Early interpolation methods such as bicubic and Lanczos <ref type="bibr" target="#b10">[11]</ref> are based on sampling theory but often produce blurry resu

nary-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" tar
roaches are specifically designed for fast inference times <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>. Thus far, realisti r" target="#b23">[24]</ref>, Bruna et al. <ref type="bibr" target="#b3">[4]</ref> and Romano et al. <ref type="bibr" target="#b41">[42]</ref> are shown in Fig. <ref type="figure">6</ref> and in the su
cation SISR have only been achieved by user-guided methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>More specifically, Dong et al. <ref type="bibr" targ
get="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b62">63]</ref>. They further inclu
ecent embedding for node i. The use of max operator is inspired from learning on general point sets <ref type="bibr" target="#b34">(Qi et al., 2017)</ref>. By applying max-pooling operator element-wis
target="#b42">Trivedi et al., 2017;</ref><ref type="bibr" target="#b31">Ngyuyen et al., 2018;</ref><ref type="bibr" target="#b50">Yu et al., 2018)</ref> assume that graph dynamics evolve as a single nd the goal is to learn embeddings that can preserve the optimality of skip-gram objective. NetWalk <ref type="bibr" target="#b50">(Yu et al., 2018)</ref> is a discrete-time dynamic embedding approach
ibr" target="#b14">(Goyal et al., 2017;</ref><ref type="bibr" target="#b54">Zhou et al., 2018;</ref><ref type="bibr" target="#b42">Trivedi et al., 2017;</ref><ref type="bibr" target="#b31">Ngyuyen et ettings, they either model simple structural and complex temporal properties in a decoupled fashion <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref> or use simple temporal models (exponenti CNN-based approaches <ref type="bibr" target="#b38">(Seo et al., 2016)</ref>, deep recurrent models <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref>, and random walks <ref type="bibr" targe "bibr" target="#b32">(Paranjape et al., 2017)</ref>. The overall training procedure is adopted from <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref> where the Backpropagation Through Time ( s, four of which has capability to model evolving graphs. Specifically, we compare with Know-Evolve <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref>-a state-of-the-art model for multi-relat space to represent an event while k still signifying different dynamic scales.</p><p>Comparison to <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref>. In the similar vein as above, the point b42">(Trivedi et al., 2017)</ref>. In the similar vein as above, the point process specification of <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref> can also be considered as a marked proce state-of-the-art Graph based Neural Self-Attention techniques which only support static graphs. As <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref> do not incorporate graph structure, they y detection which uses clique based embedding techniques to learn vertex representations. Recently, <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref> proposed Know-Evolve, a deep recurrent a ftplus function for f k which contains a dynamic specific scale parameter ψ k to achieve this while <ref type="bibr" target="#b42">(Trivedi et al. 2017</ref>) uses an exponential (exp) function for f raway nodes (as interactions are allowed between nodes that do not have an edge). Contrary to that, <ref type="bibr" target="#b42">(Trivedi et al., 2017</ref>) uses single edge level information, spec
l., 2017)</ref> or sub-graph embedding <ref type="bibr" target="#b37">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b26">Li et al., 2016;</ref><ref type="bibr" target="#b8">Dai et al., 2016) low dimensional vector representations <ref type="bibr" target="#b37">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b26">Li et al., 2016;</ref><ref type="bibr" target="#b8">Dai et al., 2016)
9)</ref>. Temporal Point Processes have previously been used to model both -dynamics on the network <ref type="bibr" target="#b12">(Farajtabar et al., 2016;</ref><ref type="bibr" target="#b52">Zarezad
architecture to capture temporal dependencies over this induced graph and perform object detection. <ref type="bibr" target="#b20">(Jerfel et al., 2017)</ref> proposes a dynamic probabilistic model in
type="bibr">2017;</ref><ref type="bibr" target="#b48">Xu et al., 2017)</ref> or sub-graph embedding <ref type="bibr" target="#b37">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b26">Li et al echniques learn to encode higher order graph structures into low dimensional vector representations <ref type="bibr" target="#b37">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b26">Li et al
l., 2017)</ref> or sub-graph embedding <ref type="bibr" target="#b37">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b26">Li et al., 2016;</ref><ref type="bibr" target="#b8">Dai et al., 2016) low dimensional vector representations <ref type="bibr" target="#b37">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b26">Li et al., 2016;</ref><ref type="bibr" target="#b8">Dai et al., 2016)
er graphs? -A key modeling choice in existing representation learning techniques for dynamic graphs <ref type="bibr" target="#b14">(Goyal et al., 2017;</ref><ref type="bibr" target="#b54">Zhou et al., llection of static graph snapshots over time <ref type="bibr" target="#b55">(Zhu et al., 2016;</ref><ref type="bibr" target="#b14">Goyal et al., 2017;</ref><ref type="bibr" target="#b54">Zhou et al., ti-relational dynamic graphs where each edge has time-stamp and type (communication events), DynGem <ref type="bibr" target="#b14">(Goyal et al., 2017</ref>)-divides timeline into discrete time points > propose a temporal latent space model for link prediction using nonnegative matrix factorization. <ref type="bibr" target="#b14">(Goyal et al., 2017)</ref> uses a warm start method to train across s

017;</ref><ref type="bibr" target="#b13">Farajtabar et al., 2017)</ref> and dynamics of the network <ref type="bibr" target="#b41">(Tran et al., 2015;</ref><ref type="bibr" target="#b11">Farajtabar et
>, as well as independent component analysis <ref type="bibr" target="#b3">[Cao et al. 2003]</ref>. <ref type="bibr" target="#b4">Cao et al. [2005]</ref> extract emotions using support vector machines lso present a user study rating the level of realism in emotion synthesis, covering several methods <ref type="bibr" target="#b4">[Cao et al. 2005;</ref><ref type="bibr" target="#b27">Liu and Osterman ing samples based on the apparent emotion <ref type="bibr" target="#b0">[Anderson et al. 2013;</ref><ref type="bibr" target="#b4">Cao et al. 2005;</ref><ref type="bibr" target="#b11">Deng et al. 2006;
semi-Markov model <ref type="bibr" target="#b37">[Schabus et al. 2014]</ref>, or recurrent networks <ref type="bibr" target="#b15">[Fan et al. 2016]</ref>.</p><p>Alternatively, machine learning has be ng has been used for learning coarticulation <ref type="bibr" target="#b11">[Deng et al. 2006;</ref><ref type="bibr" target="#b15">Ezzat et al. 2002]</ref>, followed by a concatenation stage to synthe ibr" target="#b0">[Anderson et al. 2013;</ref><ref type="bibr" target="#b9">Deena et al. 2013;</ref><ref type="bibr" target="#b15">Ezzat et al. 2002;</ref><ref type="bibr" target="#b15">Fan et al. 201 ="bibr" target="#b9">Deena et al. 2013;</ref><ref type="bibr" target="#b15">Ezzat et al. 2002;</ref><ref type="bibr" target="#b15">Fan et al. 2016;</ref><ref type="bibr" target="#b27">Liu and Osterman ef type="bibr" target="#b2">[Brand 1999;</ref><ref type="bibr" target="#b9">Deena et al. 2013;</ref><ref type="bibr" target="#b15">Fan et al. 2016]</ref>) have substantially higher latency.</p></div>
near approximation, with alternative representations such as Gaussian process latent variable model <ref type="bibr" target="#b8">[Deena and Galata 2009;</ref><ref type="bibr" target="#b9">Deena et al
uld like to drive several different meshes with audio, and we support that via deformation transfer <ref type="bibr" target="#b40">[Sumner and Popović 2004]</ref>  </p></div> <div xmlns="http://www.te
d by several authors. <ref type="bibr" target="#b6">Chuang et al. [2002]</ref> build on the work of <ref type="bibr" target="#b43">Tenenbaum and Freeman [2000]</ref> and use a bilinear model for separ
d our training setup using Theano <ref type="bibr">[Theano Development Team 2016]</ref> and Lasagne <ref type="bibr" target="#b12">[Dieleman et al. 2015</ref>] that internally use cuDNN <ref type="bib
g with minor degradation in quality, even though some coarticulation effects are known to be longer <ref type="bibr" target="#b38">[Schwartz and Savariaux 2014]</ref>. Shortening the look-ahead furthe
state. The automatic separation of speech and emotional state has been studied by several authors. <ref type="bibr" target="#b6">Chuang et al. [2002]</ref> build on the work of <ref type="bibr" targe
Galata 2009;</ref><ref type="bibr" target="#b9">Deena et al. 2013]</ref>, hidden semi-Markov model <ref type="bibr" target="#b37">[Schabus et al. 2014]</ref>, or recurrent networks <ref type="bibr" t flexible identities. The systems that do produce 3D are often based on text input instead of audio <ref type="bibr" target="#b37">[Schabus et al. 2014;</ref><ref type="bibr" target="#b47">Wampler et
earance and motion is often perceived as particularly creepy, an effect known as the uncanny valley <ref type="bibr" target="#b34">[Mori 1970</ref>]. Several authors have tried alleviating the effect
type="bibr" target="#b14">[Elgammal and Lee 2004]</ref>, as well as independent component analysis <ref type="bibr" target="#b3">[Cao et al. 2003]</ref>. <ref type="bibr" target="#b4">Cao et al. [200
)</ref>. In this paper we use Checkpoint Processing and Recovery (CPR) as the baseline architecture <ref type="bibr" target="#b1">[2]</ref> since it has been shown to outperform conventional ROB-based rview</head><p>CPR is a ROB-free proposal for building scalable large instruction window processors <ref type="bibr" target="#b1">[2]</ref>. CPR addresses the scalability and performance limitations o
tithreaded processor prior to encountering the blocked operation.</p><p>In Datascalar architectures <ref type="bibr" target="#b4">[5]</ref>, multiple processors, each tightly coupled with part of the
(post-execution), thread-based pre-execution methods have been proposed where either auxiliary code <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref> or a small subset of
target="#b10">[11]</ref> in out-of-order processors has been proposed to tolerate memory latencies <ref type="bibr" target="#b17">[18]</ref>. In runahead execution, the processor state is checkpointe
for some benchmark suites to achieve high performance. The baseline CPR uses a store sets predictor <ref type="bibr" target="#b6">[7]</ref> to predict load-store memory dependences and to issue loads
m (e.g., a backward slice of a cache miss) is pre-executed <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref> on idle contexts of a multithreaded processor prior to enco
e the same program.</p><p>Distributed large instruction window processing models have been proposed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. These processing
(post-execution), thread-based pre-execution methods have been proposed where either auxiliary code <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref> or a small subset of
for some benchmark suites to achieve high performance. The baseline CPR uses a store sets predictor <ref type="bibr" target="#b6">[7]</ref> to predict load-store memory dependences and to issue loads
overing from exceptions and the ROB is used for retiring instructions.</p><p>Balasubramonian et al. <ref type="bibr" target="#b3">[4]</ref> dynamically reserve physical registers for a future thread s
ge instruction window processing models have been proposed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. These processing models significantly change the underlyin
learn node representations by utilizing information from distant neighbors. GCNs and their variants <ref type="bibr" target="#b4">(Hamilton et al., 2017a;</ref><ref type="bibr" target="#b14">Veličkovi sed node classification <ref type="bibr">(Kipf &amp; Welling, 2017)</ref>, inductive node embedding <ref type="bibr" target="#b4">(Hamilton et al., 2017a)</ref>, link prediction <ref type="bibr" targe >Implementation Details</head><p>Training with the CV estimator is similar as with the NS estimator <ref type="bibr" target="#b4">(Hamilton et al., 2017a)</ref>. Particularly, each iteration of the al r all the other multi-class datasets. The model is GCN for the former 4 datasets and GraphSAGE-mean <ref type="bibr" target="#b4">(Hamilton et al., 2017a)</ref> for the latter 2 datasets, see Appendix orted by <ref type="bibr" target="#b2">Chen et al. (2018)</ref>, while their NS baseline, GraphSAGE <ref type="bibr" target="#b4">(Hamilton et al., 2017a)</ref>, does not implement the preprocessing t sets because of their slow convergence and the requirement to fit the entire dataset in GPU memory. <ref type="bibr" target="#b4">Hamilton et al. (2017a)</ref> make an initial attempt to develop stoch www.tei-c.org/ns/1.0"><head n="2.3.">Neighbor Sampling</head><p>To reduce the receptive field size, <ref type="bibr" target="#b4">Hamilton et al. (2017a)</ref> propose a neighbor sampling (NS) algorit id="formula_8">P (l) uv = n(u) D (l) P uv if v ∈ n(l) (u)</formula><p>, and P (l) uv = 0 otherwise. <ref type="bibr" target="#b4">Hamilton et al. (2017a)</ref> propose to perform an approximate forwar D (l) needs to be large for NS, to keep comparable predictive performance with the exact algorithm. <ref type="bibr" target="#b4">Hamilton et al. (2017a)</ref> choose D (1) = 10 and D (2) = 25, and th r, Cora, PubMed and NELL from <ref type="bibr">Kipf &amp; Welling (2017)</ref> and Reddit, PPI from <ref type="bibr" target="#b4">Hamilton et al. (2017a)</ref>, with the same train / validation / test ted to the task nor the model. Our algorithm is applicable to other models including GraphSAGE-mean <ref type="bibr" target="#b4">(Hamilton et al., 2017a</ref>) and graph attention networks (GAT) <ref e most GCNs only have two graph convolution layers <ref type="bibr">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b4">Hamilton et al., 2017a)</ref>, this gives a significant reduction of t

pyright 2018 by the author(s).</p><p>do not use the graph structure, and graph embedding approaches <ref type="bibr" target="#b9">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b13">Tang et al.
structure, and graph embedding approaches <ref type="bibr" target="#b9">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b13">Tang et al., 2015;</ref><ref type="bibr" target="#b3">Grover &amp; Le
this section, we consider introducing a third source of randomness, the random dropout of features <ref type="bibr" target="#b12">(Srivastava et al., 2014)</ref>, which is adopted in various GCN mode rom dropout (ND) of different estimators.</p><p>Our method is based on the weight scaling procedure <ref type="bibr" target="#b12">(Srivastava et al., 2014)</ref> to approximately compute the mean µ</
this section, we consider introducing a third source of randomness, the random dropout of features <ref type="bibr" target="#b12">(Srivastava et al., 2014)</ref>, which is adopted in various GCN mode rom dropout (ND) of different estimators.</p><p>Our method is based on the weight scaling procedure <ref type="bibr" target="#b12">(Srivastava et al., 2014)</ref> to approximately compute the mean µ</

structure, and graph embedding approaches <ref type="bibr" target="#b9">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b13">Tang et al., 2015;</ref><ref type="bibr" target="#b3">Grover &amp; Le
this section, we consider introducing a third source of randomness, the random dropout of features <ref type="bibr" target="#b12">(Srivastava et al., 2014)</ref>, which is adopted in various GCN mode rom dropout (ND) of different estimators.</p><p>Our method is based on the weight scaling procedure <ref type="bibr" target="#b12">(Srivastava et al., 2014)</ref> to approximately compute the mean µ</
pyright 2018 by the author(s).</p><p>do not use the graph structure, and graph embedding approaches <ref type="bibr" target="#b9">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b13">Tang et al.

>[19]</ref> as a way to study brain function. We consider the simplest of many types of perceptrons <ref type="bibr" target="#b1">[2]</ref>, a single-layer perceptron consisting of one artificial neur
y on eliminating aliasing in two-level adaptive predictors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar osed branch predictors seek to reduce the aliasing problem <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar . We compare our new predictor against gshare <ref type="bibr" target="#b16">[17]</ref> and bi-mode <ref type="bibr" target="#b15">[16]</ref>, two of the best purely dynamic global predictors from the 5 benchmarks, explaining the smaller separation between gshare and bi-mode than observed previously <ref type="bibr" target="#b15">[16]</ref>. Figures <ref type="figure">4 and 5</ref> show the mispred 14]</ref>, but our restriction to global information is typical of recent work in branch prediction <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Gathering tra
idence is high. Some branch prediction schemes explicitly compute a confidence in their predictions <ref type="bibr" target="#b10">[11]</ref>, but in our predictor this information comes for free. We
ranch is taken, and decremented otherwise. An important problem in two-level predictors is aliasing <ref type="bibr" target="#b19">[20]</ref>, and many of the recently proposed branch predictors seek
et="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref>, which occurs when two unrelated branches destructively inter et="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref> but do not change the basic prediction mechanism. Given a gen information is typical of recent work in branch prediction <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Gathering traces. Our simulations use the instrumented n a generous hardware budget, many of these two-level schemes perform about the same as one another <ref type="bibr" target="#b3">[4]</ref>.</p><p>Most two-level predictors cannot consider long histor dicator of performance in extreme conditions, and it uses the same methodology as other recent work <ref type="bibr" target="#b3">[4]</ref>. Note that previous studies have used the 8 SPEC 95 integer
et="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref>, which occurs when two unrelated branches destructively inter et="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref> but do not change the basic prediction mechanism. Given a gen information is typical of recent work in branch prediction <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Gathering traces. Our simulations use the instrumented n a generous hardware budget, many of these two-level schemes perform about the same as one another <ref type="bibr" target="#b3">[4]</ref>.</p><p>Most two-level predictors cannot consider long histor dicator of performance in extreme conditions, and it uses the same methodology as other recent work <ref type="bibr" target="#b3">[4]</ref>. Note that previous studies have used the 8 SPEC 95 integer
e distance between correlated branches is longer than the length of a global history shift register <ref type="bibr" target="#b6">[7]</ref>. Even if a PHT scheme could somehow implement longer history , and the threshold.</p><p>History length. Long history lengths can yield more accurate predictions <ref type="bibr" target="#b6">[7]</ref> but also reduce the number of table entries, thereby increas hemes, which helps because highly correlated branches can occur at a large distance from each other <ref type="bibr" target="#b6">[7]</ref>. Any global branch prediction technique that uses a fixed am of Evers et al, who show that most branches can be predicted by looking at three previous branches <ref type="bibr" target="#b6">[7]</ref>. As the best history length increases, the advantage of the study of the effects of long branch histories on branch prediction only considers lengths up to 32 <ref type="bibr" target="#b6">[7]</ref>. We have found that additional performance gains can be foun
e distance between correlated branches is longer than the length of a global history shift register <ref type="bibr" target="#b6">[7]</ref>. Even if a PHT scheme could somehow implement longer history , and the threshold.</p><p>History length. Long history lengths can yield more accurate predictions <ref type="bibr" target="#b6">[7]</ref> but also reduce the number of table entries, thereby increas hemes, which helps because highly correlated branches can occur at a large distance from each other <ref type="bibr" target="#b6">[7]</ref>. Any global branch prediction technique that uses a fixed am of Evers et al, who show that most branches can be predicted by looking at three previous branches <ref type="bibr" target="#b6">[7]</ref>. As the best history length increases, the advantage of the study of the effects of long branch histories on branch prediction only considers lengths up to 32 <ref type="bibr" target="#b6">[7]</ref>. We have found that additional performance gains can be foun
:= Û ½ end if</formula><p>Delay. A ¢ multiplier in a 0.25 m process can operate in 2.7 nanoseconds <ref type="bibr" target="#b8">[9]</ref>, which is approximately two clock cycles with a 700 MHz cloc
isting global/per-branch hybrid schemes. Per-branch and path information can yield greater accuracy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>, but our restriction an suffer a loss in performance after a context switch, having to warm up while relearning patterns <ref type="bibr" target="#b5">[6]</ref>. We simulate the effects of context switching by interleavin in the presence of context switching; this benefit of hybrid predictors has been noticed by others <ref type="bibr" target="#b5">[6]</ref>. </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n and local histories, since hybrid predictors have proven to work better than purely global schemes <ref type="bibr" target="#b5">[6]</ref>. We have preliminary experimental evidence that such hybrid
ese methods <ref type="bibr" target="#b17">[18]</ref>.</p><p>Variable length path branch prediction <ref type="bibr" target="#b22">[23]</ref> is one scheme for considering longer paths. It avoids the ld be used with a profiler to provide path length information to the variable length path predictor <ref type="bibr" target="#b22">[23]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head asible. Two cycles is also the amount of time claimed for the variable length path branch predictor <ref type="bibr" target="#b22">[23]</ref>. That work proposes pipelining the predictor to reduce del those previously considered. Variable length path prediction considers history lengths of up to 23 <ref type="bibr" target="#b22">[23]</ref>, and a study of the effects of long branch histories on br
p>Many recent sequence labeling frameworks <ref type="bibr" target="#b25">(Ma and Hovy, 2016b;</ref><ref type="bibr" target="#b27">Misawa et al., 2017)</ref> share a very basic structure: a bidirectio
g/ns/1.0"><head n="4.1">The Base Model: BLSTM-CRF</head><p>Many recent sequence labeling frameworks <ref type="bibr" target="#b25">(Ma and Hovy, 2016b;</ref><ref type="bibr" target="#b27">Misawa et al
neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) <ref type="bibr" target="#b24">(Ma and Hovy, 2016a)</ref> or bidirectional long short-term memory (B

m supervised adaptation (Lin and Lu, 2018), we assume there is no labels at all for target corpora. <ref type="bibr" target="#b37">Saito et al. (2017)</ref> and <ref type="bibr" target="#b36">Ruder an s bootstrapping with multi-task Tri-Training approach for unsupervised one-to-one domain adaptation <ref type="bibr" target="#b37">(Saito et al., 2017;</ref><ref type="bibr" target="#b36">Ruder and Pl
ww.tei-c.org/ns/1.0"><head n="5.2">Experiment Setup</head><p>For sequence labeling tasks, we follow <ref type="bibr" target="#b20">Liu et al. (2018)</ref> to build the BLSTM-CRF architecture as the ba
costs and a higher speed from non-expert contributors but suffers from some degradation in quality. <ref type="bibr" target="#b3">Dawid and Skene (1979)</ref> proposes the pioneering work to aggregate s conducted on the sequence level, addressing the problem of violating Begin/In/Out (BIO) rules. DS <ref type="bibr" target="#b3">(Dawid and Skene, 1979)</ref>, <ref type="bibr">HMM (Nguyen et al., 20
encompassing various natural language processing (NLP) tasks including part-of-speech (POS) tagging <ref type="bibr" target="#b31">(Ratnaparkhi, 1996)</ref>, word segmentation <ref type="bibr" target=
on crowd-sourced training set then feeding the generated labels to a Sequence Labeling Model (SLM) <ref type="bibr" target="#b21">(Liu et al., 2017)</ref>; (2) feeding multi-source data to a Multi-Ta
" target="#b4">(Dempster et al., 1977;</ref><ref type="bibr" target="#b6">Dredze et al., 2009;</ref><ref type="bibr" target="#b33">Raykar et al., 2010)</ref> focus on Expectation-Maximization (EM) alg
g has been demonstrated to be an effective way of fulfilling the label consumption of neural models <ref type="bibr" target="#b9">(Guan et al., 2017;</ref><ref type="bibr" target="#b18">Lin et al., 20

for human curated annotations, there exists certain label noise that hinders the model performance <ref type="bibr" target="#b43">(Wang et al., 2019)</ref>.</p><p>Unsupervised Domain Adaptation. Unsu
dels learn domain-invariant feature only <ref type="bibr" target="#b7">(Fernando et al., 2015;</ref><ref type="bibr" target="#b22">Long et al., 2014;</ref><ref type="bibr" target="#b26">Ming Harry Hsu
>Crowd-Annotation Datasets. We use crowdannotation datasets based on the 2003 CoNLL shared NER task <ref type="bibr" target="#b40">(Tjong Kim Sang and De Meulder, 2003)</ref>. The real-world datasets,
model takes Transformer as the encoder for CRF, which has shown its effectiveness in many NLP tasks <ref type="bibr" target="#b41">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b5">Devlin et a quences with self-attention and eliminates all recurrence. Following the experimental settings from <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref>, we set the number of heads for multihea
lling the label consumption of neural models <ref type="bibr" target="#b9">(Guan et al., 2017;</ref><ref type="bibr" target="#b18">Lin et al., 2019)</ref>. It collects annotations with lower costs and feature for controlling the quality of crowd-annotation in annotation frameworks such as AlpacaTag <ref type="bibr" target="#b18">(Lin et al., 2019)</ref> and LEAN-LIFE <ref type="bibr" target="#b15"
for human curated annotations, there exists certain label noise that hinders the model performance <ref type="bibr" target="#b43">(Wang et al., 2019)</ref>.</p><p>Unsupervised Domain Adaptation. Unsu
hown its effectiveness in many NLP tasks <ref type="bibr" target="#b41">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b5">Devlin et al., 2019)</ref>. Transformer models sequences with self-att
costs and a higher speed from non-expert contributors but suffers from some degradation in quality. <ref type="bibr" target="#b3">Dawid and Skene (1979)</ref> proposes the pioneering work to aggregate s conducted on the sequence level, addressing the problem of violating Begin/In/Out (BIO) rules. DS <ref type="bibr" target="#b3">(Dawid and Skene, 1979)</ref>, <ref type="bibr">HMM (Nguyen et al., 20

g et al., 2018)</ref>, and learning with entity triggers <ref type="bibr">(Lin et al., 2020)</ref>. <ref type="bibr" target="#b29">Nguyen et al. (2017)</ref>; <ref type="bibr" target="#b35">Rodrigues Multiple Annotators by EM algorithm <ref type="bibr" target="#b34">(Rodrigues et al., 2014)</ref>. <ref type="bibr" target="#b29">Nguyen et al. (2017)</ref> augments the LSTM  <ref type="table" targe he ground truth vary from 17.60% to 89.11%. Since there is no development set in AMT, we also follow<ref type="bibr" target="#b29">Nguyen et al. (2017)</ref> to use the AMT training set and CoNLL 2003
" target="#b4">(Dempster et al., 1977;</ref><ref type="bibr" target="#b6">Dredze et al., 2009;</ref><ref type="bibr" target="#b33">Raykar et al., 2010)</ref> focus on Expectation-Maximization (EM) alg
g et al., 2018)</ref>, and learning with entity triggers <ref type="bibr">(Lin et al., 2020)</ref>. <ref type="bibr" target="#b29">Nguyen et al. (2017)</ref>; <ref type="bibr" target="#b35">Rodrigues Multiple Annotators by EM algorithm <ref type="bibr" target="#b34">(Rodrigues et al., 2014)</ref>. <ref type="bibr" target="#b29">Nguyen et al. (2017)</ref> augments the LSTM  <ref type="table" targe he ground truth vary from 17.60% to 89.11%. Since there is no development set in AMT, we also follow<ref type="bibr" target="#b29">Nguyen et al. (2017)</ref> to use the AMT training set and CoNLL 2003
Lan et al., 2018;</ref><ref type="bibr" target="#b2">Clark et al., 2018)</ref>, cross-type learning <ref type="bibr" target="#b42">(Wang et al., 2018)</ref>, and learning with entity triggers <ref typ arget="#b21">(Liu et al., 2017)</ref>; (2) feeding multi-source data to a Multi-Task Learning (MTL) <ref type="bibr" target="#b42">(Wang et al., 2018)</ref> model then aggregating multiple predicted l
for one-to-one domain adaptation and does not model the differences among multiple source domains. <ref type="bibr" target="#b44">Yang and Eisenstein (2015)</ref> represents each domain with a vector
model takes Transformer as the encoder for CRF, which has shown its effectiveness in many NLP tasks <ref type="bibr" target="#b41">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b5">Devlin et a quences with self-attention and eliminates all recurrence. Following the experimental settings from <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref>, we set the number of heads for multihea
gnificant efforts in feature engineering for graphical models like conditional random fields (CRFs) <ref type="bibr" target="#b11">(Lafferty, 2001)</ref>. Recent research efforts in neural network mod
Lan et al., 2018;</ref><ref type="bibr" target="#b2">Clark et al., 2018)</ref>, cross-type learning <ref type="bibr" target="#b42">(Wang et al., 2018)</ref>, and learning with entity triggers <ref typ arget="#b21">(Liu et al., 2017)</ref>; (2) feeding multi-source data to a Multi-Task Learning (MTL) <ref type="bibr" target="#b42">(Wang et al., 2018)</ref> model then aggregating multiple predicted l
model takes Transformer as the encoder for CRF, which has shown its effectiveness in many NLP tasks <ref type="bibr" target="#b41">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b5">Devlin et a quences with self-attention and eliminates all recurrence. Following the experimental settings from <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref>, we set the number of heads for multihea
>Crowd-Annotation Datasets. We use crowdannotation datasets based on the 2003 CoNLL shared NER task <ref type="bibr" target="#b40">(Tjong Kim Sang and De Meulder, 2003)</ref>. The real-world datasets,
lling the label consumption of neural models <ref type="bibr" target="#b9">(Guan et al., 2017;</ref><ref type="bibr" target="#b18">Lin et al., 2019)</ref>. It collects annotations with lower costs and feature for controlling the quality of crowd-annotation in annotation frameworks such as AlpacaTag <ref type="bibr" target="#b18">(Lin et al., 2019)</ref> and LEAN-LIFE <ref type="bibr" target="#b15"
train the model to deal with domain shifting, which is highly dependent on prior domain knowledge. <ref type="bibr" target="#b8">(Ghifary et al., 2016)</ref> uses an auto-encoder method by jointly tr
" target="#b27">[1]</ref>) or lack a clear objective function tailored for network embedding (e.g., <ref type="bibr" target="#b42">[16]</ref>). We anticipate that a new model with a carefully designed e for both undirected and directed graphs.</p><p>The most recent work related with ours is DeepWalk <ref type="bibr" target="#b42">[16]</ref>, which deploys a truncated random walk for social network /ref> . The Flickr network is denser than the Youtube network (the same network as used in DeepWalk <ref type="bibr" target="#b42">[16]</ref>). (3) Citation Networks. Two types of citation networks ar rk can be represented as an affinity matrix, and is able to represent each vertex with a • DeepWalk <ref type="bibr" target="#b42">[16]</ref>. DeepWalk is an approach recently proposed for social netw r" target="#b39">[13]</ref>. For other networks, the dimension is set as 128 by default, as used in <ref type="bibr" target="#b42">[16]</ref>. Other default settings include: the number of negative sa

followee-follower network contains 175 million active users and around twenty billion edges in 2012 <ref type="bibr" target="#b40">[14]</ref>. Most existing graph embedding algorithms do not scale for

stochastic gradient algorithm (ASGD) <ref type="bibr" target="#b43">[17]</ref> for optimizing Eqn. <ref type="bibr" target="#b33">(7)</ref>. In each step, the ASGD algorithm samples a mini-batch of e best method for document embeddings. The readers can find advanced document embedding approaches in <ref type="bibr" target="#b33">[7]</ref>. We download the abstracts of Wikipedia pages from http://d t="#b29">3]</ref>, <ref type="bibr" target="#b30">[4,</ref><ref type="bibr" target="#b32">6]</ref>, <ref type="bibr" target="#b33">[7,</ref><ref type="bibr" target="#b38">12]</ref>, <ref type="bibr" t
achine learning literature (e.g., <ref type="bibr" target="#b30">[4,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b28">2]</ref>). They generally perform well on smaller networks. The probl DS <ref type="bibr" target="#b30">[4]</ref>, IsoMap <ref type="bibr">[20]</ref>, Laplacian eigenmap <ref type="bibr" target="#b28">[2]</ref> are at least quadratic to the number of vertices, which is reserve this first-order proximity, e.g., IsoMap <ref type="bibr">[20]</ref> and Laplacian eigenmap <ref type="bibr" target="#b28">[2]</ref>, even if they do not scale. We observe that in a real-world p <ref type="bibr">[20]</ref>, LLE <ref type="bibr" target="#b44">[18]</ref> and Laplacian Eigenmap <ref type="bibr" target="#b28">[2]</ref>. These approaches typically first construct the affinity gr orks. We categorize the vertices into different groups according to their degrees including (0, 1], <ref type="bibr" target="#b28">[2,</ref><ref type="bibr" target="#b29">3]</ref>, <ref type="bibr" ta

s of the data points, e.g., the K-nearest neighbor graph of data, and then embed the affinity graph <ref type="bibr" target="#b48">[22]</ref> into a low dimensional space. However, these algorithms us
etworks are used: an author citation network and a paper citation network. We use the DBLP data set <ref type="bibr" target="#b45">[19]</ref> <ref type="foot" target="#foot_1">3</ref> to construct the
s directly from the original Wikipedia pages and is also implicitly a matrix factorization approach <ref type="bibr" target="#b34">[8]</ref>. The window size is set as 5, the same as used for construc
stochastic gradient algorithm (ASGD) <ref type="bibr" target="#b43">[17]</ref> for optimizing Eqn. <ref type="bibr" target="#b33">(7)</ref>. In each step, the ASGD algorithm samples a mini-batch of e best method for document embeddings. The readers can find advanced document embedding approaches in <ref type="bibr" target="#b33">[7]</ref>. We download the abstracts of Wikipedia pages from http://d t="#b29">3]</ref>, <ref type="bibr" target="#b30">[4,</ref><ref type="bibr" target="#b32">6]</ref>, <ref type="bibr" target="#b33">[7,</ref><ref type="bibr" target="#b38">12]</ref>, <ref type="bibr" t
arize the distribution of zero weights, so that more all-zero rows and columns can be found. SNrram <ref type="bibr" target="#b43">[44]</ref> seeks to enable fine-grained column compression at the cos found. However, with ReCom, there are still many zero weights left in the compressed model. SNrram <ref type="bibr" target="#b43">[44]</ref> compresses the model at a finer level, i.e., allzero filte g OU rows are removed to reduce unnecessary computations. Note that we does not compare with SNrram <ref type="bibr" target="#b43">[44]</ref>, as SNrram uses model-based compression and its crossbar a /ref>, we also use arrows to indicate the weight compression ratio that can be obtained from SNrram <ref type="bibr" target="#b43">[44]</ref>. As SNrram uses model-based compression (i.e., finegrained , only all-zero rows can be removed; many zero weights still remain in the compressed model. SNrram <ref type="bibr" target="#b43">[44]</ref> is another ReRAMbased sparse DNN accelerator that compress utput indexing overhead.</p><p>Existing sparsity solutions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref> are based on an over-idealized ReRAM crossbar architecture
it sparsity, which is commonly done in digital CMOS-based accelerators to improve energy efficiency <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target ines sequentially. In this example, to compute the output neuron for the first input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target matrix-vector multiplication of OU1 and the LSB of the first two values in the input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref> is performed to obtain the output < <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref> is performed to obtain the output <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref>. The matrix-vector multiplication o e matrix-vector multiplication of OU2 and the LSB of the rest of values in the input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref> is then performed at t echanisms to exploit weight sparsity <ref type="bibr" target="#b48">[49]</ref>, activation sparsity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>, or both <ref type="bi " target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref>, the data we feed into the input register after decomposition nd the LSB of the rest of values in the input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref> is then performed at the second cycle to yield the output <re ultiplier when an input activation is zero to save energy. Another sparse DNN accelerator, Cnvlutin <ref type="bibr" target="#b0">[1]</ref>, selects only non-zero activation values for delivery as mul
rove energy efficiency <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" tar ion sparsity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>, or both <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36]</ref> in CMOS-based digi vlutin skips computations with zero weights. To jointly exploit weight and activation sparsity, EIE <ref type="bibr" target="#b15">[16]</ref> uses a compressed representation for both activations and
iques such as quantization <ref type="bibr" target="#b16">[17]</ref>, low-rank matrix factorization <ref type="bibr" target="#b11">[12]</ref>, and 1 -norm regularization <ref type="bibr" target="#b27" For instance, quantization <ref type="bibr" target="#b16">[17]</ref>, low-rank matrix factorization <ref type="bibr" target="#b11">[12]</ref>, and 1 -norm regularization <ref type="bibr" target="#b27"
an algorithmic way <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref>. Yu et al. <ref type="bibr" target="#b47">[48]</ref> propos 9,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref>. Yu et al. <ref type="bibr" target="#b47">[48]</ref> propose a method to customize DNN pruning for different ha
ected layers of DNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40]</ref>. By storing filter weights as the conductance of ReRAM cell ngs compared to CPU and GPU based DNN acceleration platforms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Despite this promising potential, the development of > are based on an over-idealized ReRAM crossbar architecture <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>. Most existing ReRAM-based DNN accelerators published in th -based DNN accelerator architecture assumed in a few works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>. The ReRAM-based deep learning accelerator is composed of m e bit line. In an ideal ReRAM-based DNN accelerator design <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>, all the wordlines in the crossbar array can be activated c lerator is very likely to deliver lower performance compared to an over-idealized design like ISAAC <ref type="bibr" target="#b39">[40]</ref> or PRIME <ref type="bibr" target="#b8">[9]</ref> since les on the bitlines. A wordline driver (WLD) such as a digital-to-analog converter (DAC) or an inverter <ref type="bibr" target="#b39">[40]</ref> is connected to each wordline of the ReRAM crossbar array ssbar array and ADC's throughput. Since an ADC consumes a significant amount of power and chip area <ref type="bibr" target="#b39">[40]</ref>, in practical designs multiple bitlines share an ADC <ref for the OU-related components (ADC and eDRAM buffer), we use the hardware configuration from ISAAC <ref type="bibr" target="#b39">[40]</ref>, a state-ofthe-art over-idealized design. The power consum , is modeled using CACTI <ref type="bibr" target="#b33">[34]</ref> at 32nm process assumed in ISAAC <ref type="bibr" target="#b39">[40]</ref>. Each PE has a 64KB on-chip eDRAM buffer to store intermed 16?16 3 . Since a 6-bit ADC is sufficient for a 16?16 OU, we use the same style of ADC as in ISAAC <ref type="bibr" target="#b39">[40]</ref> but follow the equation in <ref type="bibr" target="#b37"> when running non-SSL sparse neural networks. Finally, we compare SRE with an over-idealized design <ref type="bibr" target="#b39">[40]</ref> to show that jointly exploiting weight and activation spar ns="http://www.tei-c.org/ns/1.0"><head n="7.5">Comparison with Over-Idealized Design</head><p>ISAAC <ref type="bibr" target="#b39">[40]</ref> is a state-of-the-art but over-idealized design, which ove
it sparsity, which is commonly done in digital CMOS-based accelerators to improve energy efficiency <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target ines sequentially. In this example, to compute the output neuron for the first input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target matrix-vector multiplication of OU1 and the LSB of the first two values in the input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref> is performed to obtain the output < <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref> is performed to obtain the output <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref>. The matrix-vector multiplication o e matrix-vector multiplication of OU2 and the LSB of the rest of values in the input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref> is then performed at t echanisms to exploit weight sparsity <ref type="bibr" target="#b48">[49]</ref>, activation sparsity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>, or both <ref type="bi " target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref>, the data we feed into the input register after decomposition nd the LSB of the rest of values in the input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref> is then performed at the second cycle to yield the output <re ultiplier when an input activation is zero to save energy. Another sparse DNN accelerator, Cnvlutin <ref type="bibr" target="#b0">[1]</ref>, selects only non-zero activation values for delivery as mul
ected layers of DNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40]</ref>. By storing filter weights as the conductance of ReRAM cell ngs compared to CPU and GPU based DNN acceleration platforms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Despite this promising potential, the development of > are based on an over-idealized ReRAM crossbar architecture <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>. Most existing ReRAM-based DNN accelerators published in th -based DNN accelerator architecture assumed in a few works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>. The ReRAM-based deep learning accelerator is composed of m e bit line. In an ideal ReRAM-based DNN accelerator design <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>, all the wordlines in the crossbar array can be activated c lerator is very likely to deliver lower performance compared to an over-idealized design like ISAAC <ref type="bibr" target="#b39">[40]</ref> or PRIME <ref type="bibr" target="#b8">[9]</ref> since les on the bitlines. A wordline driver (WLD) such as a digital-to-analog converter (DAC) or an inverter <ref type="bibr" target="#b39">[40]</ref> is connected to each wordline of the ReRAM crossbar array ssbar array and ADC's throughput. Since an ADC consumes a significant amount of power and chip area <ref type="bibr" target="#b39">[40]</ref>, in practical designs multiple bitlines share an ADC <ref for the OU-related components (ADC and eDRAM buffer), we use the hardware configuration from ISAAC <ref type="bibr" target="#b39">[40]</ref>, a state-ofthe-art over-idealized design. The power consum , is modeled using CACTI <ref type="bibr" target="#b33">[34]</ref> at 32nm process assumed in ISAAC <ref type="bibr" target="#b39">[40]</ref>. Each PE has a 64KB on-chip eDRAM buffer to store intermed 16?16 3 . Since a 6-bit ADC is sufficient for a 16?16 OU, we use the same style of ADC as in ISAAC <ref type="bibr" target="#b39">[40]</ref> but follow the equation in <ref type="bibr" target="#b37"> when running non-SSL sparse neural networks. Finally, we compare SRE with an over-idealized design <ref type="bibr" target="#b39">[40]</ref> to show that jointly exploiting weight and activation spar ns="http://www.tei-c.org/ns/1.0"><head n="7.5">Comparison with Over-Idealized Design</head><p>ISAAC <ref type="bibr" target="#b39">[40]</ref> is a state-of-the-art but over-idealized design, which ove
f> is then performed at the second cycle to yield the output <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. The output of OU1 and OU2 are added together, resulting in t 1 and OU2 are added together, resulting in the summed output <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b3">4]</ref> before being assembled by the shift-and-add circuit.</p><p>Fo target="#b3">4]</ref>. The output of OU1 and OU2 are added together, resulting in the summed output <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b3">4]</ref> before being assembled
1.0"><head>Workloads</head><p>We evaluate the proposed Sparse ReRAM Engine on three datasets: MNIST <ref type="bibr" target="#b26">[27]</ref>, CIFAR-10 <ref type="bibr" target="#b24">[25]</ref>, and I ategories, each of which includes 1000 images. The neural networks used in our evaluation are LeNet <ref type="bibr" target="#b26">[27]</ref> on MNIST, a CNN with three convolution layers and two full
s (high density, low switching energy, and high endurance) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b45">46]</ref> and thus is commonly deployed in latest ReRAM-based DNN acc

on social media given users' posts, connections among users, and a small number of labelled users. <ref type="bibr" target="#b34">Rahimi et al. (2018)</ref> apply GCNs with highway connections on thi
et al., 2015)</ref>. Although convolutional networks can benefit substantially from increased depth <ref type="bibr" target="#b20">(Huang et al., 2016)</ref>, typically MLPs obtain little benefit beyo
ism Network, which is proved to be as powerful as the Weisfeiler-Lehman test for graph isomorphism. <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref> separate the non-linear transformation f
Table <ref type="table" target="#tab_4">6</ref> shows that C-SGR sets new state-ofthe-art on TACRED <ref type="bibr" target="#b54">(Zhang et al., 2017)</ref>.</p><p>Zero-shot image classification cons

citation networks <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2017)</ref>, social networks <ref type="bibr" target="#b10">(Chen et al., 2018)</ref>, applied chemistry <ref type="bibr" target= f-the-art graph neural networks. However, it is significantly faster, and even outperforms Fast-GCN <ref type="bibr" target="#b10">(Chen et al., 2018)</ref> by up to two orders of magnitude on the lar mp; Welling, 2017)</ref> GAT <ref type="bibr" target="#b42">(Velickovic et al., 2018)</ref> FastGCN <ref type="bibr" target="#b10">(Chen et al., 2018)</ref> LNet, AdaLNet <ref type="bibr" target="#b29 stacking layers of first-order Chebyshev polynomial filters with a redefined propagation matrix S. <ref type="bibr" target="#b10">Chen et al. (2018)</ref> propose an efficient variant of GCN based on irements. Previous approaches tackle this limitation by either sampling to reduce neighborhood size <ref type="bibr" target="#b10">(Chen et al., 2018;</ref><ref type="bibr" target="#b15">Hamilton et a

citation networks <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2017)</ref>, social networks <ref type="bibr" target="#b10">(Chen et al., 2018)</ref>, applied chemistry <ref type="bibr" target= f-the-art graph neural networks. However, it is significantly faster, and even outperforms Fast-GCN <ref type="bibr" target="#b10">(Chen et al., 2018)</ref> by up to two orders of magnitude on the lar mp; Welling, 2017)</ref> GAT <ref type="bibr" target="#b42">(Velickovic et al., 2018)</ref> FastGCN <ref type="bibr" target="#b10">(Chen et al., 2018)</ref> LNet, AdaLNet <ref type="bibr" target="#b29 stacking layers of first-order Chebyshev polynomial filters with a redefined propagation matrix S. <ref type="bibr" target="#b10">Chen et al. (2018)</ref> propose an efficient variant of GCN based on irements. Previous approaches tackle this limitation by either sampling to reduce neighborhood size <ref type="bibr" target="#b10">(Chen et al., 2018;</ref><ref type="bibr" target="#b15">Hamilton et a
rization methods. Graph embedding methods <ref type="bibr" target="#b46">(Weston et al., 2008;</ref><ref type="bibr" target="#b33">Perozzi et al., 2014;</ref><ref type="bibr" target="#b50">Yang et al. ikovi et al., 2019)</ref> represent nodes as high-dimensional feature vectors. Among them, DeepWalk <ref type="bibr" target="#b33">(Perozzi et al., 2014)</ref> and Deep Graph Infomax (DGI) <ref type="
br" target="#b33">Perozzi et al., 2014;</ref><ref type="bibr" target="#b50">Yang et al., 2016;</ref><ref type="bibr" target="#b43">Velikovi et al., 2019)</ref> represent nodes as high-dimensional feat , DeepWalk <ref type="bibr" target="#b33">(Perozzi et al., 2014)</ref> and Deep Graph Infomax (DGI) <ref type="bibr" target="#b43">(Velikovi et al., 2019)</ref> use unsupervised strategies to learn gr et al., 2018)</ref> LNet, AdaLNet <ref type="bibr" target="#b29">(Liao et al., 2019)</ref> and DGI <ref type="bibr" target="#b43">(Velikovi et al., 2019)</ref> using the publicly released implementat 018;</ref><ref type="bibr" target="#b15">Hamilton et al., 2017)</ref> or limiting their model sizes <ref type="bibr" target="#b43">(Velikovi et al., 2019)</ref>. By applying a fixed filter and precomp orms the previous sampling-based GCN variants, SAGE-GCN and FastGCN by more than 1%.</p><p>Notably, <ref type="bibr" target="#b43">Velikovi et al. (2019)</ref> report that the performance of a randoml
problem and can be performed with any efficient second order method or stochastic gradient descent <ref type="bibr" target="#b6">(Bottou, 2010)</ref>. Provided the graph connectivity pattern is suffi

a spectral graph-based extension of convolutional networks to graphs. In a followup work, ChebyNets <ref type="bibr" target="#b11">(Defferrard et al., 2016)</ref> define graph convolutions using Cheby
#b29">), and Liao et al. (2019)</ref> exploit multi-scale information by raising S to higher order. <ref type="bibr" target="#b48">Xu et al. (2019)</ref> study the expressiveness of graph neural netwo lementations. Since GIN is not initially evaluated on citation networks, we implement GIN following <ref type="bibr" target="#b48">Xu et al. (2019)</ref> and use hyperopt to tune weight decay and lear necessary.</p><p>Graph classification requires models to use graph structure to categorize graphs. <ref type="bibr" target="#b48">Xu et al. (2019)</ref> theoretically show that GCNs are not sufficien
ied chemistry <ref type="bibr" target="#b29">(Liao et al., 2019)</ref>, natural language processing <ref type="bibr" target="#b51">(Yao et al., 2019;</ref><ref type="bibr" target="#b16">Han et al., 20
n et al., 2012;</ref><ref type="bibr" target="#b55">Zhang et al., 2018c)</ref>, and computer vision <ref type="bibr" target="#b45">(Wang et al., 2018;</ref><ref type="bibr" target="#b21">Kampffmeyer e learning an image classifier without access to any images or labels from the test categories. GCNZ <ref type="bibr" target="#b45">(Wang et al., 2018)</ref> uses a GCN to map the category names -based
n et al., 2012;</ref><ref type="bibr" target="#b55">Zhang et al., 2018c)</ref>, and computer vision <ref type="bibr" target="#b45">(Wang et al., 2018;</ref><ref type="bibr" target="#b21">Kampffmeyer e learning an image classifier without access to any images or labels from the test categories. GCNZ <ref type="bibr" target="#b45">(Wang et al., 2018)</ref> uses a GCN to map the category names -based
et al., 2015)</ref>. Although convolutional networks can benefit substantially from increased depth <ref type="bibr" target="#b20">(Huang et al., 2016)</ref>, typically MLPs obtain little benefit beyo
et al., 2015)</ref>. Although convolutional networks can benefit substantially from increased depth <ref type="bibr" target="#b20">(Huang et al., 2016)</ref>, typically MLPs obtain little benefit beyo
l., 2019)</ref>, natural language processing <ref type="bibr" target="#b51">(Yao et al., 2019;</ref><ref type="bibr" target="#b16">Han et al., 2012;</ref><ref type="bibr" target="#b55">Zhang et al., 2
r CNNs with learned convolutional kernels <ref type="bibr" target="#b44">(Waibel et al., 1989;</ref><ref type="bibr" target="#b25">LeCun et al., 1989)</ref>. As additional algorithmic complexity tends
rovement can be achieved.</p><p>We use a stream prefetcher <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12]</ref> similar to the one in IBM's POWER 4 <ref type="bibr" target rget="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12]</ref> to indicate whether or not a cache line (or request) was br get="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12]</ref> have been proposed. Although these proposals have the simil we need to measure the run-time accuracy of the prefetcher <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref>. Therefore, we add an handling techniques <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref> aim to reduce the int .5">Comparison with Prefetch-Aware DRAM Controllers</head><p>Prefetch-Aware DRAM Controllers (PADC) <ref type="bibr" target="#b11">[12]</ref> was proposed to maximize DRAM row buffer hits for useful r . As we discuss in Section 5.5, our mechanisms are complementary to prefetch-aware DRAM controllers <ref type="bibr" target="#b11">[12]</ref> which employ an adaptive prefetch handling technique that
s Scheduling</head><p>A number of DRAM scheduling policies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
s</head><p>To measure CMP system performance, we use Individual Speedup (IS), Weighted Speedup (WS) <ref type="bibr" target="#b21">[22]</ref>, and Harmonic mean of Speedups (HS) <ref type="bibr" targe
formance improvement techniques such as prefetching, out-of-order execution, and runahead execution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> are designed to amor
-based CZone Delta Correlation (C/DC) <ref type="bibr" target="#b17">[18]</ref> and PC-based stride <ref type="bibr" target="#b0">[1]</ref>. Both the C/DC and stride prefetchers accurately capture a s
/ref>. WS corresponds to system throughput and HS corresponds to the inverse of job turnaround time <ref type="bibr" target="#b5">[6]</ref>. In the following equations, N is the number of cores in the
re the actual design cost/effort is not expensive. Prefetch bits are already used in many proposals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" targ
wo different types of prefetchers: GHB (Global History Buffer)-based CZone Delta Correlation (C/DC) <ref type="bibr" target="#b17">[18]</ref> and PC-based stride <ref type="bibr" target="#b0">[1]</ref
expensive. Prefetch bits are already used in many proposals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar
-based CZone Delta Correlation (C/DC) <ref type="bibr" target="#b17">[18]</ref> and PC-based stride <ref type="bibr" target="#b0">[1]</ref>. Both the C/DC and stride prefetchers accurately capture a s
ed number of MSHR entries (32) because MSHRs are not scalable since they require associative search <ref type="bibr" target="#b25">[26]</ref>. In this section, we study the effect of our techniques wi sing the number of MSHR entries is more costly in terms of both latency and area due to two reasons <ref type="bibr" target="#b25">[26]</ref>: 1) MSHRs require associative search, 2) MSHRs require the arget="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26]</ref>. These works define MLP as the average number of outstandin
on LR images directly and progressively reconstruct the sub-band residuals of HR images. Tai et al. <ref type="bibr" target="#b33">[34]</ref> proposed deep recursive residual network (DRRN) to address the estimated high-quality patch with the same resolution as the input low-quality patch. We follow <ref type="bibr" target="#b33">[34]</ref> to do data augmentation. For each task, we train a single
="bibr" target="#b20">[21]</ref>. Difference to DenseNet Another related work to MemNet is DenseNet <ref type="bibr" target="#b13">[14]</ref>, which also builds upon a densely connected principle. In lysis</head><p>We now illustrate how our gate unit affects different kinds of memories. Inspired by <ref type="bibr" target="#b13">[14]</ref>, we adopt a weight norm as an approximate for the dependen
e.g., image denoising <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37]</ref>, single-image super-r f> PCLR <ref type="bibr" target="#b1">[2]</ref> PGPD <ref type="bibr" target="#b36">[37]</ref> WNNM <ref type="bibr" target="#b8">[9]</ref> RED <ref type="bibr" target="#b26">[27]</ref>  </p></div> <d LR <ref type="bibr" target="#b1">[2]</ref>, PGPD <ref type="bibr" target="#b36">[37]</ref> and WNNM <ref type="bibr" target="#b8">[9]</ref>. The results are shown in Fig. <ref type="figure">7</ref>  S
ef type="bibr" target="#b37">38]</ref> and JPEG deblocking <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>As three classical image restoration tasks, image de
and Eqn. 12 are optimized via the mini-batch stochastic gradient descent (SGD) with backpropagation <ref type="bibr" target="#b23">[24]</ref>. We set the mini-batch size of SGD to 64, momentum paramet
ery deep convolutional auto-encoder network for image denoising and SISR. Very Recently, Lai et al. <ref type="bibr" target="#b22">[23]</ref> proposed LapSRN to address the problems of speed and accur
function, including batch normalization <ref type="bibr" target="#b15">[16]</ref> followed by ReLU <ref type="bibr" target="#b29">[30]</ref>, and W i m , i = 1, 2 are the weights of the i-th convolut
ion, and the popular networks, GoogleNet <ref type="bibr" target="#b32">[33]</ref>, Highway network <ref type="bibr" target="#b31">[32]</ref>, ResNet <ref type="bibr" target="#b11">[12]</ref>, reveal en MemNet and Highway Network -a very deep CNN model using a gate unit to regulate information flow <ref type="bibr" target="#b31">[32]</ref>.</p><p>To avoid information attenuation in very deep plain
="#b37">[38]</ref> and other 200 are from BSD train set. For testing, four benchmark datasets, Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b38">[39]</ref>,</p><p>Data
s a recursive unit and a gate unit. Inspired by neuroscience <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref> that recursive connections ubiquitously exist in the neocor ear function that acts like a recursive synapse in the brain <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>. Here, we use a residual building block, which is introduce
>Related Work</head><p>The success of AlexNet <ref type="bibr" target="#b21">[22]</ref> in ImageNet <ref type="bibr" target="#b30">[31]</ref> starts the era of deep learning for vision, and the popula
l.</ref> were the first to propose the concept of dead-block prediction and a trace-based predictor <ref type="bibr" target="#b15">[16]</ref>, which predicts a block dead once it has been accessed by #b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Software solutions pass hints about dead-block informatio f> and PC based <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Compared to data-address based approaches, PC-based appro ed, and time-based.</p><p>Lai et al. were the first to propose the concept of dead-block prediction <ref type="bibr" target="#b15">[16]</ref> and a trace-based dead-block predictor for the L1 cache, c nt, bypassing, power reduction, and coherence protocol optimizations.</p><p>Prefetching: Lai et al. <ref type="bibr" target="#b15">[16]</ref> and Hu et al. <ref type="bibr" target="#b6">[7]</ref> used g compared to triggering prefetches on cache misses. Ferdman and Falsafi later extended the work in <ref type="bibr" target="#b15">[16]</ref> to store correlation patterns off-chip and stream them on- div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>By references By bursts Trace</head><p>RefTrace <ref type="bibr" target="#b15">[16]</ref> BurstTrace Prediction Counting RefCount <ref type="bibr" t rg/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>RefTrace was evaluated on directly mapped caches in<ref type="bibr" target="#b15">[16]</ref>. This paper uses it on set-associative caches. In contrast
t <ref type="bibr" target="#b28">[29]</ref>, Kampe et al. proposed an Self-Correcting LRU algorithm <ref type="bibr" target="#b11">[12]</ref> to correct LRU replacement mistakes, whereas Qureshi et al
ould be invalidated. Somogyi et al. studied using PC-traces to identify last stores to cache blocks <ref type="bibr" target="#b24">[25]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
ad><p>Dead block prediction can be performed in software <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref> or in hardware <ref type="bibr" target="#b0">[1]</ref>, <r rmation collected through profiling or compiler analysis <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref> to the hardware. They are more accurate but usually have l
b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref> to improve cach
mercial applications (SPECWeb99, TPC-W, and SPECjbb) and five scientific applications from SPLASH-2 <ref type="bibr" target="#b29">[30]</ref>. Because of the cache coherence protocol, the definition o
" target="#b15">[16]</ref> to store correlation patterns off-chip and stream them on-chip as needed <ref type="bibr" target="#b3">[4]</ref>, which makes it possible to perform correlation-prefetching t on set-associative caches. In contrast to this study which evaluates RefTrace in the MRU position,<ref type="bibr" target="#b3">[4]</ref> evaluated RefTrace in the LRU position.</p></note> 		</body>
ache <ref type="bibr" target="#b19">[20]</ref>.</p><p>Bypassing: Prior work has also used bypassing <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" ype="bibr">Gonz?lez et al.</ref> proposed to bypass L1 data cache blocks with low temporal locality <ref type="bibr" target="#b5">[6]</ref>.</p><p>Power reduction: Dead block prediction has also been
t <ref type="bibr" target="#b28">[29]</ref>, Kampe et al. proposed an Self-Correcting LRU algorithm <ref type="bibr" target="#b11">[12]</ref> to correct LRU replacement mistakes, whereas Qureshi et al
2.">Prior Work on Dead Block Prediction</head><p>Dead block prediction can be performed in software <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref> or in hardware solutions pass hints about dead-block information collected through profiling or compiler analysis <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref> to the hardware
prefetch engine.</head><p>We use an existing prefetching scheme, tag correlating prefetching (TCP) <ref type="bibr" target="#b7">[8]</ref> as the baseline prefetch engine. TCP is a correlating prefet
="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr f type="figure" target="#fig_0">1</ref> shows the opportunity of temporal prefetching and what STMS <ref type="bibr" target="#b9">[10]</ref> and ISB <ref type="bibr" target="#b12">[13]</ref>, two stat e addresses that follow the missed address in the  history <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. As it is evident from Figure <ref type="figure" target="#f how that temporal prefetchers that rely on just one miss address to look up the history (e.g., STMS <ref type="bibr" target="#b9">[10]</ref>), frequently prefetch incorrectly.</p><p>Another interestin storage for STMS to have a reasonable coverage, both tables are placed off chip in the main memory <ref type="bibr" target="#b9">[10]</ref>. Consequently, every access to these tables (read or update data.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows the timing of events with STMS <ref type="bibr" target="#b9">[10]</ref>. Upon a cache miss, a request is sent to the main memory to n offers the level of performance similar to that of the non-practical always-update implementation <ref type="bibr" target="#b9">[10]</ref>.</p><p>As both metadata tables are off-chip, the on-chip st ata misses. As the size of these two tables is very large (several megabytes), just like prior work <ref type="bibr" target="#b9">[10]</ref>, both tables are stored in the main memory.</p><p>Domino pr . The delay of the search is tolerable because it is considerably smaller than the off-chip latency <ref type="bibr" target="#b9">[10]</ref>. In case a match is not found, nothing will be done, and ot ementation <ref type="bibr" target="#b12">[13]</ref>.</p><p>Sampled Temporal Memory Streaming. STMS <ref type="bibr" target="#b9">[10]</ref> records miss sequences in a global per-core HT and locates al per-core HT and locates streams through an IT. It benefits from a stream-end detection heuristic <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b39">[40]</ref> to reduce useles ISB <ref type="bibr" target="#b12">[13]</ref>, VLDP <ref type="bibr" target="#b33">[34]</ref>, STMS <ref type="bibr" target="#b9">[10]</ref>, and Digram <ref type="bibr" target="#b20">[21]</ref>. As a
">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> prefetchers are usually incapable of prefetching dependent ers are implemented on top of the baseline.</p><p>Variable Length Delta Prefetcher. We include VLDP <ref type="bibr" target="#b33">[34]</ref> because it has similarities with the lookup mechanism of o n</head><p>We compare Domino prefetcher against ISB <ref type="bibr" target="#b12">[13]</ref>, VLDP <ref type="bibr" target="#b33">[34]</ref>, STMS <ref type="bibr" target="#b9">[10]</ref>, and Digram
"#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bib anced Index Table (EIT) that is indexed by a single miss address. Unlike a conventional Index Table <ref type="bibr" target="#b10">[11]</ref> that solely stores a pointer for each address in the histo
stall the core because both misses are fetched serially <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The length of the chain of dependent misses varies across
f type="bibr" target="#b21">[22]</ref>, we use the Sequitur hierarchical data compression algorithm <ref type="bibr" target="#b34">[35]</ref> to identify the opportunity of temporal prefetching. While f type="bibr" target="#b21">[22]</ref>, we use the Sequitur hierarchical data compression algorithm <ref type="bibr" target="#b34">[35]</ref> to identify temporal prefetching opportunity for data miss 1]</ref>. As a point of reference, we also include the temporal opportunity measured using Sequitur <ref type="bibr" target="#b34">[35]</ref>. VLDP is a spatial prefetching technique while ISB, STMS,
">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>. Most of prior
">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bib
rget="#b4">[5]</ref>, are ineffective for server workloads <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, more advanced data prefetchers may eliminate or reduce the negative effect of data misses. One of the promising prefetching techniques is temporal prefetching <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" t ed address in the history and prefetch the addresses that follow the missed address in the  history <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. As it is evident
ams consist of loops, and hence, the sequence of addresses, and consequently, miss addresses repeat <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Upon a miss, t ferent chains in a particular application, ranging from a couple to hundreds of thousands of misses <ref type="bibr" target="#b17">[18]</ref>. While stride <ref type="bibr" target="#b2">[3]</ref>, <re e prior studies of measuring repetitiveness in data misses <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we use the Seq or studies of measuring repetitiveness of access sequences <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we use the Seq
f type="bibr" target="#b21">[22]</ref>, we use the Sequitur hierarchical data compression algorithm <ref type="bibr" target="#b34">[35]</ref> to identify the opportunity of temporal prefetching. While f type="bibr" target="#b21">[22]</ref>, we use the Sequitur hierarchical data compression algorithm <ref type="bibr" target="#b34">[35]</ref> to identify temporal prefetching opportunity for data miss 1]</ref>. As a point of reference, we also include the temporal opportunity measured using Sequitur <ref type="bibr" target="#b34">[35]</ref>. VLDP is a spatial prefetching technique while ISB, STMS,
rget="#b4">[5]</ref>, are ineffective for server workloads <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, more advanced data prefetchers may eliminate or reduce the negative effect of data misses. One of the promising prefetching techniques is temporal prefetching <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" t ed address in the history and prefetch the addresses that follow the missed address in the  history <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. As it is evident
t="#b13">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b27">Velickovic et al., 2018;</ref><ref type="bibr" target="#b19">Palm et al., 2018)</ref>, we propose an evidence reasoning network (E
arget="#b18">Nie et al. (2019)</ref>; <ref type="bibr" target="#b31">Yoneda et al. (2018)</ref> and <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref> have achieved the top three results a laimevidence pair individually and then aggregate all NLI predictions for final verification. Then, <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref>; <ref type="bibr" target="#b31">Yoned the task. In the document retrieval and sentence selection stages, we simply follow the method from <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref> since their method has the highest sc ose noisy evidence.</p><p>In the document retrieval step, we adopt the entity linking approach from <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref>. Given a claim, the method first util ent selects the most relevant evidence for the claim from all sentences in the retrieved documents. <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref> modify the ESIM 2 https://www.mediawi he OFEVER scores of our model and models from other teams. After running the same model proposed by <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref>, we find our OFEVER score is slightly cted to form the final evidence set in the original method.</p><p>In addition to the original model <ref type="bibr" target="#b10">(Hanselowski et al., 2018)</ref>, we add a relevance score filter wit models from the FEVER shared task as our baselines.</p><p>The Athene UKP TU Darmstadt team (Athene) <ref type="bibr" target="#b10">(Hanselowski et al., 2018)</ref> combines five inference vectors from
r" target="#b1">(Bowman et al., 2015;</ref><ref type="bibr" target="#b20">Parikh et al., 2016;</ref><ref type="bibr" target="#b23">Sha et al., 2016;</ref><ref type="bibr">Chen et al., 2017b,c;</ref><r
arget="#b18">Nie et al. (2019)</ref>; <ref type="bibr" target="#b31">Yoneda et al. (2018)</ref> and <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref> have achieved the top three results a laimevidence pair individually and then aggregate all NLI predictions for final verification. Then, <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref>; <ref type="bibr" target="#b31">Yoned the task. In the document retrieval and sentence selection stages, we simply follow the method from <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref> since their method has the highest sc ose noisy evidence.</p><p>In the document retrieval step, we adopt the entity linking approach from <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref>. Given a claim, the method first util ent selects the most relevant evidence for the claim from all sentences in the retrieved documents. <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref> modify the ESIM 2 https://www.mediawi he OFEVER scores of our model and models from other teams. After running the same model proposed by <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref>, we find our OFEVER score is slightly cted to form the final evidence set in the original method.</p><p>In addition to the original model <ref type="bibr" target="#b10">(Hanselowski et al., 2018)</ref>, we add a relevance score filter wit models from the FEVER shared task as our baselines.</p><p>The Athene UKP TU Darmstadt team (Athene) <ref type="bibr" target="#b10">(Hanselowski et al., 2018)</ref> combines five inference vectors from

ounty in the USA". Furthermore, we adopt an effective pretrained language representation model BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> to better grasp both evidence and claim se bibr" target="#b22">(Radford et al., 2018)</ref> are proven to be effective on many NLP tasks. BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> employs bidirectional transformer and well ttp://www.tei-c.org/ns/1.0"><head>Sentence Encoder</head><p>Given an input sentence, we employ BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> as our sentence encoder by extracting the ystems in our experiments. We first introduce the top-3 systems from the FEVER shared task. As BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> has achieved promising performance on seve s="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyperparameter Settings</head><p>We utilize BERT BASE <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> in all of the BERT fine-tuning baselines a
" target="#b17">Nie and Bansal, 2017;</ref><ref type="bibr" target="#b5">Conneau et al., 2017;</ref><ref type="bibr" target="#b9">Gong et al., 2018;</ref><ref type="bibr" target="#b24">Tay et al., 201
h is a modification of ESIM and achieves the best results in the competition. Unlike these methods, <ref type="bibr" target="#b30">Yin and Roth (2018)</ref> propose the TWOWINGOS system which trains t
ef type="bibr" target="#b14">Luken et al. (2018)</ref> adopt the decomposable attention model (DAM) <ref type="bibr" target="#b20">(Parikh et al., 2016)</ref> to generate NLI predictions for each clai have achieved the state-of-the-art results <ref type="bibr" target="#b1">(Bowman et al., 2015;</ref><ref type="bibr" target="#b20">Parikh et al., 2016;</ref><ref type="bibr" target="#b23">Sha et al.,
f><ref type="bibr" target="#b23">Sha et al., 2016;</ref><ref type="bibr">Chen et al., 2017b,c;</ref><ref type="bibr" target="#b16">Munkhdalai and Yu, 2017;</ref><ref type="bibr" target="#b17">Nie and
r" target="#b1">(Bowman et al., 2015;</ref><ref type="bibr" target="#b20">Parikh et al., 2016;</ref><ref type="bibr" target="#b23">Sha et al., 2016;</ref><ref type="bibr">Chen et al., 2017b,c;</ref><r
e nodes into a K-dimensional vector space, which preserves certain properties among nodes. Deepwalk <ref type="bibr" target="#b6">[Tang et al., 2015]</ref>, LINE <ref type="bibr" target="#b6">[Tang et tain properties among nodes. Deepwalk <ref type="bibr" target="#b6">[Tang et al., 2015]</ref>, LINE <ref type="bibr" target="#b6">[Tang et al., 2015]</ref> and Node2vec <ref type="bibr" target="#b2">[
the short-term pattern of nodes, we apply the contextual attention-based model which is inspired by <ref type="bibr" target="#b3">[Liu et al., 2017]</ref> and proposed by <ref type="bibr" target="#b1"
y anomalous edges detection, is then highly needed before the data are fed into the following tasks <ref type="bibr" target="#b1">[Akoglu et al., 2015;</ref><ref type="bibr" target="#b4">Ranshous et a d model which is inspired by <ref type="bibr" target="#b3">[Liu et al., 2017]</ref> and proposed by <ref type="bibr" target="#b1">[Cui et al., 2017]</ref>. In our framework, we construct short state o l></formula><p>GRU is a variant of LSTM network. It is simpler and more effective than LSTM network <ref type="bibr" target="#b1">[Chung et al., 2014]</ref>. GRU can record long-term information, and oss entropy to distinguish the existing edges and the generated ones. We then take the same idea in <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref> and use marginbased pairwise loss in train ally build the required datasets because the ground-truth for the test phase is difficult to obtain <ref type="bibr" target="#b1">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref t
ey define a density function and discover the target mainly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et a graph, while the method mainly focuses on structural features and cannot catch long-term anomalies. <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> proposed NetWalk, a dynamic graph embedding mo btain <ref type="bibr" target="#b1">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> to inject anomalous edges into two datasets.</ ure and historical behavior near an edge to measure whether the edge is anomalous or not. • NetWalk <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The method first builds node embeddings based esults are shown in Table <ref type="table">1</ref>, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. We can see that Ad-dGraph beats all baselines 3">2</ref> illustrates the results on dynamic graph, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The results indicate that AddGraph beats base
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
the data are fed into the following tasks <ref type="bibr" target="#b1">[Akoglu et al., 2015;</ref><ref type="bibr" target="#b4">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the an inly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et al., 2015]</ref> consider content feature or even tempor sides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the de cluster, and the model can also be used to produce anomalous score for a given edge. • CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref>. It uses the local structural feature an
ns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anomaly Detection in Dynamic Graph</head><p>Goutlier <ref type="bibr" target="#b0">[Aggarwal et al., 2011]</ref> is proposed with an observation that ano tasets.</p><p>Baselines. We compare AddGraph with three anomaly detection methods.</p><p>• GOutlier <ref type="bibr" target="#b0">[Aggarwal et al., 2011]</ref>. It builds a generative model for edges
ey define a density function and discover the target mainly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et a graph, while the method mainly focuses on structural features and cannot catch long-term anomalies. <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> proposed NetWalk, a dynamic graph embedding mo btain <ref type="bibr" target="#b1">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> to inject anomalous edges into two datasets.</ ure and historical behavior near an edge to measure whether the edge is anomalous or not. • NetWalk <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The method first builds node embeddings based esults are shown in Table <ref type="table">1</ref>, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. We can see that Ad-dGraph beats all baselines 3">2</ref> illustrates the results on dynamic graph, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The results indicate that AddGraph beats base
ey define a density function and discover the target mainly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et a graph, while the method mainly focuses on structural features and cannot catch long-term anomalies. <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> proposed NetWalk, a dynamic graph embedding mo btain <ref type="bibr" target="#b1">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> to inject anomalous edges into two datasets.</ ure and historical behavior near an edge to measure whether the edge is anomalous or not. • NetWalk <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The method first builds node embeddings based esults are shown in Table <ref type="table">1</ref>, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. We can see that Ad-dGraph beats all baselines 3">2</ref> illustrates the results on dynamic graph, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The results indicate that AddGraph beats base
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
ey define a density function and discover the target mainly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et a graph, while the method mainly focuses on structural features and cannot catch long-term anomalies. <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> proposed NetWalk, a dynamic graph embedding mo btain <ref type="bibr" target="#b1">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> to inject anomalous edges into two datasets.</ ure and historical behavior near an edge to measure whether the edge is anomalous or not. • NetWalk <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The method first builds node embeddings based esults are shown in Table <ref type="table">1</ref>, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. We can see that Ad-dGraph beats all baselines 3">2</ref> illustrates the results on dynamic graph, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The results indicate that AddGraph beats base
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
culty of detection. The similar anomaly pattern appears in the network attack against IP-IP network <ref type="bibr" target="#b2">[Eswaran et al., 2018]</ref>, where there are sudden large number of c tional Network) is a representative model to combine the content and structural features in a graph <ref type="bibr" target="#b2">[Kipf and Welling, 2017]</ref>. Compared with traditional graph method Tang et al., 2015]</ref>, LINE <ref type="bibr" target="#b6">[Tang et al., 2015]</ref> and Node2vec <ref type="bibr" target="#b2">[Grover and Leskovec, 2016]</ref> are the methods to yield node embedd xtends the idea of convolution model over regular graphs (i.e., image) to general graphs. The works <ref type="bibr" target="#b2">[Defferrard et al., 2016;</ref><ref type="bibr" target="#b2">Kipf and ., image) to general graphs. The works <ref type="bibr" target="#b2">[Defferrard et al., 2016;</ref><ref type="bibr" target="#b2">Kipf and Welling, 2017]</ref>improve the performance of basic GCN from
ns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anomaly Detection in Dynamic Graph</head><p>Goutlier <ref type="bibr" target="#b0">[Aggarwal et al., 2011]</ref> is proposed with an observation that ano tasets.</p><p>Baselines. We compare AddGraph with three anomaly detection methods.</p><p>• GOutlier <ref type="bibr" target="#b0">[Aggarwal et al., 2011]</ref>. It builds a generative model for edges
ey define a density function and discover the target mainly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et a graph, while the method mainly focuses on structural features and cannot catch long-term anomalies. <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> proposed NetWalk, a dynamic graph embedding mo btain <ref type="bibr" target="#b1">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> to inject anomalous edges into two datasets.</ ure and historical behavior near an edge to measure whether the edge is anomalous or not. • NetWalk <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The method first builds node embeddings based esults are shown in Table <ref type="table">1</ref>, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. We can see that Ad-dGraph beats all baselines 3">2</ref> illustrates the results on dynamic graph, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The results indicate that AddGraph beats base
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
culty of detection. The similar anomaly pattern appears in the network attack against IP-IP network <ref type="bibr" target="#b2">[Eswaran et al., 2018]</ref>, where there are sudden large number of c tional Network) is a representative model to combine the content and structural features in a graph <ref type="bibr" target="#b2">[Kipf and Welling, 2017]</ref>. Compared with traditional graph method Tang et al., 2015]</ref>, LINE <ref type="bibr" target="#b6">[Tang et al., 2015]</ref> and Node2vec <ref type="bibr" target="#b2">[Grover and Leskovec, 2016]</ref> are the methods to yield node embedd xtends the idea of convolution model over regular graphs (i.e., image) to general graphs. The works <ref type="bibr" target="#b2">[Defferrard et al., 2016;</ref><ref type="bibr" target="#b2">Kipf and ., image) to general graphs. The works <ref type="bibr" target="#b2">[Defferrard et al., 2016;</ref><ref type="bibr" target="#b2">Kipf and Welling, 2017]</ref>improve the performance of basic GCN from
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
the data are fed into the following tasks <ref type="bibr" target="#b1">[Akoglu et al., 2015;</ref><ref type="bibr" target="#b4">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the an inly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et al., 2015]</ref> consider content feature or even tempor sides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the de cluster, and the model can also be used to produce anomalous score for a given edge. • CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref>. It uses the local structural feature an
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
ey define a density function and discover the target mainly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et a graph, while the method mainly focuses on structural features and cannot catch long-term anomalies. <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> proposed NetWalk, a dynamic graph embedding mo btain <ref type="bibr" target="#b1">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> to inject anomalous edges into two datasets.</ ure and historical behavior near an edge to measure whether the edge is anomalous or not. • NetWalk <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The method first builds node embeddings based esults are shown in Table <ref type="table">1</ref>, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. We can see that Ad-dGraph beats all baselines 3">2</ref> illustrates the results on dynamic graph, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The results indicate that AddGraph beats base
ns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anomaly Detection in Dynamic Graph</head><p>Goutlier <ref type="bibr" target="#b0">[Aggarwal et al., 2011]</ref> is proposed with an observation that ano tasets.</p><p>Baselines. We compare AddGraph with three anomaly detection methods.</p><p>• GOutlier <ref type="bibr" target="#b0">[Aggarwal et al., 2011]</ref>. It builds a generative model for edges
the data are fed into the following tasks <ref type="bibr" target="#b1">[Akoglu et al., 2015;</ref><ref type="bibr" target="#b4">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the an inly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et al., 2015]</ref> consider content feature or even tempor sides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the de cluster, and the model can also be used to produce anomalous score for a given edge. • CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref>. It uses the local structural feature an
the data are fed into the following tasks <ref type="bibr" target="#b1">[Akoglu et al., 2015;</ref><ref type="bibr" target="#b4">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the an inly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et al., 2015]</ref> consider content feature or even tempor sides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the de cluster, and the model can also be used to produce anomalous score for a given edge. • CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref>. It uses the local structural feature an
the data are fed into the following tasks <ref type="bibr" target="#b1">[Akoglu et al., 2015;</ref><ref type="bibr" target="#b4">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the an inly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et al., 2015]</ref> consider content feature or even tempor sides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the de cluster, and the model can also be used to produce anomalous score for a given edge. • CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref>. It uses the local structural feature an
culty of detection. The similar anomaly pattern appears in the network attack against IP-IP network <ref type="bibr" target="#b2">[Eswaran et al., 2018]</ref>, where there are sudden large number of c tional Network) is a representative model to combine the content and structural features in a graph <ref type="bibr" target="#b2">[Kipf and Welling, 2017]</ref>. Compared with traditional graph method Tang et al., 2015]</ref>, LINE <ref type="bibr" target="#b6">[Tang et al., 2015]</ref> and Node2vec <ref type="bibr" target="#b2">[Grover and Leskovec, 2016]</ref> are the methods to yield node embedd xtends the idea of convolution model over regular graphs (i.e., image) to general graphs. The works <ref type="bibr" target="#b2">[Defferrard et al., 2016;</ref><ref type="bibr" target="#b2">Kipf and ., image) to general graphs. The works <ref type="bibr" target="#b2">[Defferrard et al., 2016;</ref><ref type="bibr" target="#b2">Kipf and Welling, 2017]</ref>improve the performance of basic GCN from
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
the data are fed into the following tasks <ref type="bibr" target="#b1">[Akoglu et al., 2015;</ref><ref type="bibr" target="#b4">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the an inly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et al., 2015]</ref> consider content feature or even tempor sides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the de cluster, and the model can also be used to produce anomalous score for a given edge. • CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref>. It uses the local structural feature an
tant measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 201 orks <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density functio
vely search the tree-structured architecture space. Motivated by these AutoML frameworks, He et al. <ref type="bibr" target="#b10">[11]</ref> leveraged the reinforcement learning to automatically prun
improve the performance of deep neural networks by searching the network architectures: Zoph et al. <ref type="bibr" target="#b35">[36]</ref> proposed the Neural Architecture Search (NAS) to explore a
ers: e.g., vanilla convolution has more data reuse and better locality, while depthwise convolution <ref type="bibr" target="#b3">[4]</ref> has less reuse and worse locality, which makes it memory bou r" target="#b24">[25]</ref>. Both MobileNets are inspired from the depthwise separable convolutions <ref type="bibr" target="#b3">[4]</ref> and replace the regular convolutions with the pointwise and
ef type="bibr" target="#b4">[5]</ref> binarized the network weights into {-1, +1}; Rastegari et al. <ref type="bibr" target="#b23">[24]</ref> and Zhou et al. <ref type="bibr" target="#b32">[33]</ref>
to the range of [0, 1].</p><p>Exploration. Optimization of the DDPG agent is carried out using ADAM <ref type="bibr" target="#b15">[16]</ref> with ? 1 = 0.9 and ? 2 = 0.999. We use a fixed learning ra
. For instance, the coarse-grained channel pruning methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> prune away the enti
ers: e.g., vanilla convolution has more data reuse and better locality, while depthwise convolution <ref type="bibr" target="#b3">[4]</ref> has less reuse and worse locality, which makes it memory bou r" target="#b24">[25]</ref>. Both MobileNets are inspired from the depthwise separable convolutions <ref type="bibr" target="#b3">[4]</ref> and replace the regular convolutions with the pointwise and
s we need a specialized quantization solution for different hardware architectures. (HW1: BitFusion <ref type="bibr" target="#b25">[26]</ref>, HW2: BISMO <ref type="bibr" target="#b26">[27]</ref> edge >[27]</ref> proposed the bit-serial multiplier to support multiplications of 1 to 8 bits; BitFusion <ref type="bibr" target="#b25">[26]</ref> supports multiplications of 2, 4, 8 and 16 bits in a spati by shifting over time.</p><p>Spatial Architecture. BitFusion architecture proposed by Hardik et al. <ref type="bibr" target="#b25">[26]</ref> is a state-of-the-art spatial ASIC design for neural netwo rk with PACT <ref type="bibr" target="#b2">[3]</ref> under the latency constraints on the BitFusion <ref type="bibr" target="#b25">[26]</ref> architecture (Table <ref type="table" target="#tab_6">4</r
approaches to slim neural network models. For instance, the coarse-grained channel pruning methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta
improve the hardware efficiency, many researchers have proposed to directly design efficient models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" ta factors influencing the hardware performance, such as memory access cost and degree of parallelism <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21]</ref>. Taking computatio e proxy signals (e.g., FLOPs, number of memory references) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. However, as different hardware behaves very differently, t y study the quantization of MobileNet-V1 <ref type="bibr" target="#b12">[13]</ref> and MobileNet-V2 <ref type="bibr" target="#b24">[25]</ref>. Both MobileNets are inspired from the depthwise separable
ef type="bibr" target="#b4">[5]</ref> binarized the network weights into {-1, +1}; Rastegari et al. <ref type="bibr" target="#b23">[24]</ref> and Zhou et al. <ref type="bibr" target="#b32">[33]</ref>
timated that up to 82% Americans take one or more drugs, and 29% take more than four drugs together <ref type="bibr" target="#b2">[3]</ref>. Consider all drugs have a small chance of side effects, tak DDAAs from MedHelp.org <ref type="bibr" target="#b22">[23]</ref>, a popular online health community <ref type="bibr" target="#b2">[3]</ref>. They first built a heterogeneous healthcare network compris elp online. A recent survey shows that 72% of Internet users went online to seek health information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>. Several attempts ha
few fields, such as image retrieval, and anomaly detection <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. One-class SVM is trained with validated positive samples o
interactions only <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t t="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" t
ting surveillance studies focus on predicting ADRs for a single drug or drug-drug interactions only <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta
has achieved remarkable performance in a few fields, such as image retrieval, and anomaly detection <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. One-class SVM is
f all ADRs <ref type="bibr" target="#b3">[4]</ref>, resulting in significant fatality and morbidity <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Consequently, early i
n Web search logs to detect a specific DDAA, i.e., paroxetine-pravastatin-hyperglycemia association <ref type="bibr" target="#b21">[22]</ref>. Yang et al. proposed to discover drugdrug interactions an
">[4]</ref>, resulting in significant fatality and morbidity <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Consequently, early identification of potential ADRs for com ata mining results, it is unrealistic to expect every highly ranked ADRs valuable to domain experts <ref type="bibr" target="#b5">[6]</ref>. Therefore, we shortlist the top 40 ADRs according to their
use the PubChem fingerprint, which corresponds to 881 substructures defined in the PubChem database <ref type="bibr" target="#b29">[30]</ref>, to encode the drug chemical structure. Each drug is repre
="#b11">[12]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" t
et proteins in DrugBank are quite sparse, we also integrate the target information from DrugCentral <ref type="bibr" target="#b26">[27]</ref>, an openaccess online drug compendium. The association dat
arget="#b6">7]</ref>. This important finding attracts great interests in edge partitioning recently <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target xisting partitioners, METIS gives the lowest replication factor which is consistent with literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>. However, METIS runs titioning algorithm METIS <ref type="bibr" target="#b7">[8]</ref> is extended for edge partitioning <ref type="bibr" target="#b2">[3]</ref>, which makes full access to the graph structure by partition alanced if max i ∈[p] {|E i |} ≤ ⌈ α |E | /p⌉.<label>(1)</label></formula><p>The replication factor <ref type="bibr" target="#b2">[3]</ref> of a partitioning is defined as</p><formula xml:id="formula_ <head n="2.3">NP-Hardness</head><p>The p-edge partitioning problem has been proved to be NP-hard in <ref type="bibr" target="#b2">[3]</ref> when p grows with n = |V |. To our best knowledge, it has no weight. One can turn a vertex-partitioner into an edge-partitioner while preserving its performance <ref type="bibr" target="#b2">[3]</ref>. To transform METIS to an edge-partitioner, we first call ME </table></figure> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In<ref type="bibr" target="#b2">[3]</ref>, the NP-hardness is proved by a reduction from 3-partition p
ypically yield high replication factor. The notable multi-level vertex partitioning algorithm METIS <ref type="bibr" target="#b7">[8]</ref> is extended for edge partitioning <ref type="bibr" target="# ting partitioners. We compare our NE algorithm with six existing edge partitioners, including METIS <ref type="bibr" target="#b7">[8]</ref>, RAND <ref type="bibr" target="#b5">[6]</ref>, DBH <ref type ge partitioner Neighbor Expansion (NE) that outperforms other state-of-the-art ones including METIS <ref type="bibr" target="#b7">[8]</ref> and Sheep <ref type="bibr" target="#b12">[13]</ref> in terms
ation of |E | /n and σ /n. Here we apply the elegant treatment of random power-law graphs by Newman <ref type="bibr" target="#b13">[14]</ref> using generating functions, and derive the expected number e term E[ σ /n]. When τ &gt; 2,</formula><p>it is almost sure that the graph is not fully connected <ref type="bibr" target="#b13">[14]</ref>; when 2 &lt; τ &lt; 3.4785 and κ → ∞, besides small connec probability that this edge leads to a small component of size k. Here we follow Newman's assumption <ref type="bibr" target="#b13">[14]</ref> that there is no loop in the small components.</p><p>Hence
tore vertex set. For instance, our implementation of NE takes about 90 GB RAM to partition uk-union <ref type="bibr" target="#b1">[2]</ref>. We have made some preliminary attempts to extend the NE alg streaming algorithm via sampling methods (Appendix B), which is able to partition the clueweb graph <ref type="bibr" target="#b1">[2]</ref> (|V | = 978M, |E| = 42.5B) whose edge set exceeds the volume " target="#tab_7">4</ref>. To evaluate the scalability of the streaming we add a new graph clue-web <ref type="bibr" target="#b1">[2]</ref> of 978, 408, 098 vertices and 42, 574, 107, 469 edges. Compa
igure" target="#fig_0">1</ref>) is found to be more effective in advanced distributed graph engines <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target ly, researchers demonstrated that edge partitioning performs better on many large real-world graphs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. This important findin nding attracts great interests in edge partitioning recently <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targe ">17]</ref>. Edge partitioning has been widely adopted in recent graph systems including PowerGraph <ref type="bibr" target="#b5">[6]</ref>, Spark GraphX <ref type="bibr" target="#b6">[7]</ref>, and C tleneck in graph computing due to the intrinsic dependency and expensive random access in the graph <ref type="bibr" target="#b5">[6]</ref>. While it is relatively easy to solve the first issue by par ree distribution of power-law graphs, but it leverages little graph structure, like RAND. Oblivious <ref type="bibr" target="#b5">[6]</ref> is a streaming algorithm that considers the distribution of e the communication cost and execution time on distributed graph processing systems like PowerGraph <ref type="bibr" target="#b5">[6]</ref> and PowerLyra <ref type="bibr" target="#b3">[4]</ref> in Sec with six existing edge partitioners, including METIS <ref type="bibr" target="#b7">[8]</ref>, RAND <ref type="bibr" target="#b5">[6]</ref>, DBH <ref type="bibr" target="#b16">[17]</ref>, Oblivious <r D <ref type="bibr" target="#b5">[6]</ref>, DBH <ref type="bibr" target="#b16">[17]</ref>, Oblivious <ref type="bibr" target="#b5">[6]</ref>, HDRF <ref type="bibr" target="#b14">[15]</ref>, and Sheep <
ation of |E | /n and σ /n. Here we apply the elegant treatment of random power-law graphs by Newman <ref type="bibr" target="#b13">[14]</ref> using generating functions, and derive the expected number e term E[ σ /n]. When τ &gt; 2,</formula><p>it is almost sure that the graph is not fully connected <ref type="bibr" target="#b13">[14]</ref>; when 2 &lt; τ &lt; 3.4785 and κ → ∞, besides small connec probability that this edge leads to a small component of size k. Here we follow Newman's assumption <ref type="bibr" target="#b13">[14]</ref> that there is no loop in the small components.</p><p>Hence
ypically yield high replication factor. The notable multi-level vertex partitioning algorithm METIS <ref type="bibr" target="#b7">[8]</ref> is extended for edge partitioning <ref type="bibr" target="# ting partitioners. We compare our NE algorithm with six existing edge partitioners, including METIS <ref type="bibr" target="#b7">[8]</ref>, RAND <ref type="bibr" target="#b5">[6]</ref>, DBH <ref type ge partitioner Neighbor Expansion (NE) that outperforms other state-of-the-art ones including METIS <ref type="bibr" target="#b7">[8]</ref> and Sheep <ref type="bibr" target="#b12">[13]</ref> in terms
target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. Edge partitioning ncoming edge -an edge is more likely to be assigned to the partition with more adjacent edges. HDRF <ref type="bibr" target="#b14">[15]</ref> is another streaming algorithm that extends Oblivious by f ower-law graphs</head><p>Prior to this work, DBH <ref type="bibr" target="#b16">[17]</ref> and HDRF <ref type="bibr" target="#b14">[15]</ref> give upper bounds of expected replication factor of their with the ones in literature. Prior to ours, DBH <ref type="bibr" target="#b16">[17]</ref> and HDRF <ref type="bibr" target="#b14">[15]</ref> give upper bounds of expected replication factor for rando 4">[15]</ref> give upper bounds of expected replication factor for random power-law graphs. In HDRF <ref type="bibr" target="#b14">[15]</ref>, an average-case analysis is applied to their streaming me <ref type="bibr" target="#b16">[17]</ref>, Oblivious <ref type="bibr" target="#b5">[6]</ref>, HDRF <ref type="bibr" target="#b14">[15]</ref>, and Sheep <ref type="bibr" target="#b12">[13]</ref>. METI is randomly assigned to one of its adjacent vertices' partition. For Oblivious and HDRF, following <ref type="bibr" target="#b14">[15]</ref>, we feed the edges in a random order, to balance the resul
ypically yield high replication factor. The notable multi-level vertex partitioning algorithm METIS <ref type="bibr" target="#b7">[8]</ref> is extended for edge partitioning <ref type="bibr" target="# ting partitioners. We compare our NE algorithm with six existing edge partitioners, including METIS <ref type="bibr" target="#b7">[8]</ref>, RAND <ref type="bibr" target="#b5">[6]</ref>, DBH <ref type ge partitioner Neighbor Expansion (NE) that outperforms other state-of-the-art ones including METIS <ref type="bibr" target="#b7">[8]</ref> and Sheep <ref type="bibr" target="#b12">[13]</ref> in terms
uted graph engines <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> partition the input graph and parallelize the computation o tributed graph engines <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref>, which evenly assigns edges to machines in a way that minim type="bibr" target="#b5">[6]</ref>, Spark GraphX <ref type="bibr" target="#b6">[7]</ref>, and Chaos <ref type="bibr" target="#b15">[16]</ref>, to divide the graph across machines. It turns out that th ed to different machines.</p><p>Quite a few methods have been proposed for edge partitioning. Chaos <ref type="bibr" target="#b15">[16]</ref> distributes the edges randomly (RAND) under high network b
uted graph processing systems like PowerGraph <ref type="bibr" target="#b5">[6]</ref> and PowerLyra <ref type="bibr" target="#b3">[4]</ref> in Section 5.2.</p><p>Testbed. We evaluate all partitioners
puter vision methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar xperiments, we follow the protocol used in previous papers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b14">15]</ref>, which uses unseen #foot_0">1</ref> , 10 2 ]), where the implementation of traditional classifiers becomes challenging <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Arguably, t rix of pairwise distances of a subset of the training set (i.e., the mini-batch) allows Song et al. <ref type="bibr" target="#b20">[21]</ref> to design of a new loss function that integrates all posit > (with and without FANNG <ref type="bibr" target="#b4">[5]</ref>), (2) lifted structured embedding <ref type="bibr" target="#b20">[21]</ref>, (3) N-pairs metric loss <ref type="bibr" target="#b19">[2 hod, and ( <ref type="formula">5</ref>) ), we use the same training and test set split described in <ref type="bibr" target="#b20">[21]</ref> across all datasets. Specifically, the means CUB200-2011 < target="#b22">[23]</ref> weights and randomly initialize the final fully connected layer similar to <ref type="bibr" target="#b20">[21]</ref>. We set the embedding size to 64 <ref type="bibr" target=" nnected layer similar to <ref type="bibr" target="#b20">[21]</ref>. We set the embedding size to 64 <ref type="bibr" target="#b20">[21]</ref> and the learning rate for the randomly initialized fully c omly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type="bibr" target="#b20">[21]</ref>.</p><p>For the experiments using triplet combined with glo ="bibr" target="#b22">[23]</ref> and randomly initialize the final fully connected layer similar to <ref type="bibr" target="#b20">[21]</ref> . The learning rate for the randomly initialized fully con omly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type="bibr" target="#b20">[21]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
ead n="4.">Experiments</head><p>For the experiments, we follow the protocol used in previous papers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta (2) lifted structured embedding <ref type="bibr" target="#b20">[21]</ref>, (3) N-pairs metric loss <ref type="bibr" target="#b19">[20]</ref>, (4) clustering <ref type="bibr" target="#b14">[15]</ref>,
target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
sing training samples acquired from a smart sampling method that has low computational com- plexity <ref type="bibr" target="#b4">[5]</ref> and can find effective training samples that produce gradien ue to training with very few samples per class). A FANNG (Fast Approximate Nearest Neighbour Graph) <ref type="bibr" target="#b4">[5]</ref> is a graph based index that can find these neighbourhoods qu ining set T to generate the current feature embedding f (x, θ f ). The indexing graph used in FANNG <ref type="bibr" target="#b4">[5]</ref> is then constructed using the traverse-add algorithm (Alg. 4 type="bibr" target="#b4">[5]</ref> is then constructed using the traverse-add algorithm (Alg. 4 in <ref type="bibr" target="#b4">[5]</ref>) with the embedding of each element of T forming a vertex in putationally efficient collection of the approximate nearest neighbour set S.</p><p>As described in <ref type="bibr" target="#b4">[5]</ref>, the traverse-add algorithm can be repeatedly applied until from the original building process for FANNGs. Rather than applying the backtrack search (Alg. 3 in <ref type="bibr" target="#b4">[5]</ref>) to further refine the graph, we instead use the same backtr ="bibr" target="#b7">[8]</ref>. Our proposed method combining triplet and global losses using FANNG <ref type="bibr" target="#b4">[5]</ref> with and without automated hyper-parameter selection (i.e., ing with semihard negative mining <ref type="bibr" target="#b15">[16]</ref> (with and without FANNG <ref type="bibr" target="#b4">[5]</ref>), (2) lifted structured embedding <ref type="bibr" target="#
get="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar
ich uses unseen classes from the CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref> datasets in order to assess the clustering quality and k nea h we take the first 100 species for training and use the remaining 100 species for testing. Cars196 <ref type="bibr" target="#b8">[9]</ref> has 16, 185 images from 196 car models, from which we take t tate of the art for the datasets CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref>. From these tables, we can first see that Triplet + FANNG si quality of the embedding. Using CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref>, we show that our proposed method achieves fast and more acc
tive feature embedding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ incorporation of loss functions that take into account the global structure of the embedding space <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar Triplet + FANNG + Global + Adaptive significantly outperforms the current state of the art methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> in terms of cluster /ref>.</p><p>In this paper, we propose a novel deep metric learning approach that combines a global <ref type="bibr" target="#b9">[10]</ref> and a triplet loss <ref type="bibr" target="#b6">[7,</ref>< he development of approaches that explore the global structure of the embedding space. Kumar et al. <ref type="bibr" target="#b9">[10]</ref> proposed a global loss function that uses first and second d a loss function that optimises a global clustering quality metric (NMI). As shown by Kumar et al. <ref type="bibr" target="#b9">[10]</ref>, it appears that a combination of local and global losses c ade more robust with the introduction of a loss that explores the global structure of the embedding <ref type="bibr" target="#b9">[10]</ref>. In particular, the triplet loss in (2) can be extended wit (4) clustering <ref type="bibr" target="#b14">[15]</ref>, and (5) triplet combined with global loss <ref type="bibr" target="#b9">[10]</ref>. For the approaches (1), ( <ref type="formula" target="#for ="bibr" target="#b20">[21]</ref>.</p><p>For the experiments using triplet combined with global loss <ref type="bibr" target="#b9">[10]</ref> and for our proposed approach, we let the training procedur for batches comprised of only random triplets. Similar to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10]</ref>, we set the margin for the triplet and global loss to 0.2 an
presents a normalisation function, and r l (.) denotes a non-linear activation function (e.g., ReLU <ref type="bibr" target="#b13">[14]</ref>). Also in <ref type="bibr" target="#b0">(1)</ref>, note th
get="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. The main advantage of such models lies in their ability to
presents a normalisation function, and r l (.) denotes a non-linear activation function (e.g., ReLU <ref type="bibr" target="#b13">[14]</ref>). Also in <ref type="bibr" target="#b0">(1)</ref>, note th
ich uses unseen classes from the CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref> datasets in order to assess the clustering quality and k nea h we take the first 100 species for training and use the remaining 100 species for testing. Cars196 <ref type="bibr" target="#b8">[9]</ref> has 16, 185 images from 196 car models, from which we take t tate of the art for the datasets CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref>. From these tables, we can first see that Triplet + FANNG si quality of the embedding. Using CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref>, we show that our proposed method achieves fast and more acc
6">[7,</ref><ref type="bibr" target="#b25">26]</ref>, which are an extension of the siamese network <ref type="bibr" target="#b0">[1]</ref>. Triplet networks are composed of three identical convolutio es a non-linear activation function (e.g., ReLU <ref type="bibr" target="#b13">[14]</ref>). Also in <ref type="bibr" target="#b0">(1)</ref>, note that f l = [f l,1 , ..., f l,n l ] represents an array
#b8">[9]</ref> datasets in order to assess the clustering quality and k nearest neighbour retrieval <ref type="bibr" target="#b7">[8]</ref>. Our proposed method combining triplet and global losses usi
target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar neighbour search that forms the basis of our proposed method. As pointed out by Shrivastava et al. <ref type="bibr" target="#b16">[17]</ref>, hard negative and positive mining is a relabeling of the
ich uses unseen classes from the CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref> datasets in order to assess the clustering quality and k nea h we take the first 100 species for training and use the remaining 100 species for testing. Cars196 <ref type="bibr" target="#b8">[9]</ref> has 16, 185 images from 196 car models, from which we take t tate of the art for the datasets CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref>. From these tables, we can first see that Triplet + FANNG si quality of the embedding. Using CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref>, we show that our proposed method achieves fast and more acc
get="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar
tive feature embedding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ incorporation of loss functions that take into account the global structure of the embedding space <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar Triplet + FANNG + Global + Adaptive significantly outperforms the current state of the art methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> in terms of cluster /ref>.</p><p>In this paper, we propose a novel deep metric learning approach that combines a global <ref type="bibr" target="#b9">[10]</ref> and a triplet loss <ref type="bibr" target="#b6">[7,</ref>< he development of approaches that explore the global structure of the embedding space. Kumar et al. <ref type="bibr" target="#b9">[10]</ref> proposed a global loss function that uses first and second d a loss function that optimises a global clustering quality metric (NMI). As shown by Kumar et al. <ref type="bibr" target="#b9">[10]</ref>, it appears that a combination of local and global losses c ade more robust with the introduction of a loss that explores the global structure of the embedding <ref type="bibr" target="#b9">[10]</ref>. In particular, the triplet loss in (2) can be extended wit (4) clustering <ref type="bibr" target="#b14">[15]</ref>, and (5) triplet combined with global loss <ref type="bibr" target="#b9">[10]</ref>. For the approaches (1), ( <ref type="formula" target="#for ="bibr" target="#b20">[21]</ref>.</p><p>For the experiments using triplet combined with global loss <ref type="bibr" target="#b9">[10]</ref> and for our proposed approach, we let the training procedur for batches comprised of only random triplets. Similar to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10]</ref>, we set the margin for the triplet and global loss to 0.2 an
rget="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. The main advantage can estimate feature embedding is based on triplet networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>, which are an extension of the siamese network <ref type="b ampling techniques <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref> that stochastically under-samples the set of triplets. Here <ref type="bibr" target="#b9">[10]</ref> and a triplet loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref> computed using training samples acquired from a smart sampl ples, and 2) what is the definition of challenging positive and negative samples.</p><p>Wang et al. <ref type="bibr" target="#b25">[26]</ref> described a way to build triplets based on a manual annota /1.0"><head n="3.">Proposed Method</head><p>We first describe the architecture of a triplet network <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" targ iv xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Triplet Networks</head><p>The triplet network <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" targ
type="bibr" target="#b5">6]</ref> is at the core of many recently proposed computer vision methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" targ
<p>The development of deep metric learning models for the estimation of effective feature embedding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target gative is. Positive samples are then chosen to guarantee a non-zero response from the loss function <ref type="bibr" target="#b1">(2)</ref>.</p><p>More formally, we define a smart negative as any nega
get="#b18">19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" tar rget="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref> and the loss function used in its training. Then, we descri rget="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref> is trained with an input composed of an anchor point x i (f
get="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar the embedding space <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>In this paper, we propose a novel deep metric learni s, but still relies on stochastic sampling of positive and negative samples. Ustinova and Lempitsky <ref type="bibr" target="#b23">[24]</ref> presented a loss function that minimises the integral of t uce the most effective embedding spaces, so we believe that the last two approaches mentioned above <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15]</ref> still have room fo
statistics, there are tens of millions of compounds available in compound and bioactivity databases <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" targe cade. In this sense, the prominent bioactivity and compound data resources can be listed as PubChem <ref type="bibr" target="#b0">[1]</ref>, ChEMBL <ref type="bibr" target="#b1">[2]</ref>, DrugBank <r and nearly 2700 of human proteins are known to be targeted by either approved or experimental drugs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. The 3D structure info
rpose specific data sets <ref type="bibr" target="#b189">[190]</ref>, often using the ZINC database <ref type="bibr" target="#b174">[175]</ref> as their resource. With the increased volume of open acc
f type="bibr" target="#b170">[171]</ref>, HMDB <ref type="bibr" target="#b173">[174]</ref> and T3DB <ref type="bibr" target="#b171">[172]</ref>.</p><p>Although the discussed databases have common prop
ref type="bibr" target="#b169">[170]</ref>, SIDER <ref type="bibr" target="#b172">[173]</ref>, DCDB <ref type="bibr" target="#b170">[171]</ref>, HMDB <ref type="bibr" target="#b173">[174]</ref> and T3
b238">239,</ref><ref type="bibr" target="#b239">240,</ref><ref type="bibr" target="#b249">250,</ref><ref type="bibr" target="#b254">255,</ref><ref type="bibr" target="#b255">256]</ref>. In some of the
rug pairs <ref type="bibr" target="#b10">[11]</ref> and the prediction of ATC codes for known drugs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. In addition, the methods are given below.</p><p>A supervised machine learning methodology was proposed by Liu et al. <ref type="bibr" target="#b11">[12]</ref> using a combination of both similarity and feature-based a
0">[121]</ref><ref type="bibr" target="#b121">[122]</ref><ref type="bibr" target="#b122">[123]</ref><ref type="bibr" target="#b123">[124]</ref>. Descriptors based on functional sites describe certain
f type="bibr" target="#b170">[171]</ref>, HMDB <ref type="bibr" target="#b173">[174]</ref> and T3DB <ref type="bibr" target="#b171">[172]</ref>.</p><p>Although the discussed databases have common prop
2">[113]</ref><ref type="bibr" target="#b113">[114]</ref><ref type="bibr" target="#b114">[115]</ref><ref type="bibr" target="#b115">[116]</ref><ref type="bibr" target="#b116">[117]</ref><ref type="bib
adable cross-platform software by integrating open-access screening tools using the Docker platform <ref type="bibr" target="#b40">[41]</ref>. Qiu et al. introduced the emergence of PCM and mentioned presentations/descriptors for computational drug discovery <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b212">213,</ref><ref type="bibr" t
the prediction of the aqueous solubility of compounds, which takes SMILES representations as input <ref type="bibr" target="#b253">[254]</ref>.</p><p>There are several review articles on deep learnin
side, where individual routing nodes may act strategically <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>. We instead consider consumer incentives and assume a singl
. Centralized traffic engineering (TE) techniques have been proposed to improve network utilization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> without affecting ineering with pricing. Pricing can guard existing TE techniques that improve WAN utilization (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>) against strategic ble portion of inter-datacenter transfers have deadlines, and can be modeled using this abstraction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>. For example, peri ase of our solution ( ?4.4). Other portions of the WAN traffic may not be governed by any TE scheme <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" ta rther, by using prices, SAM is mostly protected from strategic users ( ?4.2). Similar to prior work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, we execute SAM on that the bandwidth required for such requests is known a priori (e.g., from historical usage, as in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>), and is appropria fic engineering for datacenter WANs has been drawn recent attention from both industry and academia <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" ta for ad hoc high priority traffic; the volume to be set aside is estimated based on historical usage <ref type="bibr" target="#b17">[18]</ref>. When unexpected congestion occurs, perhaps because of mor 7">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>. SWAN <ref type="bibr" target="#b17">[18]</ref> and B4 <ref type="bibr" target="#b19">[20]</ref> aim to im
g and machine learning. The existence of optimal "market" prices is a classical result in economics <ref type="bibr" target="#b2">[3]</ref>, and given sufficient data it is possible to learn those pri
<ref type="bibr" target="#b20">[21]</ref>, oblivious routing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>, and finding suitable routing parameters for given protocol r" target="#b13">14]</ref>, and finding suitable routing parameters for given protocols (e.g., OSPF <ref type="bibr" target="#b13">[14]</ref>). Recent papers consider the objective of imposing fairnes
incorporate pricing for Internet congestion control (e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>) and inter-ISP tran
br" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>) and inter-ISP transit networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>. More recently, th
to the cloud, and utilizing capacity that is already paid for but would go unused otherwise (e.g., <ref type="bibr" target="#b6">[7]</ref>). It is reasonable to assume that bulk discounts would be le
ISPs through flexible pricing structures, leading to better utilization and user satisfaction. TUBE <ref type="bibr" target="#b15">[16]</ref> deals with pricing mobile data. TUBE uses time-dependent d
chniques have been proposed to improve network utilization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> without affecting low latency traffic and with explicit sup existing TE techniques that improve WAN utilization (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>) against strategic users. By quoting lower prices for more tected from strategic users ( ?4.2). Similar to prior work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, we execute SAM once every few minutes. This leaves enough ns of the WAN traffic may not be governed by any TE scheme <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. For example, there een drawn recent attention from both industry and academia <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>. SWAN <ref type="bi ref><ref type="bibr" target="#b23">24]</ref>. SWAN <ref type="bibr" target="#b17">[18]</ref> and B4 <ref type="bibr" target="#b19">[20]</ref> aim to improve the utilization of inter-DC WAN. However, t
based on dual pricing, and recent advances in combinatorial market design and statistical learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target time window, the price selection will be approximately optimal for the upcoming window as well. See <ref type="bibr" target="#b5">[6]</ref> and Theorem 52 of <ref type="bibr" target="#b18">[19]</ref> type="bibr" target="#b2">[3]</ref>, and given sufficient data it is possible to learn those prices <ref type="bibr" target="#b5">[6]</ref>. Hsu et al. <ref type="bibr" target="#b18">[19]</ref> point
oose 10% of the timesteps in a window). We address this issue by using sorting-network inequalities <ref type="bibr" target="#b24">[25]</ref>, which reduces the number of constraints to polynomial wit for the construction and proof. We note that our solution improves upon the techniques proposed in <ref type="bibr" target="#b24">[25]</ref> by requiring 40% fewer "sorting" constraints per link (det details in appendix). Furthermore, we provide a rigorous proof of correctness, which was missing in <ref type="bibr" target="#b24">[25]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head </p><p>x + y = m + M, m ? x, m ? y. Note this implies M ? max{x, y} and m ? min{x, y}. We note that <ref type="bibr" target="#b24">[25]</ref> uses five sorting constraints in their solution; hence, ou
t="#b13">[14]</ref>). Recent papers consider the objective of imposing fairness in a shared network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Traffic engi
ider the objective of imposing fairness in a shared network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Traffic engineering for datacenter WANs has been dra
<ref type="bibr" target="#b20">[21]</ref>, oblivious routing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>, and finding suitable routing parameters for given protocol r" target="#b13">14]</ref>, and finding suitable routing parameters for given protocols (e.g., OSPF <ref type="bibr" target="#b13">[14]</ref>). Recent papers consider the objective of imposing fairnes
ge pricing, to offload  delay-tolerant users in peak traffic periods. Similar ideas can be found in <ref type="bibr" target="#b34">[35]</ref>. Our work also incorporates time-varying prices, but our s
chniques have been proposed to improve network utilization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> without affecting low latency traffic and with explicit sup existing TE techniques that improve WAN utilization (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>) against strategic users. By quoting lower prices for more tected from strategic users ( ?4.2). Similar to prior work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, we execute SAM once every few minutes. This leaves enough ns of the WAN traffic may not be governed by any TE scheme <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. For example, there een drawn recent attention from both industry and academia <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>. SWAN <ref type="bi ref><ref type="bibr" target="#b23">24]</ref>. SWAN <ref type="bibr" target="#b17">[18]</ref> and B4 <ref type="bibr" target="#b19">[20]</ref> aim to improve the utilization of inter-DC WAN. However, t
r" target="#b29">30]</ref>) and inter-ISP transit networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>. More recently, there has been work on increasing profits o
g and machine learning. The existence of optimal "market" prices is a classical result in economics <ref type="bibr" target="#b2">[3]</ref>, and given sufficient data it is possible to learn those pri
nces in combinatorial market design and statistical learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19]</ref>. Second, Pretium plan used to guide online allocation. An alternative line of work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref> employs online learni
deadlines and demands, and the provider uses traffic engineering to accommodate multiple requests. <ref type="bibr" target="#b31">[32]</ref> proposes destination-based tiered pricing for transit ISPs
ow latency traffic and with explicit support for deadlines <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>. Such techniques crucially depend on detailed traffic infor lated to recent work on multi-timestep traffic engineering <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref> but extends it in two ways: (1) it considers potentially no
ISPs through flexible pricing structures, leading to better utilization and user satisfaction. TUBE <ref type="bibr" target="#b15">[16]</ref> deals with pricing mobile data. TUBE uses time-dependent d
challenged by their vulnerability to adversarial examples <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5]</ref>, which are crafted by adding small, human-imperceptible noise icted labels and probabilities of these images given by the Inception v3. more varied training data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>With the knowl >, adversarial training is the most extensively investigated way to increase the robustness of DNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe ef type="bibr" target="#b22">[23]</ref>, one-step gradient-based methods such as fast gradient sign <ref type="bibr" target="#b4">[5]</ref> and iterative variants of gradient-based methods <ref type=" t-based methods that iteratively perturb the input with the gradients to maximize the loss function <ref type="bibr" target="#b4">[5]</ref>, momentum-based methods accumulate a velocity vector in the e-box attacks and the transferability, and act as a stronger attack algorithm than one-step methods <ref type="bibr" target="#b4">[5]</ref> and vanilla iterative methods <ref type="bibr" target="#b8"> ply derived.</p><p>One-step gradient-based approaches, such as the fast gradient sign method (FGSM) <ref type="bibr" target="#b4">[5]</ref>, find an adversarial example x * by maximizing the loss func
</head><p>Deep neural networks (DNNs) are challenged by their vulnerability to adversarial examples <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5]</ref>, which are crafted b 8">[9]</ref>. In general, a more severe issue of adversarial examples is their good transferability <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta amples in the white-box manner, including optimization-based methods such as box-constrained L-BFGS <ref type="bibr" target="#b22">[23]</ref>, one-step gradient-based methods such as fast gradient sig " target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Optimization-based methods <ref type="bibr" target="#b22">[23]</ref> directly optimize the distance between the real and advers
rget="#b14">15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11]</ref>, adversarial traini
od transferability <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>, i.e., the adversarial examples crafted for one model remai
ssue of adversarial examples is their good transferability <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>, i.e., the adversar l example fools multiple models, it is more likely to remain adversarial for other black-box models <ref type="bibr" target="#b11">[12]</ref>. We show that the adversarial examples generated by the mo he data point. However in practice, the linear assumption may not hold when the distortion is large <ref type="bibr" target="#b11">[12]</ref>, which makes the adversarial example generated by FGSM "un tion that always fools these models and is more likely to transfer to other models at the same time <ref type="bibr" target="#b11">[12]</ref>, thus enabling powerful black-box attacks.</p><p>We propos For comparison, we also introduce two alternative ensemble schemes, one of which is already studied <ref type="bibr" target="#b11">[12]</ref>. Specifically, K models can be averaged in predictions <re ed <ref type="bibr" target="#b11">[12]</ref>. Specifically, K models can be averaged in predictions <ref type="bibr" target="#b11">[12]</ref> as p(x) = K k=1 w k p k (x), where p k (x) is the predicte ansferability comes from the fact that models learn similar decision boundaries around a data point <ref type="bibr" target="#b11">[12]</ref>. Although the decision boundaries are similar, they are un data point for a model (holes as shown in Fig. <ref type="figure" target="#fig_5">4&amp;5</ref> in <ref type="bibr" target="#b11">[12]</ref>), which are hard to transfer to other models. These region e inappropriate assumption of the linearity of the decision boundary when the perturbation is large <ref type="bibr" target="#b11">[12]</ref>. For the black-box attacks, although the success rates of
div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Defense methods</head><p>Among many attempts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" targ
x models, especially for those with a defense mechanism. For example, ensemble adversarial training <ref type="bibr" target="#b23">[24]</ref> significantly improves the robustness of deep neural netwo for multiple models, can successfully fool robust models obtained by ensemble adversarial training <ref type="bibr" target="#b23">[24]</ref> in the blackbox manner. The findings in this paper raise n generation of adversarial examples and the parameters being trained. Ensemble adversarial training <ref type="bibr" target="#b23">[24]</ref> augments the training data with the adversarial samples pr ree of which are trained by ensemble adversarial training-Inc-v3 ens3 , Inc-v3 ens4 , IncRes-v2 ens <ref type="bibr" target="#b23">[24]</ref>. We will simply call the last three models as "adversarial than one-step methods at the cost of worse transferability <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Optimization-based methods <ref type="bibr" target=" he robustness of DNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref>. By injecting adversarial examples into the training proced target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
vious gradients helps to barrel through narrow valleys, small humps and poor local minima or maxima <ref type="bibr" target="#b3">[4]</ref>. The momentum method also shows its effectiveness in stochas
s for enhancing the performance and improving the robustness <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2]</ref>. The idea of ensemble c
images given by the Inception v3. more varied training data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>With the knowledge of the structure and parameters of dversarial examples generated by optimization-based and iterative methods have poor transferability <ref type="bibr" target="#b9">[10]</ref>, and thus make black-box attacks less effective. On the oth sferable adversarial examples, however they usually have a low success rate for the white-box model <ref type="bibr" target="#b9">[10]</ref>, making it ineffective for blackbox attacks. Given the diff ble of Inc-v3, Inc-v4, IncRes-v2, Res-152, Inc-v3 ens3 , Inc-v3 ens4 , IncRes-v2 ens and Inc-v3 adv <ref type="bibr" target="#b9">[10]</ref>. We adopt the ensemble in logits scheme. The ensemble weigh ethods are stronger whitebox adversaries than one-step methods at the cost of worse transferability <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Optimization rget="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" targ ensively investigated way to increase the robustness of DNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref>. By injecting advers
nerated by the proposed momentum iterative fast gradient sign method (MI-FGSM) for the Inception v3 <ref type="bibr" target="#b21">[22]</ref> model. Left column: the original images. Middle column: th tup</head><p>We study seven models, four of which are normally trained models-Inception v3 (Inc-v3) <ref type="bibr" target="#b21">[22]</ref>, Inception v4 (Inc-v4), Inception Resnet v2 (IncRes-v2) <r
primarily focused on intra-line compression to minimize decompression latency. Even the recent work <ref type="bibr" target="#b24">[24]</ref> which does compress across cache lines, is optimized for s er metric to measure area overhead is dictionary size. C-Pack in Adaptive and Decoupled requires    <ref type="bibr" target="#b24">[24]</ref>. As MORC allocates 512-bytes for each compression and deco type="bibr" target="#b18">[18]</ref>, Decoupled <ref type="bibr" target="#b19">[19]</ref>, and SC2 <ref type="bibr" target="#b24">[24]</ref> compressed caches. These schemes are evaluated with perfec cient than larger uncompressed caches thanks to significantly lower static and dynamic energy power <ref type="bibr" target="#b24">[24]</ref>. Figure <ref type="figure" target="#fig_15">9a</ref> compa ype="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22]</ref>. While one recent scheme <ref type="bibr" target="#b24">[24]</ref> compresses inter-line, it still prioritizes single-stream type="bibr" target="#b18">[18]</ref>, Decoupled <ref type="bibr" target="#b19">[19]</ref>, and SC2 <ref type="bibr" target="#b24">[24]</ref>.</p><p>Adaptive <ref type="bibr" target="#b18">[18]</ref> , but is designed to be easier to implement <ref type="bibr" target="#b42">[42]</ref>. Finally, SC2 <ref type="bibr" target="#b24">[24]</ref> is most similar to MORC because it maintains a system-wide
" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target=
lar compression performance.</p><p>Memory link compression <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b45">45]</ref> is complementary to cache compression. MORC does not compre
lar compression performance.</p><p>Memory link compression <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b45">45]</ref> is complementary to cache compression. MORC does not compre
ditional inputs are indicated by an underscore and number.</p><p>We run a set of publicly available <ref type="bibr" target="#b36">[36]</ref> profiled representative regions <ref type="bibr" target="#
nable to keep pace with transistor scaling. Due to the Operation Energy Scale 64b comparison (65nm) <ref type="bibr" target="#b12">[12]</ref> 2pJ 1x 64b access 128KB SRAM (32nm) <ref type="bibr" targe
6]</ref>, hardware <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b43">43]</ref>). LBE is loosely based on C-Pack, but is optimized for inte
6]</ref>, hardware <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b43">43]</ref>). LBE is loosely based on C-Pack, but is optimized for inte
ughput-oriented computing is becoming increasingly important <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>. Emerging examples of throughput computing occur in the enter
64b comparison (65nm) <ref type="bibr" target="#b12">[12]</ref> 2pJ 1x 64b access 128KB SRAM (32nm) <ref type="bibr" target="#b13">[13]</ref> 4pJ 2x 64b floating point op (45nm) <ref type="bibr" targe ly increase the size eightfold to .08mm 2 . For reference, a 16-way 256KB cache in 32nm is 2.12mm 2 <ref type="bibr" target="#b13">[13]</ref>.</p><p>Another metric to measure area overhead is dictiona ss; access energy numbers are per cache line. For SRAM and caches we use the 32nm models from CACTI <ref type="bibr" target="#b13">[13]</ref> L1  <ref type="bibr" target="#b17">[17]</ref>, assuming a
ess 128KB SRAM (32nm) <ref type="bibr" target="#b13">[13]</ref> 4pJ 2x 64b floating point op (45nm) <ref type="bibr" target="#b14">[14]</ref> 45pJ 22.5x 64b transfer across 15mm on-chip <ref type="bib
on-based models <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b52">[43]</ref> widely used in information retrieval. The interaction-base d by each person to 100.</p><p>The hyper-parameters of the RBF kernel functions are set the same as <ref type="bibr" target="#b52">[43]</ref>. We use 11 RBF kernels, with the hyper-parameters m = f1;
ome well-known academic websites such as Scopus <ref type="bibr" target="#b29">[28]</ref> CiteSeerX <ref type="bibr" target="#b54">[45]</ref>, Web of Science <ref type="bibr" target="#b0">[1]</ref> an g 1013 models <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b49">[41]</ref>, <ref type="bibr" target="#b54">[45]</ref>, <ref type="bibr" target="#b57">[47]</ref> and then adopt >,</p><p>1015 <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b49">[41]</ref>, <ref type="bibr" target="#b54">[45]</ref>, <ref type="bibr" target="#b57">[47]</ref>, K-means <ref t
even if they are semantically similar. On the other hand, recently, some representationbased models <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b14">[14]</ref> can successfull ct and soft matches, we adopt the interaction-based models <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b52">[43]</ref> widely used in ERT <ref type="bibr" target="#b6">[6]</ref>.</p><p>Aggregation Function. For sentence matching, CNN <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b25">[25]</ref> and RNN <ref ty

>601GBDT.</head><label></label><figDesc>Is a widely used model to solve KDD Cup 2013 602 challenge-1<ref type="bibr" target="#b7">[7]</ref>,<ref type="bibr" target="#b19">[19]</ref>,<ref type="bibr" t t person exists was not considered and all the participations devoted to featureengineering methods <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b60">[48]</ref>.</p><p>Entity Lin

nts the embedding of the jth token in the candidate person c, which can be pre-trained by Word2-Vec <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b23">[23]</ref> or BERT <ref ty
andidate person c, which can be pre-trained by Word2-Vec <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b23">[23]</ref> or BERT <ref type="bibr" target="#b6">[6]</ref>.</p><p>Agg
ther fields. Besides, each person 99 publishes multiple papers, which also take different effects.  <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b34">[32]</ref> or if the top mat m is widely studied in entity linking. The main solutions usually include the NIL threshold methods <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b34">[32]</ref> predicting the me
ther fields. Besides, each person 99 publishes multiple papers, which also take different effects.  <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b34">[32]</ref> or if the top mat m is widely studied in entity linking. The main solutions usually include the NIL threshold methods <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b34">[32]</ref> predicting the me
all the papers into several disjoint  engineering methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b38">[35]</ref>, <ref type="bib -means <ref type="bibr" target="#b47">[40]</ref>, <ref type="bibr" target="#b61">[49]</ref>, DBSCAN <ref type="bibr" target="#b13">[13]</ref> or 1016 semi-supervised clustering <ref type="bibr" target
n, that were applied to short-text summarization. We propose a novel variant of the coverage vector <ref type="bibr" target="#b25">(Tu et al., 2016)</ref> from Neural Machine Translation, which we use d n="2.3">Coverage mechanism</head><p>Repetition is a common problem for sequenceto-sequence models <ref type="bibr" target="#b25">(Tu et al., 2016;</ref><ref type="bibr" target="#b14">Mi et al., 2016 erating multi-sentence text (see Figure <ref type="figure">1</ref>). We adapt the coverage model of <ref type="bibr" target="#b25">Tu et al. (2016)</ref> to solve the problem. In our coverage model, w ine Translation <ref type="bibr" target="#b10">(Koehn, 2009)</ref>, coverage was adapted for NMT by <ref type="bibr" target="#b25">Tu et al. (2016)</ref> and <ref type="bibr" target="#b14">Mi et al. (
has made abstractive summarization viable <ref type="bibr" target="#b3">(Chopra et al., 2016;</ref><ref type="bibr" target="#b17">Nallapati et al., 2016;</ref><ref type="bibr" target="#b20">Rush et a arget="#b7">Gulcehre et al., 2016;</ref><ref type="bibr" target="#b15">Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b17">Nallapati et al., 2016;</ref><ref type="bibr" target="#b28">Zeng et a cently-introduced CNN/ Daily Mail dataset <ref type="bibr" target="#b8">(Hermann et al., 2015;</ref><ref type="bibr" target="#b17">Nallapati et al., 2016)</ref>, which contains news articles (39 sente head><p>We use the CNN/Daily Mail dataset <ref type="bibr" target="#b8">(Hermann et al., 2015;</ref><ref type="bibr" target="#b17">Nallapati et al., 2016)</ref>, which contains online news articles (7 ad n="2.1">Sequence-to-sequence attentional model</head><p>Our baseline model is similar to that of <ref type="bibr" target="#b17">Nallapati et al. (2016)</ref>, and is depicted in Figure <ref type="f e on those datasets.</p><p>However, large-scale datasets for summarization of longer text are rare. <ref type="bibr" target="#b17">Nallapati et al. (2016)</ref> adapted the DeepMind question-answering considerably different from that of <ref type="bibr" target="#b7">Gulcehre et al. (2016)</ref> and <ref type="bibr" target="#b17">Nallapati et al. (2016)</ref>. Those works train their pointer compon with multi-sentence summaries (3.75 sentences or 56 tokens on average). We used scripts supplied by <ref type="bibr" target="#b17">Nallapati et al. (2016)</ref> to obtain the same version of the the d and b ptr in equation 8), and coverage adds 512 extra parameters (w c in equation 11).</p><p>Unlike <ref type="bibr" target="#b17">Nallapati et al. (2016)</ref>, we do not pretrain the word embeddings is (+1.1 ROUGE-1, +2.0 ROUGE-2, +1.1 ROUGE-L) points respectively, and our best model scores exceed <ref type="bibr" target="#b17">Nallapati et al. (2016)</ref> by (+4.07 ROUGE-1, +3.98 ROUGE-2, +3.73 scene. (...) Summary: more questions than answers emerge in controversial s.c. police shooting. of <ref type="bibr" target="#b17">Nallapati et al. (2016)</ref> by several ROUGE points. Despite the br g Representations <ref type="bibr" target="#b24">(Takase et al., 2016)</ref>, hierarchical networks <ref type="bibr" target="#b17">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type="b nique that has been applied to <ref type="bibr">NMT (Sankaran et al., 2016)</ref> and summarization <ref type="bibr" target="#b17">(Nallapati et al., 2016)</ref>. In this approach, each attention dist rget="#tab_2">1</ref>), compared to the smaller boost given by temporal attention for the same task <ref type="bibr" target="#b17">(Nallapati et al., 2016)</ref>.</p></div> <div xmlns="http://www.tei- he first three sentences of the article as a summary), and compare to the only existing abstractive <ref type="bibr" target="#b17">(Nallapati et al., 2016)</ref> and extractive <ref type="bibr" target training pairs, 13,368 validation pairs and 11,490 test pairs. Both the dataset's published results <ref type="bibr" target="#b17">(Nallapati et al., 2016</ref><ref type="bibr" target="#b16">(Nallapat le online. <ref type="foot" target="#foot_5">6</ref>Given that we generate plain-text summaries but <ref type="bibr" target="#b17">Nallapati et al. (2016;</ref><ref type="bibr" target="#b16">2017)</re
<p>Our hybrid pointer-generator network facilitates copying words from the source text via pointing <ref type="bibr" target="#b26">(Vinyals et al., 2015)</ref>, which improves accuracy and handling of twork</head><p>Our pointer-generator network is a hybrid between our baseline and a pointer network <ref type="bibr" target="#b26">(Vinyals et al., 2015)</ref>, as it allows both copying words via poi plore sentence fusion using dependency trees.</p><p>Pointer-generator networks. The pointer network <ref type="bibr" target="#b26">(Vinyals et al., 2015)</ref> is a sequence-tosequence model that uses
f><ref type="bibr" target="#b14">Mi et al., 2016;</ref><ref type="bibr">Sankaran et al., 2016;</ref><ref type="bibr" target="#b24">Suzuki and Nagata, 2016)</ref>, and is especially pronounced when gen ecoders <ref type="bibr" target="#b3">(Chopra et al., 2016)</ref>, Abstract Meaning Representations <ref type="bibr" target="#b24">(Takase et al., 2016)</ref>, hierarchical networks <ref type="bibr" t
r" target="#b27">Xu et al. (2015)</ref>, who apply a coverage-like method to image cap-tioning, and <ref type="bibr" target="#b1">Chen et al. (2016)</ref>, who also incorporate a coverage mechanism (w

to the difficulty of abstractive summarization, the great majority of past work has been extractive <ref type="bibr" target="#b11">(Kupiec et al., 1995;</ref><ref type="bibr" target="#b18">Paice, 1990
d approaches for NMT <ref type="bibr" target="#b7">(Gulcehre et al., 2016)</ref>, language modeling <ref type="bibr" target="#b13">(Merity et al., 2016)</ref>, and summarization <ref type="bibr" targe
n distributions to obtain the coverage vector -suffices. In this respect our approach is similar to <ref type="bibr" target="#b27">Xu et al. (2015)</ref>, who apply a coverage-like method to image cap
f><ref type="bibr" target="#b14">Mi et al., 2016;</ref><ref type="bibr">Sankaran et al., 2016;</ref><ref type="bibr" target="#b24">Suzuki and Nagata, 2016)</ref>, and is especially pronounced when gen ecoders <ref type="bibr" target="#b3">(Chopra et al., 2016)</ref>, Abstract Meaning Representations <ref type="bibr" target="#b24">(Takase et al., 2016)</ref>, hierarchical networks <ref type="bibr" t
r" target="#b27">Xu et al. (2015)</ref>, who apply a coverage-like method to image cap-tioning, and <ref type="bibr" target="#b1">Chen et al. (2016)</ref>, who also incorporate a coverage mechanism (w
ents from the input sequence. The pointer network has been used to create hybrid approaches for NMT <ref type="bibr" target="#b7">(Gulcehre et al., 2016)</ref>, language modeling <ref type="bibr" targ (Merity et al., 2016)</ref>, and summarization <ref type="bibr" target="#b6">(Gu et al., 2016;</ref><ref type="bibr" target="#b7">Gulcehre et al., 2016;</ref><ref type="bibr" target="#b15">Miao and Bl ple occurrences of it in the source text.</p><p>Our approach is considerably different from that of <ref type="bibr" target="#b7">Gulcehre et al. (2016)</ref> and <ref type="bibr" target="#b17">Nallap
br" target="#b15">(Miao and Blunsom, 2016)</ref>, and direct optimization of the performance metric <ref type="bibr" target="#b19">(Ranzato et al., 2016)</ref>, further improving performance on those
target="#b3">(Chopra et al., 2016;</ref><ref type="bibr" target="#b17">Nallapati et al., 2016;</ref><ref type="bibr" target="#b20">Rush et al., 2015;</ref><ref type="bibr" target="#b28">Zeng et al., 2
ive and abstractive approaches, is similar to <ref type="bibr">Gu et al.'s (2016)</ref> CopyNet and <ref type="bibr" target="#b15">Miao and Blunsom's (2016)</ref> Forced-Attention Sentence Compression al networks <ref type="bibr" target="#b17">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type="bibr" target="#b15">(Miao and Blunsom, 2016)</ref>, and direct optimization of the perfor bibr" target="#b6">(Gu et al., 2016;</ref><ref type="bibr" target="#b7">Gulcehre et al., 2016;</ref><ref type="bibr" target="#b15">Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b17">Nallapati al., 2016)</ref>.</p><p>Our approach is close to the Forced-Attention Sentence Compression model of <ref type="bibr" target="#b15">Miao and Blunsom (2016)</ref> and the CopyNet model of <ref type="bib
vided the first abstractive baselines. The same authors then published a neural extractive approach <ref type="bibr" target="#b16">(Nallapati et al., 2017)</ref>, which uses hierarchical RNNs to selec y existing abstractive <ref type="bibr" target="#b17">(Nallapati et al., 2016)</ref> and extractive <ref type="bibr" target="#b16">(Nallapati et al., 2017)</ref> models on the full dataset. The output es not quite surpass the ROUGE scores of the lead-3 baseline, nor the current best extractive model <ref type="bibr" target="#b16">(Nallapati et al., 2017)</ref>. We discuss this issue in section 7.1. s. Both the dataset's published results <ref type="bibr" target="#b17">(Nallapati et al., 2016</ref><ref type="bibr" target="#b16">(Nallapati et al., , 2017) )</ref> use the anonymized version of the at we generate plain-text summaries but <ref type="bibr" target="#b17">Nallapati et al. (2016;</ref><ref type="bibr" target="#b16">2017)</ref> generate anonymized summaries (see Section 4), our ROUGE
br" target="#b15">(Miao and Blunsom, 2016)</ref>, and direct optimization of the performance metric <ref type="bibr" target="#b19">(Ranzato et al., 2016)</ref>, further improving performance on those
majority of past work has been extractive <ref type="bibr" target="#b11">(Kupiec et al., 1995;</ref><ref type="bibr" target="#b18">Paice, 1990;</ref><ref type="bibr" target="#b21">Saggion and Poibeau,
majority of past work has been extractive <ref type="bibr" target="#b11">(Kupiec et al., 1995;</ref><ref type="bibr" target="#b18">Paice, 1990;</ref><ref type="bibr" target="#b21">Saggion and Poibeau,
ive and abstractive approaches, is similar to <ref type="bibr">Gu et al.'s (2016)</ref> CopyNet and <ref type="bibr" target="#b15">Miao and Blunsom's (2016)</ref> Forced-Attention Sentence Compression al networks <ref type="bibr" target="#b17">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type="bibr" target="#b15">(Miao and Blunsom, 2016)</ref>, and direct optimization of the perfor bibr" target="#b6">(Gu et al., 2016;</ref><ref type="bibr" target="#b7">Gulcehre et al., 2016;</ref><ref type="bibr" target="#b15">Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b17">Nallapati al., 2016)</ref>.</p><p>Our approach is close to the Forced-Attention Sentence Compression model of <ref type="bibr" target="#b15">Miao and Blunsom (2016)</ref> and the CopyNet model of <ref type="bib
to the difficulty of abstractive summarization, the great majority of past work has been extractive <ref type="bibr" target="#b11">(Kupiec et al., 1995;</ref><ref type="bibr" target="#b18">Paice, 1990
ents from the input sequence. The pointer network has been used to create hybrid approaches for NMT <ref type="bibr" target="#b7">(Gulcehre et al., 2016)</ref>, language modeling <ref type="bibr" targ (Merity et al., 2016)</ref>, and summarization <ref type="bibr" target="#b6">(Gu et al., 2016;</ref><ref type="bibr" target="#b7">Gulcehre et al., 2016;</ref><ref type="bibr" target="#b15">Miao and Bl ple occurrences of it in the source text.</p><p>Our approach is considerably different from that of <ref type="bibr" target="#b7">Gulcehre et al. (2016)</ref> and <ref type="bibr" target="#b17">Nallap
f><ref type="bibr" target="#b14">Mi et al., 2016;</ref><ref type="bibr">Sankaran et al., 2016;</ref><ref type="bibr" target="#b24">Suzuki and Nagata, 2016)</ref>, and is especially pronounced when gen ecoders <ref type="bibr" target="#b3">(Chopra et al., 2016)</ref>, Abstract Meaning Representations <ref type="bibr" target="#b24">(Takase et al., 2016)</ref>, hierarchical networks <ref type="bibr" t
target="#b3">(Chopra et al., 2016;</ref><ref type="bibr" target="#b17">Nallapati et al., 2016;</ref><ref type="bibr" target="#b20">Rush et al., 2015;</ref><ref type="bibr" target="#b28">Zeng et al., 2
target="#b3">(Chopra et al., 2016;</ref><ref type="bibr" target="#b17">Nallapati et al., 2016;</ref><ref type="bibr" target="#b20">Rush et al., 2015;</ref><ref type="bibr" target="#b28">Zeng et al., 2
ive and abstractive approaches, is similar to <ref type="bibr">Gu et al.'s (2016)</ref> CopyNet and <ref type="bibr" target="#b15">Miao and Blunsom's (2016)</ref> Forced-Attention Sentence Compression al networks <ref type="bibr" target="#b17">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type="bibr" target="#b15">(Miao and Blunsom, 2016)</ref>, and direct optimization of the perfor bibr" target="#b6">(Gu et al., 2016;</ref><ref type="bibr" target="#b7">Gulcehre et al., 2016;</ref><ref type="bibr" target="#b15">Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b17">Nallapati al., 2016)</ref>.</p><p>Our approach is close to the Forced-Attention Sentence Compression model of <ref type="bibr" target="#b15">Miao and Blunsom (2016)</ref> and the CopyNet model of <ref type="bib
ted by the decoder), and has decoder state s t . The attention distribution a t is calculated as in <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>:</p><formula xml:id="formula_0">e t i = v als et al., 2015)</ref> is a sequence-tosequence model that uses the soft attention distribution of <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> to produce an output sequence consisting
ted by the decoder), and has decoder state s t . The attention distribution a t is calculated as in <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>:</p><formula xml:id="formula_0">e t i = v als et al., 2015)</ref> is a sequence-tosequence model that uses the soft attention distribution of <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> to produce an output sequence consisting
pe="bibr" target="#b11">(Kupiec et al., 1995;</ref><ref type="bibr" target="#b18">Paice, 1990;</ref><ref type="bibr" target="#b21">Saggion and Poibeau, 2013)</ref>. However, the recent success of sequ
pe="bibr" target="#b11">(Kupiec et al., 1995;</ref><ref type="bibr" target="#b18">Paice, 1990;</ref><ref type="bibr" target="#b21">Saggion and Poibeau, 2013)</ref>. However, the recent success of sequ
pyrouge package. <ref type="foot" target="#foot_3">4</ref> We also evaluate with the METEOR metric <ref type="bibr" target="#b4">(Denkowski and Lavie, 2014)</ref>, both in exact match mode (rewarding
d approaches for NMT <ref type="bibr" target="#b7">(Gulcehre et al., 2016)</ref>, language modeling <ref type="bibr" target="#b13">(Merity et al., 2016)</ref>, and summarization <ref type="bibr" targe
common problem for sequenceto-sequence models <ref type="bibr" target="#b25">(Tu et al., 2016;</ref><ref type="bibr" target="#b14">Mi et al., 2016;</ref><ref type="bibr">Sankaran et al., 2016;</ref><r 9)</ref>, coverage was adapted for NMT by <ref type="bibr" target="#b25">Tu et al. (2016)</ref> and <ref type="bibr" target="#b14">Mi et al. (2016)</ref>, who both use a GRU to update the coverage vec
veral techniques for nonsequential instruction prefetching <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targe
rget="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" targe
target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. Their results show r from frequent context switches, causing significant increases in the instruction cache miss rates <ref type="bibr" target="#b10">[11]</ref>. Lo et al. <ref type="bibr" target="#b11">[12]</ref> also
arget="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. Thus the key challenge in improving the performance of mem orkloads with a large main memory buffer pool is memory bound rather than I/O bound. Shatdal et al. <ref type="bibr" target="#b19">[20]</ref> proposed cache-conscious performance tuning techniques tha
al features on DBMSs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targ
a subset of CPU-2000 benchmarks and a database workload that consists of a subset of the Wisconsin <ref type="bibr" target="#b2">[3]</ref> and TPC-H <ref type="bibr" target="#b7">[8]</ref> queries.</ a database workload that consists of eight queries (1 through 7 and 9) from the Wisconsin benchmark <ref type="bibr" target="#b2">[3]</ref>, and five queries (1,2,3,5, and 6) from the TPC-H benchmark
systems with their large code and data footprints suffer significantly from poor cache performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target y recently that researchers have examined the performance impact of architectural features on DBMSs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ ds, the instruction cache miss rate is nearly three times the data cache miss rate. Ailamaki et al. <ref type="bibr" target="#b0">[1]</ref> analyzed three commercial DBMSs on a Xeon processor and show
rget="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" targ
ure" target="#fig_1">2</ref> shows a segment of a call graph for adding a record to a file in SHORE <ref type="bibr" target="#b5">[6]</ref>. SHORE is a storage manager that provides storage volumes, B ness of CGP we implemented a subset of the relational operators on top of the SHORE storage manager <ref type="bibr" target="#b5">[6]</ref>. SHORE is a fully functional storage manager which has been
a subset of CPU-2000 benchmarks and a database workload that consists of a subset of the Wisconsin <ref type="bibr" target="#b2">[3]</ref> and TPC-H <ref type="bibr" target="#b7">[8]</ref> queries.</ a database workload that consists of eight queries (1 through 7 and 9) from the Wisconsin benchmark <ref type="bibr" target="#b2">[3]</ref>, and five queries (1,2,3,5, and 6) from the TPC-H benchmark
target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. Thus the key chall s have proposed several techniques to improve the I/O bottleneck of database systems. Nyberg et al. <ref type="bibr" target="#b14">[15]</ref> suggested that if data intensive applications use software
ome video generation methods have dealt with this problem by generating the entire sequence at once <ref type="bibr" target="#b24">[25]</ref> or in small batches <ref type="bibr" target="#b19">[20]</r d to handle videos <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Straight-for ibr" target="#b23">24]</ref>.</p><p>Straight-forward adaptations of GANs for videos are proposed in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20]</ref>, replacing the 2D
enerating the entire sequence at once <ref type="bibr" target="#b24">[25]</ref> or in small batches <ref type="bibr" target="#b19">[20]</ref>. However, this introduces a lag in the generation process, to capture temporal dependencies but requires fixed length videos. This limitation was overcome in <ref type="bibr" target="#b19">[20]</ref> but constraints need to be imposed in the latent space to ght-forward adaptations of GANs for videos are proposed in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20]</ref>, replacing the 2D convolutional layers with 3D convolutiona
on. The frame sharpness is evaluated using the cumulative probability blur detection (CPBD) measure <ref type="bibr" target="#b16">[17]</ref>, which determines blur based on the presence of edges in t
strided transposed convolutions to produce the video frames from the latent representation. A U-Net <ref type="bibr" target="#b18">[19]</ref> architecture is used with skip connections between the Ide
and ReLU activations. The initial convolutional layer starts with a large kernel, as recommended in <ref type="bibr" target="#b6">[7]</ref>, which helps limit the depth of the network while ensuring t
aintained throughout the sequence. Evaluation is performed in a subject independent way on the GRID <ref type="bibr" target="#b5">[6]</ref> and TCD TIMIT <ref type="bibr" target="#b10">[11]</ref> data
ures (e.g. landmarks, visemes) and using computer graphics (CG) methods to generate realistic faces <ref type="bibr" target="#b11">[12]</ref>. Some methods avoid the use of CG by selecting frames from or re-targeting steps to adapt to new faces.</p><p>Convolutional neural networks (CNN) are used in <ref type="bibr" target="#b11">[12]</ref> to transform audio features to 3D meshes of a specific per
i-c.org/ns/1.0"><head n="2.2">GAN-Based Video Synthesis</head><p>The recent introduction of GANs in <ref type="bibr" target="#b9">[10]</ref> has shifted the focus of the machine learning community to
ures (e.g. landmarks, visemes) and using computer graphics (CG) methods to generate realistic faces <ref type="bibr" target="#b11">[12]</ref>. Some methods avoid the use of CG by selecting frames from or re-targeting steps to adapt to new faces.</p><p>Convolutional neural networks (CNN) are used in <ref type="bibr" target="#b11">[12]</ref> to transform audio features to 3D meshes of a specific per
><p>Subject independent approaches have been proposed that transform audio features to video frames <ref type="bibr" target="#b4">[5]</ref> but there is still no method to directly transform raw audio rating natural facial expressions. Some methods generate frames based solely on present information <ref type="bibr" target="#b4">[5]</ref>, without taking into account the facial dynamics. This makes for capturing articulation dynamics and estimating the 3D points of the mesh. Finally, Chung et al. <ref type="bibr" target="#b4">[5]</ref> proposed a CNN applied on Mel-frequency cepstral coefficient works that are closest to ours are those proposed in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b4">[5]</ref>. The former method is subject dependent and requires a large static method that produces video frames using a sliding window of audio samples like that used in <ref type="bibr" target="#b4">[5]</ref>. This is a GAN-based method that uses a combination of an L rator but also due to the use of the conditional Sequence Discriminator. Unlike previous approaches <ref type="bibr" target="#b4">[5]</ref> that prohibit the generation of facial expressions, the adve
was used to recover the most likely mouth shapes for a speech signal. A similar approach is used in <ref type="bibr" target="#b25">[26]</ref> to estimate the sequence of lip parameters. Finally, the V
) and GANs in its generating network and a 3D CNN as a sequence discriminator. Finally, Chen et al. <ref type="bibr" target="#b3">[4]</ref> propose a GAN-based encoder-decoder architecture that uses C
pendent. Similar deep architectures based on recurrent neural networks (RNNs) have been proposed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>, producing realistic
st of these methods aim at performing a featureto-feature translation. A typical example of this is <ref type="bibr" target="#b22">[23]</ref>, which uses a deep neural network (DNN) to transform a pho
dden Markov models (HMMs) to capture the dynamics of the video and speech sequences. Simons and Cox <ref type="bibr" target="#b20">[21]</ref> used vector quantization to achieve a compact representati
i-c.org/ns/1.0"><head n="2.2">GAN-Based Video Synthesis</head><p>The recent introduction of GANs in <ref type="bibr" target="#b9">[10]</ref> has shifted the focus of the machine learning community to
and ReLU activations. The initial convolutional layer starts with a large kernel, as recommended in <ref type="bibr" target="#b6">[7]</ref>, which helps limit the depth of the network while ensuring t
aintained throughout the sequence. Evaluation is performed in a subject independent way on the GRID <ref type="bibr" target="#b5">[6]</ref> and TCD TIMIT <ref type="bibr" target="#b10">[11]</ref> data
><p>Subject independent approaches have been proposed that transform audio features to video frames <ref type="bibr" target="#b4">[5]</ref> but there is still no method to directly transform raw audio rating natural facial expressions. Some methods generate frames based solely on present information <ref type="bibr" target="#b4">[5]</ref>, without taking into account the facial dynamics. This makes for capturing articulation dynamics and estimating the 3D points of the mesh. Finally, Chung et al. <ref type="bibr" target="#b4">[5]</ref> proposed a CNN applied on Mel-frequency cepstral coefficient works that are closest to ours are those proposed in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b4">[5]</ref>. The former method is subject dependent and requires a large static method that produces video frames using a sliding window of audio samples like that used in <ref type="bibr" target="#b4">[5]</ref>. This is a GAN-based method that uses a combination of an L rator but also due to the use of the conditional Sequence Discriminator. Unlike previous approaches <ref type="bibr" target="#b4">[5]</ref> that prohibit the generation of facial expressions, the adve
features and a weak coupling between head motion and the fundamental frequency of the speech signal <ref type="bibr" target="#b27">[28]</ref>.</p><p>Some of the earliest methods for facial animation r
) and GANs in its generating network and a 3D CNN as a sequence discriminator. Finally, Chen et al. <ref type="bibr" target="#b3">[4]</ref> propose a GAN-based encoder-decoder architecture that uses C
features and a weak coupling between head motion and the fundamental frequency of the speech signal <ref type="bibr" target="#b27">[28]</ref>.</p><p>Some of the earliest methods for facial animation r
et="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>[8] <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta d 100003FA, which is often observed and utilized for prediction in the Markov prefetching algorithm <ref type="bibr" target="#b9">[10]</ref> . The following section discusses data prefetching methodol strides are recognizable. To capture repetitiveness in data reference addresses, Markov prefetching <ref type="bibr" target="#b9">[10]</ref> was proposed. This strategy assumes the history might repea
target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>[8] <ref type="bibr" target="#b9">[10]</ref><ref type="bibr"
4">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>[19] <ref type="bibr" target="#b22">[23]</ref> . These studies concluded that prefetching is a promising p><p>Other recent efforts in hardware prefetching include Zhou's dual-core execution (DCE) approach <ref type="bibr" target="#b22">[23]</ref> , Ganusov et al's future execution (FE) approach <ref type
lti-Level Difference Table (MLDT) prediction algorithm is such a representative aggressive strategy <ref type="bibr" target="#b20">[21]</ref> . This prediction strategy forms a difference table of dep n (FE) approach <ref type="bibr" target="#b7">[8]</ref> , Sun et al's data push server architecture <ref type="bibr" target="#b20">[21]</ref> and Solihin et al.'s memory-side prefetching <ref type="bi
3.2">SPEC CPU2000 Benchmark Simulation</head><p>We conducted several sets of SPEC CPU2000 benchmark <ref type="bibr" target="#b23">[24]</ref> simulation for performance evaluation. Twenty-one of the t
ng area. Data prefetching is frequently classified as software prefetching and hardware prefetching <ref type="bibr" target="#b21">[22]</ref> . Software prefetching instruments prefetch instructions t uch examples. The techniques include simple prefetching, unrolling the loop and software pipelining <ref type="bibr" target="#b21">[22]</ref> . Software prefetching is usually used for large amount of
t="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" t
and many strategies have been proposed for data prefetching <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" targe pattern is a general case of stride pattern. However, the conventional stride prefetching approach <ref type="bibr" target="#b2">[3]</ref> is unable to detect it without the DAHC support. This exampl ic defined to characterize a program's spatial locality at runtime. The stride prefetching approach <ref type="bibr" target="#b2">[3]</ref> observes the pattern among strides of past accesses and thus
3.2">SPEC CPU2000 Benchmark Simulation</head><p>We conducted several sets of SPEC CPU2000 benchmark <ref type="bibr" target="#b23">[24]</ref> simulation for performance evaluation. Twenty-one of the t
approach <ref type="bibr" target="#b22">[23]</ref> , Ganusov et al's future execution (FE) approach <ref type="bibr" target="#b7">[8]</ref> , Sun et al's data push server architecture <ref type="bibr"
4">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>[19] <ref type="bibr" target="#b22">[23]</ref> . These studies concluded that prefetching is a promising p><p>Other recent efforts in hardware prefetching include Zhou's dual-core execution (DCE) approach <ref type="bibr" target="#b22">[23]</ref> , Ganusov et al's future execution (FE) approach <ref type
rarchy.</p><p>To separate the metadata and data hierarchies, we build upon the Direct-to-Data (D2D) <ref type="bibr" target="#b0">[1]</ref> cache hierarchy, which replaces TLB lookups with access to l proposed cache optimizations under one common framework. These include:</p><p>? Direct data access <ref type="bibr" target="#b0">[1]</ref> to lower latency by using the metadata information to skip l ations. As a first step, we extend the energy-efficient, but single-core only, Direct-to-Data (D2D) <ref type="bibr" target="#b0">[1]</ref> cache framework to support multiple cores.</p></div> <div xm type="bibr" target="#b38">[39]</ref> and Sembrant et al. <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b0">[1]</ref> extend the TLB with cacheline way information to reduce L1 c s handled by adding tracking pointers to each cacheline (e, f). More details on this can be found in<ref type="bibr" target="#b0">[1]</ref>.</p></note> 			<note xmlns="http://www.tei-c.org/ns/1.0" pla
here it is needed <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" t re delays in LLCs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" t er NUCA placement <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and for filtering c
"#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>(Using classification from the sharing information
ect accesses to data in multiple cache levels and memory (D2D), as discussed in Section II. Hallnor <ref type="bibr" target="#b40">[41]</ref> suggests an indirect index cache to locate data in a manag
ref type="bibr" target="#b28">[29]</ref>), HPC (Splash2x <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>), Mobile (Chrome browser with Telemetry <ref type="bibr" t
nt and replication to lower latency and reduce traffic by keeping data closer to where it is needed <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t "><head>VI. RELATED WORK</head><p>NUCA caches have been proposed to address the wire delays in LLCs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t , but may have high storage requirements. Data classification has been used to steer NUCA placement <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t
here it is needed <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" t re delays in LLCs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" t er NUCA placement <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and for filtering c
reon graphics stack to avoid X11 overhead and NoMali GPU <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> emulator to avoid measuring software rendering for the mob
has been proposed based on hardware detection mechanisms <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> and OS support <ref type="bibr" target="#b7">[8]</ref>, <r
categories of workloads, Parallel (Parsec <ref type="bibr" target="#b28">[29]</ref>), HPC (Splash2x <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>), Mobile (Chrom
contemporary energy-efficient processor as shown in Table <ref type="table">III</ref>. We use CACTI <ref type="bibr" target="#b26">[27]</ref> and McPAT <ref type="bibr" target="#b27">[28]</ref> with 2
#b17">17</ref> , face recognition <ref type="bibr" target="#b18">18</ref> , and playing Atari games <ref type="bibr" target="#b19">19</ref> . They use many layers of neurons, each arranged in overlapp
lue network and rollout evaluations with weighting parameter λ. All updates are performed lock-free <ref type="bibr" target="#b56">56</ref> . Expansion (Fig. <ref type="figure" target="#fig_1">3b</ref
zing action value disagree. Time controls were otherwise shaped to use most time in the middle-game <ref type="bibr" target="#b57">57</ref> . AlphaGo resigns when its overall evaluation drops below an
e-playing, classical planning, partially observed planning, scheduling, and constraint satisfaction <ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36</ref> . By combining tree
53">53</ref> ; however, it is known that rollout-based position evaluation is frequently inaccurate <ref type="bibr" target="#b54">54</ref> . AlphaGo uses relatively simple rollouts, and instead addre
l network to approximate the optimal value function, achieving superhuman performance in backgammon <ref type="bibr" target="#b46">46</ref> ; and achieving weak kyu-level performance in small-board Go
for both players <ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref> </p><formula xml:id="formula_2">E ( ) = | = . v s z s s a > , checkers <ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45</ref> and Go <ref type="bibr" target="#b30">30</ref> ; or using linear regression in othello <ref type="bibr" tar ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52</ref> or online adaptation <ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b53">53</ref> ; however, it is kno
ms, and a mini-batch size of m = 16. Updates were applied asynchronously on 50 GPUs using DistBelief<ref type="bibr" target="#b61">61</ref> ; gradients older than 100 steps were discarded. Training to
s) that predicts the outcome from state s. This approach has led to superhuman performance in chess <ref type="bibr" target="#b4">4</ref> , checkers <ref type="bibr" target="#b5">5</ref> and othello < evaluated thousands of times fewer positions than Deep Blue did in its chess match against Kasparov <ref type="bibr" target="#b4">4</ref> ; compensating by selecting those positions more intelligently a-beta pruning <ref type="bibr" target="#b40">40</ref> has achieved superhuman performance in chess <ref type="bibr" target="#b4">4</ref> , checkers <ref type="bibr" target="#b5">5</ref> and othello <

y ways of the difficulties faced by artificial intelligence <ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34</ref> : a challenging decision-making task, an intractable search
finition of black-box access as query access <ref type="bibr" target="#b6">(Chen et al., 2017;</ref><ref type="bibr" target="#b15">Liu et al., 2017;</ref><ref type="bibr" target="#b11">Hayes &amp; Dan rediction API with small datasets like MNIST and successfully demonstrated an untargeted attack. As <ref type="bibr" target="#b15">Liu et al. (2017)</ref> demonstrated, it is more difficult to transfe ularly when attacking models trained on large datasets like ImageNet. Using ensemble-based methods, <ref type="bibr" target="#b15">Liu et al. (2017)</ref> overcame these limitations to attack the Clar
te networks do not always transfer to the target model, especially when conducting targeted attacks <ref type="bibr" target="#b6">(Chen et al., 2017;</ref><ref type="bibr" target="#b19">Narodytska &am :</p><p>Black-box setting. In this paper, we use the definition of black-box access as query access <ref type="bibr" target="#b6">(Chen et al., 2017;</ref><ref type="bibr" target="#b15">Liu et al., 20 orders of magnitude more query-efficient than previous methods based on gradient estimation such as <ref type="bibr" target="#b6">Chen et al. (2017)</ref>. We show that our approach reliably produces imated by querying the classifier rather than computed by autodifferentiation. This idea is used in <ref type="bibr" target="#b6">Chen et al. (2017)</ref>, where the gradient is estimated via pixel-by ="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">BLACK-BOX ATTACKS WITH GRADIENT</head><p>ESTIMATION <ref type="bibr" target="#b6">Chen et al. (2017)</ref> explore black-box gradient estimation methods mpability of the 2 and ∞ metric as well as the fixed-budget nature of the optimization algorithm in <ref type="bibr" target="#b6">Chen et al. (2017)</ref>, our method takes far fewer queries to genera


hat we have described here can be seen as a finite-differences estimate on a random Gaussian basis. <ref type="bibr" target="#b10">Gorban et al. (2016)</ref> shows that for an n-dimensional space and
hat we have described here can be seen as a finite-differences estimate on a random Gaussian basis. <ref type="bibr" target="#b10">Gorban et al. (2016)</ref> shows that for an n-dimensional space and
"#b9">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b4">Carlini &amp; Wagner, 2017;</ref><ref type="bibr" target="#b18">Moosavi-Dezfooli et al., 2016;</ref><ref type="bibr" target="#b17">Mo
Carlini &amp; Wagner, 2017;</ref><ref type="bibr" target="#b18">Moosavi-Dezfooli et al., 2016;</ref><ref type="bibr" target="#b17">Moosavi-Dezfooli et al., 2017;</ref><ref type="bibr" target="#b11">Ha
tack this setting, we can use "standard" first-order techniques for generating adversarial examples <ref type="bibr" target="#b9">Goodfellow et al. (2015)</ref>; <ref type="bibr" target="#b23">Paperno ers but is  classified as "dog," as shown in Figure <ref type="figure" target="#fig_4">5</ref> 5 .  <ref type="bibr" target="#b9">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b4">Carlini & orks or gradient estimation techniques.  <ref type="bibr" target="#b27">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b9">Goodfellow et al., 2015)</ref>. <ref type="bibr" target="#b22">Paperno
e investigated practical black-box attacks on real-world systems such as speech recognition systems <ref type="bibr" target="#b5">(Carlini et al., 2016)</ref>, malware detectors <ref type="bibr" targe
e investigated practical black-box attacks on real-world systems such as speech recognition systems <ref type="bibr" target="#b5">(Carlini et al., 2016)</ref>, malware detectors <ref type="bibr" targe
e investigated practical black-box attacks on real-world systems such as speech recognition systems <ref type="bibr" target="#b5">(Carlini et al., 2016)</ref>, malware detectors <ref type="bibr" targe
tack this setting, we can use "standard" first-order techniques for generating adversarial examples <ref type="bibr" target="#b9">Goodfellow et al. (2015)</ref>; <ref type="bibr" target="#b23">Paperno ers but is  classified as "dog," as shown in Figure <ref type="figure" target="#fig_4">5</ref> 5 .  <ref type="bibr" target="#b9">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b4">Carlini & orks or gradient estimation techniques.  <ref type="bibr" target="#b27">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b9">Goodfellow et al., 2015)</ref>. <ref type="bibr" target="#b22">Paperno
hat we have described here can be seen as a finite-differences estimate on a random Gaussian basis. <ref type="bibr" target="#b10">Gorban et al. (2016)</ref> shows that for an n-dimensional space and
hat we have described here can be seen as a finite-differences estimate on a random Gaussian basis. <ref type="bibr" target="#b10">Gorban et al. (2016)</ref> shows that for an n-dimensional space and
ini et al., 2016)</ref>, malware detectors <ref type="bibr" target="#b13">(Hu &amp; Tan, 2017;</ref><ref type="bibr" target="#b30">Xu et al., 2016)</ref>, and face recognition systems <ref type="bibr" adversarial examples use similar techniques but with different adversarial goals or threat models. <ref type="bibr" target="#b30">Xu et al. (2016)</ref> explore black-box adversarial examples to fool


tack this setting, we can use "standard" first-order techniques for generating adversarial examples <ref type="bibr" target="#b9">Goodfellow et al. (2015)</ref>; <ref type="bibr" target="#b23">Paperno ers but is  classified as "dog," as shown in Figure <ref type="figure" target="#fig_4">5</ref> 5 .  <ref type="bibr" target="#b9">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b4">Carlini & orks or gradient estimation techniques.  <ref type="bibr" target="#b27">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b9">Goodfellow et al., 2015)</ref>. <ref type="bibr" target="#b22">Paperno



SMARTS <ref type="bibr" target="#b28">[29]</ref>) and representative sampling (as done in SimPoint <ref type="bibr" target="#b21">[22]</ref>). Our experimental results using the SPEC CPU2000 benchmar st well known representative sampling approach is the SimPoint approach proposed by Sherwood et al. <ref type="bibr" target="#b21">[22]</ref>. SimPoint picks a small number of sampling units that accu onsiders multiple randomly chosen cluster centers and uses the Bayesian Information Criterion (BIC) <ref type="bibr" target="#b21">[22]</ref> to assess the quality of the clustering: the clustering wi ifferent ways for doing so, such as code working sets <ref type="bibr" target="#b5">[6]</ref>, BBVs <ref type="bibr" target="#b21">[22]</ref>, procedure calls <ref type="bibr" target="#b12">[13]</ref>
arge gap between maximum and typical power consumption. Dynamic thermal management (DTM) techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> seek to exploit this
s a wealth of literature covering this area, see for example <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targe
target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar
type="bibr" target="#b5">[6]</ref>, BBVs <ref type="bibr" target="#b21">[22]</ref>, procedure calls <ref type="bibr" target="#b12">[13]</ref>, and performance data <ref type="bibr" target="#b6">[7]</r
ergy, etc. There are two common ways in sampled simulation, statistical sampling (as done in SMARTS <ref type="bibr" target="#b28">[29]</ref>) and representative sampling (as done in SimPoint <ref typ pling units.</p><p>The SMARTS (Sampling Microarchitecture Simulation) approach by Wunderlich et al. <ref type="bibr" target="#b28">[29]</ref> proposes systematic sampling, which selects sampling units ">Sampled Simulation</head><p>For statistical sampling, we use periodic sampling, as done in SMARTS <ref type="bibr" target="#b28">[29]</ref>, i.e., we select a sampling unit every n intervals. We wil get="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>There are basically two major ways for determining w
of the spectrum, stressmarks are being used to explore a microprocessor's maximum power consumption <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, maximum thermal hots maximum power consumption of a microprocessor. This is common practice in industry, see for example <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
wer consumption. Dynamic thermal management (DTM) techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> seek to exploit this gap: the microprocessor cooling appara 1364). Power is estimated using Wattch v1.02 <ref type="bibr" target="#b1">[2]</ref> and HotLeakage <ref type="bibr" target="#b22">[23]</ref> assuming a 70nm technology, 5.6GHz clock frequency and 1V cessor floorplan, could provide valuable information in terms of power density and thermal hotspots <ref type="bibr" target="#b22">[23]</ref>. Figures <ref type="figure">3 and 4</ref> quantify the err
of the spectrum, stressmarks are being used to explore a microprocessor's maximum power consumption <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, maximum thermal hots maximum power consumption of a microprocessor. This is common practice in industry, see for example <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
an attempt to provide a representative cross-cut of the entire program execution.</p><p>Laha et al. <ref type="bibr" target="#b19">[20]</ref> propose statistical sampling for evaluating cache performa
y more accurate than representative sampling for estimating average behavior as shown in prior work <ref type="bibr" target="#b29">[30]</ref>, however, the new insight provided in this paper is that r e" target="#fig_1">5</ref>. The results in the left graph confirm the earlier findings by Yi et al. <ref type="bibr" target="#b29">[30]</ref> who provide a detailed comparison of statistical and repre
roaches for representation learning of graphs <ref type="bibr" target="#b23">(Li et al., 2016;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b21">Kipf &amp arget="#b6">Defferrard et al., 2016;</ref><ref type="bibr" target="#b8">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b19">Kearnes e erage via attention <ref type="bibr" target="#b34">(Velickovic et al., 2018)</ref> and LSTM pooling <ref type="bibr" target="#b13">(Hamilton et al., 2017a;</ref><ref type="bibr" target="#b24">Murphy e
arget="#b8">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b19">Kearnes et al., 2016;</ref><ref type="bibr" target="#b21">Kipf &amp;


a surge of interest in Graph Neural Network (GNN) approaches for representation learning of graphs <ref type="bibr" target="#b23">(Li et al., 2016;</ref><ref type="bibr" target="#b13">Hamilton et al. get="#b19">Kearnes et al., 2016;</ref><ref type="bibr" target="#b21">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b23">Li et al., 2016;</ref><ref type="bibr" target="#b34">Velickovic et al

t="#b13">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b21">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b34">Velickovic et al., 2018;</ref><ref type="bibr" target="#b37">Xu et al " target="#b21">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b23">Li et al., 2016;</ref><ref type="bibr" target="#b34">Velickovic et al., 2018;</ref><ref type="bibr" target="#b28">Santoro on-standard neighbor aggregation schemes that we do not cover, e.g., weighted average via attention <ref type="bibr" target="#b34">(Velickovic et al., 2018)</ref> and LSTM pooling <ref type="bibr" tar
ks to the universal approximation theorem <ref type="bibr" target="#b16">(Hornik et al., 1989;</ref><ref type="bibr" target="#b15">Hornik, 1991)</ref>. In practice, we model f (k+1) • ϕ (k) with one M sets to unique embeddings. It can be parameterized by an MLP by the universal approximation theorem <ref type="bibr" target="#b15">(Hornik, 1991)</ref>. Nonetheless, many existing GNNs instead use a 1

et="#b31">(Scarselli et al., 2009b;</ref><ref type="bibr" target="#b3">Battaglia et al., 2016;</ref><ref type="bibr" target="#b6">Defferrard et al., 2016;</ref><ref type="bibr" target="#b8">Duvenaud e
cial graphs; (2) the batch size ∈ {32, 128}; (3) the dropout ratio ∈ {0, 0.5} after the dense layer <ref type="bibr" target="#b33">(Srivastava et al., 2014)</ref>; (4) the number of epochs, i.e., a si
="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. While these metho mance degradation as the prefetching depth grows. Since lbm has a variety of memory access patterns <ref type="bibr" target="#b8">[9]</ref>, deeper prefetching with a simple delta predictor wastes ban p><p>To address both prefetching coverage and accuracy, prior work has adopted lookahead mechanisms <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, or do not implement adaptive throttling <ref type="bibr" target="#b8">[9]</ref>. Moreover, most hardware prefetchers work in the physical ad st hardware prefetchers work in the physical address space <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" nt, best of class, lookahead and non-lookahead prefetchers <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, including the win offset prefetchers take longer to train or fail to select the optimal offset. Lookahead prefetchers <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref> efficiently encod nto the pattern table and generate further predictions. This recursion allows lookahead prefetchers <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref> to prefetch far a o avoid over-prefetching, existing lookahead prefetchers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref> globally and statically limit the depth to which lookahead i e transition. Previously proposed history-based techniques <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref> address this by making predictions using the first offset in op performing recently proposed prefetching algorithms: the Variable Length Delta Prefetcher (VLDP) <ref type="bibr" target="#b8">[9]</ref>, the Best Offset Prefetcher (BOP) <ref type="bibr" target="#
t sequences of addresses that differ by a constant value and fail to capture diverse delta patterns <ref type="bibr" target="#b13">[14]</ref>. For example, Figure <ref type="figure" target="#fig_0">2<
t sequences of addresses that differ by a constant value and fail to capture diverse delta patterns <ref type="bibr" target="#b13">[14]</ref>. For example, Figure <ref type="figure" target="#fig_0">2<
<body> <div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The "Memory Wall" <ref type="bibr" target="#b0">[1]</ref>, the vast gulf between processor execution speed and memory
h as the Best Offset prefetcher <ref type="bibr" target="#b9">[10]</ref> and the Sandbox prefetcher <ref type="bibr" target="#b14">[15]</ref> evaluate multiple offsets at runtime and issue prefetches ysical address space. Therefore, offset-based prefetchers <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b14">[15]</ref> work better than SPP for these, because they are trained b
target="#b1">[2]</ref>, a set of lines following a strided pattern with respect to the current miss <ref type="bibr" target="#b2">[3]</ref>, or a set of blocks spatially around the miss <ref type="bib
current miss <ref type="bibr" target="#b2">[3]</ref>, or a set of blocks spatially around the miss <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. More recent prefet ibr" target="#b8">[9]</ref>. Moreover, most hardware prefetchers work in the physical address space <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" t 0"><head>D. Other Prior Prefetchers</head><p>Somogyi et al. proposed Spatial Memory Streaming (SMS) <ref type="bibr" target="#b3">[4]</ref> which leverages the correlation between memory request instr when they are broken up by a physical page transition. Previously proposed history-based techniques <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref> address this by mak l be discussed further in the next section. We also modeled and compared the performance of the SMS <ref type="bibr" target="#b3">[4]</ref> prefetcher originally designed for server workloads, however
and accuracy, prior work has adopted lookahead mechanisms <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. These studies, type="bibr" target="#b12">[13]</ref>. These studies, however, suffer from high hardware complexity <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, or do not impl lly in order to balance prefetch coverage with accuracy. ? Unlike prior lookahead based prefetchers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, SPP does not r r fail to select the optimal offset. Lookahead prefetchers <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref> efficiently encode the relationship between accesses to yi r predictions. This recursion allows lookahead prefetchers <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref> to prefetch far ahead of the current program execution, an esired prefetching depth must be limited. To avoid over-prefetching, existing lookahead prefetchers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref> globally and stat
2">[3]</ref>, or a set of blocks spatially around the miss <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. More recent prefetchers d bwaves lbm Fig. <ref type="figure
t sequences of addresses that differ by a constant value and fail to capture diverse delta patterns <ref type="bibr" target="#b13">[14]</ref>. For example, Figure <ref type="figure" target="#fig_0">2<
n two different SPEC CPU 2006 benchmarks using a simple Program Counter (PC) based delta prefetcher <ref type="bibr" target="#b10">[11]</ref>. As the prefetching depth d grows, the PC delta prefetcher "#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, where the mapping between virtual and physical memory is
oupled Compressed Cache (DCC) [Sardashti and Wood 2013a,  2013b]  and Skewed Compressed Cache (SCC) <ref type="bibr" target="#b26">[Sardashti et al. 2014</ref>]-with a more practical and simpler desig br">[Sardashti and</ref><ref type="bibr">Wood 2013a, 2013b]</ref> and Skewed Compressed Cache (SCC) <ref type="bibr" target="#b26">[Sardashti et al. 2014</ref>]-try to achieve some of these goals, alt he additional area and complexity of backward pointers to maintain the decoupled mapping.</p><p>SCC <ref type="bibr" target="#b26">[Sardashti et al. 2014</ref>] also uses super-block tags but eliminat (i.e., backward pointers) and the need to separately manage block and super-block replacements. SCC <ref type="bibr" target="#b26">[Sardashti et al. 2014</ref>] eliminates DCC's backward pointers and ="http://www.tei-c.org/ns/1.0"><head n="2.3.">SCC: A State-of-the-Art Compressed Cache</head><p>SCC <ref type="bibr" target="#b26">[Sardashti et al. 2014</ref>] is a state-of-the-art compressed cache >Kim et al. 2002;</ref><ref type="bibr">Sardashti and</ref><ref type="bibr">Wood 2013a, 2013b;</ref><ref type="bibr" target="#b26">Sardashti et al. 2014]</ref>.</p><p>A compressed cache organization m ACC design in more detail.</p><p>3.1.1 YACC Tag Format. Like SCC, YACC uses sparse super-block tags <ref type="bibr" target="#b26">[Sardashti et al. 2014]</ref> to map the blocks that are compressed i ef type="table">II</ref> shows the main parameters. We use 64-byte cache block sizes. For YACC, SCC <ref type="bibr" target="#b26">[Sardashti et al. 2014]</ref> and DCC <ref type="bibr">[Sardashti and pe="bibr">Wood 2013a, 2013b]</ref>. We consider this extra overhead in our simulation. ? SCC models <ref type="bibr" target="#b26">[Sardashti et al. 2014]</ref> with four-block super-blocks and 16-byt xtra hardware complexity induced for storing and retrieving compressed data blocks. Compared to SCC <ref type="bibr" target="#b26">[Sardashti et al. 2014]</ref>, we addressed most of the issues of the

ation framework that was used to evaluate SCC and DCC-the full-system cycle-accurate GEMS simulator <ref type="bibr" target="#b20">[Martin et al. 2005</ref>]. We model YACC with an eight-core multicor
ation framework that was used to evaluate SCC and DCC-the full-system cycle-accurate GEMS simulator <ref type="bibr" target="#b20">[Martin et al. 2005</ref>]. We model YACC with an eight-core multicor
everal compressed cache proposals. Early work <ref type="bibr" target="#b17">[Kim et al. 2002;</ref><ref type="bibr" target="#b19">Lee et al. 2000</ref>] limits the maximum benefit of compression to a ssible. These proposals, which we call FixedC <ref type="bibr" target="#b17">[Kim et al. 2002;</ref><ref type="bibr" target="#b19">Lee et al. 2000]</ref>), facilitate direct tag-data mappings but limi
b3">[Alameldeen and Wood 2004;</ref><ref type="bibr" target="#b13">Hallnor and Reinhardt 2005;</ref><ref type="bibr" target="#b17">Kim et al. 2002;</ref><ref type="bibr">Sardashti and</ref><ref type=" er-block granularity. Several designs <ref type="bibr" target="#b3">[Alameldeen and Wood 2004;</ref><ref type="bibr" target="#b17">Kim et al. 2002;</ref><ref type="bibr" target="#b6">Baek et al. 2013] ED WORK</head><p>The past decade and a half has seen several compressed cache proposals. Early work <ref type="bibr" target="#b17">[Kim et al. 2002;</ref><ref type="bibr" target="#b19">Lee et al. 2000 an uncompressed block) even if data were highly compressible. These proposals, which we call FixedC <ref type="bibr" target="#b17">[Kim et al. 2002;</ref><ref type="bibr" target="#b19">Lee et al. 2000

t="#b3">[Alameldeen and Wood 2004;</ref><ref type="bibr" target="#b23">Pekhimenko et al. 2012;</ref><ref type="bibr" target="#b9">Chen et al. 2010]</ref>.</p><p>Our work is largely independent of the he C-PACK+Z algorithm <ref type="bibr">[Sardashti and</ref><ref type="bibr">Wood 2013a, 2013b;</ref><ref type="bibr" target="#b9">Chen et al. 2010]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns ti and</ref><ref type="bibr">Wood 2013a, 2013b]</ref>, which is a variation of the C-PACK algorithm <ref type="bibr" target="#b9">[Chen et al. 2010]</ref> with support to detect zero blocks. C-PACK+Z hown to have low hardware overheads and decompression latency, with a fairly high compression ratio <ref type="bibr" target="#b9">[Chen et al. 2010;</ref><ref type="bibr">Sardashti and</ref><ref type=
" target="#b3">[Alameldeen and Wood 2004;</ref><ref type="bibr" target="#b17">Kim et al. 2002;</ref><ref type="bibr" target="#b6">Baek et al. 2013]</ref> simply increase the number of tags, such as do
d-associative design, where each (group of) cache way(s) is indexed using a different hash function <ref type="bibr" target="#b29">[Seznec 1993</ref><ref type="bibr" target="#b31">[Seznec , 2004]]</re cult to understand behavior. Skewed-associative caches tend to eliminate conflict misses on average <ref type="bibr" target="#b29">[Seznec 1993</ref>], but the multiple hash functions make it difficul
acement policies (e.g., RRIP). Skewed-associative caches can use the replacement policy from Zcache <ref type="bibr" target="#b25">[Sanchez and Kozyrakis 2010]</ref>, although at the expense of signif
]</ref>, they still belong to the family of linear models and are claimed to be di cult to estimate <ref type="bibr" target="#b27">[28]</ref>. Moreover, they are known to have only marginal improvemen ag recommendation. With one hidden layer only, our NFM signi cantly outperforms FM (the ocial LibFM <ref type="bibr" target="#b27">[28]</ref> implementation) with a 7.3% improvement. Compared to the s te) linear models. In other words, the predicted target ˆ (x) is linear w.r.t. each model parameter <ref type="bibr" target="#b27">[28]</ref>. Formally, for each model parameter θ ∈ {w 0 , {w i }, { i itive embedding-based models that are speci cally designed for sparse data prediction:</p><p>-LibFM <ref type="bibr" target="#b27">[28]</ref>. is is the o cial implementation <ref type="foot" target="
eature interactions from raw data automatically. A popular approach is factorization machines (FMs) <ref type="bibr" target="#b26">[27]</ref>, which embeds features into a latent space and models the underlying structure, FM may not be expressive enough. Although higher-order FMs have been proposed <ref type="bibr" target="#b26">[27]</ref>, they still belong to the family of linear models and are ing the second-order factorized interactions between features. By specifying input features, Rendle <ref type="bibr" target="#b26">[27]</ref> showed that FM can mimic many speci c factorization models value, rather than simply an embedding table lookup, so as to account for the real valued features <ref type="bibr" target="#b26">[27]</ref>.</p><p>Bi-Interaction Layer. We then feed the embedding se nsorFlow implementation<ref type="foot" target="#foot_7">7</ref> of higherorder FM, as described in <ref type="bibr" target="#b26">[27]</ref>. We experimented with order size 3, since the MovieLens da n Machines</head><p>Factorization machines are originally proposed for collaborative recommendation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref>. Given a real valu
ef type="bibr" target="#b20">[21]</ref>, to search ranking <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>, visual analysis <ref type="bibr" target="#b34">[35]</ref>, [38]</ref> proposed a entional FM, using an a ention network <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref> to learn the importance of each feature interaction. Howeve
echniques on FM to improve its learning and generalization ability. For example, we can use dropout <ref type="bibr" target="#b32">[33]</ref> -a well-known technique in deep learning community to prev models have strong representation ability, they are also easy to over t the training data. Dropout <ref type="bibr" target="#b32">[33]</ref> is a regularization technique for neural networks to preve ˆ (x). As such, dropout can also be seen as performing model averaging with smaller neural networks <ref type="bibr" target="#b32">[33]</ref>.</p><p>In NFM, to avoid feature embeddings co-adapt to eac eters in each update numerically, while using dropout can be seen as ensembling multiple sub-models <ref type="bibr" target="#b32">[33]</ref>, which can be more e ective. Considering the genericity of
target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. erea er, standard rget="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar s are originally proposed for collaborative recommendation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref>. Given a real valued feature vector x ∈ R n , FM estimates egression tasks such as recommendation with explicit ratings <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref> and click-through rate prediction <ref type="bibr" target=" e. It has shown strong performance for personalized tag recommendation and context-aware prediction <ref type="bibr" target="#b29">[30]</ref>. We used the SGD learner for a fair comparison with other
target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>, we argue that its performance can be limited by its linear ctions from raw data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar ogy, many variants <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref> of FM have been dev type="bibr" target="#b17">[18]</ref> proposed Co-FMs to learn from multi-view data; Oentaryo et al. <ref type="bibr" target="#b23">[24]</ref> encoded prior knowledge of features to FM by designing a h r" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref> and click-through rate prediction <ref type="bibr" target="#b23">[24]</ref>. We rounded up the prediction of each model to 1 or −1 if
generality -in contrast to matrix factorization (MF) that models the relation of two entities only <ref type="bibr" target="#b16">[17]</ref>, FM is a general predictor working with any real valued fe
ating nonlinearities is to extend the objective function with regularizers like the graph Laplacian <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref>. Lastly, we are in
d items. Later, the NCF framework was extended to model a ribute interactions for a ribute-aware CF <ref type="bibr" target="#b36">[37]</ref>. However, their methods are only applicable to learn inter g task, we can optimize pairwise personalized ranking loss <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37]</ref> or contrastive max-margin loss <ref type="bibr" target="#b4
imic many speci c factorization models such as the standard MF, parallel factor analysis, and SVD++ <ref type="bibr" target="#b21">[22]</ref>, Owing to such genericity, FM has been recognized as one o
s and updates model parameters based on the batch. In our implementation, we use mini-batch Adagrad <ref type="bibr" target="#b9">[10]</ref> as the optimizer, rather than the vanilla SGD. Its main adv FM that optimized FM with the vanilla SGD, all other methods were optimized with mini-batch Adagrad <ref type="bibr" target="#b9">[10]</ref>, where the batch size was set to 128 for Frappe and 4096 fo
t simply concatenate <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref> or average <ref type="bibr" target="#b15">[16,</ref><ref ty get="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>. By embedding high-dimensional sparse features into a low-d arget="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref> started to explore DNNs for some scenarios of sparse predic en two entities and do not directly support the general se ing of supervised learning. Zhang et al. <ref type="bibr" target="#b43">[44]</ref> developed a FM-supported Neural Network (FNN), which uses the very low training error but high test error implies that the model is overing. Inspired by FNN <ref type="bibr" target="#b43">[44]</ref>, we further explore the use of feature embeddings learned ype="bibr" target="#b15">16]</ref>, constant <ref type="bibr" target="#b30">[31]</ref>, and diamond <ref type="bibr" target="#b43">[44]</ref>, among others.</p><p>Prediction Layer. At last, the output
t simply concatenate <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref> or average <ref type="bibr" target="#b15">[16,</ref><ref ty get="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>. By embedding high-dimensional sparse features into a low-d arget="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref> started to explore DNNs for some scenarios of sparse predic en two entities and do not directly support the general se ing of supervised learning. Zhang et al. <ref type="bibr" target="#b43">[44]</ref> developed a FM-supported Neural Network (FNN), which uses the very low training error but high test error implies that the model is overing. Inspired by FNN <ref type="bibr" target="#b43">[44]</ref>, we further explore the use of feature embeddings learned ype="bibr" target="#b15">16]</ref>, constant <ref type="bibr" target="#b30">[31]</ref>, and diamond <ref type="bibr" target="#b43">[44]</ref>, among others.</p><p>Prediction Layer. At last, the output

et al. <ref type="bibr" target="#b37">[38]</ref> proposed a entional FM, using an a ention network <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref> to learn the importa
ove the e ciency of NFM by resorting to hashing techniques <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41]</ref> to make it more suitable for large-scale applications and s
ality prediction. In future, we will improve the e ciency of NFM by resorting to hashing techniques <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41]</ref> to make it more su
imic many speci c factorization models such as the standard MF, parallel factor analysis, and SVD++ <ref type="bibr" target="#b21">[22]</ref>, Owing to such genericity, FM has been recognized as one o
NFM to prevent the learning of higher-order feature interactions from co-adaptations and over ing. <ref type="bibr" target="#b19">[20]</ref>. It means that the distribution of each layer's inputs cha g its parameters, which adversely slows down the training. To address the problem, Io e and Szegedy <ref type="bibr" target="#b19">[20]</ref> proposed batch normalization (BN), which normalizes layer eneralization can be achieved. enabled with a ratio of 0.  <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref> showed that by addressing the internal covariate shi with B
<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>, visual analysis <ref type="bibr" target="#b34">[35]</ref>, and event detection <ref type="bibr" target="#b39">[40]</
et="#b38">39]</ref>, visual analysis <ref type="bibr" target="#b34">[35]</ref>, and event detection <ref type="bibr" target="#b39">[40]</ref>. Typically, a predictive task is formulated as estimating
br" target="#b28">[29,</ref><ref type="bibr" target="#b36">37]</ref> or contrastive max-margin loss <ref type="bibr" target="#b42">[43]</ref>. In this work, we focus on the regression task and optimiz
<body> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Score matching <ref type="bibr" target="#b12">(Hyvärinen, 2005)</ref> is particularly suitable for learning unnorma MLE) can be difficult due to the intractable partition function Z θ . To avoid this, score matching <ref type="bibr" target="#b12">(Hyvärinen, 2005)</ref> minimizes the Fisher divergence between p d a not have access to the score function of the data s d (x).</p><p>By applying integration by parts, <ref type="bibr" target="#b12">Hyvärinen (2005)</ref> shows that L(θ) can be written as L(θ) = J(θ) </p><p>Other than our requirements on p v , the assumptions are exactly the same as in Theorem 1 of <ref type="bibr" target="#b12">Hyvärinen (2005)</ref>. We advise the interested readers to read Appe he consistency of <ref type="bibr">MLE (van der Vaart, 1998)</ref>. We also adopt the assumption in <ref type="bibr" target="#b12">Hyvärinen (2005)</ref> that all densities are strictly positive (Assu M . These two facts lead to consistency. For a complete proof, see Appendix B.3.</p><p>Remark 1. In <ref type="bibr" target="#b12">Hyvärinen (2005)</ref>, the authors only showed that J(θ) = 0 ⇔ θ = θ s a constant w.r.t. θ.</p><p>Proof. The basic idea of this proof is similar to that of Theorem 1 in <ref type="bibr" target="#b12">Hyvärinen (2005)</ref>. First, note that L(θ, p v ) can be expanded t

an-vector products, thus can be easily and efficiently implemented in frameworks such as TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and PyTorch <ref type="bibr" target="#b1">(
e="bibr" target="#b29">(Sun et al., 2019)</ref>, and models defined by complex simulation processes <ref type="bibr" target="#b32">(Tran et al., 2017)</ref>. In many cases learning and inference becom

onential families <ref type="bibr" target="#b35">(Wenliang et al., 2019)</ref> and NICE flow models <ref type="bibr" target="#b3">(Dinh et al., 2015)</ref> show that our method is either more scalable p Flow Models</head><p>Setup. As a sanity check, we also evaluate SSM by training a NICE flow model <ref type="bibr" target="#b3">(Dinh et al., 2015)</ref>, whose likelihood is tractable and can be co yers, each with five hidden layers, for a total of 20 hidden layers, as well as a final scale layer <ref type="bibr" target="#b3">(Dinh et al., 2015)</ref>. Softplus nonlinearities are used between hi
frameworks such as TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and PyTorch <ref type="bibr" target="#b1">(Adam et al., 2017)</ref>.</p><p>Beyond training unnormalized models,

e="bibr" target="#b29">(Sun et al., 2019)</ref>, and models defined by complex simulation processes <ref type="bibr" target="#b32">(Tran et al., 2017)</ref>. In many cases learning and inference becom
turally adapted as an objective for estimating the score function of a data generating distribution <ref type="bibr" target="#b24">(Sasaki et al., 2014;</ref><ref type="bibr" target="#b28">Strathmann
e="bibr" target="#b29">(Sun et al., 2019)</ref>, and models defined by complex simulation processes <ref type="bibr" target="#b32">(Tran et al., 2017)</ref>. In many cases learning and inference becom
that in their experiment, ignoring warm-up could result in an error as high as 15% in simulated CPI <ref type="bibr" target="#b11">[12]</ref>. Thus adequate warm-up is critical to the accuracy of samp hniques. For example, MRRL is claimed to have achieved 90% of the maximum possible simulation speed <ref type="bibr" target="#b11">[12]</ref>. However, careful analysis of the experiment reveals funct <ref type="foot" target="#foot_0">1</ref> . Fifty 1 million-instruction sampling units are used in <ref type="bibr" target="#b11">[12]</ref>. Suppose that a benchmark is 100 billion instructions long struction caches. In their second technique, they measure the Memory Reference Reuse Latency (MRRL) <ref type="bibr" target="#b11">[12]</ref>, which refers to the elapsed time measured in number of in oose a sampling unit size of 1 million instructions. This sampling unit size was used in MRRL paper <ref type="bibr" target="#b11">[12]</ref>, and Variance SimPoint <ref type="bibr" target="#b14">[15] redicts.</p><p>For MRRL, we choose the p-value to be 99.9%, which is the default value suggested in <ref type="bibr" target="#b11">[12]</ref>. For BLRL, we use the p-value of 90%. Both methods are als p://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>No warm up length number is given in<ref type="bibr" target="#b11">[12]</ref>. This number is based on our experiment with MRRL. See Sec
eme is to devote a fixed number of instructions to warm up the cache. This is called "PRIME" scheme <ref type="bibr" target="#b5">[6]</ref> following Crowley et. al.'s terminology <ref type="bibr" tar
he sample size. Recently, Wunderlich et al. applied sampling theory to microarchitecture simulation <ref type="bibr" target="#b1">[2]</ref>. Under the assumption of no measuring error, they showed tha caches is to do cache simulation throughout the benchmark execution. This is how the SMARTS scheme <ref type="bibr" target="#b1">[2]</ref> does the warm-up. The simulator switches between functional path execution on the caches during functional warm-up. It has been shown that this error is small <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b2">[3]</ref>. Although this warm processor configuration used in our experiment. This configuration is adapted from the SMARTS paper <ref type="bibr" target="#b1">[2]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n= imulator executes 4,000 instructions in cycle accurate mode to warm up the pipeline as suggested in <ref type="bibr" target="#b1">[2]</ref>, and then the CPI of 1 million instruction sampling unit is rm-up. In this simulation the caches are always simulated between every sampling units as in SMARTS <ref type="bibr" target="#b1">[2]</ref>. The sampling units and the cycleaccurate warm-up are the sa
e for the cold-start references is equivalent to the miss rate for all other references. Wood et al <ref type="bibr" target="#b7">[8]</ref> show that this assumption is usually not true. The miss rate
e for the cold-start references is equivalent to the miss rate for all other references. Wood et al <ref type="bibr" target="#b7">[8]</ref> show that this assumption is usually not true. The miss rate
s not easy, but monitoring the warm-up process with Vengroff et al's deterministic finite automaton <ref type="bibr" target="#b16">[17]</ref> may be much simpler. Extending SMA to warm up other struct
block will be a hit or a miss. Such references are referred to as coldstart references. Laha et al <ref type="bibr" target="#b6">[7]</ref> proposed not counting these cold-start references when calcu
e for the cold-start references is equivalent to the miss rate for all other references. Wood et al <ref type="bibr" target="#b7">[8]</ref> show that this assumption is usually not true. The miss rate
lled "PRIME" scheme <ref type="bibr" target="#b5">[6]</ref> following Crowley et. al.'s terminology <ref type="bibr" target="#b3">[4]</ref>.</p><p>At an extreme, zero instructions are used to warm up
e for the cold-start references is equivalent to the miss rate for all other references. Wood et al <ref type="bibr" target="#b7">[8]</ref> show that this assumption is usually not true. The miss rate
he sample size. Recently, Wunderlich et al. applied sampling theory to microarchitecture simulation <ref type="bibr" target="#b1">[2]</ref>. Under the assumption of no measuring error, they showed tha caches is to do cache simulation throughout the benchmark execution. This is how the SMARTS scheme <ref type="bibr" target="#b1">[2]</ref> does the warm-up. The simulator switches between functional path execution on the caches during functional warm-up. It has been shown that this error is small <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b2">[3]</ref>. Although this warm processor configuration used in our experiment. This configuration is adapted from the SMARTS paper <ref type="bibr" target="#b1">[2]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n= imulator executes 4,000 instructions in cycle accurate mode to warm up the pipeline as suggested in <ref type="bibr" target="#b1">[2]</ref>, and then the CPI of 1 million instruction sampling unit is rm-up. In this simulation the caches are always simulated between every sampling units as in SMARTS <ref type="bibr" target="#b1">[2]</ref>. The sampling units and the cycleaccurate warm-up are the sa

tences contribute to the classification decision which can be of value in applications and analysis <ref type="bibr" target="#b21">(Shen et al., 2014;</ref><ref type="bibr">Gao et al., 2014)</ref>.</p
r text classification. The architecture is a direct application of CNNs, as used in computer vision <ref type="bibr" target="#b12">(LeCun et al., 1998)</ref>, albeit with NLP interpretations. <ref typ
used the most frequent 500,000 n-grams (up to 5-grams). Bag-of-means The average word2vec embedding <ref type="bibr" target="#b18">(Mikolov et al., 2013</ref>) is used as feature set.</p></div> <div x s with a special UNK token.</p><p>We obtain the word embedding by training an unsupervised word2vec <ref type="bibr" target="#b18">(Mikolov et al., 2013)</ref> model on the training and validation spl
. (2015)</ref> apply a character-level CNN for text classification and achieve competitive results. <ref type="bibr" target="#b22">Socher et al. (2013)</ref>   explore the structure of a sentence and
rocessing. The goal is to assign labels to text. It has broad applications including topic labeling <ref type="bibr" target="#b28">(Wang and Manning, 2012)</ref>, sentiment classification <ref type="b cal features, such as n-grams, and then use a linear model or kernel methods on this representation <ref type="bibr" target="#b28">(Wang and Manning, 2012;</ref><ref type="bibr" target="#b6">Joachims,
r text classification. The architecture is a direct application of CNNs, as used in computer vision <ref type="bibr" target="#b12">(LeCun et al., 1998)</ref>, albeit with NLP interpretations. <ref typ
ic labeling <ref type="bibr" target="#b28">(Wang and Manning, 2012)</ref>, sentiment classification <ref type="bibr" target="#b16">(Maas et al., 2011;</ref><ref type="bibr" target="#b19">Pang and Lee,
rocessing. The goal is to assign labels to text. It has broad applications including topic labeling <ref type="bibr" target="#b28">(Wang and Manning, 2012)</ref>, sentiment classification <ref type="b cal features, such as n-grams, and then use a linear model or kernel methods on this representation <ref type="bibr" target="#b28">(Wang and Manning, 2012;</ref><ref type="bibr" target="#b6">Joachims,
">(Blunsom et al., 2014)</ref> and recurrent neural networks based on long short-term memory (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997)</ref> to learn text representations
igrams and bagof-bigrams as features respectively.</p><p>Text Features are constructed according to <ref type="bibr" target="#b9">(Kiritchenko et al., 2014)</ref>, including word and character n-grams
in sequence generation <ref type="bibr" target="#b13">(Li et al., 2015)</ref> and language modeling <ref type="bibr" target="#b14">(Lin et al., 2015)</ref>.</p><p>The attention mechanism was proposed
ning</head><p>We split documents into sentences and tokenize each sentence using Stanford's CoreNLP <ref type="bibr" target="#b17">(Manning et al., 2014)</ref>. We only retain words appearing more tha
">(Blunsom et al., 2014)</ref> and recurrent neural networks based on long short-term memory (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997)</ref> to learn text representations
r text classification. The architecture is a direct application of CNNs, as used in computer vision <ref type="bibr" target="#b12">(LeCun et al., 1998)</ref>, albeit with NLP interpretations. <ref typ
f>. There are five levels of ratings from 1 to 5 (higher is better). IMDB reviews are obtained from <ref type="bibr" target="#b2">(Diao et al., 2014)</ref>. The ratings range from 1 to 10. Yahoo answe
f>. There are five levels of ratings from 1 to 5 (higher is better). IMDB reviews are obtained from <ref type="bibr" target="#b2">(Diao et al., 2014)</ref>. The ratings range from 1 to 10. Yahoo answe
igrams and bagof-bigrams as features respectively.</p><p>Text Features are constructed according to <ref type="bibr" target="#b9">(Kiritchenko et al., 2014)</ref>, including word and character n-grams
. (2015)</ref> apply a character-level CNN for text classification and achieve competitive results. <ref type="bibr" target="#b22">Socher et al. (2013)</ref>   explore the structure of a sentence and
igrams and bagof-bigrams as features respectively.</p><p>Text Features are constructed according to <ref type="bibr" target="#b9">(Kiritchenko et al., 2014)</ref>, including word and character n-grams
target="#b31">Zhang et al., 2015;</ref><ref type="bibr" target="#b7">Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b26">Tang et al., 2015)</ref>, in this paper we test the hypothesis that b seline methods and results are reported in <ref type="bibr" target="#b31">(Zhang et al., 2015;</ref><ref type="bibr" target="#b26">Tang et al., 2015)</ref>.</p></div> <div xmlns="http://www.tei-c.org/ d otherwise.</p><p>Yelp reviews are obtained from the Yelp Dataset Challenge in 2013, 2014 and 2015 <ref type="bibr" target="#b26">(Tang et al., 2015)</ref>. There are five levels of ratings from 1 to mlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">SVMs</head><p>SVMs-based methods are reported in <ref type="bibr" target="#b26">(Tang et al., 2015)</ref>, including SVM+Unigrams, Bigrams, Text Feat 0"><head n="3.2.3">Neural Network methods</head><p>The neural network based methods are reported in <ref type="bibr" target="#b26">(Tang et al., 2015)</ref> and <ref type="bibr" target="#b31">(Zhang e states of all words is used as feature for classification. Conv-GRNN and LSTM-GRNN were proposed by <ref type="bibr" target="#b26">(Tang et al., 2015)</ref>. They also explore the hierarchical structu ibr" target="#b11">(Lai et al., 2015;</ref><ref type="bibr" target="#b32">Zhou et al., 2015)</ref>. <ref type="bibr" target="#b26">Tang et al. (2015)</ref> use hierarchical structure in sentiment clas
target="#b31">Zhang et al., 2015;</ref><ref type="bibr" target="#b7">Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b26">Tang et al., 2015)</ref>, in this paper we test the hypothesis that b seline methods and results are reported in <ref type="bibr" target="#b31">(Zhang et al., 2015;</ref><ref type="bibr" target="#b26">Tang et al., 2015)</ref>.</p></div> <div xmlns="http://www.tei-c.org/ d otherwise.</p><p>Yelp reviews are obtained from the Yelp Dataset Challenge in 2013, 2014 and 2015 <ref type="bibr" target="#b26">(Tang et al., 2015)</ref>. There are five levels of ratings from 1 to mlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">SVMs</head><p>SVMs-based methods are reported in <ref type="bibr" target="#b26">(Tang et al., 2015)</ref>, including SVM+Unigrams, Bigrams, Text Feat 0"><head n="3.2.3">Neural Network methods</head><p>The neural network based methods are reported in <ref type="bibr" target="#b26">(Tang et al., 2015)</ref> and <ref type="bibr" target="#b31">(Zhang e states of all words is used as feature for classification. Conv-GRNN and LSTM-GRNN were proposed by <ref type="bibr" target="#b26">(Tang et al., 2015)</ref>. They also explore the hierarchical structu ibr" target="#b11">(Lai et al., 2015;</ref><ref type="bibr" target="#b32">Zhou et al., 2015)</ref>. <ref type="bibr" target="#b26">Tang et al. (2015)</ref> use hierarchical structure in sentiment clas
ost impossible to hand-craft all the meaningful combinations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>. One may ask that we can enumerate all the possible high-or e modeling different orders of feature combinations.</p><p>For example, Factorization Machines (FM) <ref type="bibr" target="#b25">[26]</ref>, which combine polynomial regression models with factoriza herefore extensively studied in the literature. A well-known example is Factorization Machines (FM) <ref type="bibr" target="#b25">[26]</ref>, which were proposed to mainly capture the first-and secon of involved feature fields, and ?(?) is a non-additive combination function, such as multiplication <ref type="bibr" target="#b25">[26]</ref> and outer product <ref type="bibr" target="#b18">[19,</ref th model names accordingly.</p><p>LR (A). LR only models the linear combination of raw features. FM <ref type="bibr" target="#b25">[26]</ref> (B). FM uses factorization techniques to model secondorder arget="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>. Specifically, we d
ion) is a critical problem for many applications such as online advertising and recommender systems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe hallenging for several reasons. First, the input features are extremely sparse and high-dimensional <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ atures, the machine learning models are easily overfitted. Second, as shown in extensive literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ on domain experts. Moreover, it is almost impossible to hand-craft all the meaningful combinations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>. One may ask that we ature interactions and impractical to capture high-order feature interactions. Recently, many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ is regarded significant for CTR prediction task, which has also been pointed out in existing works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ implicit feature interactions and have been widely integrated into existing CTR prediction methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ nies <ref type="bibr">[8-10, 15, 21, 29, 43]</ref>. For example, Google developed the Wide&amp;Deep <ref type="bibr" target="#b7">[8]</ref> learning system for recommender systems, which combines the br" target="#b40">[41]</ref>, DeepCrossing <ref type="bibr" target="#b31">[32]</ref>, Wide&amp;Deep <ref type="bibr" target="#b7">[8]</ref> and DeepFM <ref type="bibr" target="#b10">[11]</ref> utilize e name the joint model AutoInt+ and compare it with the following algorithms:</p><p>? Wide&amp;Deep <ref type="bibr" target="#b7">[8]</ref>. Wide&amp;Deep integrates the outputs of logistic regression combinatorial features to yield good prediction performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targe

"foot" target="#foot_8">8</ref> . To prevent overfitting, we use grid search to select dropout rate <ref type="bibr" target="#b33">[34]</ref> from {0.1 -0.9} for MovieLens-1M data set, and we found dr
the input features are extremely sparse and high-dimensional <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar easily overfitted. Second, as shown in extensive literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar apture high-order feature interactions. Recently, many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar prediction performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar tion task, which has also been pointed out in existing works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref>.</p></div> <div xml been widely integrated into existing CTR prediction methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>. To investigate whe e="bibr" target="#b31">[32]</ref>, Wide&amp;Deep <ref type="bibr" target="#b7">[8]</ref> and DeepFM <ref type="bibr" target="#b10">[11]</ref> utilized feed-forward neural networks to model high-order e&amp;Deep integrates the outputs of logistic regression and feed-forward neural networks. ? DeepFM <ref type="bibr" target="#b10">[11]</ref>. DeepFM combines trainditional second-order factorization Space Complexity. The embedding layer, which is a shared component in neural network-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta inatorial feature" and "feature interaction" interchangeably as they are both used in the literature<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19</ref></p></note> 			<note
eractions and have been proved effective for various tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. However, limited by its polynomial fitting time, it is onl een proved effective for many tasks in recommender systems <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Afterwards, different variants of factorization machines h
the input features are extremely sparse and high-dimensional <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar easily overfitted. Second, as shown in extensive literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar apture high-order feature interactions. Recently, many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar prediction performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar tion task, which has also been pointed out in existing works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref>.</p></div> <div xml been widely integrated into existing CTR prediction methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>. To investigate whe e="bibr" target="#b31">[32]</ref>, Wide&amp;Deep <ref type="bibr" target="#b7">[8]</ref> and DeepFM <ref type="bibr" target="#b10">[11]</ref> utilized feed-forward neural networks to model high-order e&amp;Deep integrates the outputs of logistic regression and feed-forward neural networks. ? DeepFM <ref type="bibr" target="#b10">[11]</ref>. DeepFM combines trainditional second-order factorization Space Complexity. The embedding layer, which is a shared component in neural network-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta inatorial feature" and "feature interaction" interchangeably as they are both used in the literature<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19</ref></p></note> 			<note
tions to model higher-order features. Similarly, PNN <ref type="bibr" target="#b24">[25]</ref>, FNN <ref type="bibr" target="#b40">[41]</ref>, DeepCrossing <ref type="bibr" target="#b31">[32]</ref>, W
">[35]</ref>, text summarization <ref type="bibr" target="#b29">[30]</ref>, and recommender systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" ta
rget="#b1">[2]</ref> and has been proved effective in a variety of tasks such as question answering <ref type="bibr" target="#b34">[35]</ref>, text summarization <ref type="bibr" target="#b29">[30]</r
ctions, it is not trivial to explain which combinations are useful. Second, some tree-based methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" ta
puter vision and speech processing with deep neural networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. Space Complexity. The embedding layer, which is a shared c can see that AutoInt is able to identify the meaningful combinatorial feature &lt;Gender=Male, Age= <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" t

dependency in machine translation <ref type="bibr" target="#b35">[36]</ref> and sentence embedding <ref type="bibr" target="#b19">[20]</ref>, and has been successfully applied to capturing node simil Gender=Male, Age= <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" t

Our proposed model makes use of the latest techniques in the literature of deep learning: attention <ref type="bibr" target="#b1">[2]</ref> and residual networks <ref type="bibr" target="#b11">[12]</r target="#b11">[12]</ref>. Attention is first proposed in the context of neural machine translation <ref type="bibr" target="#b1">[2]</ref> and has been proved effective in a variety of tasks such as
the input features are extremely sparse and high-dimensional <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar easily overfitted. Second, as shown in extensive literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar apture high-order feature interactions. Recently, many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar prediction performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar tion task, which has also been pointed out in existing works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref>.</p></div> <div xml been widely integrated into existing CTR prediction methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>. To investigate whe e="bibr" target="#b31">[32]</ref>, Wide&amp;Deep <ref type="bibr" target="#b7">[8]</ref> and DeepFM <ref type="bibr" target="#b10">[11]</ref> utilized feed-forward neural networks to model high-order e&amp;Deep integrates the outputs of logistic regression and feed-forward neural networks. ? DeepFM <ref type="bibr" target="#b10">[11]</ref>. DeepFM combines trainditional second-order factorization Space Complexity. The embedding layer, which is a shared component in neural network-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta inatorial feature" and "feature interaction" interchangeably as they are both used in the literature<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19</ref></p></note> 			<note
eractions and have been proved effective for various tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. However, limited by its polynomial fitting time, it is onl een proved effective for many tasks in recommender systems <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Afterwards, different variants of factorization machines h
tree-based methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> combined the power of embedding-based models and tree-based
iques, are developed to model feature interactions and have been proved effective for various tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. However, limited ond-order feature interactions and have been proved effective for many tasks in recommender systems <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Afterwards, diffe
rget="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38]</ref> based on deep neural networks have been proposed to model t ref type="bibr" target="#b25">[26]</ref> and outer product <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref>. For example, x i 1 ? x i 2 is a second-order combinatorial ut in existing works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n three lines of works that learn feature interactions in an explicit fashion. First, Deep&amp;Cross <ref type="bibr" target="#b37">[38]</ref> and xDeepFM <ref type="bibr" target="#b18">[19]</ref> took feature interactions are implicitly captured by the nonlinearity of neural networks.</p><p>CrossNet <ref type="bibr" target="#b37">[38]</ref> (C). Cross Network, which is the core of Deep&amp;Cross mo orization machines and feed-forward neural network, with a shared embedding layer. ? Deep&amp;Cross <ref type="bibr" target="#b37">[38]</ref>. Deep&amp;Cross is the extension of CrossNet by integratin
the input features are extremely sparse and high-dimensional <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar easily overfitted. Second, as shown in extensive literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar apture high-order feature interactions. Recently, many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar prediction performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar tion task, which has also been pointed out in existing works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref>.</p></div> <div xml been widely integrated into existing CTR prediction methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>. To investigate whe e="bibr" target="#b31">[32]</ref>, Wide&amp;Deep <ref type="bibr" target="#b7">[8]</ref> and DeepFM <ref type="bibr" target="#b10">[11]</ref> utilized feed-forward neural networks to model high-order e&amp;Deep integrates the outputs of logistic regression and feed-forward neural networks. ? DeepFM <ref type="bibr" target="#b10">[11]</ref>. DeepFM combines trainditional second-order factorization Space Complexity. The embedding layer, which is a shared component in neural network-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta inatorial feature" and "feature interaction" interchangeably as they are both used in the literature<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19</ref></p></note> 			<note
sociations. Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in <ref type="bibr" target="#b6">(Johnson and Zhang, 2015b</ref>) and later tested with large datasets gion embedding enhanced with unsupervised embeddings (embeddings trained in an unsupervised manner) <ref type="bibr" target="#b6">(Johnson and Zhang, 2015b)</ref> for improving accuracy.</p></div> <di ing each word in the text to a word vector (word embedding). We take a more general viewpoint as in <ref type="bibr" target="#b6">(Johnson and Zhang, 2015b)</ref> and consider text region embedding -e w-1 as input, serves as an unsupervised embedding function in the model for text categorization. In <ref type="bibr" target="#b6">(Johnson and Zhang, 2015b)</ref> unsupervised embeddings obtained this ings. Note that ShallowCNN enhanced with unsupervised embeddings (row 2) was originally proposed in <ref type="bibr" target="#b6">(Johnson and Zhang, 2015b)</ref> as a semi-supervised extension of <re
be effective for text categorization. While simple and shallow convolutional neural networks (CNNs) <ref type="bibr" target="#b9">(Kim, 2014;</ref><ref type="bibr">John-son and Zhang, 2015a)</ref> wer on and Zhang, 2016)</ref>. That is, upper-case letters were converted to lower-case letters. Unlike <ref type="bibr" target="#b9">(Kim, 2014;</ref><ref type="bibr" target="#b14">Zhang et al., 2015;</r
b0">Conneau et al., 2016)</ref>, a complex combination of CNNs and recurrent neural networks (RNNs) <ref type="bibr" target="#b12">(Tang et al., 2015)</ref>, and RNNs in a wordsentence hierarchy <ref ex than DPCNN due to the use of RNNs and linguistic knowledge for sentence segmentation. Similarly, <ref type="bibr" target="#b12">Tang et al. (2015)</ref> proposed to use CNN or LSTM to represent eac
NNs) <ref type="bibr" target="#b12">(Tang et al., 2015)</ref>, and RNNs in a wordsentence hierarchy <ref type="bibr" target="#b13">(Yang et al., 2016)</ref>.</p><p>A CNN is a feedforward network with over ShallowCNN indicates that the added depth is indeed useful, capturing more global information. <ref type="bibr" target="#b13">Yang et al. (2016)</ref>'s hierarchical attention network (row 3) con
in <ref type="bibr" target="#b6">(Johnson and Zhang, 2015b)</ref> as a semi-supervised extension of <ref type="bibr" target="#b5">(Johnson and Zhang, 2015a)</ref>, and then it was tested on the large
linearity' eases training of deep networks, similar to the role of constant error carousels in LSTM <ref type="bibr" target="#b4">(Hochreiter and Schmidhuder, 1997)</ref>. We empirically observed that
in <ref type="bibr" target="#b6">(Johnson and Zhang, 2015b)</ref> as a semi-supervised extension of <ref type="bibr" target="#b5">(Johnson and Zhang, 2015a)</ref>, and then it was tested on the large
arge amounts of training data (e.g., one million documents). Examples are deep character-level CNNs <ref type="bibr" target="#b14">(Zhang et al., 2015;</ref><ref type="bibr" target="#b0">Conneau et al ry deep 32-layer character-level CNNs were shown to outperform deep 9-layer character-level CNNs of <ref type="bibr" target="#b14">(Zhang et al., 2015)</ref>. However, in <ref type="bibr" target="#b7" erely replacing characters with words in character-level CNNs; doing so rather degraded accuracy in <ref type="bibr" target="#b14">(Zhang et al., 2015)</ref>.</p><p>We carefully studied deepening of w w our model as much more useful in practice. Row 7 shows the performance of deep wordlevel CNN from <ref type="bibr" target="#b14">(Zhang et al., 2015)</ref>, which was designed to match their charact eprocessing To facilitate comparisons with previous results, we used the eight datasets compiled by <ref type="bibr" target="#b14">Zhang et al. (2015)</ref>, summarized in Table <ref type="table" targ that their model outperformed <ref type="bibr">Tang et al.'s model. Conneau et al. (2016)</ref> and <ref type="bibr" target="#b14">Zhang et al. (2015)</ref> proposed deep character-level CNNs (row 4&a letters were converted to lower-case letters. Unlike <ref type="bibr" target="#b9">(Kim, 2014;</ref><ref type="bibr" target="#b14">Zhang et al., 2015;</ref><ref type="bibr" target="#b0">Conneau et al.
b0">Conneau et al., 2016)</ref>, a complex combination of CNNs and recurrent neural networks (RNNs) <ref type="bibr" target="#b12">(Tang et al., 2015)</ref>, and RNNs in a wordsentence hierarchy <ref ex than DPCNN due to the use of RNNs and linguistic knowledge for sentence segmentation. Similarly, <ref type="bibr" target="#b12">Tang et al. (2015)</ref> proposed to use CNN or LSTM to represent eac
b0">Conneau et al., 2016)</ref>, a complex combination of CNNs and recurrent neural networks (RNNs) <ref type="bibr" target="#b12">(Tang et al., 2015)</ref>, and RNNs in a wordsentence hierarchy <ref ex than DPCNN due to the use of RNNs and linguistic knowledge for sentence segmentation. Similarly, <ref type="bibr" target="#b12">Tang et al. (2015)</ref> proposed to use CNN or LSTM to represent eac
in <ref type="bibr" target="#b6">(Johnson and Zhang, 2015b)</ref> as a semi-supervised extension of <ref type="bibr" target="#b5">(Johnson and Zhang, 2015a)</ref>, and then it was tested on the large
ng ability of SNN. Several research efforts have explored STDP algorithms to enable learning in SNN <ref type="bibr" target="#b2">[3]</ref> [4] <ref type="bibr" target="#b4">[5]</ref>. Simulations of im shows comparable accuracy with the stateof-the-art SNN design with deterministic STDP from Diehl <ref type="bibr" target="#b2">[3]</ref>. In Diehl's work, the network yields an accuracy of 91.9% fo
st, while networks demonstrate good accuracy for simple tasks such as MNIST-based digit recognition <ref type="bibr" target="#b11">[12]</ref>, the learning accuracy for difficult tasks such as Fashion els. ? We demonstrate that the stochastic STDP allows good learning accuracy for both simple (MNIST <ref type="bibr" target="#b11">[12]</ref>, 96.1% accuracy) and complex (feature-rich) (Fashion-MNIST
odels based on spiking activity in a region, but they did not discuss STDP-based learning using SNN <ref type="bibr" target="#b10">[11]</ref>.</p><p>The existing SNN simulators such as NEST and CARLSi
t-Plasticity (STDP) in the synapse models. The STDP is a phenomenon observed in biology experiments <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, which can be used
eed of SNNs. Several SNN simulators with integrated STDP learning have been presented such as Brain <ref type="bibr" target="#b7">[8]</ref>, NEST <ref type="bibr" target="#b8">[9]</ref> and CARLsim <r
st, while networks demonstrate good accuracy for simple tasks such as MNIST-based digit recognition <ref type="bibr" target="#b11">[12]</ref>, the learning accuracy for difficult tasks such as Fashion els. ? We demonstrate that the stochastic STDP allows good learning accuracy for both simple (MNIST <ref type="bibr" target="#b11">[12]</ref>, 96.1% accuracy) and complex (feature-rich) (Fashion-MNIST
es at the synapse. Stochastic STDP is achieved with an algorithm inspired by the work of Srinivasan <ref type="bibr" target="#b13">[14]</ref>. The probabilities for potentiation and depression are def
ionship between two neurons can be encoded as conductance of the synapse. Using model introduced in <ref type="bibr" target="#b3">[4]</ref>, conductance is modulated by equations ?G p = ? p e -?p(G-Gm
st, while networks demonstrate good accuracy for simple tasks such as MNIST-based digit recognition <ref type="bibr" target="#b11">[12]</ref>, the learning accuracy for difficult tasks such as Fashion els. ? We demonstrate that the stochastic STDP allows good learning accuracy for both simple (MNIST <ref type="bibr" target="#b11">[12]</ref>, 96.1% accuracy) and complex (feature-rich) (Fashion-MNIST
ionship between two neurons can be encoded as conductance of the synapse. Using model introduced in <ref type="bibr" target="#b3">[4]</ref>, conductance is modulated by equations ?G p = ? p e -?p(G-Gm
eed of SNNs. Several SNN simulators with integrated STDP learning have been presented such as Brain <ref type="bibr" target="#b7">[8]</ref>, NEST <ref type="bibr" target="#b8">[9]</ref> and CARLsim <r
and hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. However, these approaches rely on manual feature engineeri (3) Hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> combine the above two categories and learn user/item embedd ecommender systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar ized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type="bibr" target="#b27">[28]</ref>, we use a trainable and personalized relation scoring func where GCNs can be used directly, while here we investigate GCNs for heterogeneous KGs. Wang et al. <ref type="bibr" target="#b27">[28]</ref> use GCNs in KGs for recommendation, but simply applying GC o a user-personalized weighted graph that characterizes user's preferences. To this end, similar to <ref type="bibr" target="#b27">[28]</ref>, we use a user-specific relation scoring function s u (r ) ear that the performance of KGNN-LS with a non-zero λ is better than λ = 0 (the case of Wang et al. <ref type="bibr" target="#b27">[28]</ref>), which justifies our claim that LS regularization can ass
ef type="bibr" target="#b33">34]</ref>, and hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. However, these app hs, which are hard to tune in practice. (3) Hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> combine the above t ht for KG part is 0.1 for all datasets. The learning rate are the same as in SVD.</p><p>• RippleNet <ref type="bibr" target="#b23">[24]</ref> is a representative of hybrid methods, which is a memory-n

(2) Edge weights are parameterized and therefore learnable <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>. Inspired by these ount of prior works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar
weights are assumed to be given as input and therefore fixed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>; (2) Edge weights a
Internet applications to meet user's personalized interests and alleviate the information overload <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" targ
on the graph directly and apply "convolution" (i.e., weighted average) to local neighbors of a node <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target
all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.</p><p>• SVD <ref type="bibr" target="#b11">[12]</ref> is a classic CF-based model using inner product to model u
ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>, and hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" ta on manually designed metapaths/meta-graphs, which are hard to tune in practice. (3) Hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" ta
ecommender systems can be classified into path-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>, embedding-based me y lack an end-to-end way of training. (2) Path-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> explore various pat "#b1">[2]</ref> to each user-item pair. The dimension of TransE is 32 for all datasets.</p><p>• PER <ref type="bibr" target="#b32">[33]</ref> is a representative of path-based methods, which treats th
weights are assumed to be given as input and therefore fixed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>; (2) Edge weights a
ing process, which leads to better generalization. We develop an approach based on label smoothness <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref>, which assumes tha therefore learnable <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>. Inspired by these methods, we design a module of label smo get="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>. Therefore, more re r the unlabeled nodes l * u (E\V). To solve the issue, we propose minimizing the leave-one-out loss <ref type="bibr" target="#b34">[35]</ref>. Suppose we hold out a single item v and treat it unlabele
works developed GNNs architecture for recommender systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar ipartite graph in Pinterest. Monti et al. <ref type="bibr" target="#b13">[14]</ref> and Berg et al. <ref type="bibr" target="#b18">[19]</ref> model recommender systems as matrix completion and design
ution" (i.e., weighted average) to local neighbors of a node <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Recently, rese s for the architecture of our model, e.g., GCN<ref type="bibr" target="#b10">[11]</ref> or GraphSAGE<ref type="bibr" target="#b6">[7]</ref>. Here we use GCN<ref type="bibr" target="#b10">[11]</ref> as
systems, but the KGE algorithms focus more on modeling rigorous semantic relatedness (e.g., TransE <ref type="bibr" target="#b1">[2]</ref> assumes head +relation = tail), which are more suitable for ll datasets. • LibFM + TransE extends LibFM by attaching an entity representation learned by TransE <ref type="bibr" target="#b1">[2]</ref> to each user-item pair. The dimension of TransE is 32 for al other and semantically related; (2) Treating A u symmetric will greatly increase the matrix density.<ref type="bibr" target="#b1">2</ref> There are several candidate designs for the architecture of ou
ype="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>, embedding-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" targ g KG-aware recommender systems can be classified into three categories: (1) Embedding-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" targ
type="bibr" target="#b35">36]</ref>, embedding-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar lassified into three categories: (1) Embedding-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar
rget="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>, and hybrid methods <ref type="bibr" target="#b17">[18,</re rget="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> pre-process a KG with knowledge graph embedding (KGE) <ref nt" for Dianping-Food. The settings of dimension and learning rate are the same as SVD.</p><p>• CKE <ref type="bibr" target="#b33">[34]</ref> is a representative of embedding-based methods, which comb
al neighbors of a node <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Recently, researchers also deployed GCNs in recommen
rmance degradation as we will show later. Schlichtkrull et al. also propose using GCNs to model KGs <ref type="bibr" target="#b16">[17]</ref>, but not for the purpose of recommendations.</p></div> <di
ommender systems that are based on collaborative filtering <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> usually suffer from the coldstart problem and have trouble information such as user/item profiles <ref type="bibr" target="#b22">[23]</ref> or social networks <ref type="bibr" target="#b21">[22]</ref>.</p><p>Knowledge graphs (KGs) capture structured informati
captured by the KG. Existing KG-aware recommender systems can be classified into path-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" targ ddition, embedding-based methods usually lack an end-to-end way of training. (2) Path-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" targ
issue can be addressed by introducing additional sources of information such as user/item profiles <ref type="bibr" target="#b22">[23]</ref> or social networks <ref type="bibr" target="#b21">[22]</re
all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.</p><p>• SVD <ref type="bibr" target="#b11">[12]</ref> is a classic CF-based model using inner product to model u
all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.</p><p>• SVD <ref type="bibr" target="#b11">[12]</ref> is a classic CF-based model using inner product to model u
on graphs, as highlighted by a large amount of prior works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar
target="#b31">32]</ref>. Traditional recommender systems that are based on collaborative filtering <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> usually suffer fro
on in Fourier domain and calculate the eigendecomposition of the graph Laplacian, Defferrard et al. <ref type="bibr" target="#b4">[5]</ref> approximate the convolutional filters by Chebyshev expansion
nping-Food R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 SVD 0. • LibFM <ref type="bibr" target="#b15">[16]</ref> is a widely used feature-based factorization model for CTR
rmance degradation as we will show later. Schlichtkrull et al. also propose using GCNs to model KGs <ref type="bibr" target="#b16">[17]</ref>, but not for the purpose of recommendations.</p></div> <di
ecommender systems can be classified into path-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>, embedding-based me y lack an end-to-end way of training. (2) Path-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> explore various pat "#b1">[2]</ref> to each user-item pair. The dimension of TransE is 32 for all datasets.</p><p>• PER <ref type="bibr" target="#b32">[33]</ref> is a representative of path-based methods, which treats th
/ref><ref type="bibr" target="#b33">34]</ref> pre-process a KG with knowledge graph embedding (KGE) <ref type="bibr" target="#b29">[30]</ref> algorithms, then incorporate learned entity embeddings int
nping-Food R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 SVD 0. • LibFM <ref type="bibr" target="#b15">[16]</ref> is a widely used feature-based factorization model for CTR
, such as graphs which encode the pairwise relationships. This includes examples of social networks <ref type="bibr" target="#b2">[3]</ref>, protein interfaces <ref type="bibr" target="#b3">[4]</ref>, assification, including Cora, Citeseer, Pubmed <ref type="bibr" target="#b10">[11]</ref> and Reddit <ref type="bibr" target="#b2">[3]</ref>. Intensive experiments verify the effectiveness of our metho lf-attention strategy.</p><p>More recently, two kinds of sampling-based methods including GraphSAGE <ref type="bibr" target="#b2">[3]</ref> and FastGCN <ref type="bibr" target="#b20">[21]</ref> were d and Extensions</head><p>Relation to other sampling methods. We contrast our approach with GraphSAGE <ref type="bibr" target="#b2">[3]</ref> and FastGC-N <ref type="bibr" target="#b20">[21]</ref> regar and Pubmed <ref type="bibr" target="#b10">[11]</ref>  community different posts belong to in Reddit <ref type="bibr" target="#b2">[3]</ref>. These graphs are varying in sizes from small to large. Part set to be 16. For the Reddit dataset, the hidden dimensions are selected to be 256 as suggested by <ref type="bibr" target="#b2">[3]</ref>. The numbers of the sampling nodes for all layers excluding "><head n="7.1">Alation Studies on the Adaptive Sampling</head><p>Baselines. The codes of GraphSAGE <ref type="bibr" target="#b2">[3]</ref> and FastGCNN <ref type="bibr" target="#b20">[21]</ref> provi
s learning the graph embedding is already an important topic <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, this paper mainly focu use the nodes of the (l − 1)-th layer to preserve the second-order proximity (see the definition in <ref type="bibr" target="#b6">[7]</ref>). For the (l + 1)-th layer, the nodes of the (l − 1)-th laye
one. We detail the comparisons in § 6. Another related work is the control-variate-based method by <ref type="bibr" target="#b21">[22]</ref>. However, the sampling process of this method is node-wise
nes multiple-hop neighborhoods by using the powers series of a transition matrix, and other authors <ref type="bibr" target="#b18">[19]</ref> extracted normalized neighborhoods that contain a fixed nu
data, such as image classification <ref type="bibr" target="#b0">[1]</ref> and machine translation <ref type="bibr" target="#b1">[2]</ref>. By making use of local connection and weight sharing, CNNs
ady an important topic <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, this paper mainly focus on learning the representations for
te-of-the-art methods. We contrast the performance of our methods with the graph kernel method KLED <ref type="bibr" target="#b24">[25]</ref> and Diffusion Convolutional Network (DCN) <ref type="bibr"
one. We detail the comparisons in § 6. Another related work is the control-variate-based method by <ref type="bibr" target="#b21">[22]</ref>. However, the sampling process of this method is node-wise
e arbitrarily structured graphs. Whereas learning the graph embedding is already an important topic <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target >Moreover, we explore how to enable efficient message passing across distant nodes. Current methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref> resort to random walk s, we can sample the multi-hop neighborhoods for the GCN update in a similar way as the random walk <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. However, the random
volutionized various machine learning tasks with grid-like input data, such as image classification <ref type="bibr" target="#b0">[1]</ref> and machine translation <ref type="bibr" target="#b1">[2]</r ar, our motivation of applying the skip connection is different to the residual function in ResNets <ref type="bibr" target="#b0">[1]</ref>. The purpose of employing the skip connection in <ref type=" in ResNets <ref type="bibr" target="#b0">[1]</ref>. The purpose of employing the skip connection in <ref type="bibr" target="#b0">[1]</ref> is to gain accuracy by increasing the network depth. Here, w
one. We detail the comparisons in § 6. Another related work is the control-variate-based method by <ref type="bibr" target="#b21">[22]</ref>. However, the sampling process of this method is node-wise
based methods <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b51">[52]</ref> have demonstrated
n, i.e. T = {? id , ? exp , s, pitch, yaw, roll, t x , t y }. The network is based on the Resnet-18 <ref type="bibr" target="#b19">[20]</ref> with the modification of changing the output number of the
Then, the irradiance of a vertex v i with surface normal n i and scalar albedo b i is expressed as <ref type="bibr" target="#b39">[40]</ref>:</p><formula xml:id="formula_4">L(n i , b i | ?) = b i ? B
ce reconstruction from monocular video. For quantitative evaluation, we test on the FaceCap dataset <ref type="bibr" target="#b54">[55]</ref>. The dataset consists of 200 frames along with 3D meshes c ates the suitability of  From top to bottom: input face video frame and groundtruth mesh in dataset <ref type="bibr" target="#b54">[55]</ref>, results of the inverse rendering approach, results of our On the subregion, the mean error is 2.20mm for CoarseNet and 2.03mm for FineNet.</p><p>Input Stereo <ref type="bibr" target="#b54">[55]</ref> [18] Ours Figure <ref type="figure" target="#fig_2">12:</r with much faster processing speed. The groundtruth mesh is constructed using the binocular approach <ref type="bibr" target="#b54">[55]</ref>.</p><p>Figure <ref type="figure" target="#fig_4">13</ref>: to the groundtruth 3D face shape generated by the binocular facial performance capture proposed in <ref type="bibr" target="#b54">[55]</ref>. We can see that the result of our learning based face tra
, the symmetry of the human face has often been employed <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Kemelmacher et
lly automatically fitting a 3D Morphable Model to a single image using landmarks and edge features. <ref type="bibr" target="#b45">[46]</ref> introduces a framework to fit a parametric face model with 41">[42]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b13">[14]</ref> on single-image of the methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b13">[14]</ref> are generated u 41">[42]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b13">[14]</ref> and ours. It ca
aphics, and has many applications such as face recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b53">[54]</ref> and face animation <ref type="bibr" target="#b22">[23]</re single image <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bib in various computer vision tasks, such as face recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b53">[54]</ref>, face alignment <ref type="bibr" target="#b59">[60]</ref>, b41">[42]</ref> extends these methods by also learning detailed geometry in an unsupervised manner. <ref type="bibr" target="#b53">[54]</ref> proposes to regress robust and discriminative 3DMM with a
sel Face Model (BFM) <ref type="bibr" target="#b36">[37]</ref> for A id and A alb and FaceWarehouse <ref type="bibr" target="#b9">[10]</ref> for A exp .</p><p>Fine-scale details. As 3DMM is a low-dime
ructure of a 3D face from a single image <ref type="bibr" target="#b3">[4]</ref> or multiple images <ref type="bibr" target="#b1">[2]</ref>, the facial details like wrinkles and folds are not possible
sel Face Model (BFM) <ref type="bibr" target="#b36">[37]</ref> for A id and A alb and FaceWarehouse <ref type="bibr" target="#b9">[10]</ref> for A exp .</p><p>Fine-scale details. As 3DMM is a low-dime
aches for face shape reconstruction have grown in popularity over the last decade. Blanz and Vetter <ref type="bibr" target="#b3">[4]</ref> proposed to represent a textured 3D face with principal comp ef>. Although such a model is able to capture the global structure of a 3D face from a single image <ref type="bibr" target="#b3">[4]</ref> or multiple images <ref type="bibr" target="#b1">[2]</ref>, " target="#b59">[60]</ref>, <ref type="bibr" target="#b30">[31]</ref> use 3D Morphable Model (3DMM) <ref type="bibr" target="#b3">[4]</ref> to represent 3D faces and use CNN to learn the 3DMM and pose oposes to use an analysis-by-synthesis energy function as the loss function during network training <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b52">[53]</ref>. <ref type="bibr" rtex v i (i = 1, 2, . . . , n).</formula><p>Parametric face model. We use 3D Morphable Model (3DMM) <ref type="bibr" target="#b3">[4]</ref> as the parametric face model to encode 3D face geometry and t = {s, t} and ? l = {? alb , r}. The fitting process is based on the analysisby-synthesis strategy <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b52">[53]</ref>, and we seek a so
://www.tei-c.org/ns/1.0"><p>We present a variational approximation to the information bottleneck of <ref type="bibr" target="#b33">Tishby et al. (1999)</ref>. This variational approach allows us to pa oot_2">3</ref> This approach is known as the information bottleneck (IB), and was first proposed in <ref type="bibr" target="#b33">Tishby et al. (1999)</ref>. Intuitively, the first term in R IB encou challenging. There are two notable exceptions: the first is when X, Y and Z are all discrete, as in <ref type="bibr" target="#b33">Tishby et al. (1999)</ref>; this can be used to cluster discrete data

o cluster discrete data, such as words. The second case is when X, Y and Z are all jointly Gaussian <ref type="bibr" target="#b8">(Chechik et al., 2005)</ref>. However, these assumptions both severely
el of adversarial robustness even when β → 0. This can be explained by the theoretical framework of <ref type="bibr" target="#b11">Fawzi et al. (2016)</ref>. Their work proves that quadratic classifie er which is bounded by a quadratic function, which is interesting because the theoretical framework <ref type="bibr" target="#b11">Fawzi et al. (2016)</ref> proves that quadratic classifiers have grea
sentation of the network. This form of the information bottleneck is known as predictive information<ref type="bibr" target="#b4">(Bialek et al., 2001;</ref><ref type="bibr" target="#b24">Palmer et al
OF ADVERSARIES</head><p>Since the initial work by <ref type="bibr">Szegedy et al. (2013)</ref> and <ref type="bibr" target="#b31">Goodfellow et al. (2014)</ref>, many different adversaries have been mlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b31">Goodfellow et al. (2014)</ref> and the L 2 optimization method propos versarial example. As originally described, FGS generates untargeted adversarial examples. On MNIST,<ref type="bibr" target="#b31">Goodfellow et al. (2014)</ref> reported that FGS could generate adver a minimal perturbation that changes the model's classification; single-step gradient-based attacks <ref type="bibr" target="#b31">(Goodfellow et al., 2014;</ref><ref type="bibr">Kurakin et al., 2016; the perturbation using L 0 , L 1 , L 2 , and L ∞ norms <ref type="bibr">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b31">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b7">Carlini &

approximate the problem. Variational bounds on mutual information have previously been explored in <ref type="bibr" target="#b1">Agakov (2004)</ref>, though not in conjunction with the information bo



o-Encoder (CAE) to learn the joint representation using Denoising Auto-Encoder (DAE) style learning <ref type="bibr" target="#b14">[15]</ref>. Fig. <ref type="figure" target="#fig_4">5</ref> shows a s
</ref>, we combine the following features as the visual middle-level representation: experiments in <ref type="bibr" target="#b10">[11]</ref> find out that people under stress and anxiety prefer lower
</ref>, we combine the following features as the visual middle-level representation: experiments in <ref type="bibr" target="#b10">[11]</ref> find out that people under stress and anxiety prefer lower
</ref>, we combine the following features as the visual middle-level representation: experiments in <ref type="bibr" target="#b10">[11]</ref> find out that people under stress and anxiety prefer lower
e representations in speech video <ref type="bibr" target="#b12">[13]</ref> or images with text tag <ref type="bibr" target="#b13">[14]</ref>.  We propose a Cross-media Auto-Encoder (CAE) to learn the
d zero to hurt back propagation optimization while creating an easy-to-train sparse representations <ref type="bibr" target="#b11">[12]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
d zero to hurt back propagation optimization while creating an easy-to-train sparse representations <ref type="bibr" target="#b11">[12]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
e representations in speech video <ref type="bibr" target="#b12">[13]</ref> or images with text tag <ref type="bibr" target="#b13">[14]</ref>.  We propose a Cross-media Auto-Encoder (CAE) to learn the
ers turning to non-invasive ways to automatically detect psychological stress from social networks. <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref> presented method
tress more efficiently and timely recent years. Most of them <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> focus on detecting stress in real-time via body-worn sensors eless sensors to collect physiological data to support real-time detection of stress. Hong Lu. etc. <ref type="bibr" target="#b2">[3]</ref> proposed StressSense for unobtrusively recognizing stress fr ide-in-china</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><ref type="bibr" target="#b2">3</ref> Xinhuanet News, http://news.xinhuanet.com/ newmedia/2013-02/21
xtraction of the middle-level representations is based on psychological principles and art theories <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Finally, a deep neura ions from tweets' low level features based on previous psychological principles and art theories in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. The definitions are a /1.0"><head>2) Visual Attributes:</head><p>Based on previous work on affective image classification <ref type="bibr" target="#b5">[6]</ref> and color psychology theories <ref type="bibr" target="#b9"> eels dull, otherwise clear. To extract the five-color theme feature, we use the method described in <ref type="bibr" target="#b5">[6]</ref>. Saturation, brightness and warm or cool color features of a
data sources. We use natural language processing methods, such as Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b5">[8]</ref> and topical phrase mining <ref type="bibr" target="#b12">[16 as their topical similarity using a process they call Phrase LDA.</p><p>Latent Dirichlet Allocation <ref type="bibr" target="#b5">[8]</ref> is the most common topic modeling process and PLDA+ is a sca
ion system to utilize the entire MEDLINE data set. By using state-of-the-art tools, such as ToPMine <ref type="bibr" target="#b12">[16]</ref> and FastText <ref type="bibr" target="#b6">[9]</ref>, we a Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b5">[8]</ref> and topical phrase mining <ref type="bibr" target="#b12">[16]</ref>, along with other data mining techniques to conceptually l word phrases from that corpus such as "asthma a ack," allowing us to treat phrases as single tokens <ref type="bibr" target="#b12">[16]</ref>. Next, we send the corpus through FastText, the most recen urned to the UMLS SPECIALIST NLP toolset <ref type="bibr" target="#b0">[1]</ref> as well as ToPMine <ref type="bibr" target="#b12">[16]</ref> and FastText <ref type="bibr" target="#b6">[9,</ref><ref t o . It is also important to note that we modify the version of ToP-Mine distributed by El-Kishky in <ref type="bibr" target="#b12">[16]</ref> to allow phrases containing numbers, such as gene names li over weigh a specialized language. However, phrase mining approaches that recover n-grams, such as <ref type="bibr" target="#b12">[16]</ref>, produce accurate methods without limiting the dictionary.
ch contain knowledge representative of the path as a whole. PLDA+, a scalable implementation of LDA <ref type="bibr" target="#b24">[28]</ref>, allows us to quickly nd topic models in these clouds. Unl truction process, all share common topics. We perform topic modeling on these documents using PLDA+ <ref type="bibr" target="#b24">[28]</ref>. e result is a set of plain text topics which represent di s and PLDA+ is a scalable implementation of this algorithm <ref type="bibr" target="#b16">[20,</ref><ref type="bibr" target="#b24">28]</ref>. Developed by Zhiyuan Liu et al., PLDA+ quickly identi es g
mor drug development <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b8">11,</ref><ref type="bibr" target="#b33">37]</ref> and represents a case for repurposing target anti-viral dru
athering methods struggle to keep up with the growing wealth of forgo en and hard to nd information <ref type="bibr" target="#b13">[17]</ref>. eir work in the eld of hypothesis generation has included
to query for concepts intersecting two keywords, is a notable contribution to hypothesis generation <ref type="bibr" target="#b23">[27]</ref>. is system, proposed by Liu et al., is similar to both our
r" target="#b44">48]</ref>. Currently, DDX3 is an established target for antitumor drug development <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b8">11,</ref><ref type="bibr" targ
onnect to simply 3 of the 130 semantic types. ey supplement this subset with concepts from the OMIM <ref type="bibr" target="#b15">[19]</ref> and GWAS <ref type="bibr" target="#b3">[6]</ref> databases e network analytic approach to understand biomedical data in both their work on the disease network <ref type="bibr" target="#b15">[19]</ref> as well as their more generalized symptoms-disease network more generalized symptoms-disease network <ref type="bibr" target="#b47">[51]</ref>. In the former <ref type="bibr" target="#b15">[19]</ref>, the authors construct a bipartite network of disease phon
ect from El-Kishky et al., is focused on discovering multi-word phrases from a large corpus of text <ref type="bibr" target="#b16">[20]</ref>. is project intelligently groups unigrams together to crea is the most common topic modeling process and PLDA+ is a scalable implementation of this algorithm <ref type="bibr" target="#b16">[20,</ref><ref type="bibr" target="#b24">28]</ref>. Developed by Zhiy
athering methods struggle to keep up with the growing wealth of forgo en and hard to nd information <ref type="bibr" target="#b13">[17]</ref>. eir work in the eld of hypothesis generation has included
o verify the ability of MOLIERE to make predictions similar to previous systems with restricted LDA <ref type="bibr" target="#b45">[49]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head .</p><p>Bio-LDA is a modi cation of LDA which limits the set of keywords to the set present in UMLS <ref type="bibr" target="#b45">[49]</ref>. is reduction improves the meaning and readability of topi e choices with methods described in Section 6.</p><p>We repeat an experiment done by Wang et al. in <ref type="bibr" target="#b45">[49]</ref> wherein we discover the implicit connections between the d <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Venlafaxine to HTR1A</head><p>Wang et al. in <ref type="bibr" target="#b45">[49]</ref> use a similar topic modeling approach, and nd during one o es, Venlafaxine to HTR1A, and Venlafaxine to HTR2A, we can corroborate the ndings of Wang et al. in <ref type="bibr" target="#b45">[49]</ref>. We nd that neither pair of keywords is directly connected ion space or the size of their dictionary. For example, specialized versions of LDA such as Bio-LDA <ref type="bibr" target="#b45">[49]</ref> uncover latent topics using a dictionary that gives a prio generation systems <ref type="bibr" target="#b37">[41,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" target="#b45">49]</ref>. However, most of these improve their performance by restri
e the network nodes by minimizing such objectives as the minimum logarithmic or linear arrangements <ref type="bibr" target="#b31">[35,</ref><ref type="bibr" target="#b32">36]</ref>. On a mixture of K
ion, and metastasis <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b20">24,</ref><ref type="bibr" target="#b39">43,</ref><ref type="bibr" target="#b44">48]</ref>. Currently, DDX3 is
A − A edges we anticipate an improvement of at least 20% in the number of cache misses according to <ref type="bibr" target="#b30">[34]</ref>. Mass Evaluation: We note that evaluation techniques are l
xt-based search application meant to help doctors make connections from within the MEDLINE data set <ref type="bibr" target="#b34">[38,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" ta
mor drug development <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b8">11,</ref><ref type="bibr" target="#b33">37]</ref> and represents a case for repurposing target anti-viral dru
Cell-matrix adhesion, tumor cells invasion, and metastasis <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b20">24,</ref><ref type="bibr" target="#b39">43,</ref><ref type="bibr" tar
ref type="bibr" target="#b15">[19]</ref> as well as their more generalized symptoms-disease network <ref type="bibr" target="#b47">[51]</ref>. In the former <ref type="bibr" target="#b15">[19]</ref>, ojected networks, the human disease network (HDN), and the Disease Gene Network (DGN). In the la er <ref type="bibr" target="#b47">[51]</ref>, they construct a more generalized human symptoms disease
research pipeline. Several promising observations in this direction have been done by Foster et al. <ref type="bibr" target="#b14">[18]</ref> who examined this through Bourdieu's eld theory of science
) to a new application <ref type="bibr" target="#b1">[4,</ref><ref type="bibr" target="#b2">5,</ref><ref type="bibr" target="#b29">33]</ref>. As an example, the drugs developed for the treatment of in
e the network nodes by minimizing such objectives as the minimum logarithmic or linear arrangements <ref type="bibr" target="#b31">[35,</ref><ref type="bibr" target="#b32">36]</ref>. On a mixture of K
ion, and metastasis <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b20">24,</ref><ref type="bibr" target="#b39">43,</ref><ref type="bibr" target="#b44">48]</ref>. Currently, DDX3 is
onnect to simply 3 of the 130 semantic types. ey supplement this subset with concepts from the OMIM <ref type="bibr" target="#b15">[19]</ref> and GWAS <ref type="bibr" target="#b3">[6]</ref> databases e network analytic approach to understand biomedical data in both their work on the disease network <ref type="bibr" target="#b15">[19]</ref> as well as their more generalized symptoms-disease network more generalized symptoms-disease network <ref type="bibr" target="#b47">[51]</ref>. In the former <ref type="bibr" target="#b15">[19]</ref>, the authors construct a bipartite network of disease phon
ref type="bibr" target="#b15">[19]</ref> as well as their more generalized symptoms-disease network <ref type="bibr" target="#b47">[51]</ref>. In the former <ref type="bibr" target="#b15">[19]</ref>, ojected networks, the human disease network (HDN), and the Disease Gene Network (DGN). In the la er <ref type="bibr" target="#b47">[51]</ref>, they construct a more generalized human symptoms disease
xt-based search application meant to help doctors make connections from within the MEDLINE data set <ref type="bibr" target="#b34">[38,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" ta
rget="#b6">[9,</ref><ref type="bibr" target="#b19">23,</ref><ref type="bibr" target="#b26">30,</ref><ref type="bibr" target="#b27">31]</ref>. Word2vec is a method which utilizes the skip-gram model to mples presented in <ref type="bibr" target="#b19">[23,</ref><ref type="bibr" target="#b26">30,</ref><ref type="bibr" target="#b27">31]</ref> and others, we set the dimensionality of our vector space d
ion, and metastasis <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b20">24,</ref><ref type="bibr" target="#b39">43,</ref><ref type="bibr" target="#b44">48]</ref>. Currently, DDX3 is
that novel discoveries could be found by carefully studying the existing body of scienti c research <ref type="bibr" target="#b41">[45]</ref>. Since then, many groups have a empted to mine the wealth study and exploration of undiscovered public knowledge began in 1986 with Swanson's landmark paper <ref type="bibr" target="#b41">[45]</ref>. Swanson hypothesized that fragments of information from t
involved cancer development and progression mainly through regulation of the Wnt signaling pathway <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b46">50]</ref> and associated reg
e MEDLINE data set <ref type="bibr" target="#b34">[38,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" target="#b42">46]</ref>. To use Arrowsmith, researchers supply two UMLS keywords wh ionally expensive, and queries were typically run on a subset of the MEDLINE data set (according to <ref type="bibr" target="#b42">[46]</ref> around 1,000 documents).</p><p>Spangler has also been a dr
se the Fast Library for Approximate Nearest Neighbors (FLANN) to generate a nearest neighbors graph <ref type="bibr" target="#b28">[32]</ref>. e result is a network of MEDLINE papers, each of which ar possible sub-words from ve to eight in order to increase data quality.</p><p>Finally, we used FLANN <ref type="bibr" target="#b28">[32]</ref> to create nearest neighbors graph from all i ∈ A in order
ect from El-Kishky et al., is focused on discovering multi-word phrases from a large corpus of text <ref type="bibr" target="#b16">[20]</ref>. is project intelligently groups unigrams together to crea is the most common topic modeling process and PLDA+ is a scalable implementation of this algorithm <ref type="bibr" target="#b16">[20,</ref><ref type="bibr" target="#b24">28]</ref>. Developed by Zhiy
se the Fast Library for Approximate Nearest Neighbors (FLANN) to generate a nearest neighbors graph <ref type="bibr" target="#b28">[32]</ref>. e result is a network of MEDLINE papers, each of which ar possible sub-words from ve to eight in order to increase data quality.</p><p>Finally, we used FLANN <ref type="bibr" target="#b28">[32]</ref> to create nearest neighbors graph from all i ∈ A in order
ads rely on temporal streaming to record, and subsequently replay, entire sequences of instructions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. As a class, these r's inability to predict instruction cache misses that are not contiguous, stream-based prefetchers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> exploit the recurr to record instruction fetch streams in a similar vein to prior stream-based instruction prefetchers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. We augment each c ory record and replay mechanisms from previously proposed per-core data and instruction prefetchers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" ta ge of 90% of instruction cache misses, but necessitates over 200KB per core for its history storage <ref type="bibr" target="#b13">[14]</ref>.</p><p>Stream-based instruction prefetchers were proposed core area by 16% and 59% for two lean-core designs over the state-of-the-art instruction prefetcher <ref type="bibr" target="#b13">[14]</ref>. The absolute performance improvement on a suite of divers /p><p>The state-of-the-art stream-based instruction prefetcher is Proactive Instruction Fetch (PIF) <ref type="bibr" target="#b13">[14]</ref>, which extends earlier work on temporal streaming <ref typ l noise in streams introduced by the instruction cache replacement policy and branch mispredictions <ref type="bibr" target="#b13">[14]</ref>. To mitigate increased history storage requirements result re it to the state-of-the-art instruction prefetcher with per-core private instruction history, PIF <ref type="bibr" target="#b13">[14]</ref>, using trace-based and cycle-accurate simulations of a 16- effectiveness and history storage requirements with the state-ofthe-art instruction prefetcher, PIF <ref type="bibr" target="#b13">[14]</ref>. Like other stream-based instruction and data prefetchers F requires 32K spatial region records in the history buffer targeting 90% instruction miss coverage <ref type="bibr" target="#b13">[14]</ref>, also validated with our experiments. As a result, the his effectiveness, we first compare the fraction of instruction cache misses predicted by SHIFT to PIF <ref type="bibr" target="#b13">[14]</ref>. For the purposes of this study, we only track the predict eliminates 92% of the instruction cache misses with 13% overprediction, corroborating prior results <ref type="bibr" target="#b13">[14]</ref>. However, PIF_2K, which has the same aggregate storage ove treams of discontinuities in its history, enhancing the lookahead of discontinuity prefetching. PIF <ref type="bibr" target="#b13">[14]</ref>, records the complete retire-order instruction cache acces
methodology <ref type="bibr" target="#b44">[44]</ref>, which extends the SMARTS sampling framework <ref type="bibr" target="#b46">[46]</ref>. Our samples are collected over 10-30 seconds of workload
"#b39">39]</ref>, exposing instruction-fetch stalls as a dominant performance bottleneck in servers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ >Instruction fetch stalls have long been recognized as a dominant performance bottleneck in servers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
reach of today's practical first-level instruction caches <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b35">35]</ref>. Instruction cache get="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" tar rget="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" tar 36">36,</ref><ref type="bibr" target="#b42">42]</ref>. To explore future code paths, an idle thread <ref type="bibr" target="#b20">[20]</ref>, a helper thread <ref type="bibr" target="#b0">[1]</ref>,
r the history. As an alternative, a cache partitioning scheme (e.g., <ref type="bibr">Van</ref>tage <ref type="bibr" target="#b31">[31]</ref>) can easily guarantee the required cache partition for the
reach of today's practical first-level instruction caches <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b35">35]</ref>. Instruction cache get="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" tar rget="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" tar 36">36,</ref><ref type="bibr" target="#b42">42]</ref>. To explore future code paths, an idle thread <ref type="bibr" target="#b20">[20]</ref>, a helper thread <ref type="bibr" target="#b0">[1]</ref>,
predict instruction cache misses caused by discontinuities <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" tar
uction prefetchers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b43">43]</ref>. To facilitate shar
get="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b41">41]</ref>. Simple next-line instruction prefetchers varying in prefet
0]</ref>, a helper thread <ref type="bibr" target="#b0">[1]</ref>, speculative threading mechanisms <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b48">48]</ref> or run-ahead execu
e limitations, we embed the SHIFT history buffer in the LLC leveraging the virtualization framework <ref type="bibr" target="#b7">[8]</ref>.</p><p>History Virtualization. To virtualize the instruction ruction history, SHIFT embeds the history buffer in the LLC as proposed in predictor virtualization <ref type="bibr" target="#b7">[8]</ref>.</p><p>A number of orthogonal studies mitigate instruction c
p://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge bases (KBs), such as Freebase <ref type="bibr" target="#b1">(Bollacker et al., 2008)</ref>, NELL <ref type="bibr" target="#b19">(M
ollowed by TransE <ref type="bibr" target="#b3">(Bordes et al., 2013)</ref>, Neural Tensor Networks <ref type="bibr" target="#b25">(Socher et al., 2013)</ref>, and many variants on all of these algori
h, its primary use has been in KB completion <ref type="bibr" target="#b15">(Lao et al., 2012;</ref><ref type="bibr" target="#b8">Gardner et al., 2013;</ref><ref type="bibr" target="#b9">Gardner et al r" target="#b15">(Lao et al., 2012)</ref>, to introducing better representations of the text corpus <ref type="bibr" target="#b8">(Gardner et al., 2013;</ref><ref type="bibr" target="#b9">Gardner et a
s/1.0"><head n="2">The Path Ranking Algorithm</head><p>The path ranking algorithm was introduced by <ref type="bibr" target="#b13">Lao and Cohen (2010)</ref>. It is a two-step process for generating a
h, its primary use has been in KB completion <ref type="bibr" target="#b15">(Lao et al., 2012;</ref><ref type="bibr" target="#b8">Gardner et al., 2013;</ref><ref type="bibr" target="#b9">Gardner et al r" target="#b15">(Lao et al., 2012)</ref>, to introducing better representations of the text corpus <ref type="bibr" target="#b8">(Gardner et al., 2013;</ref><ref type="bibr" target="#b9">Gardner et a
RA can be applied to any link prediction task in a graph, its primary use has been in KB completion <ref type="bibr" target="#b15">(Lao et al., 2012;</ref><ref type="bibr" target="#b8">Gardner et al., s ranged from incorporating a parsed corpus as additional evidence when doing random walk inference <ref type="bibr" target="#b15">(Lao et al., 2012)</ref>, to introducing better representations of th
arget="#b4">(Chang et al., 2014;</ref><ref type="bibr" target="#b7">García-Durán et al., 2014;</ref><ref type="bibr" target="#b29">Wang et al., 2014)</ref>. These methods perform well when there is st
arget="#b4">(Chang et al., 2014;</ref><ref type="bibr" target="#b7">García-Durán et al., 2014;</ref><ref type="bibr" target="#b29">Wang et al., 2014)</ref>. These methods perform well when there is st
s of facts about common or popular entities <ref type="bibr" target="#b30">(West et al., 2014;</ref><ref type="bibr" target="#b5">Choi et al., 2015)</ref>. The task of knowledge base completion-fillin
ture matrices over node pairs in the graph. The method has a strong connection to logical inference <ref type="bibr" target="#b10">(Gardner et al., 2015)</ref>, as the feature space considered by PRA
ibr" target="#b15">(Lao et al., 2012;</ref><ref type="bibr" target="#b8">Gardner et al., 2013;</ref><ref type="bibr" target="#b9">Gardner et al., 2014)</ref>.</p><p>PRA is a two-step process, where th better representations of the text corpus <ref type="bibr" target="#b8">(Gardner et al., 2013;</ref><ref type="bibr" target="#b9">Gardner et al., 2014)</ref>, and using PRA in a broader context as par al. (2011)</ref> use measures of the precision and recall of each feature in this selection, while <ref type="bibr" target="#b9">Gardner et al. (2014)</ref> simply pick those most frequently seen.</p e -GENDER-would be "COMPARISON:-GENDER-:/m/Male:/m/Female".</p><p>Vector space similarity features. <ref type="bibr" target="#b9">Gardner et al. (2014)</ref> introduced a modification of PRA's random tractors we introduced, we learned models for 10 relations in the NELL KB. We used the same data as <ref type="bibr" target="#b9">Gardner et al. (2014)</ref>, using both the formal KB relations and th SFE significantly outperforms PRA, both with and without the vector space random walks presented by <ref type="bibr" target="#b9">Gardner et al. (2014)</ref>.</p></div> <div xmlns="http://www.tei-c.or 6.1. The Freebase data consists of 24 relations from the Freebase KB; we used the same data used by<ref type="bibr" target="#b9">Gardner et al. (2014)</ref>.</note> 			<note xmlns="http://www.tei-c.o
to guide the pixel denoiser; in contrast, our denoising is applied directly on features. Guo et al. <ref type="bibr" target="#b7">[8]</ref> transform the images via non-differentiable image preprocess ts of their non-differentiable computations <ref type="bibr" target="#b0">[1]</ref>. In contrast to <ref type="bibr" target="#b7">[8]</ref>, our feature denoising models are differentiable, but are st y increases as the image is propagated through the network <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b7">8]</ref>, and non-existing activations in the feature maps are halluci
#b7">[8]</ref> transform the images via non-differentiable image preprocessing, like image quilting <ref type="bibr" target="#b3">[4]</ref>, total variance minimization <ref type="bibr" target="#b16"> </p><p>Variants of denoising operations. Next, we evaluate variants of denoising operations in Sec. <ref type="bibr" target="#b3">4</ref>. In these ablations, we add blocks of different kinds to basel
rocessing, like image quilting <ref type="bibr" target="#b3">[4]</ref>, total variance minimization <ref type="bibr" target="#b16">[17]</ref>, and quantization. While these defenses may be effective i
s adversarially perturbed counterpart (bottom). The adversarial perturbation was produced using PGD <ref type="bibr" target="#b15">[16]</ref> with maximum perturbation ǫ =16 (out of 256). In this exam e-box attacks on ImageNet <ref type="bibr" target="#b17">[18]</ref>. Under 10-iteration PGD attacks <ref type="bibr" target="#b15">[16]</ref>, we report 55.7% classification accuracy on ImageNet, larg tacker based on the current parameters of the models. We use the Projected Gradient Descent (PGD) 2 <ref type="bibr" target="#b15">[16]</ref> as the white-box attacker for adversarial training.</p><p> ining, we can initialize the adversarial image by the clean image, or randomly within the allowed ǫ <ref type="bibr" target="#b15">[16]</ref>. We randomly choose from both initializations in the PGD a ot_2">4</ref> We evaluate with ǫ =16, a challenging case for defenders on ImageNet.</p><p>Following <ref type="bibr" target="#b15">[16]</ref>, the PGD white-box attacker initializes the adversarial pe ts adversarially perturbed counterpart (bottom). The adversarial perturbation was produced using PGD<ref type="bibr" target="#b15">[16]</ref> with maximum perturbation ǫ =16 (out of 256). In this exam >Adversarial training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref> defends against adversarial perturbations by training netwo riven by a successful implementation of adversarial training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. In this section, we describe our implementation of adversa noising models.</p><p>The basic idea of adversarial training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> is to train the network on adversarially perturbed images.
t="#b5">[6]</ref>, iterative FGSM<ref type="bibr" target="#b11">[12]</ref>, and its momentum variant<ref type="bibr" target="#b2">[3]</ref>. Similar to<ref type="bibr" target="#b9">[10]</ref>, we foun
#b7">[8]</ref> transform the images via non-differentiable image preprocessing, like image quilting <ref type="bibr" target="#b3">[4]</ref>, total variance minimization <ref type="bibr" target="#b16"> </p><p>Variants of denoising operations. Next, we evaluate variants of denoising operations in Sec. <ref type="bibr" target="#b3">4</ref>. In these ablations, we add blocks of different kinds to basel
.org/ns/1.0"><head n="1.">Introduction</head><p>Adversarial attacks to image classification systems <ref type="bibr" target="#b19">[20]</ref> add small perturbations to images that lead these systems
epochs; we decrease the learning rate by 10× at the 35th, 70-th, and 95-th epoch. A label smoothing <ref type="bibr" target="#b18">[19]</ref> of 0.1 is used. The total time needed for adversarial trai valuated under 10-iteration PGD attack in <ref type="bibr" target="#b9">[10]</ref>, on Inception-v3 <ref type="bibr" target="#b18">[19]</ref>. It achieves 27.9% accuracy on ImageNet validation images
t="#b5">[6]</ref>, iterative FGSM<ref type="bibr" target="#b11">[12]</ref>, and its momentum variant<ref type="bibr" target="#b2">[3]</ref>. Similar to<ref type="bibr" target="#b9">[10]</ref>, we foun
Figure <ref type="figure" target="#fig_0">1</ref> shows a randomly selected feature map of a ResNet <ref type="bibr" target="#b8">[9]</ref> applied on a clean image (top) and on its ad- * Work done du ed on a clean image (top) and on its ad- * Work done during an internship at Facebook AI Research.  <ref type="bibr" target="#b8">[9]</ref> applied on a clean image (top) and on its adversarially pert clean image and its adversarially perturbed counterpart, we use the same network (here, a ResNet-50 <ref type="bibr" target="#b8">[9]</ref>) to compute its activations in the hidden layers. Figure <re ocessed by a 1×1 convolutional layer, and then added to the block's input via a residual connection <ref type="bibr" target="#b8">[9]</ref>. <ref type="foot" target="#foot_0">1</ref>The design in Figu value of ǫ is relative to the pixel intensity scale of 256.</p><p>Our baselines are ResNet-101/152 <ref type="bibr" target="#b8">[9]</ref>. By default, we add 4 denoising blocks to a ResNet: each is ual block with a single 1×1 convolution. Further, we also compare with adding 4 standard bottleneck <ref type="bibr" target="#b8">[9]</ref> blocks -essentially, ResNet-164. All models are trained by a ead><label>1</label><figDesc>Figure1. Feature map in the res3 block of an ImageNet-trained ResNet-50<ref type="bibr" target="#b8">[9]</ref> applied on a clean image (top) and on its adversarially pert
t="#b5">[6]</ref>, iterative FGSM<ref type="bibr" target="#b11">[12]</ref>, and its momentum variant<ref type="bibr" target="#b2">[3]</ref>. Similar to<ref type="bibr" target="#b9">[10]</ref>, we foun
feedback as a composition of user result examination and relevance judgment. Examination hypothesis <ref type="bibr" target="#b7">[8]</ref>, which is a fundamental assumption in click modeling, postul ns; and among them result examination plays a central role <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref>. Unfortunately, most applications of bandit algorithms simply the selected arm a t . Based on the examination hypothesis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref>, when C at = 1, the chosen a t must be relevant to the user's
ed a 8.9% CTR lift compared to standard contextual bandits <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9]</ref> which do not model users' examination behavior.</p></div> <di incurs a space complexity at least O(T ) and time complexity at least O(T 2 ) (e.g., Filippi et al. <ref type="bibr" target="#b8">[9]</ref> requires exact optimum of logistic regression on all histori
ch as clicks, which is known to be biased and incomplete about users' evaluation of system's output <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11]</ref>. For example, a us y for log x (See Appendix B.2), a lower bound of the required form is achievable. When C = 1, by Eq <ref type="bibr" target="#b15">(16)</ref> in Appendix B.3, we have,</p><formula xml:id="formula_7">l
those clusters. Kawale et al. developed a Thompson sampling scheme for online matrix-factorization <ref type="bibr" target="#b14">[15]</ref>. Latent features are extracted via an online low-rank matr
make things even worse, the two popular bandit learning paradigms, upper confidence bound principle <ref type="bibr" target="#b0">[1]</ref> and Thompson sampling <ref type="bibr" target="#b2">[3]</ref
that skipped documents are less attractive than later clicked ones in a ranked list, Kveton et al. <ref type="bibr" target="#b16">[17]</ref> developed a cascading bandit model to learn from both clic mula><p>More specifically, g(x, ⇠) is a polynomial of degree 2 with respect to x. When C = 0, by Eq <ref type="bibr" target="#b16">(17)</ref> in Appendix B.3, we have,</p><formula xml:id="formula_9">l
than 100 thousands student video watching sessions. Based on the unbiased offline evaluation policy <ref type="bibr" target="#b20">[21]</ref>, our algorithm achieved a 8.9% CTR lift compared to standa again justifies our decomposition of examination and relevance in click feedback.</p><p>We followed <ref type="bibr" target="#b20">[21]</ref> to develop our online data collection policy in our MOOC p ility of examination at different position is known, and is estimated from offline data.  Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed a method to calculate a near-unbiased estimate of tensively used for online advertisement CTR optimization. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>, the authors model user clicks by a regularized logistic re
>Related Works</head><p>As having been extensively studied in click modeling of user search results <ref type="bibr" target="#b6">[7]</ref>, various factors affect users' click decisions; and among th
arning from multiple clicks in the same result ranking list, they adopted the dependent click model <ref type="bibr" target="#b9">[10]</ref> to infer user satisfaction after a sequence of clicks <ref
arning from multiple clicks in the same result ranking list, they adopted the dependent click model <ref type="bibr" target="#b9">[10]</ref> to infer user satisfaction after a sequence of clicks <ref
arning from multiple clicks in the same result ranking list, they adopted the dependent click model <ref type="bibr" target="#b9">[10]</ref> to infer user satisfaction after a sequence of clicks <ref
ve high fetch bandwidth, while maintaining the complexity under control, is the stream fetch engine <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>This fetch div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fetch Models</head><p>The stream fetch engine <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> model is shown in for the stream fetch engine to provide high fetch bandwidth while requiring low implementation cost <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>. However, having h ww.tei-c.org/ns/1.0"><head n="5.1">The Multiple Stream Predictor</head><p>The next stream predictor <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>, which is shown in zed code layout. In addition, data are shown for the original single-stream predictor, described in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>, and a 2-stream mu p>To avoid this increase in the fetch engine complexity, we propose to use long instruction streams <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> as basic predictio /p><p>Our instruction cache setup uses wide cache lines, that is, 4 times the processor fetch width <ref type="bibr" target="#b10">[11]</ref>, and 64KB total hardware budget. The trace fetch architect
gh instructions to hide the prediction table access latency.</p><p>Using a fetch target queue (FTQ) <ref type="bibr" target="#b11">[12]</ref> is also helpful for taking advantage of this fact. The FTQ kew predictor <ref type="bibr" target="#b18">[19]</ref>, the fetch target buffer (FTB) architecture <ref type="bibr" target="#b11">[12]</ref> using a perceptron predictor <ref type="bibr" target="#b7" type="bibr" target="#b4">[5]</ref>. All these architectures use an 8-entry fetch target queue (FTQ) <ref type="bibr" target="#b11">[12]</ref> to decouple branch prediction from the instruction cache a rmation <ref type="bibr" target="#b18">[19]</ref>. Our FTB model is similar to the one described in <ref type="bibr" target="#b11">[12]</ref> but using a perceptron branch predictor <ref type="bibr" t similar to the one described in <ref type="bibr" target="#b13">[14]</ref> but enhanced using an FTQ <ref type="bibr" target="#b11">[12]</ref> to decouple the trace predictor from the trace cache, as s
et buffer (FTB) architecture <ref type="bibr" target="#b11">[12]</ref> using a perceptron predictor <ref type="bibr" target="#b7">[8]</ref>, and the trace cache fetch architecture using a trace predic one described in <ref type="bibr" target="#b11">[12]</ref> but using a perceptron branch predictor <ref type="bibr" target="#b7">[8]</ref> to predict the direction of conditional branches.  Our trace
direction of conditional branches.  Our trace cache fetch model is similar to the one described in <ref type="bibr" target="#b13">[14]</ref> but enhanced using an FTQ <ref type="bibr" target="#b11">[
>A common solution for this problem is prediction overriding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. A small and fast predictor is used to obtain a first predi res.</p><p>Our fetch models also use an overriding mechanism <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref> to complete a branch prediction each cycle. A small branch t of complex hardware mechanisms, like prediction overriding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>, to hide the prediction table access delay.</p><p>To avoid urate. A similar mechanism is used in the Alpha EV6 <ref type="bibr" target="#b2">[3]</ref> and EV8 <ref type="bibr" target="#b18">[19]</ref> processors.</p><p>The problem of prediction overriding is the-art fetch architectures: a fetch architecture using an interleaved BTB and a 2bcgskew predictor <ref type="bibr" target="#b18">[19]</ref>, the fetch target buffer (FTB) architecture <ref type="bib p><p>Our interleaved BTB fetch model (iBTB) is inspired by the EV8 fetch engine design described in <ref type="bibr" target="#b18">[19]</ref>. This iBTB model decouples the branch prediction mechanism prediction block, which combines the outcome of the last branch in the block with path information <ref type="bibr" target="#b18">[19]</ref>. Our FTB model is similar to the one described in <ref typ
used by modern technologies, prevent branch prediction tables from being accessed in a single cycle <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. This fact limits fetc slower wires in modern technologies, cause branch prediction tables to require multi-cycle accesses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>The trace predi ies and larger wire delays cause branch prediction tables to require multiple cycles to be accessed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, limiting the fetch en ggressive 8 fan-out-of-four delays clock period, that is, a 3.47 GHz clock frequency as reported in <ref type="bibr" target="#b0">[1]</ref>. It has been claimed in <ref type="bibr" target="#b3">[4]</r
used by modern technologies, prevent branch prediction tables from being accessed in a single cycle <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. This fact limits fetc slower wires in modern technologies, cause branch prediction tables to require multi-cycle accesses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>The trace predi ies and larger wire delays cause branch prediction tables to require multiple cycles to be accessed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, limiting the fetch en ggressive 8 fan-out-of-four delays clock period, that is, a 3.47 GHz clock frequency as reported in <ref type="bibr" target="#b0">[1]</ref>. It has been claimed in <ref type="bibr" target="#b3">[4]</r
anch prediction tables from being accessed in a single cycle <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. This fact limits fetch engine performance because each branc use branch prediction tables to require multi-cycle accesses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>The trace predictor <ref type="bibr" target="#b4">[5]< prediction tables to require multiple cycles to be accessed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, limiting the fetch engine performance. This fact has led to ing address of the following one.</p><p>A common solution for this problem is prediction overriding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. A small and fast pr architectures use some separate structures.</p><p>Our fetch models also use an overriding mechanism <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref> to complete a branch ce. This fact has led to the development of complex hardware mechanisms, like prediction overriding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>, to hide the predict pe of this work.</p><p>A different approach is the overriding mechanism described by Jimenez et al. <ref type="bibr" target="#b6">[7]</ref>. This mechanism provides two predictions, a first prediction
type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>The trace predictor <ref type="bibr" target="#b4">[5]</ref> is a latency tolerant mechanism, since each trace prediction type="bibr" target="#b7">[8]</ref>, and the trace cache fetch architecture using a trace predictor <ref type="bibr" target="#b4">[5]</ref>. All these architectures use an 8-entry fetch target queue ( rect jumps and function calls.</p><p>The trace fetch architecture uses a return history stack (RHS) <ref type="bibr" target="#b4">[5]</ref> instead of a RAS. This mechanism is more efficient than a RA f the simulated predictors. We have explored a wide range of history lengths, as well as DOLC index <ref type="bibr" target="#b4">[5]</ref> configurations, and selected the best one found for each set
>A common solution for this problem is prediction overriding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. A small and fast predictor is used to obtain a first predi res.</p><p>Our fetch models also use an overriding mechanism <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref> to complete a branch prediction each cycle. A small branch t of complex hardware mechanisms, like prediction overriding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>, to hide the prediction table access delay.</p><p>To avoid urate. A similar mechanism is used in the Alpha EV6 <ref type="bibr" target="#b2">[3]</ref> and EV8 <ref type="bibr" target="#b18">[19]</ref> processors.</p><p>The problem of prediction overriding is the-art fetch architectures: a fetch architecture using an interleaved BTB and a 2bcgskew predictor <ref type="bibr" target="#b18">[19]</ref>, the fetch target buffer (FTB) architecture <ref type="bib p><p>Our interleaved BTB fetch model (iBTB) is inspired by the EV8 fetch engine design described in <ref type="bibr" target="#b18">[19]</ref>. This iBTB model decouples the branch prediction mechanism prediction block, which combines the outcome of the last branch in the block with path information <ref type="bibr" target="#b18">[19]</ref>. Our FTB model is similar to the one described in <ref typ
anch prediction tables from being accessed in a single cycle <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. This fact limits fetch engine performance because each branc use branch prediction tables to require multi-cycle accesses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>The trace predictor <ref type="bibr" target="#b4">[5]< prediction tables to require multiple cycles to be accessed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, limiting the fetch engine performance. This fact has led to ing address of the following one.</p><p>A common solution for this problem is prediction overriding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. A small and fast pr architectures use some separate structures.</p><p>Our fetch models also use an overriding mechanism <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref> to complete a branch ce. This fact has led to the development of complex hardware mechanisms, like prediction overriding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>, to hide the predict pe of this work.</p><p>A different approach is the overriding mechanism described by Jimenez et al. <ref type="bibr" target="#b6">[7]</ref>. This mechanism provides two predictions, a first prediction







rate for the total of all 14 loops is calculated as the harmonic mean of the individual issue rates <ref type="bibr" target="#b20">[23]</ref>. For reasons of brevity, we shall present all subsequent s



uto-encoder. Most recently, a Deep Structured Semantic Models (DSSM) for Web search was proposed in <ref type="bibr" target="#b5">[6]</ref>, which is reported to outperform significantly semantic hash in an input word sequence into a feature vector using the technique called word hashing proposed in <ref type="bibr" target="#b5">[6]</ref>. For example, the word is represented by a count vector of i hastic gradient ascent. Learning of the C-DSSM is similar to that of learning the DSSM described in <ref type="bibr" target="#b5">[6]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n=
nd other conventional semantic models.</p><p>In this study, based on a convolutional neural network <ref type="bibr" target="#b0">[1]</ref>, we present a new Convolutional Deep Structured Semantic Mod


ng-standing research topic in information retrieval (IR) <ref type="bibr" target="#b1">[2]</ref>[4] <ref type="bibr" target="#b7">[8]</ref>. Usually, the contextual information captured by models such
and its extensions, are able to map a query to its relevant documents at the semantic level (e.g., <ref type="bibr" target="#b1">[2]</ref>). However, most latent semantic models still view a query (o ion in search queries and documents is a long-standing research topic in information retrieval (IR) <ref type="bibr" target="#b1">[2]</ref>[4] <ref type="bibr" target="#b7">[8]</ref>. Usually, the con
ng-standing research topic in information retrieval (IR) <ref type="bibr" target="#b1">[2]</ref>[4] <ref type="bibr" target="#b7">[8]</ref>. Usually, the contextual information captured by models such
and its extensions, are able to map a query to its relevant documents at the semantic level (e.g., <ref type="bibr" target="#b1">[2]</ref>). However, most latent semantic models still view a query (o ion in search queries and documents is a long-standing research topic in information retrieval (IR) <ref type="bibr" target="#b1">[2]</ref>[4] <ref type="bibr" target="#b7">[8]</ref>. Usually, the con
ffective. As an alternative, there are retrieval methods such as the phrase-based translation model <ref type="bibr" target="#b4">[5]</ref> that directly model phrases (or word n-grams), but they ofte mercial search engine. On average, each query is associated with 65 Web documents (URLs). Following <ref type="bibr" target="#b4">[5]</ref>, we only used the title field of a Web document for ranking.
ffective. As an alternative, there are retrieval methods such as the phrase-based translation model <ref type="bibr" target="#b4">[5]</ref> that directly model phrases (or word n-grams), but they ofte mercial search engine. On average, each query is associated with 65 Web documents (URLs). Following <ref type="bibr" target="#b4">[5]</ref>, we only used the title field of a Web document for ranking.
ffective. As an alternative, there are retrieval methods such as the phrase-based translation model <ref type="bibr" target="#b4">[5]</ref> that directly model phrases (or word n-grams), but they ofte mercial search engine. On average, each query is associated with 65 Web documents (URLs). Following <ref type="bibr" target="#b4">[5]</ref>, we only used the title field of a Web document for ranking.
ffective. As an alternative, there are retrieval methods such as the phrase-based translation model <ref type="bibr" target="#b4">[5]</ref> that directly model phrases (or word n-grams), but they ofte mercial search engine. On average, each query is associated with 65 Web documents (URLs). Following <ref type="bibr" target="#b4">[5]</ref>, we only used the title field of a Web document for ranking.
ng-standing research topic in information retrieval (IR) <ref type="bibr" target="#b1">[2]</ref>[4] <ref type="bibr" target="#b7">[8]</ref>. Usually, the contextual information captured by models such

ng-standing research topic in information retrieval (IR) <ref type="bibr" target="#b1">[2]</ref>[4] <ref type="bibr" target="#b7">[8]</ref>. Usually, the contextual information captured by models such
br" target="#b2">[3]</ref>[6][9] <ref type="bibr" target="#b9">[10]</ref>. Salakhutdinov and Hinton <ref type="bibr" target="#b8">[9]</ref> demonstrated that the semantic structures can be extracted v
ffective. As an alternative, there are retrieval methods such as the phrase-based translation model <ref type="bibr" target="#b4">[5]</ref> that directly model phrases (or word n-grams), but they ofte mercial search engine. On average, each query is associated with 65 Web documents (URLs). Following <ref type="bibr" target="#b4">[5]</ref>, we only used the title field of a Web document for ranking.
and its extensions, are able to map a query to its relevant documents at the semantic level (e.g., <ref type="bibr" target="#b1">[2]</ref>). However, most latent semantic models still view a query (o ion in search queries and documents is a long-standing research topic in information retrieval (IR) <ref type="bibr" target="#b1">[2]</ref>[4] <ref type="bibr" target="#b7">[8]</ref>. Usually, the con
ng-standing research topic in information retrieval (IR) <ref type="bibr" target="#b1">[2]</ref>[4] <ref type="bibr" target="#b7">[8]</ref>. Usually, the contextual information captured by models such
ng-standing research topic in information retrieval (IR) <ref type="bibr" target="#b1">[2]</ref>[4] <ref type="bibr" target="#b7">[8]</ref>. Usually, the contextual information captured by models such

ffective. As an alternative, there are retrieval methods such as the phrase-based translation model <ref type="bibr" target="#b4">[5]</ref> that directly model phrases (or word n-grams), but they ofte mercial search engine. On average, each query is associated with 65 Web documents (URLs). Following <ref type="bibr" target="#b4">[5]</ref>, we only used the title field of a Web document for ranking.
ei-c.org/ns/1.0"><head n="4.1">Stressor Event and Subject Dictionaries</head><p>The word embeddings <ref type="bibr" target="#b8">[Mikolov et al., 2013]</ref> have been found effective in estimating t
n recent years, there exist some studies on leveraging social media data for mental health care. De <ref type="bibr" target="#b4">Choudhury et al. [2013]</ref> were the first to explore social media d
gical stress has been well studied by psychologists in the past decades. Most of these measurements <ref type="bibr" target="#b9">[Rowlison and Felner, 1988;</ref><ref type="bibr" target="#b1">Brantle a gradient boosted decision tree model with features associated with each tweet.</p><p>• Lasso-MTL <ref type="bibr" target="#b9">[Nie et al., 2010]</ref>: It is the Lasso regularized MTL which can ac n methods with the same feature settings. For the SVM, SR and GBDT methods, we use the scikit-learn <ref type="bibr" target="#b9">[Pedregosa et al., 2011]</ref> implementation. As for the comparison m
ncodes grouped sparsity by assuming that all tasks share a common set of features.</p><p>• cASO-MTL <ref type="bibr" target="#b3">[Chen et al., 2009]</ref>: It is a convex relaxation of the alternatin
ressor Event Dictionary. Based on the event categories in the professional life events stress scale <ref type="bibr" target="#b6">[Holmes and Rahe, 1967]</ref>, we first selected the top 12 categories are based on questionnaires and interviews. Among them, the Social Readjustment Rating Scale (SRRS) <ref type="bibr" target="#b6">[Holmes and Rahe, 1967]</ref> is one of the widely-accepted metrics. I tegorized the stressor events into 43 categories based on the professional life events stress scale <ref type="bibr" target="#b6">[Holmes and Rahe, 1967]</ref>.</p><p>We then manually defined a set of rds of different social relations. However, stressor subjects expressed in  Inspired by the work in <ref type="bibr" target="#b6">[Haslam, 1994]</ref>, we categorized the subjects into six categories.
ncodes grouped sparsity by assuming that all tasks share a common set of features.</p><p>• cASO-MTL <ref type="bibr" target="#b3">[Chen et al., 2009]</ref>: It is a convex relaxation of the alternatin
we regard each event or subject detection as a task and propose to use a multi-task learning (MTL) <ref type="bibr" target="#b2">[Caruana, 1997]</ref> model. The MTL model is able to adaptively captu results in this paper were based on 10-fold cross validation:</p><p>• Support Vector Machine (SVM) <ref type="bibr" target="#b2">[Chang and Lin, 2011]</ref>: It is a popular binary classifier that is
ncodes grouped sparsity by assuming that all tasks share a common set of features.</p><p>• cASO-MTL <ref type="bibr" target="#b3">[Chen et al., 2009]</ref>: It is a convex relaxation of the alternatin
gical stress has been well studied by psychologists in the past decades. Most of these measurements <ref type="bibr" target="#b9">[Rowlison and Felner, 1988;</ref><ref type="bibr" target="#b1">Brantle a gradient boosted decision tree model with features associated with each tweet.</p><p>• Lasso-MTL <ref type="bibr" target="#b9">[Nie et al., 2010]</ref>: It is the Lasso regularized MTL which can ac n methods with the same feature settings. For the SVM, SR and GBDT methods, we use the scikit-learn <ref type="bibr" target="#b9">[Pedregosa et al., 2011]</ref> implementation. As for the comparison m
ncodes grouped sparsity by assuming that all tasks share a common set of features.</p><p>• cASO-MTL <ref type="bibr" target="#b3">[Chen et al., 2009]</ref>: It is a convex relaxation of the alternatin
decades. Most of these measurements <ref type="bibr" target="#b9">[Rowlison and Felner, 1988;</ref><ref type="bibr" target="#b1">Brantley and Jones, 1993]</ref> are based on questionnaires and interv fier that is found to effective in several classification problems.</p><p>• Softmax Regression (SR) <ref type="bibr" target="#b1">[Böhning, 1992]</ref>: It is a model that is used to predict the proba
structions a di cult task, even in the presence of aiding devices like a hardware trace cache (HTC) <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b3">4]</ref>. Indeed, a trace cach










oc bottom-up fashion, where PMU designers attempted to cover key issues via "dedicated miss events" <ref type="bibr" target="#b0">[1]</ref>. Yet, how does one pin-point performance issues that were no rchy's top level. This accurate classification distinguishes our method from previous approaches in <ref type="bibr" target="#b0">[1]</ref>[5][6].</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><h same symptom. Such scenarios of L1 hits and near caches' misses, are not handled by some approaches <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b4">[5]</ref>.</p><p>Note performa t="#b5">[6]</ref>, nor complex structures with latency counters as in Accurate CPI Stacks proposals <ref type="bibr" target="#b0">[1]</ref>[8] <ref type="bibr" target="#b8">[9]</ref>.</p></div> <div x tempted to accurately classify performance impacts on out-of-order architectures. Eyerman et al. in <ref type="bibr" target="#b0">[1]</ref>[9] use a simulation-based interval analysis model in order t reference model) is that it restricts all stalls to a fixed set of eight predefined miss events. In <ref type="bibr" target="#b0">[1]</ref>[4] <ref type="bibr" target="#b4">[5]</ref> there is no consi
ware cost is implied due to fairly complex tracking structures as authors themselves later state in <ref type="bibr" target="#b7">[8]</ref>. While <ref type="bibr" target="#b7">[8]</ref> replaces the king structures as authors themselves later state in <ref type="bibr" target="#b7">[8]</ref>. While <ref type="bibr" target="#b7">[8]</ref> replaces the original structure with smaller FIFO; extra log
rg/ns/1.0"><head>Figure 1: Out-of-order CPU block diagram -Intel Core™</head><p>Traditional methods <ref type="bibr" target="#b3">[4]</ref>[5] do simple estimations of stalls. E.g. the numbers of miss what has caused that. Familiar i-cache and i-TLB misses fit here, but not only these. For example, <ref type="bibr" target="#b3">[4]</ref> has flagged Instruction Length Decoding as a fetch bottlenec arlier.  It is interesting to see that the characterization of DBMS workloads generally conforms to <ref type="bibr" target="#b3">[4]</ref> who reported these workloads are limited by last-level data w.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>The widely-used naïve-approach is adopted by <ref type="bibr" target="#b3">[4]</ref>[5] to name a few. While this might work for in-order CPUs, i workloadsIt is interesting to see that the characterization of DBMS workloads generally conforms to<ref type="bibr" target="#b3">[4]</ref> who reported these workloads are limited by last-level data
tional counters/logic. We have pointed to more drawbacks in previous sections.</p><p>More recently, <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b11">[12]</ref> proposed ins d instrumentationbased tools to analyze data-locality and scalability bottlenecks, respectively. In <ref type="bibr" target="#b12">[13]</ref>, average memory latency is sampled with a PMU and coupled better metrics based on our MemStalls.L3Miss event e.g. can be used instead of raw latency value in <ref type="bibr" target="#b12">[13]</ref> to quantify when speedup may apply. Examining metrics at h
rg/ns/1.0"><head>Figure 1: Out-of-order CPU block diagram -Intel Core™</head><p>Traditional methods <ref type="bibr" target="#b3">[4]</ref>[5] do simple estimations of stalls. E.g. the numbers of miss what has caused that. Familiar i-cache and i-TLB misses fit here, but not only these. For example, <ref type="bibr" target="#b3">[4]</ref> has flagged Instruction Length Decoding as a fetch bottlenec arlier.  It is interesting to see that the characterization of DBMS workloads generally conforms to <ref type="bibr" target="#b3">[4]</ref> who reported these workloads are limited by last-level data w.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>The widely-used naïve-approach is adopted by <ref type="bibr" target="#b3">[4]</ref>[5] to name a few. While this might work for in-order CPUs, i workloadsIt is interesting to see that the characterization of DBMS workloads generally conforms to<ref type="bibr" target="#b3">[4]</ref> who reported these workloads are limited by last-level data
tional counters/logic. We have pointed to more drawbacks in previous sections.</p><p>More recently, <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b11">[12]</ref> proposed ins d instrumentationbased tools to analyze data-locality and scalability bottlenecks, respectively. In <ref type="bibr" target="#b12">[13]</ref>, average memory latency is sampled with a PMU and coupled better metrics based on our MemStalls.L3Miss event e.g. can be used instead of raw latency value in <ref type="bibr" target="#b12">[13]</ref> to quantify when speedup may apply. Examining metrics at h
ware cost is implied due to fairly complex tracking structures as authors themselves later state in <ref type="bibr" target="#b7">[8]</ref>. While <ref type="bibr" target="#b7">[8]</ref> replaces the king structures as authors themselves later state in <ref type="bibr" target="#b7">[8]</ref>. While <ref type="bibr" target="#b7">[8]</ref> replaces the original structure with smaller FIFO; extra log
rg/ns/1.0"><head>Figure 1: Out-of-order CPU block diagram -Intel Core™</head><p>Traditional methods <ref type="bibr" target="#b3">[4]</ref>[5] do simple estimations of stalls. E.g. the numbers of miss what has caused that. Familiar i-cache and i-TLB misses fit here, but not only these. For example, <ref type="bibr" target="#b3">[4]</ref> has flagged Instruction Length Decoding as a fetch bottlenec arlier.  It is interesting to see that the characterization of DBMS workloads generally conforms to <ref type="bibr" target="#b3">[4]</ref> who reported these workloads are limited by last-level data w.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>The widely-used naïve-approach is adopted by <ref type="bibr" target="#b3">[4]</ref>[5] to name a few. While this might work for in-order CPUs, i workloadsIt is interesting to see that the characterization of DBMS workloads generally conforms to<ref type="bibr" target="#b3">[4]</ref> who reported these workloads are limited by last-level data
ware cost is implied due to fairly complex tracking structures as authors themselves later state in <ref type="bibr" target="#b7">[8]</ref>. While <ref type="bibr" target="#b7">[8]</ref> replaces the king structures as authors themselves later state in <ref type="bibr" target="#b7">[8]</ref>. While <ref type="bibr" target="#b7">[8]</ref> replaces the original structure with smaller FIFO; extra log
rg/ns/1.0"><head>Figure 1: Out-of-order CPU block diagram -Intel Core™</head><p>Traditional methods <ref type="bibr" target="#b3">[4]</ref>[5] do simple estimations of stalls. E.g. the numbers of miss what has caused that. Familiar i-cache and i-TLB misses fit here, but not only these. For example, <ref type="bibr" target="#b3">[4]</ref> has flagged Instruction Length Decoding as a fetch bottlenec arlier.  It is interesting to see that the characterization of DBMS workloads generally conforms to <ref type="bibr" target="#b3">[4]</ref> who reported these workloads are limited by last-level data w.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>The widely-used naïve-approach is adopted by <ref type="bibr" target="#b3">[4]</ref>[5] to name a few. While this might work for in-order CPUs, i workloadsIt is interesting to see that the characterization of DBMS workloads generally conforms to<ref type="bibr" target="#b3">[4]</ref> who reported these workloads are limited by last-level data
tional counters/logic. We have pointed to more drawbacks in previous sections.</p><p>More recently, <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b11">[12]</ref> proposed ins d instrumentationbased tools to analyze data-locality and scalability bottlenecks, respectively. In <ref type="bibr" target="#b12">[13]</ref>, average memory latency is sampled with a PMU and coupled better metrics based on our MemStalls.L3Miss event e.g. can be used instead of raw latency value in <ref type="bibr" target="#b12">[13]</ref> to quantify when speedup may apply. Examining metrics at h
researchers have found convolutional networks (ConvNets) <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b17">[18]</ref> are useful in extracting information from raw signals, ran
a summary. Sogou news corpus. This dataset is a combination of the SogouCA and SogouCS news corpora <ref type="bibr" target="#b31">[32]</ref>, containing in total 2,909,551 news articles in various to
ref type="foot" target="#foot_0">1</ref> project. That thesaurus in turn was obtained from Word-Net <ref type="bibr" target="#b6">[7]</ref>, where every synonym to a word or phrase is ranked by the se
ls is temporal max-pooling. It is the 1-D version of the max-pooling module used in computer vision <ref type="bibr" target="#b1">[2]</ref>. Given a discrete input function g(x) ∈ [1, l] → R, the max-
s <ref type="bibr" target="#b14">[15]</ref>, and incorporating character-level features to ConvNets <ref type="bibr" target="#b27">[28]</ref>  <ref type="bibr" target="#b28">[29]</ref>. In particular, these ConvNet approaches use words as a basis, in which character-level features extracted at word <ref type="bibr" target="#b27">[28]</ref> or word n-gram <ref type="bibr" target="#b28">[29]</ref> l

s <ref type="bibr" target="#b14">[15]</ref>, and incorporating character-level features to ConvNets <ref type="bibr" target="#b27">[28]</ref>  <ref type="bibr" target="#b28">[29]</ref>. In particular, these ConvNet approaches use words as a basis, in which character-level features extracted at word <ref type="bibr" target="#b27">[28]</ref> or word n-gram <ref type="bibr" target="#b28">[29]</ref> l
seem to play a role in deciding which method is better.</p><p>Bag-of-means is a misuse of word2vec <ref type="bibr" target="#b19">[20]</ref>. One of the most obvious facts one could observe from tabl
the counts of each word as the features. For the TFIDF (term-frequency inverse-document-frequency) <ref type="bibr" target="#b13">[14]</ref> version, we use the counts as the term-frequency. The inve

shown that ConvNets can be directly applied to distributed <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b15">[16]</ref> or discrete <ref type="bibr" target="#b12">[13]</ref> embe arisons with both using the pretrained word2vec <ref type="bibr" target="#b22">[23]</ref> embedding <ref type="bibr" target="#b15">[16]</ref> and using lookup tables <ref type="bibr" target="#b4">[5]<

target="#b35">Sperduti &amp; Starita, 1997)</ref>. Graph Neural Networks (GNNs) were introduced in <ref type="bibr" target="#b15">Gori et al. (2005)</ref> and <ref type="bibr" target="#b33">Scarselli
iEmb) <ref type="bibr" target="#b39">(Weston et al., 2012)</ref>, manifold regularization (ManiReg) <ref type="bibr" target="#b3">(Belkin et al., 2006)</ref>, skip-gram based graph embeddings (Deep-Wa se the metrics already reported in <ref type="bibr" target="#b16">Hamilton et al. (2017)</ref> for  <ref type="bibr" target="#b3">(Belkin et al., 2006)</ref> 59.5% 60.1% 70.7% SemiEmb <ref type="bibr"
target="#b35">Sperduti &amp; Starita, 1997)</ref>. Graph Neural Networks (GNNs) were introduced in <ref type="bibr" target="#b15">Gori et al. (2005)</ref> and <ref type="bibr" target="#b33">Scarselli
k, which produces an output for each node based on its state. This idea was adopted and improved by <ref type="bibr" target="#b24">Li et al. (2016)</ref>, which propose to use gated recurrent units <r
rks.</p><p>Attention mechanisms have become almost a de facto standard in many sequence-based tasks <ref type="bibr" target="#b2">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b13">Gehring et sed in our experiments. The particular attentional setup utilized by us closely follows the work of <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref>-but the framework is agnostic to the part terpretability, as was the case in the machine translation domain (e.g. the qualitative analysis of <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref>). • The attention mechanism is applied in d across all eight attention heads). Properly interpreting these coefficients (as performed by e.g. <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref>) will require further domain knowledge ab
image classification <ref type="bibr" target="#b17">(He et al., 2016)</ref>, semantic segmentation <ref type="bibr" target="#b21">(Jégou et al., 2017)</ref> or machine translation <ref type="bibr" ta
type="bibr" target="#b3">(Belkin et al., 2006)</ref>, skip-gram based graph embeddings (Deep-Walk) <ref type="bibr" target="#b30">(Perozzi et al., 2014)</ref>, the iterative classification algorithm % 59.6% 71.7% LP <ref type="bibr" target="#b42">(Zhu et al., 2003)</ref> 68.0% 45.3% 63.0% DeepWalk <ref type="bibr" target="#b30">(Perozzi et al., 2014)</ref> 67.2% 43.2% 65.3% ICA <ref type="bibr" t

="#b11">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b1">Atwood &amp; Towsley, 2016;</ref><ref type="bibr" target="#b16">Hamilton et al., 2017)</ref>, which define convolutions directly on t ial approach which provides a unified generalization of CNN architectures to graphs. More recently, <ref type="bibr" target="#b16">Hamilton et al. (2017)</ref> introduced GraphSAGE, a method for compu tely unobserved during training. To construct the graphs, we used the preprocessed data provided by <ref type="bibr" target="#b16">Hamilton et al. (2017)</ref>. The average number of nodes per graph i ing task, we compare against the four different supervised GraphSAGE inductive methods presented in <ref type="bibr" target="#b16">Hamilton et al. (2017)</ref>. These provide a variety of approaches t es of the two unseen test graphs, averaged after 10 runs, and reuse the metrics already reported in <ref type="bibr" target="#b16">Hamilton et al. (2017)</ref> for  <ref type="bibr" target="#b3">(Belk
fication algorithm (ICA) <ref type="bibr" target="#b26">(Lu &amp; Getoor, 2003)</ref> and Planetoid <ref type="bibr" target="#b41">(Yang et al., 2016)</ref>. We also directly compare our model against 65.3% ICA <ref type="bibr" target="#b26">(Lu &amp; Getoor, 2003)</ref> 75.1% 69.1% 73.9% Planetoid <ref type="bibr" target="#b41">(Yang et al., 2016)</ref> 75.7% 64.7% 77.2% Chebyshev <ref type="bibr br" target="#b34">(Sen et al., 2008)</ref>-and closely follow the transductive experimental setup of<ref type="bibr" target="#b41">Yang et al. (2016)</ref>. In all of these datasets, nodes correspond
gle-person pose estimation adopt the probabilistic graphical model or the pictorial structure model <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b49">50]</ref>, which is recently

ation and segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" tar
ty and parameters.</p><p>There are related multi-scale networks for classification and segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
get="#b46">47,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar

a high-to-low resolution network (e.g., VGGNet or ResNet) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b76">77]</ref>.</p><p>We present a novel architecture, namely High-Resolut dely-used networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b71">72]</ref> for pose estimation and its follow-ups <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b30">31]</ref> design the low-to-h d person boxes instead of detected person boxes. Following <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b61">62]</ref>, a six-scale pyrami and its extensions <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar >62]</ref>. Both the two processes are possibly repeated several times for boosting the performance <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" ta r" target="#b63">[64]</ref>. Hourglass <ref type="bibr" target="#b39">[40]</ref> and its extensions <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b30">31]</ref> combine low-level s computed as the average of the heatmaps of the original and flipped images for testing. Following <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b61">62]</ref>, we also perform s
ed multi-resolution images separately into multiple networks and aggregate the output response maps <ref type="bibr" target="#b63">[64]</ref>. Hourglass <ref type="bibr" target="#b39">[40]</ref> and i ion quality, e.g., <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" targe

get="#b42">43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b15">16]</ref>. There are two mainstream methods: regressing the position
get="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar
bination of bidirectional LSTM and CTC has been applied to characterlevel speech recognition before <ref type="bibr" target="#b5">(Eyben et al., 2009)</ref>, however the relatively shallow architectur
="#b10">(Graves et al., 2013)</ref> with a Connectionist Temporal Classification (CTC) output layer <ref type="bibr" target="#b9">(Graves et al., 2006;</ref><ref type="bibr">Graves, 2012, Chapter 7)</ _8">15</ref>) can be efficiently evaluated and differentiated using a dynamic programming algorithm <ref type="bibr" target="#b9">(Graves et al., 2006)</ref>. Given a target transcription y * , the ne
mal preprocessing scheme.</p><p>The spectrograms are processed by a deep bidirectional LSTM network <ref type="bibr" target="#b10">(Graves et al., 2013)</ref> with a Connectionist Temporal Classificat M is used for the hidden layers the complete architecture is referred to as deep bidirectional LSTM <ref type="bibr" target="#b10">(Graves et al., 2013)</ref>.</p></div> <div xmlns="http://www.tei-c.o
="#fig_1">1</ref> illustrates a single LSTM memory cell. For the version of LSTM used in this paper <ref type="bibr" target="#b7">(Gers et al., 2002)</ref> H is implemented by the following composite
directly train HMM-neural network hybrids to maximise the probability of the correct transcription <ref type="bibr" target="#b0">(Bahl et al., 1986;</ref><ref type="bibr" target="#b16">Jaitly et al.,

="#fig_1">1</ref> illustrates a single LSTM memory cell. For the version of LSTM used in this paper <ref type="bibr" target="#b7">(Gers et al., 2002)</ref> H is implemented by the following composite
he introduction of neural networks <ref type="bibr" target="#b2">(Bourlard &amp; Morgan, 1993;</ref><ref type="bibr" target="#b13">Hinton et al., 2012)</ref>, the networks are at present only a single

was created using alignments from an SGMM-HMM system trained using Kaldi recipe 's5', model 'tri4b' <ref type="bibr" target="#b20">(Povey et al., 2011)</ref>.</p><p>The 14 hour subset was first used t
was created using alignments from an SGMM-HMM system trained using Kaldi recipe 's5', model 'tri4b' <ref type="bibr" target="#b20">(Povey et al., 2011)</ref>.</p><p>The 14 hour subset was first used t
="#fig_1">1</ref> illustrates a single LSTM memory cell. For the version of LSTM used in this paper <ref type="bibr" target="#b7">(Gers et al., 2002)</ref> H is implemented by the following composite

directly train HMM-neural network hybrids to maximise the probability of the correct transcription <ref type="bibr" target="#b0">(Bahl et al., 1986;</ref><ref type="bibr" target="#b16">Jaitly et al.,
he introduction of neural networks <ref type="bibr" target="#b2">(Bourlard &amp; Morgan, 1993;</ref><ref type="bibr" target="#b13">Hinton et al., 2012)</ref>, the networks are at present only a single



rks are now able to directly classify raw pixels into high-level concepts such as object categories <ref type="bibr" target="#b17">(Krizhevsky et al., 2012)</ref> and messages on traffic signs <ref ty

rks are now able to directly classify raw pixels into high-level concepts such as object categories <ref type="bibr" target="#b17">(Krizhevsky et al., 2012)</ref> and messages on traffic signs <ref ty


="#fig_1">1</ref> illustrates a single LSTM memory cell. For the version of LSTM used in this paper <ref type="bibr" target="#b7">(Gers et al., 2002)</ref> H is implemented by the following composite
ave been considered for HMMbased recognisers as a way of dealing with out of vocabulary (OOV) words <ref type="bibr" target="#b6">(Galescu, 2003;</ref><ref type="bibr" target="#b1">Bisani &amp; Ney, 2


="#fig_1">1</ref> illustrates a single LSTM memory cell. For the version of LSTM used in this paper <ref type="bibr" target="#b7">(Gers et al., 2002)</ref> H is implemented by the following composite

ave been considered for HMMbased recognisers as a way of dealing with out of vocabulary (OOV) words <ref type="bibr" target="#b6">(Galescu, 2003;</ref><ref type="bibr" target="#b1">Bisani &amp; Ney, 2
rks are now able to directly classify raw pixels into high-level concepts such as object categories <ref type="bibr" target="#b17">(Krizhevsky et al., 2012)</ref> and messages on traffic signs <ref ty
he introduction of neural networks <ref type="bibr" target="#b2">(Bourlard &amp; Morgan, 1993;</ref><ref type="bibr" target="#b13">Hinton et al., 2012)</ref>, the networks are at present only a single

ine learning tasks involve graph structured datasets, such as classifying posts in a social network <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref>, predicting interfaces between protein o aggregate a local set of lower-level features. We refer to such an operator as a graph aggregator <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref> and the set of local nodes as the rece ref type="bibr" target="#b15">Kipf and Welling, 2017</ref>) can be interpreted as graph aggregators <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref>.</p><p>Graph aggregators are the basic t to the inductive node classification problem. We also improve the sampling strategy introduced in <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref> to reduce the memory cost and increase oral forecasting problem. Extensive experiments on two node classification datasets, PPI and Reddit <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref>, and one traffic speed forecasting dat rtional to the total number of nodes, which could be hundreds of thousands of nodes in large graphs <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computatio "bibr" target="#b10">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computational cost, <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref> proposed the GraphSAGE framework that ble and the goal is to predict the labels of the unseen testing nodes. Our approach follows that of <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref>, where a mini-batch of nodes are sampl dels in our framework and a two-layer fully connected neural network on the PPI and Reddit datasets <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref>. The five baseline aggregators include the effectiveness of incorporating graph structures, we also evaluate a two-layer fully-connected  <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref> (61.2)<ref type="foot" target="#foot_0 hyperparameters for training. The training, validation, and testing splits are the same as that in <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref>. The micro-averaged F1 score is used t ith the previous state-of-the-art methods on inductive node classification. This includes GraphSAGE <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref>, GAT <ref type="bibr" target="#b27">(V can see steady improvement with larger sampling sizes, which is consistent with the observation in <ref type="bibr" target="#b10">(Hamilton et al., 2017a)</ref>.</p><p>Effect of output dimensions in r" target="#b15">Kipf and Welling, 2017;</ref><ref type="bibr" target="#b8">Fout et al., 2017;</ref><ref type="bibr" target="#b10">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b27">Veličkovi d on either pooling over neighborhoods <ref type="bibr" target="#b15">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b10">Hamilton et al., 2017a)</ref> or computing a weighted sum of the neig rget="#b7">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b15">Kipf and Welling, 2017;</ref><ref type="bibr" target="#b10">Hamilton et al., 2017a)</ref>, while others integrated edge features
)</ref>.</p><p>Recent research, however, has pivoted to solving these problems by graph convolution <ref type="bibr" target="#b7">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b0">Atwood and ses of the graph pooling aggregators. Some models only integrate the node features of neighborhoods <ref type="bibr" target="#b7">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b15">Kipf and W
aplacian matrix, which is computationally intractable on large graphs, while the localized versions <ref type="bibr" target="#b6">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b15">Kipf and M <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref> with the ChebNet operator <ref type="bibr" target="#b6">(Defferrard et al., 2016)</ref>, and applied it to a synthetic video p

convolution over a regular grid topology <ref type="bibr" target="#b9">(Gehring et al., 2017;</ref><ref type="bibr" target="#b16">Krizhevsky et al., 2012)</ref> to 'convolution' over graph structures
h Convolutional Recurrent Neural Network (GCRNN), which replaced the fully-connected layers in LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref> with the ChebNet operator <r
convolution over a regular grid topology <ref type="bibr" target="#b9">(Gehring et al., 2017;</ref><ref type="bibr" target="#b16">Krizhevsky et al., 2012)</ref> to 'convolution' over graph structures
h Convolutional Recurrent Neural Network (GCRNN), which replaced the fully-connected layers in LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref> with the ChebNet operator <r
aplacian matrix, which is computationally intractable on large graphs, while the localized versions <ref type="bibr" target="#b6">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b15">Kipf and M <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref> with the ChebNet operator <ref type="bibr" target="#b6">(Defferrard et al., 2016)</ref>, and applied it to a synthetic video p
These two authors contributed equally. dard definition of convolution over a regular grid topology <ref type="bibr" target="#b9">(Gehring et al., 2017;</ref><ref type="bibr" target="#b16">Krizhevsky
ving these problems by graph convolution <ref type="bibr" target="#b7">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b0">Atwood and Towsley, 2016;</ref><ref type="bibr" target="#b15">Kipf and RU <ref type="bibr" target="#b5">(Chung et al., 2014)</ref> with the diffusion convolution operator <ref type="bibr" target="#b0">(Atwood and Towsley, 2016)</ref>. Furthermore, DCRNN takes the directi e="bibr" target="#b10">Hamilton et al., 2017a)</ref>, while others integrated edge features as well <ref type="bibr" target="#b0">(Atwood and Towsley, 2016;</ref><ref type="bibr" target="#b8">Fout et
amounts of supervision. However, even systems that have relied extensively on unsupervised features <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b36">Turian et s, and present a hybrid tagging architecture. This architecture is similar to the ones presented by <ref type="bibr" target="#b7">Collobert et al. (2011)</ref> and <ref type="bibr" target="#b19">Huang ></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pretrained embeddings</head><p>As in <ref type="bibr" target="#b7">Collobert et al. (2011)</ref>, we use pretrained word embeddings to in g of our    Several other neural architectures have previously been proposed for NER. For instance, <ref type="bibr" target="#b7">Collobert et al. (2011)</ref> uses a CNN over a sequence of word embed
ds, including the ones using external labeled data. The only exception is Dutch, where the model of <ref type="bibr" target="#b14">Gillick et al. (2015)</ref> can perform better by leveraging the info ally, there is currently a lot of interest in models for NER that use letter-based representations. <ref type="bibr" target="#b14">Gillick et al. (2015)</ref> model the task of sequencelabeling as a s
do not use gazetteers or any external labeled resources. The best score reported on this task is by <ref type="bibr" target="#b26">Luo et al. (2015)</ref>. They obtained a F 1 of 91.2 by jointly model
representations. To encourage the model to depend on both representations, we use dropout training <ref type="bibr" target="#b17">(Hinton et al., 2012)</ref>, applying a dropout mask to the final emb
ure">2</ref> that is inspired by transition-based parsers, in particular the arc-standard parser of <ref type="bibr" target="#b29">Nivre (2004)</ref>. In this algorithm, we make use of two stacks (des
. (2015)</ref> presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. <ref type="bibr" target="#b40">Zhou and Xu (2015)</ref> also used a similar model and adapted it to


ed extensively on unsupervised features <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b36">Turian et al., 2010;</ref><ref type="bibr" target="#b23">Lin and Wu,
do not use gazetteers or any external labeled resources. The best score reported on this task is by <ref type="bibr" target="#b26">Luo et al. (2015)</ref>. They obtained a F 1 of 91.2 by jointly model

procedures such as generating forced alignments and decision trees. Meanwhile, another line of work <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target ction to infer speech-label alignments automatically. This CTC technique is further investigated in <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target incorporate lexicons and language models into decoding. When decoding CTC-trained models, past work <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target enchmark show that Eesen results in superior performance than the existing end-to-end ASR pipelines <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref>. The WERs of Eesen are ained using frame-level labels with respect to the cross-entropy (CE) criterion. Instead, following <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target /1.0"><head n="3.1.">Decoding with WFSTs</head><p>Previous work has introduced a variety of methods <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target ARPA format (which we will consistently refer to as standard). To be consistent with previous work <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref>, we report our results e">3</ref> lists the results of end-to-end ASR systems that have been reported in the previous work <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref> and on the same datase ature differ not only in their model architectures but also in their decoding methods. For example, <ref type="bibr" target="#b6">[6]</ref> and <ref type="bibr" target="#b8">[8]</ref> adopt two distin ">[10]</ref> or achieve the integration under constrained conditions (e.g., nbest list rescoring in <ref type="bibr" target="#b6">[6]</ref>). In this work, we propose a generalized decoding approach b ed in decoding. When only the lexicon is used, our decoding behaves similarly as the beam search in <ref type="bibr" target="#b6">[6]</ref>. In this case, the WER rises quickly to 26.92%. This obvious dard language model, the character-based system gets the WER of 9.07%. CTC experiments in past work <ref type="bibr" target="#b6">[6]</ref> have adopted an expanded vocabulary, and re-trained the lang ref type="bibr" target="#b8">8]</ref> and on the same dataset. Our Eesen framework outperforms both <ref type="bibr" target="#b6">[6]</ref> and <ref type="bibr" target="#b8">[8]</ref> in terms of WERs ]</ref> in terms of WERs on the testing set. It is worth pointing out that the 8.7% WER reported in <ref type="bibr" target="#b6">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the bibr" target="#b6">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the authors of <ref type="bibr" target="#b6">[6]</ref> generate a nbest list of hypotheses from a hybrid DNN model, e WERs of Eesen systems via more advanced learning techniques (e.g., expected transcription loss in <ref type="bibr" target="#b6">[6]</ref>) and alternative decoding approach (e.g., dynamic decoders <
viewed as a sequence-to-sequence learning problem. We exploit deep recurrent neural networks (RNNs) <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b17">16]</ref> as the acoustic mo
the total probability of all CTC paths that end with label l u at frame t. As with the case of HMMs <ref type="bibr" target="#b27">[26]</ref>, α u t can be recursively computed from α u t−1 and α u−1
n the GMM models.</p><p>Previous work has made various attempts to reduce the complexity of ASR. In <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>, researchers propose t
" target="#b24">[23,</ref><ref type="bibr" target="#b25">24]</ref> or convolutional neural networks <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b20">19]</ref>, is straightforwar
we propose a generalized decoding approach based on WFSTs <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref>. A WFST is a finite-state acceptor (FSA) in which each tran erage any language models that can be converted into WFSTs. Following conventions in the literature <ref type="bibr" target="#b29">[28]</ref>, the language model WFST is denoted as G.</p><p>Lexicon. A hybrid HMM/DNN system. The hybrid system is constructed by following the standard Kaldi recipe "s5" <ref type="bibr" target="#b29">[28]</ref>. Inputs of the DNN model are 11 neighboring frames of filt
type="bibr" target="#b37">36]</ref> and adaptive training <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b39">38]</ref> techniques for the CTC models.</p></div> <div xmlns="http:/
type="bibr" target="#b32">[31]</ref>). Also, we are interested to apply Eesen to various languages <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" ta
ref> depicts the structure of the LSTM units we use. The blue curves represent peephole connections <ref type="bibr" target="#b23">[22]</ref> that link the memory cells to the gates to learn precise t
type="bibr" target="#b37">36]</ref> and adaptive training <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b39">38]</ref> techniques for the CTC models.</p></div> <div xmlns="http:/
="bibr" target="#b17">16]</ref> as the acoustic models, and the Long Short-Term Memory (LSTM) units <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" ta problem <ref type="bibr" target="#b22">[21]</ref>. To overcome this issue, we apply the LSTM units <ref type="bibr" target="#b18">[17]</ref> as the building blocks of RNNs. LSTM contains memory cells
r" target="#b6">[6]</ref>). In this work, we propose a generalized decoding approach based on WFSTs <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref>. A WFST is a finit
emory (LSTM) units <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b21">20]</ref> as the RNN building " target="#b25">24]</ref> or convolutional neural networks <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b20">19]</ref>, is straightforward to achieve.</p></div> <div xmlns="http:
troduction of deep neural networks (DNNs) as acoustic models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. In the hybrid HMM/DNN
" target="#b24">[23,</ref><ref type="bibr" target="#b25">24]</ref> or convolutional neural networks <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b20">19]</ref>, is straightforwar
bing LSTMs with other network structures, e.g., time-delay <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b25">24]</ref> or convolutional neural networks <ref type="bibr" target="#
" target="#b24">[23,</ref><ref type="bibr" target="#b25">24]</ref> or convolutional neural networks <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b20">19]</ref>, is straightforwar
="bibr" target="#b17">16]</ref> as the acoustic models, and the Long Short-Term Memory (LSTM) units <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" ta problem <ref type="bibr" target="#b22">[21]</ref>. To overcome this issue, we apply the LSTM units <ref type="bibr" target="#b18">[17]</ref> as the building blocks of RNNs. LSTM contains memory cells
RNNs to learn long-term temporal dependency can be difficult due to the vanishing gradients problem <ref type="bibr" target="#b22">[21]</ref>. To overcome this issue, we apply the LSTM units <ref type

we propose a generalized decoding approach based on WFSTs <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref>. A WFST is a finite-state acceptor (FSA) in which each tran erage any language models that can be converted into WFSTs. Following conventions in the literature <ref type="bibr" target="#b29">[28]</ref>, the language model WFST is denoted as G.</p><p>Lexicon. A hybrid HMM/DNN system. The hybrid system is constructed by following the standard Kaldi recipe "s5" <ref type="bibr" target="#b29">[28]</ref>. Inputs of the DNN model are 11 neighboring frames of filt
type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b37">36]</ref> and adaptive training <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b39">38]</ref> techniques for the
b16">[17]</ref>.</p><p>A more direct approach to LTR metric optimization was proposed by Qin et al. <ref type="bibr" target="#b13">[14]</ref>, where the rank variable in the definition of metrics like Recent hardware and software advances in the training of neural networks, however, make the work in <ref type="bibr" target="#b13">[14]</ref> relevant again and potentially allow us to harvest the eff ng state-of-the-art LTR algorithms such as LambdaMART. We give an overview of LTR and in particular <ref type="bibr" target="#b13">[14]</ref> in Section 2. We discuss experimental results in Section 3 "#b14">[15]</ref>, boosting <ref type="bibr" target="#b18">[19]</ref>, and approximating the metric <ref type="bibr" target="#b13">[14]</ref>. It is the latter that can tightly bound any ranking metri " target="#b13">[14]</ref>. It is the latter that can tightly bound any ranking metric such as NDCG <ref type="bibr" target="#b13">[14]</ref> and can be easily optimized with gradient descent.</p><p>S adient descent.</p><p>Surprisingly, despite its attractive theoretical properties, the framework in <ref type="bibr" target="#b13">[14]</ref> has received little attention in LTR studies in the decade ts to optimize NDCG-referred to as ApproxNDCG. Our results show that  the theoretical guarantees in <ref type="bibr" target="#b13">[14]</ref> materialize in practice. Before we go any further, we give the indicator which is 1 if s &lt; t and 0 otherwise. <ref type="bibr">Qin et al.</ref> propose in <ref type="bibr" target="#b13">[14]</ref> a smooth approximation of Equation <ref type="formula" tar ture of ranking utility functions.</p><p>In this work, we set out to revisit the work of Qin et al. <ref type="bibr" target="#b13">[14]</ref> which formulates a smooth approximation to any ranking met any ranking metric such as NDCG. Unlike many other existing surrogate LTR losses, the framework in <ref type="bibr" target="#b13">[14]</ref> offers a way to directly optimize ranking metrics. Because g metrics rather than loosely related surrogate losses; and (b) that the approximation framework in <ref type="bibr" target="#b13">[14]</ref> could lay out the foundation of deep neural networks in LT
leased our implementation of ApproxNDCG in Tensorflow in the open-source Tensorflow Ranking library <ref type="bibr" target="#b11">[12]</ref>. <ref type="foot" target="#foot_0">1</ref></p></div> <div
n="3.2">Models</head><p>We have compared our results with existing ranking models including ListMLE <ref type="bibr" target="#b17">[18]</ref>, RankNet and LambdaMART <ref type="bibr" target="#b2">[3]<

/p><p>The hyperparameters for LambdaMART models are based on those reported in previous work (e.g., <ref type="bibr" target="#b15">[16]</ref>) and further fine-tuned on the validation set. Specificall Note that our NDCG measurements for λMART GBM are lower than those reported in previous work (e.g., <ref type="bibr" target="#b15">[16]</ref>). This is because LightGBM computes an NDCG of 1.0 for que
n="3.2">Models</head><p>We have compared our results with existing ranking models including ListMLE <ref type="bibr" target="#b17">[18]</ref>, RankNet and LambdaMART <ref type="bibr" target="#b2">[3]<
/p><p>The hyperparameters for LambdaMART models are based on those reported in previous work (e.g., <ref type="bibr" target="#b15">[16]</ref>) and further fine-tuned on the validation set. Specificall Note that our NDCG measurements for λMART GBM are lower than those reported in previous work (e.g., <ref type="bibr" target="#b15">[16]</ref>). This is because LightGBM computes an NDCG of 1.0 for que
ew notable exceptions that attempt to directly maximize a ranking metric by using coordinate ascent <ref type="bibr" target="#b10">[11]</ref>, smoothing scores <ref type="bibr" target="#b14">[15]</ref
he legacy RankLib implementation (λMART RankLib ). We implemented ListMLE and RankNet in Tensorflow <ref type="bibr" target="#b0">[1]</ref>, a deep learning framework. In all of our experiments, we ru
#b5">6]</ref>, support vector machines <ref type="bibr" target="#b8">[9]</ref>, and neural networks <ref type="bibr" target="#b1">[2]</ref>. In this paper, we model f using the latter.</p><p>The loss
ides, as demonstrated for instance by LambdaMART's winning of the Yahoo! Learning to Rank Challenge <ref type="bibr" target="#b4">[5]</ref>, and since regression trees cannot be optimized globally, su ilable LTR datasets: MSLR-WEB30K <ref type="bibr" target="#b12">[13]</ref> and Yahoo! LTR Challenge <ref type="bibr" target="#b4">[5]</ref>. Both datasets contain roughly 30,000 queries. Web30K has an
as SoftRank <ref type="bibr" target="#b14">[15]</ref> and indirect boosting methods like LambdaMART <ref type="bibr" target="#b16">[17]</ref>.</p><p>A more direct approach to LTR metric optimization w
ides, as demonstrated for instance by LambdaMART's winning of the Yahoo! Learning to Rank Challenge <ref type="bibr" target="#b4">[5]</ref>, and since regression trees cannot be optimized globally, su ilable LTR datasets: MSLR-WEB30K <ref type="bibr" target="#b12">[13]</ref> and Yahoo! LTR Challenge <ref type="bibr" target="#b4">[5]</ref>. Both datasets contain roughly 30,000 queries. Web30K has an
the latter.</p><p>The loss function, ℓ, is ideally derived from a utility of interest such as NDCG <ref type="bibr" target="#b7">[8]</ref>, a popular ranking metric. However, most ranking metrics are
as SoftRank <ref type="bibr" target="#b14">[15]</ref> and indirect boosting methods like LambdaMART <ref type="bibr" target="#b16">[17]</ref>.</p><p>A more direct approach to LTR metric optimization w
he legacy RankLib implementation (λMART RankLib ). We implemented ListMLE and RankNet in Tensorflow <ref type="bibr" target="#b0">[1]</ref>, a deep learning framework. In all of our experiments, we ru
ides, as demonstrated for instance by LambdaMART's winning of the Yahoo! Learning to Rank Challenge <ref type="bibr" target="#b4">[5]</ref>, and since regression trees cannot be optimized globally, su ilable LTR datasets: MSLR-WEB30K <ref type="bibr" target="#b12">[13]</ref> and Yahoo! LTR Challenge <ref type="bibr" target="#b4">[5]</ref>. Both datasets contain roughly 30,000 queries. Web30K has an
ides, as demonstrated for instance by LambdaMART's winning of the Yahoo! Learning to Rank Challenge <ref type="bibr" target="#b4">[5]</ref>, and since regression trees cannot be optimized globally, su ilable LTR datasets: MSLR-WEB30K <ref type="bibr" target="#b12">[13]</ref> and Yahoo! LTR Challenge <ref type="bibr" target="#b4">[5]</ref>. Both datasets contain roughly 30,000 queries. Web30K has an
#b5">6]</ref>, support vector machines <ref type="bibr" target="#b8">[9]</ref>, and neural networks <ref type="bibr" target="#b1">[2]</ref>. In this paper, we model f using the latter.</p><p>The loss
the latter.</p><p>The loss function, ℓ, is ideally derived from a utility of interest such as NDCG <ref type="bibr" target="#b7">[8]</ref>, a popular ranking metric. However, most ranking metrics are
"bibr" target="#b2">[3]</ref>. To train LambdaMART models, we used the recent open-source Light-GBM <ref type="bibr" target="#b9">[10]</ref> implementation (denoted by λMART GBM ). We also used the le
ides, as demonstrated for instance by LambdaMART's winning of the Yahoo! Learning to Rank Challenge <ref type="bibr" target="#b4">[5]</ref>, and since regression trees cannot be optimized globally, su ilable LTR datasets: MSLR-WEB30K <ref type="bibr" target="#b12">[13]</ref> and Yahoo! LTR Challenge <ref type="bibr" target="#b4">[5]</ref>. Both datasets contain roughly 30,000 queries. Web30K has an
is nondifferentiable. Therefore, for S with model parameters Φ, we use the policy gradient based RL <ref type="bibr" target="#b13">[Sutton et al., 2000]</ref> to derive its gradient:</p><formula xml:i
dle and Freudenthaler, 2014]</ref>, and frequency-based sampling that subsamples frequent instances <ref type="bibr" target="#b2">[Caselles-Dupré et al., 2018]</ref>. The other type is auxiliary infor ds, the previous GAN-based models, such as <ref type="bibr">IRGAN [Wang et al., 2017]</ref>, CF-GAN <ref type="bibr" target="#b2">[Chae et al., 2018]</ref> and AdvIR [Park and Chang, 2019], focuses on ning better recommender models with adversarial instances <ref type="bibr">[Wang et al., 2017;</ref><ref type="bibr" target="#b2">Chae et al., 2018;</ref><ref type="bibr">Park and Chang, 2019</ref>].
t will cause selection bias, making the model under-trained and resulting in suboptimal performance <ref type="bibr" target="#b10">[Lian et al., 2017;</ref><ref type="bibr" target="#b4">Ding et al., 2 m mean discrepancy (MMD) between the empirical distribution of feature vectors in these two subsets <ref type="bibr" target="#b10">[Li et al., 2015;</ref><ref type="bibr">Zhang et al., 2017]</ref>. Co
t will cause selection bias, making the model under-trained and resulting in suboptimal performance <ref type="bibr" target="#b10">[Lian et al., 2017;</ref><ref type="bibr" target="#b4">Ding et al., 2 m mean discrepancy (MMD) between the empirical distribution of feature vectors in these two subsets <ref type="bibr" target="#b10">[Li et al., 2015;</ref><ref type="bibr">Zhang et al., 2017]</ref>. Co
egative instances to train a better recommender as prediction model. The most related work is KBGAN <ref type="bibr" target="#b1">[Cai and Wang, 2018</ref>] that generates hard negative instances to t selects the item with the highest prediction score among X randomly sampled negatives.</p><p>-KBGAN <ref type="bibr" target="#b1">[Cai and Wang, 2018]</ref>. With the generator serving as an adversari
is to uniformly sample negative instances from the missing data (i.e., the unobserved interactions) <ref type="bibr" target="#b8">[Jiang et al., 2018;</ref><ref type="bibr" target="#b7">He et al., 201
ch cannot be simply integrated like other auxiliary information in multiple feedback recommendation <ref type="bibr" target="#b5">[Ding et al., 2018b;</ref><ref type="bibr" target="#b6">Gao et al., 20
ng data (i.e., the unobserved interactions) <ref type="bibr" target="#b8">[Jiang et al., 2018;</ref><ref type="bibr" target="#b7">He et al., 2018;</ref><ref type="bibr" target="#b12">Lin et al., 2019]
is to uniformly sample negative instances from the missing data (i.e., the unobserved interactions) <ref type="bibr" target="#b8">[Jiang et al., 2018;</ref><ref type="bibr" target="#b7">He et al., 201
dle and Freudenthaler, 2014]</ref>, and frequency-based sampling that subsamples frequent instances <ref type="bibr" target="#b2">[Caselles-Dupré et al., 2018]</ref>. The other type is auxiliary infor ds, the previous GAN-based models, such as <ref type="bibr">IRGAN [Wang et al., 2017]</ref>, CF-GAN <ref type="bibr" target="#b2">[Chae et al., 2018]</ref> and AdvIR [Park and Chang, 2019], focuses on ning better recommender models with adversarial instances <ref type="bibr">[Wang et al., 2017;</ref><ref type="bibr" target="#b2">Chae et al., 2018;</ref><ref type="bibr">Park and Chang, 2019</ref>].
egative instances to train a better recommender as prediction model. The most related work is KBGAN <ref type="bibr" target="#b1">[Cai and Wang, 2018</ref>] that generates hard negative instances to t selects the item with the highest prediction score among X randomly sampled negatives.</p><p>-KBGAN <ref type="bibr" target="#b1">[Cai and Wang, 2018]</ref>. With the generator serving as an adversari
ad><p>We formalize our notion of spatial correlation similar to prior studies of spatial footprints <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. We define a spatial ed in hardware by correlating patterns with the code and/or data address that initiates the pattern <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. Whereas existing sp tracking of spatial correlation. We show that the cache-coupled structures used in previous work ( <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>) are suboptimal for dex consistently provides the most accurate predictions when correlation table storage is unbounded <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. By combining both q lications, PC+address indexing can be approximated by combining the PC with a spatial region offset <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. The spatial region ="http://www.tei-c.org/ns/1.0"><head n="4.2.">Indexing</head><p>Prior studies of spatial predictors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> advocate predictor i coverage and/or fragment prediction entries, consequently polluting the PHT.</p><p>Past predictors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> couple the predictor eas existing spatial pattern prefetching designs are effective for desktop/engineering applications <ref type="bibr" target="#b3">[4]</ref>, the only practical implementation evaluated on server workl e highest coverage.</p><p>For scientific applications, we corroborate the conclusions of prior work <ref type="bibr" target="#b3">[4]</ref> that indicate PC+offset indexing generally approaches the pe led sectored cache <ref type="bibr" target="#b21">[22]</ref>, whereas the spatial pattern predictor <ref type="bibr" target="#b3">[4]</ref> provided a logical sectored-cache tag array alongside a trad
l correlation similar to prior studies of spatial footprints <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. We define a spatial region as a fixed-size portion of the with the code and/or data address that initiates the pattern <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. Whereas existing spatial pattern prefetching designs are e ow that the cache-coupled structures used in previous work ( <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>) are suboptimal for observing spatial correlation. Accesses rate predictions when correlation table storage is unbounded <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. By combining both quantities, which we call PC+address ind pproximated by combining the PC with a spatial region offset <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. The spatial region offset of a data address is the distanc "4.2.">Indexing</head><p>Prior studies of spatial predictors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> advocate predictor indices that include address information tries, consequently polluting the PHT.</p><p>Past predictors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> couple the predictor training structure to a sectored (i.e. y practical implementation evaluated on server workloads provides less than 20% miss rate reduction <ref type="bibr" target="#b16">[17]</ref>.</p><p>In this paper, we reconsider prediction and streami is work demonstrates: • Effective spatial correlation and prediction. Contrary to previous findings <ref type="bibr" target="#b16">[17]</ref>, address-based correlation is not needed to predict the ac region generation is defined can significantly impact the accuracy and coverage of spatial patterns <ref type="bibr" target="#b16">[17]</ref>. A generation must be defined to ensure that, when SMS str t distinguish among distinct access patterns to different data structures by the same code (e.g.,   <ref type="bibr" target="#b16">[17]</ref>, which indicated that PC+address provides superior coverag experience worse conflict behavior. To mitigate this disadvantage, the spatial footprint predictor <ref type="bibr" target="#b16">[17]</ref> employed a decoupled sectored cache <ref type="bibr" targe
r streaming (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar atial relationships can be exploited through simple prefetching schemes, such as stride prefetching <ref type="bibr" target="#b23">[24]</ref>.</p><p>Commercial applications exhibit complex access patt
ace-driven and cycle-accurate full-system simulation of a shared-memory multiprocessor using FLEXUS <ref type="bibr" target="#b13">[14]</ref>. FLEXUS can execute unmodified commercial applications and
[1,</ref><ref type="bibr" target="#b9">10]</ref>. We perform speculative load and store prefetching <ref type="bibr" target="#b8">[9]</ref>, and speculatively relax memory ordering constraints at memo
isited. Database scan and join operations, which dominate the execution of decision support queries <ref type="bibr" target="#b22">[23]</ref>, contain long repetitive access patterns that visit data o Database Server. We select four queries from the TPC-H DSS workload based on the categorization in <ref type="bibr" target="#b22">[23]</ref>: one scan-dominated query, two join-dominated queries, and
. Our most significant departure from prior designs is that those designs target decoupled sectored <ref type="bibr" target="#b21">[22]</ref> or sub-blocked caches. Integrating spatial pattern predict l footprint predictor <ref type="bibr" target="#b16">[17]</ref> employed a decoupled sectored cache <ref type="bibr" target="#b21">[22]</ref>, whereas the spatial pattern predictor <ref type="bibr" ta
get="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>), few studies have demonstrated success at improving memory 7">[28]</ref> propose offline profiling to select fetch size upon a miss. Guided region prefetching <ref type="bibr" target="#b28">[29]</ref> uses compiler hints to direct both spatial and non-spatial
20">[21]</ref>. Software prefetching can accelerate certain database operations, such as hash joins <ref type="bibr" target="#b4">[5]</ref>. Temporal streaming reduces coherence stalls by streaming re
pplications through memory prefetching or streaming (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" tar up of 1.22 and at best 1.48 over a system without SMS. In contrast, the global history buffer (GHB) <ref type="bibr" target="#b19">[20]</ref>, the best proposed prefetcher for desktop/engineering appl ry access patterns for commercial workloads. We compare SMS against the Global History Buffer (GHB) <ref type="bibr" target="#b19">[20]</ref>, whose PC/DC (program counter / delta correlation) variant uffer sizes: 256 entries (sufficient for SPEC applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>) and 16k entries (to roughly match the capacity of the SMS
><ref type="bibr" target="#b29">30]</ref>. Although modern servers provide copious memory bandwidth <ref type="bibr" target="#b6">[7]</ref>, the memory system remains underutilized because these depen
system-related stalls <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar
emory latency in desktop and scientific applications through memory prefetching or streaming (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" ta
the dual data cache <ref type="bibr" target="#b10">[11]</ref>, the spatial locality detection table <ref type="bibr" target="#b15">[16]</ref>, and caches that dynamically adjust block size <ref type="
system-related stalls <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar
ementation of the total store order memory consistency model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>. We perform speculative load and store prefetching <ref type relax memory ordering constraints at memory barrier and atomic read-modify-write memory operations <ref type="bibr" target="#b9">[10]</ref>. We list other relevant parameters of our system model in T
r streaming (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar atial relationships can be exploited through simple prefetching schemes, such as stride prefetching <ref type="bibr" target="#b23">[24]</ref>.</p><p>Commercial applications exhibit complex access patt
p><p>For cycle-accurate simulations, we use a sampling approach developed in accordance with SMARTS <ref type="bibr" target="#b31">[32]</ref>. Our samples are drawn over an interval of 10 to 30 second
just 1.3 parallel offchip misses for these applications on a current-generation outof-order system <ref type="bibr" target="#b5">[6]</ref>. Enhancing the parallelism and hiding the latency of memory coincide with misses that the out-of-order core is able to overlap. Even though overall MLP is low <ref type="bibr" target="#b5">[6]</ref>, misses that the core can issue in parallel also tend to be
ementation of the total store order memory consistency model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>. We perform speculative load and store prefetching <ref type relax memory ordering constraints at memory barrier and atomic read-modify-write memory operations <ref type="bibr" target="#b9">[10]</ref>. We list other relevant parameters of our system model in T
p><p>For cycle-accurate simulations, we use a sampling approach developed in accordance with SMARTS <ref type="bibr" target="#b31">[32]</ref>. Our samples are drawn over an interval of 10 to 30 second
20">[21]</ref>. Software prefetching can accelerate certain database operations, such as hash joins <ref type="bibr" target="#b4">[5]</ref>. Temporal streaming reduces coherence stalls by streaming re
of high inter-class similarities. Most recently, an additional center loss was introduced into CNNs <ref type="bibr" target="#b43">[44]</ref> to reduce the intra-class variations of the learned featur suffers from drastic data expansion when constructing image pairs from the training set. Wen et al. <ref type="bibr" target="#b43">[44]</ref> introduced a center loss for face recognition, which targe eview of Center Loss</head><p>As illustrated in Fig. <ref type="figure">1</ref>(b), the center loss <ref type="bibr" target="#b43">[44]</ref> explicitly reduces the intra-class variations by pushing s the CNN training.</p><p>1) Forward propagation: The center loss denoted as L C is defined in Eq. 1 <ref type="bibr" target="#b43">[44]</ref> as the summation of squared distances between samples and
">[28]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bib coding (MSR <ref type="bibr" target="#b31">[32]</ref>), and CNN-based methods (3DCNN and 3DCNN-DAP <ref type="bibr" target="#b19">[20]</ref>, Inception <ref type="bibr" target="#b27">[28]</ref>, IACN <div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. Classes Feature Strategy 3DCNN <ref type="bibr" target="#b19">[20]</ref> 85.9 7 Dynamic 15 folds ITBN <ref type="bibr" target="#b42 6 Dynamic 4 folds Cov3D <ref type="bibr" target="#b33">[34]</ref> 92.3 7 Dynamic 5 folds 3DCNN-DAP <ref type="bibr" target="#b19">[20]</ref> 92.4 7 Dynamic 15 folds Inception <ref type="bibr" target= ynamic 20 folds HOG 3D <ref type="bibr" target="#b16">[17]</ref> 60.89 6 Dynamic 10 folds 3DCNN-DAP <ref type="bibr" target="#b19">[20]</ref> 63.4 6 Dynamic 20 folds DTAGN <ref type="bibr" target="#b1
d 2D features <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bib
achieved promising results on facial expression recognition especially in more challenging settings <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bib mising recognition performance under real-world conditions as demonstrated in the recent EmotiW2015 <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bib ranked at the third place for the testing set among all the methods compared. Note that, Kim et al. <ref type="bibr" target="#b15">[16]</ref> and Yu et al. <ref type="bibr" target="#b47">[48]</ref>, w an ensemble of IL-CNNs has been constructed and achieves comparable performance as the best methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b47">[48]</ref> on the SFEW dat
Gabor phase shifts (F-Bases) <ref type="bibr" target="#b35">[36]</ref>, Latent Ordinal Model (LOMo) <ref type="bibr" target="#b37">[38]</ref>, spatiotemporal covariance descriptors (Cov3D) <ref type=" <ref type="bibr" target="#b4">[5]</ref>, STM-ExpLet <ref type="bibr" target="#b20">[21]</ref>, LOMo <ref type="bibr" target="#b37">[38]</ref>, ITBN <ref type="bibr" target="#b42">[43]</ref> and F-Base 94.19 7 Dynamic 10 folds F-Bases <ref type="bibr" target="#b35">[36]</ref> 94.81 7 Static LOSO LOMo <ref type="bibr" target="#b37">[38]</ref> 95.1 7 Dynamic 10 folds IACNN <ref type="bibr" target="#b2
">[16]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" ">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" "bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b53">[54]</ref> and EmotiW2016 challenge <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr"
ype="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref>, Oulu-CASIA database <ref type="bibr" target="#b49">[50]</ref> and MMI database <ref type="bibr" target="#b30">[31]</ref> ">[51]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b49">[50]</ref>, features are deliberately designed to utilize both spatia [24]</ref>, the MMI database <ref type="bibr" target="#b30">[31]</ref>, and the Oulu-CASIA database <ref type="bibr" target="#b49">[50]</ref>, and more importantly, on a spontaneous facial expression
Gabor phase shifts (F-Bases) <ref type="bibr" target="#b35">[36]</ref>, Latent Ordinal Model (LOMo) <ref type="bibr" target="#b37">[38]</ref>, spatiotemporal covariance descriptors (Cov3D) <ref type=" <ref type="bibr" target="#b4">[5]</ref>, STM-ExpLet <ref type="bibr" target="#b20">[21]</ref>, LOMo <ref type="bibr" target="#b37">[38]</ref>, ITBN <ref type="bibr" target="#b42">[43]</ref> and F-Base 94.19 7 Dynamic 10 folds F-Bases <ref type="bibr" target="#b35">[36]</ref> 94.81 7 Static LOSO LOMo <ref type="bibr" target="#b37">[38]</ref> 95.1 7 Dynamic 10 folds IACNN <ref type="bibr" target="#b2
b1">[2]</ref>, histograms of Local Binary Patterns (LBP) <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b14">[15]</ref>, histograms of Local Phase Quantization (LPQ) <ref type="b
re similar. Rather than minimizing the distance to the cluster center as the center loss, Li et al. <ref type="bibr" target="#b17">[18]</ref> proposed a Deep Locality-Preserving CNN (DLP-CNN), which p <ref type="bibr" target="#b27">[28]</ref>, IACNN <ref type="bibr" target="#b25">[26]</ref>, DLP-CNN <ref type="bibr" target="#b17">[18]</ref>, FN2EN <ref type="bibr" target="#b6">[7]</ref>, PPDN <ref 1 7 Dynamic 10 folds IACNN <ref type="bibr" target="#b25">[26]</ref> 95.37 7 Static 8 folds DLP-CNN <ref type="bibr" target="#b17">[18]</ref> 95.78 6 Static 5 folds STM <ref type="bibr" target="#b4">[
21">[22]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bib
pplied to the task of single image super-resolution (SISR) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" tar rget="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar of image super-resolution. Thus, in recent image SR networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42]</ref>, batch normalizatio rget="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar rget="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar f type="bibr" target="#b13">[14]</ref>, SRResNet <ref type="bibr" target="#b16">[17]</ref> and EDSR <ref type="bibr" target="#b18">[19]</ref>). The increasing of depth brings benefits to representatio ding blocks for image super-resolution networks. Compared with vanilla residual blocks used in EDSR <ref type="bibr" target="#b18">[19]</ref>, we introduce WDSR-A which has a slim identity mapping pat 5]</ref> leads to better accuracy for deep super-resolution networks. Previous works including EDSR <ref type="bibr" target="#b18">[19]</ref>, BTSRN <ref type="bibr" target="#b6">[7]</ref> and RDN <re e for training SR networks. However, with the increasing depth of neural networks for SR (e.g. MDSR <ref type="bibr" target="#b18">[19]</ref> has depth around 180), the networks without batch normaliz ing deeper and deeper (from 3-layer SRCNN <ref type="bibr" target="#b3">[4]</ref> to 160-layer MDSR <ref type="bibr" target="#b18">[19]</ref>), training becomes more difficult. Batch normalization lay f type="figure">1</ref>. Two-layer residual blocks are specifically studied following baseline EDSR <ref type="bibr" target="#b18">[19]</ref>. Assume the width of identity mapping pathway (Fig. <ref t <p>Figure <ref type="figure">2</ref>: Demonstration of our simplified SR network compared with EDSR <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this part, we overview the WDSR network architec his part, we overview the WDSR network architectures. We made two major modifications based on EDSR <ref type="bibr" target="#b18">[19]</ref> super-resolution network.</p><p>Global residual pathway Fi set <ref type="bibr" target="#b34">[35]</ref>  In this part, we show results of baseline model EDSR <ref type="bibr" target="#b18">[19]</ref> and our proposed WDSR-A and WDSR-B for the task of image b e results suggest that our proposed WDSR-A and WDSR-B have better accuracy and efficiency than EDSR <ref type="bibr" target="#b18">[19]</ref>. WDSR-B with wider activation also has better or similar p curacy drop with our simpler form.</p><p>Upsampling layer Different from previous state-of-the-arts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref> where one or more lips and rotations following common data augmentation methods<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. During training, the input images are also subtracted with
resolution networks. Previous works including EDSR <ref type="bibr" target="#b18">[19]</ref>, BTSRN <ref type="bibr" target="#b6">[7]</ref> and RDN <ref type="bibr" target="#b41">[42]</ref> found that br" target="#b30">31]</ref>. It is also experimentally proved in single image super-resolution task <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ "#b11">[12]</ref> hinders the accuracy of image super-resolution. Thus, in recent image SR networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" targ small image patches (e.g. 48 × 48) and small mini-batch size (e.g. 16) are used to speedup training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ ny kinds of regularizers, for examples, weight decaying and dropout, are not adopted in SR networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ a is augmented with random horizontal flips and rotations following common data augmentation methods<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. During training, th
target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref> but meanwhile under-use the feature information from shallo
networks (CNNs) have been successfully applied to the task of single image super-resolution (SISR) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta om 3 to 5). They are inferior in accuracy compared with later proposed deep SR networks (e.g., VDSR <ref type="bibr" target="#b13">[14]</ref>, SRResNet <ref type="bibr" target="#b16">[17]</ref> and ED uper-resolution task <ref type="bibr" target="#b41">[42]</ref>. To address this contradictory, VDSR <ref type="bibr" target="#b13">[14]</ref> proposed a very deep VGG-like <ref type="bibr" target="#b2 experimentally proved in single image super-resolution task <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar small mini-batch size (e.g. 16) are used to speedup training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar weight decaying and dropout, are not adopted in SR networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
et="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42]</ref>. SISR aims at recovery of a high resolution (HR) image from get="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>. These very deep networks (usually more than 10 layers) sta nt image SR networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42]</ref>, batch normalization is abandoned.</p></div> <div xmlns="ht get="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>, thus the mean and variance of small image patches differ a get="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>. 3) Unlike image classification tasks where softmax (scale- Upsampling layer Different from previous state-of-the-arts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref> where one or more convolutional layers are inserted after u To address this issue, methods including SRDenseNet <ref type="bibr" target="#b35">[36]</ref>, RDN <ref type="bibr" target="#b41">[42]</ref>, MemNet <ref type="bibr" target="#b32">[33]</ref> introduc SR <ref type="bibr" target="#b18">[19]</ref>, BTSRN <ref type="bibr" target="#b6">[7]</ref> and RDN <ref type="bibr" target="#b41">[42]</ref> found that batch normalization <ref type="bibr" target="#b [30]</ref>, on the other hand low-level features are also important for image super-resolution task <ref type="bibr" target="#b41">[42]</ref>. To address this contradictory, VDSR <ref type="bibr" targ exploit the hierarchical features from all the convolutional layers, residual dense networks (RDN) <ref type="bibr" target="#b41">[42]</ref> are proposed. All these works benefit from additional skip
uper-resolution significantly outperform conventional ones <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref> in terms of peak signal-to-noise ratio (PSNR) and structura
RCNN <ref type="bibr" target="#b3">[4]</ref>, FSRCNN <ref type="bibr" target="#b2">[3]</ref>, ESPCN <ref type="bibr" target="#b28">[29]</ref> utilized relatively shallow convolutional neural networks layers compute on LR feature space. Another non-parametric efficient alternative is pixel shuffling <ref type="bibr" target="#b28">[29]</ref> (a.k.a., sub-pixel convolution). Pixel shuffling is also b × 3) convolutions and have higher accuracy than shallow ones <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>. However, the increasing depth of convolutional neural netw
et="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42]</ref>. SISR aims at recovery of a high resolution (HR) image from get="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>. These very deep networks (usually more than 10 layers) sta nt image SR networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42]</ref>, batch normalization is abandoned.</p></div> <div xmlns="ht get="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>, thus the mean and variance of small image patches differ a get="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>. 3) Unlike image classification tasks where softmax (scale- Upsampling layer Different from previous state-of-the-arts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref> where one or more convolutional layers are inserted after u To address this issue, methods including SRDenseNet <ref type="bibr" target="#b35">[36]</ref>, RDN <ref type="bibr" target="#b41">[42]</ref>, MemNet <ref type="bibr" target="#b32">[33]</ref> introduc SR <ref type="bibr" target="#b18">[19]</ref>, BTSRN <ref type="bibr" target="#b6">[7]</ref> and RDN <ref type="bibr" target="#b41">[42]</ref> found that batch normalization <ref type="bibr" target="#b [30]</ref>, on the other hand low-level features are also important for image super-resolution task <ref type="bibr" target="#b41">[42]</ref>. To address this contradictory, VDSR <ref type="bibr" targ exploit the hierarchical features from all the convolutional layers, residual dense networks (RDN) <ref type="bibr" target="#b41">[42]</ref> are proposed. All these works benefit from additional skip
uper-resolution significantly outperform conventional ones <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref> in terms of peak signal-to-noise ratio (PSNR) and structura
ve neural networks</head><p>The depth of neural networks is of central importance for deep learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" targ omes more difficult. Batch normalization layers are one of the cures for this problem in many tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>. It is also introduc skip connection) for SISR. SRResNet <ref type="bibr" target="#b16">[17]</ref> proposed a ResNetlike <ref type="bibr" target="#b8">[9]</ref> network. Densely connected networks <ref type="bibr" target= ions.</p><p>1 × 1 convolutions are widely used for channel number expansion or reduction in ResNets <ref type="bibr" target="#b8">[9]</ref>, ResNeXts <ref type="bibr" target="#b37">[38]</ref> and Mobi
-resolution (SISR) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42]</ref>. SISR aims at recov
ng pathway, and explore more efficient ways to expand features. We first consider group convolution <ref type="bibr" target="#b37">[38]</ref> and depthwise separable convolution <ref type="bibr" targe of input planes, X and Y denote filter width and height.</p><p>Group convolution Group convolutions <ref type="bibr" target="#b37">[38]</ref> divide features into groups channel-wisely and perform con the group number. Group convolutions are the key components to many efficient models (e.g. ResNeXt <ref type="bibr" target="#b37">[38]</ref>).</p><p>Depthwise separable convolution Depthwise separabl channel number expansion or reduction in ResNets <ref type="bibr" target="#b8">[9]</ref>, ResNeXts <ref type="bibr" target="#b37">[38]</ref> and MobileNetV2 <ref type="bibr" target="#b26">[27]</ref>.
uper-resolution significantly outperform conventional ones <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref> in terms of peak signal-to-noise ratio (PSNR) and structura
ous image super-resolution networks including SRCNN <ref type="bibr" target="#b3">[4]</ref>, FSRCNN <ref type="bibr" target="#b2">[3]</ref>, ESPCN <ref type="bibr" target="#b28">[29]</ref> utilized re e, where S is the upscaling factor. To accelerate processing speed without loss of accuracy, FSRCNN <ref type="bibr" target="#b2">[3]</ref> utilized parametric deconvolution layer at the end of SR net type="bibr" target="#b2">[3]</ref> utilized parametric deconvolution layer at the end of SR network <ref type="bibr" target="#b2">[3]</ref>, making all convolution layers compute on LR feature space. yers) stack many small-kernel (i.e., 3 × 3) convolutions and have higher accuracy than shallow ones <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>. However, the increa
t-in module for other image restoration or recognition tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" tar
ng pathway, and explore more efficient ways to expand features. We first consider group convolution <ref type="bibr" target="#b37">[38]</ref> and depthwise separable convolution <ref type="bibr" targe of input planes, X and Y denote filter width and height.</p><p>Group convolution Group convolutions <ref type="bibr" target="#b37">[38]</ref> divide features into groups channel-wisely and perform con the group number. Group convolutions are the key components to many efficient models (e.g. ResNeXt <ref type="bibr" target="#b37">[38]</ref>).</p><p>Depthwise separable convolution Depthwise separabl channel number expansion or reduction in ResNets <ref type="bibr" target="#b8">[9]</ref>, ResNeXts <ref type="bibr" target="#b37">[38]</ref> and MobileNetV2 <ref type="bibr" target="#b26">[27]</ref>.
ver-parameterization and difficulty of training. To address these issues, recursive neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref> are proposed by re
networks (CNNs) have been successfully applied to the task of single image super-resolution (SISR) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta om 3 to 5). They are inferior in accuracy compared with later proposed deep SR networks (e.g., VDSR <ref type="bibr" target="#b13">[14]</ref>, SRResNet <ref type="bibr" target="#b16">[17]</ref> and ED uper-resolution task <ref type="bibr" target="#b41">[42]</ref>. To address this contradictory, VDSR <ref type="bibr" target="#b13">[14]</ref> proposed a very deep VGG-like <ref type="bibr" target="#b2 experimentally proved in single image super-resolution task <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar small mini-batch size (e.g. 16) are used to speedup training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar weight decaying and dropout, are not adopted in SR networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
networks (CNNs) have been successfully applied to the task of single image super-resolution (SISR) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta om 3 to 5). They are inferior in accuracy compared with later proposed deep SR networks (e.g., VDSR <ref type="bibr" target="#b13">[14]</ref>, SRResNet <ref type="bibr" target="#b16">[17]</ref> and ED uper-resolution task <ref type="bibr" target="#b41">[42]</ref>. To address this contradictory, VDSR <ref type="bibr" target="#b13">[14]</ref> proposed a very deep VGG-like <ref type="bibr" target="#b2 experimentally proved in single image super-resolution task <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar small mini-batch size (e.g. 16) are used to speedup training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar weight decaying and dropout, are not adopted in SR networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
learning-based methods for single image super-resolution significantly outperform conventional ones <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref> in terms of peak s
learning-based methods for single image super-resolution significantly outperform conventional ones <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref> in terms of peak s
es before ReLU, since simply adding more parameters is inefficient for real-time image SR scenarios <ref type="bibr" target="#b7">[8]</ref>. We first introduce SR residual network WDSR-A, which has a
ency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target
="#b11">[12]</ref>, Li and Chu <ref type="bibr" target="#b15">[16]</ref>, and Govindarajan, et al., <ref type="bibr" target="#b9">[10]</ref> all discuss architectures that feature simultaneous multith
ments to processor utilization -long latencies and limited per-thread parallelism. Tullsen, et al., <ref type="bibr" target="#b26">[27]</ref> showed the potential of Proceedings of the 23rd Annual Int head><p>The methodology in this paper closely follows the simulation and measurement methodology of <ref type="bibr" target="#b26">[27]</ref>. Our simulator uses emulationbased, instruction-level simu compiler <ref type="bibr" target="#b16">[17]</ref>, modified to produce Alpha code. In contrast to <ref type="bibr" target="#b26">[27]</ref>, we turn off trace scheduling in the compiler for this stu sor utilization, at less than 50% of the 8-issue processor, is well short of the potential shown in <ref type="bibr" target="#b26">[27]</ref>.</p><p>We make several conclusions about the potential bot shion here) that simultaneous multithreading uses to improve the throughput of the functional units <ref type="bibr" target="#b26">[27]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head tectures have been proposed that exhibit simultaneous multithreading in some form. Tullsen, et al., <ref type="bibr" target="#b26">[27]</ref> demonstrated the potential for simultaneous multithreading
f fourinstruction blocks, each block holding instructions from a single thread.</p><p>The M-Machine <ref type="bibr" target="#b8">[9]</ref> and the Multiscalar project <ref type="bibr" target="#b24">[
terleave operations from VLIW instructions onto individual functional units.</p><p>Daddis and Torng <ref type="bibr" target="#b5">[6]</ref> plot increases in instruction throughput as a function of th
ect caches and a homogeneous workload (all threads running the same trace). Yamamoto and Nemirovsky <ref type="bibr" target="#b27">[28]</ref> simulate an SMT architecture with separate instruction que
r simulator uses emulationbased, instruction-level simulation, and borrows significantly from MIPSI <ref type="bibr" target="#b21">[22]</ref>, a MIPS-based simulator. The simulator executes unmodified
le" target="#tab_1">1</ref> shows the instruction latencies, which are derived from the Alpha 21164 <ref type="bibr" target="#b7">[8]</ref>.</p><p>We assume a 32-entry integer instruction queue (which
="#b11">[12]</ref>, Li and Chu <ref type="bibr" target="#b15">[16]</ref>, and Govindarajan, et al., <ref type="bibr" target="#b9">[10]</ref> all discuss architectures that feature simultaneous multith
ation of the benchmarks.</p><p>We compile each program with the Multiflow trace scheduling compiler <ref type="bibr" target="#b16">[17]</ref>, modified to produce Alpha code. In contrast to <ref type=
register file.</p><p>Keckler and Dally <ref type="bibr" target="#b13">[14]</ref> and Prasadh and Wu <ref type="bibr" target="#b18">[19]</ref> describe architectures that dynamically interleave operati
they have recently been shown to be particularly unstable to adversarial perturbations of the data <ref type="bibr" target="#b17">[18]</ref>. In fact, very small and often imperceptible perturbations e classifier, it seems that the adversarial perturbations are generalizable across different models <ref type="bibr" target="#b17">[18]</ref>. This can actually become a real concern from a security p of the relevant work. The phenomenon of adversarial instability was first introduced and studied in <ref type="bibr" target="#b17">[18]</ref>. The authors estimated adversarial examples by solving pen explaining the presence of adversarial examples. Unfortunately, the optimization method employed in <ref type="bibr" target="#b17">[18]</ref> is time-consuming and therefore does not scale to large da he training procedure that allows to boost the robustness of the classifier. Notably, the method in <ref type="bibr" target="#b17">[18]</ref> was applied in order to generate adversarial perturbations he proposed DeepFool approach to stateof-the-art techniques to compute adversarial perturbations in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b3">[4]</ref>. The method in ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b3">[4]</ref>. The method in <ref type="bibr" target="#b17">[18]</ref> solves a series of penalized optimization problems to find ver that the proposed approach also yields slightly smaller perturbation vectors than the method in <ref type="bibr" target="#b17">[18]</ref>. The proposed approach is hence more accurate in detecting mplexity aspect, the proposed approach is substantially faster than the standard method proposed in <ref type="bibr" target="#b17">[18]</ref>. In fact, while the approach <ref type="bibr" target="#b17 standard method proposed in <ref type="bibr" target="#b17">[18]</ref>. In fact, while the approach <ref type="bibr" target="#b17">[18]</ref> involves a costly minimization of a series of objective fu
two-layer fully connected network, and a two-layer LeNet convoluational neural network architecture <ref type="bibr" target="#b8">[9]</ref>. Both networks are trained with SGD with momentum using the


>[1,</ref><ref type="bibr" target="#b15">16]</ref>, speech <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>, and computer vision <ref type="bibr" target="#b9">[10,</ref>
bibr" target="#b17">[18]</ref> is time-consuming and therefore does not scale to large datasets. In <ref type="bibr" target="#b13">[14]</ref>, the authors showed that convolutional networks are not in
enerated synthetic unrecognizable images, which are classified with high confidence. The authors of <ref type="bibr" target="#b2">[3]</ref> also studied a related problem of finding the minimal geomet
(NIN) architecture <ref type="bibr" target="#b10">[11]</ref>.</p><p>• ILSVRC 2012: We used CaffeNet <ref type="bibr" target="#b6">[7]</ref> and GoogLeNet <ref type="bibr" target="#b16">[17]</ref> pre-
on performance in many research areas such as bioinformatics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, speech <ref type="bibr" target="#b11">[12,</ref><ref type=
>[1,</ref><ref type="bibr" target="#b15">16]</ref>, speech <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>, and computer vision <ref type="bibr" target="#b9">[10,</ref>
informatics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, speech <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>, and computer vision
get="#foot_0">3</ref>  <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref>. They analyzed the relationship between internal properties bibr" target="#b4">[5]</ref>, narcissism <ref type="bibr" target="#b6">[7]</ref>, self-presentation <ref type="bibr" target="#b12">[13]</ref> and the corresponding user profiles on Facebook.</p><p>We
ucted for Facebook<ref type="foot" target="#foot_0">3</ref>  <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref>. They analyzed the re n internal properties such as self-construction <ref type="bibr" target="#b4">[5]</ref>, narcissism <ref type="bibr" target="#b6">[7]</ref>, self-presentation <ref type="bibr" target="#b12">[13]</ref>
ucted for Facebook<ref type="foot" target="#foot_0">3</ref>  <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref>. They analyzed the re n internal properties such as self-construction <ref type="bibr" target="#b4">[5]</ref>, narcissism <ref type="bibr" target="#b6">[7]</ref>, self-presentation <ref type="bibr" target="#b12">[13]</ref>
e real world but represent information sharing relationships <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="
]</ref>, or based on the name, location, and sex of the user <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. The reason behind using the aforementioned features for user
]</ref>, or based on the name, location, and sex of the user <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. The reason behind using the aforementioned features for user
ot represent social relationships in the real world but represent information sharing relationships <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div> <div xmlns=
terest in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, which are known as user profiling studies.</p><p>When users A based user recommendation by considering hashtags(#), replies (@), and emoticons in tweet content <ref type="bibr" target="#b9">[10]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n
</ref>) friends (a link between friends), (3) information seeker (a link from a user to a hub user) <ref type="bibr" target="#b5">[6]</ref>. Cha et al. and Kwak et al. investigated the large-scale inf
]</ref>, or based on the name, location, and sex of the user <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. The reason behind using the aforementioned features for user
has attracted considerable research interest in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, which are known as us heir introductory information or the content of their tweets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, or based on the name, location, and sex of the user <ref typ
ct and have high intersubject variability. To learn personalized models for each student, we follow <ref type="bibr" target="#b3">(Jaques et al., 2017)</ref> and use a Multitask approach which compris
ariability in behavioural and environmental patterns have stymied predictive modeling of this kind. <ref type="bibr" target="#b8">(Mikelsons et al., 2018)</ref> have tried to predict stress of student tp://www.tei-c.org/ns/1.0"><head n="3.2.1.">LOCATION FEATURE BASED MLP</head><p>In the work done by <ref type="bibr" target="#b8">(Mikelsons et al., 2018)</ref>, a Multilayer Perceptron (MLP) with 4 f ><head n="4.">Result</head><p>Due to a heavy imbalance of class labels on a scale of 1-5, we follow <ref type="bibr" target="#b8">(Mikelsons et al., 2018)</ref>, converting the five stress label scale ses of 23 students, totaling to 1183 data points achieving roughly equal amount of training data in <ref type="bibr" target="#b8">(Mikelsons et al., 2018)</ref>. These 1183 data points have the follow
hnologies for detecting stress has been accomplished. Few use heart rate and heart rate variability <ref type="bibr" target="#b17">(Vrijkotte et al., 2000)</ref>, cortisol levels <ref type="bibr" targ
however people have used Auto-encoders for compression and reduction of the temporal dimensions. In <ref type="bibr" target="#b7">(Lngkvist et al., 2014)</ref>  </p></div> <div xmlns="http://www.tei-c

d by shared fully connected layers and a MLP for each student. A similar approach was also taken in <ref type="bibr" target="#b4">(Kandemir et al., 2014)</ref> for the prediction of affect (mood) by l


hnologies for detecting stress has been accomplished. Few use heart rate and heart rate variability <ref type="bibr" target="#b17">(Vrijkotte et al., 2000)</ref>, cortisol levels <ref type="bibr" targ
sol levels <ref type="bibr" target="#b1">(Dickerson &amp; Kemenyr, 2004</ref>) and skin conductance <ref type="bibr" target="#b13">(Setz et al., 2010)</ref>. Other techniques do not depend on sensors
k. A prolonged exposure to stressful academic and social environment causes cardiovascular diseases <ref type="bibr" target="#b10">(Rozanski et al., 1999;</ref><ref type="bibr" target="#b5">Kario et a
however people have used Auto-encoders for compression and reduction of the temporal dimensions. In <ref type="bibr" target="#b7">(Lngkvist et al., 2014)</ref>  </p></div> <div xmlns="http://www.tei-c
however people have used Auto-encoders for compression and reduction of the temporal dimensions. In <ref type="bibr" target="#b7">(Lngkvist et al., 2014)</ref>  </p></div> <div xmlns="http://www.tei-c
cognition <ref type="bibr" target="#b14">(SJ et al., 2009)</ref>, suppression of the immune system <ref type="bibr" target="#b6">(Khansari et al., 1990)</ref>, and poor academic performance <ref type





ironment causes cardiovascular diseases <ref type="bibr" target="#b10">(Rozanski et al., 1999;</ref><ref type="bibr" target="#b5">Kario et al., 2003)</ref>, alterations of the brain causing difference
ef type="bibr" target="#b9">(Rahman et al., 2014)</ref> and surveys like the Perceived Stress Scale <ref type="bibr" target="#b0">(Cohen et al., 1983)</ref>.</p><p>With the induction of high quality,
d by shared fully connected layers and a MLP for each student. A similar approach was also taken in <ref type="bibr" target="#b4">(Kandemir et al., 2014)</ref> for the prediction of affect (mood) by l
generally performed at two levels: feature level or early fusion and decision level or late fusion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" targ inear kernel function.</p><p>Many existing literature use the SVM-based fusion scheme. Adams et al. <ref type="bibr" target="#b2">[3]</ref> adopted a late fusion approach in order to detect semantic c the NLF method is used for nonlinear combination of multimodal information. This method is based on <ref type="bibr" target="#b2">[3]</ref>, where SVM is first used as a classifier for the individual f> SVM based score space classification of combined information from multiple intermediate concepts <ref type="bibr" target="#b2">[3]</ref> The key idea was to utilize the synchrony measure between th their correlation over time. This approach is used for speech recognition. The work by Adams et al. <ref type="bibr" target="#b2">[3]</ref> also used a Bayesian network in addition to SVM and showed t ibr" target="#b24">[25]</ref>, Xie et al. <ref type="bibr" target="#b145">[145]</ref>, Adams et al. <ref type="bibr" target="#b2">[3]</ref> Photo and video annotation Linear weighted fusion Iyenger et fusion Iyenger et al. <ref type="bibr" target="#b57">[58]</ref> Support vector machine Adams et al. <ref type="bibr" target="#b2">[3]</ref>, Iyenger et al. <ref type="bibr" target="#b57">[58]</ref>, W
e of correlation, confidence, and the context), also related to how to fuse, are described in Sect. <ref type="bibr" target="#b3">4</ref>. This section further elaborates the issues when to fuse (the hich provided the decision about the identity of the talking face. On another front, Aguilar et al. <ref type="bibr" target="#b3">[4]</ref> provided a comparison between the rule-based fusion and lear
ech recognition <ref type="bibr" target="#b105">[106]</ref>; biometric audiovisual speech synchrony <ref type="bibr" target="#b19">[20]</ref>; multi-sensor management for information fusion <ref type= le and widely used forms of the correlation coefficient is the Pearson's product-moment coefficient <ref type="bibr" target="#b19">[20]</ref>, which is computed as follows. Assuming that I i and I j a mprovement in error rate with CCA and 61% improvement with LSA. In another work, Bredin and Chollet <ref type="bibr" target="#b19">[20]</ref> also demonstrated the utility of considering CCA for audio
ct of the analysis task, such as multimodal video indexing <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b120">120]</ref>; automatic audio-visual speech recognition <ref type="bib
t al. <ref type="bibr" target="#b101">[102]</ref> for audio-visual speech recognition. Meyer et al. <ref type="bibr" target="#b84">[85]</ref> and Xu and Chua <ref type="bibr" target="#b149">[149]</ref o proposed to model the measurement of noise uncertainty.</p><p>At the decision level, Meyer et al. <ref type="bibr" target="#b84">[85]</ref> fused the decisions obtained from speech and visual modali
e Bayes approach.</p><p>There are other works such as Jeon and Manmatha [63] and Argillander et al. <ref type="bibr" target="#b6">[7]</ref>, which have used maximum entropy model for multimedia analys
n contrast to the above methods that used the static confidence, some efforts (e.g. Tavakoli et al. <ref type="bibr" target="#b127">[127]</ref>, Atrey et al. <ref type="bibr" target="#b9">[10]</ref> h d towards the dynamic computation of the confidence levels of different modalities. Tavakoli et al. <ref type="bibr" target="#b127">[127]</ref> have used spatial and temporal information in clusters i
is happy, a smart mirror should select and play a romantic song when s/he enters into a smart house <ref type="bibr" target="#b49">[50]</ref>.</p><p>The contextual information can be determined by exp
eature level for performing various multimedia analysis tasks. Examples include Foresti and Snidaro <ref type="bibr" target="#b39">[40]</ref>, Yang et al. <ref type="bibr" target="#b152">[152]</ref> f person identification. We briefly describe these works in the following.</p><p>Foresti and Snidaro <ref type="bibr" target="#b39">[40]</ref> used a linear weighted sum method to fuse trajectory infor ifferent sensors; however, the determination of these weights has been left to the user. Similar to <ref type="bibr" target="#b39">[40]</ref>, Yang et al. <ref type="bibr" target="#b152">[152]</ref> a ear weighted fusion of the location information of the objects. However, unlike Foresti and Snidaro <ref type="bibr" target="#b39">[40]</ref>, Yang et al. <ref type="bibr" target="#b152">[152]</ref> a "#b66">67,</ref><ref type="bibr" target="#b136">136]</ref>, and have left it to the users to decide <ref type="bibr" target="#b39">[40]</ref>.</p><p>Other works, such as Neti et al. <ref type="bibr" t
"#b153">[153]</ref>; multimodal human computer interaction <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b96">97]</ref>; audio-visual biometric <ref type="bibr" target="#b4">[5]</
t al. <ref type="bibr" target="#b101">[102]</ref> for audio-visual speech recognition. Meyer et al. <ref type="bibr" target="#b84">[85]</ref> and Xu and Chua <ref type="bibr" target="#b149">[149]</ref o proposed to model the measurement of noise uncertainty.</p><p>At the decision level, Meyer et al. <ref type="bibr" target="#b84">[85]</ref> fused the decisions obtained from speech and visual modali

een addressed by performing the multimedia analysis tasks (such as event detection) over a timeline <ref type="bibr" target="#b28">[29]</ref>. A timeline refers to a measurable span of time with infor
the nodes. Second, by using them, the temporal dynamics of multimodal data can easily be integrated <ref type="bibr" target="#b119">[119]</ref>. These advantages make them suitable for various multime
each a similar decision <ref type="bibr" target="#b113">[113]</ref>. For example, Radova and Psutka <ref type="bibr" target="#b107">[108]</ref> have presented a speaker identification system by employ
trate the use of the NN fusion method for performing the multimedia analysis tasks. Gandetto et al. <ref type="bibr" target="#b40">[41]</ref> have used the NN fusion method to combine sensory data for >[131]</ref>, Beal et al. <ref type="bibr" target="#b14">[15]</ref> Neural networks Gandetto et al. <ref type="bibr" target="#b40">[41]</ref>, Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref>
the fusion incorporating their correlation. A similar approach was also presented by Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref> for tracking humans in a cluttered environment. Recently r" target="#b33">[34]</ref>, Ni et al. <ref type="bibr" target="#b87">[88]</ref>, and Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref> for speaker tracking. Cutler and Davis <ref type="bibr" r locating the speaking person in the scene. A similar approach was also presented by Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref>. In <ref type="bibr" target="#b158">[158]</ref>, the aut imilar approach was also presented by Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref>. In <ref type="bibr" target="#b158">[158]</ref>, the authors have also compared the TDNN approach with t decision.</p><p>While Cutler and Davis <ref type="bibr" target="#b33">[34]</ref> and Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref> used the NN fusion approach at the feature level, Ni et [15]</ref> Neural networks Gandetto et al. <ref type="bibr" target="#b40">[41]</ref>, Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref> Kalman filter Talantzis et al. <ref type="bibr" target="
ed in other domains such as machine learning <ref type="bibr" target="#b47">[48]</ref>, data mining <ref type="bibr" target="#b23">[24]</ref> and information retrieval <ref type="bibr" target="#b133">
rget="#b41">[42]</ref> that contains face, speech, fingerprint, hand and signature modalities; MYCT <ref type="bibr" target="#b92">[93]</ref> that contains 10-print fingerprint and signature modalitie
><p>The particle methods have been widely used in multimedia analysis. For instance, Vermaak et al. <ref type="bibr" target="#b132">[132]</ref> used particle filters to estimate the predictions from a to fuse 2D object shapes and audio information for speaker tracking. However, unlike Vermaak et al. <ref type="bibr" target="#b132">[132]</ref>, the latter uses the concept of importance particle filt m parameters and the tracking position in the audio-visual state-space model. Unlike Vermaak et al. <ref type="bibr" target="#b132">[132]</ref> or Perez et al. <ref type="bibr" target="#b98">[99]</ref
6]</ref>, relevance vector machines <ref type="bibr" target="#b35">[36]</ref>, logistics regression <ref type="bibr" target="#b70">[71]</ref> and boosting <ref type="bibr" target="#b74">[75]</ref>. Ho
searchers have successfully adopted the decision level fusion strategy. For example, Iyenger et al. <ref type="bibr" target="#b56">[57]</ref> performed fusion of decisions obtained from a face detecto ="bibr" target="#b86">[87]</ref> for speaker recognition and speech event detection, Iyengar et al. <ref type="bibr" target="#b56">[57]</ref> for monologue detection, Iyengar et al. <ref type="bibr" t modalities and accordingly adjusted their weights. Similar to this fusion approach, Iyengar et al. <ref type="bibr" target="#b56">[57]</ref> fused multiple modalities (face, speech and the synchrony ion and annotation in video by Iyengar et al. <ref type="bibr" target="#b57">[58]</ref>. Similar to <ref type="bibr" target="#b56">[57]</ref>, the linear weighted product fusion strategy has also been mutual information as a measure of synchrony between audio and video. For instance, Iyengar et al. <ref type="bibr" target="#b56">[57]</ref> computed the synchrony between face and speech using mutua >.</p><p>Other works, such as Neti et al. <ref type="bibr" target="#b86">[87]</ref>, Iyenger et al. <ref type="bibr" target="#b56">[57]</ref>, Tatbul et al. <ref type="bibr" target="#b126">[126]</ref> et="#b46">[47]</ref>, Slaney and Covell <ref type="bibr" target="#b117">[117]</ref>, Iyengar et al. <ref type="bibr" target="#b56">[57]</ref>, Nock et al. <ref type="bibr" target="#b90">[91]</ref> and
2">[3,</ref><ref type="bibr" target="#b1">2]</ref> and FPGAs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Activation unit is a non-linear function that some l
e required memory bandwidth.</p><p>Other domain-specific instructions set for CNN were presented in <ref type="bibr" target="#b14">[15]</ref>, can be added into Snowball, because they also use vector
timizations.</p><p>Memory transfer friendly computation tiling for CNN accelerators was explored in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. In <ref type="bib
s and processing units. Custom hardware and instruction generation software was developed for Caffe <ref type="bibr" target="#b10">[11]</ref> in <ref type="bibr" target="#b24">[25,</ref><ref type="bib
s and processing units. Custom hardware and instruction generation software was developed for Caffe <ref type="bibr" target="#b10">[11]</ref> in <ref type="bibr" target="#b24">[25,</ref><ref type="bib
f type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref> on large image datasets <ref type="bibr" target="#b19">[20]</ref>. Those models are embarrassingly parallel, but exploiting r representations can be used in the system. Pretrained ResNet18 2 was profiled on ImageNet dataset <ref type="bibr" target="#b19">[20]</ref> using Q8.8 and Q5.11 fixed point precisions. Top-5 accurac
ry. We designed a generic software structure to go from high level model representation from Torch7 <ref type="bibr" target="#b3">[4]</ref> down to an instruction stream that runs Snowflake. The main ref>. Snowball is the first to generate to custom instructions for hardware accelerator from Torch7 <ref type="bibr" target="#b3">[4]</ref> or Pytorch models.</p><p>The system in <ref type="bibr" targ
insignificant accuracy degradation compared to neural networks implemented in 32 bit floating point <ref type="bibr" target="#b9">[10]</ref>. Nevertheless, other number representations can be used in
representation and the techniques presented can be also be integrated into conventional frameworks <ref type="bibr" target="#b13">[14]</ref>, which is left for future work.</p></div> <div xmlns="http
y bandwidth. While a  GPU's optimized memory interconnects can achieve a high bandwidth of 112 GB/s <ref type="bibr" target="#b15">[16]</ref>, Xilinx ZC706 board <ref type="bibr" target="#b23">[24]</r
representation and the techniques presented can be also be integrated into conventional frameworks <ref type="bibr" target="#b13">[14]</ref>, which is left for future work.</p></div> <div xmlns="http
unknown true labels and some behavior assumptions, with examples of the Dawid-Skene (DS) estimator <ref type="bibr" target="#b4">[5]</ref>, the minimax entropy (Entropy) estimator<ref type="foot" tar ://www.tei-c.org/ns/1.0"><head n="2.2">Dawid-Skene Estimator</head><p>The method of Dawid and Skene <ref type="bibr" target="#b4">[5]</ref> is a generative approach by considering worker confusability ed majority voting (IWMV) <ref type="bibr" target="#b10">[11]</ref>, the Dawid-Skene (DS) estimator <ref type="bibr" target="#b4">[5]</ref>, and the minimax entropy (Entropy) estimator <ref type="bibr m c = 2ˆ[−8 : 0] and = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> by the method in Sec. 6.2. As for Gibbs-CrowdSVM, we generate
ing the workload into small parts, then distributing micro-tasks to a crowd of ordinary web workers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>. However, the labe
ues (i.e., 0 or ∞). We investigate two choices on defining the max-margin posterior regularization: <ref type="bibr" target="#b0">(1)</ref> an averaging model with a variational inference algorithm; a dSVM and Gibbs-CrowdSVM, the regularization parameters (c, ) are selected from c = 2ˆ[−8 : 0] and = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target
11">[12]</ref>. Recently, spectral methods have also been applied to better initialize the DS model <ref type="bibr" target="#b22">[23]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
stimator</head><p>CrowdSVM adopts an averaging model to define the posterior constraints in problem <ref type="bibr" target="#b8">(9)</ref>. Here, we further provide an alternative strategy which lead eresting to investigate more on active learning, such as selecting reliable workers to reduce costs <ref type="bibr" target="#b8">[9]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id
11">[12]</ref>. Recently, spectral methods have also been applied to better initialize the DS model <ref type="bibr" target="#b22">[23]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
eness of our methods.</p><p>Our model is flexible to fit specific complicated application scenarios <ref type="bibr" target="#b21">[22]</ref>. One seminal feature of Bayesian methods is their sequenti
r" target="#b3">[4]</ref>. So it can be efficiently solved by welldeveloped SVM solvers like LIBSVM <ref type="bibr" target="#b1">[2]</ref>. For updating y, we define (x) + := max(0, x), and then it i
e majority voting and the weighted majority voting that takes worker reliability into consideration <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>In this pape eighted majority voting (WMV) by putting different weights on workers to measure worker reliability <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div> <div xml e it is intractable to directly solve problem ( <ref type="formula" target="#formula_9">9</ref>) or <ref type="bibr" target="#b9">(10)</ref>, we introduce the structured mean-field assumption on the p <p>Infer y: Fixing the distributions of Φ and η at their optimum q * , we find y by solving problem <ref type="bibr" target="#b9">(10)</ref>. To make the prediction more efficient, we approximate the
istributing micro-tasks to a crowd of ordinary web workers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>. However, the labeling accuracy of web workers could be low
eness of our methods.</p><p>Our model is flexible to fit specific complicated application scenarios <ref type="bibr" target="#b21">[22]</ref>. One seminal feature of Bayesian methods is their sequenti
cles respectively, which are determined to be a good set up experimentally. In fact, Jim?nez et al. <ref type="bibr" target="#b2">[3]</ref> shows a 50:1 of "execution epoch vs sampling period" is a re ">[31]</ref> studies the effect of hardware prefetching in virtualized environments. Jimenez et al. <ref type="bibr" target="#b2">[3]</ref> proposed an adaptive prefetch control to independently adjus
tching on will probably hurt other programs' performance due to interference. Prior work Liu et al. <ref type="bibr" target="#b24">[25]</ref> (Paragraph 5 in the introduction) shows that maximum weigh control described in this paper can be applied to a variety of prefetchers. In addition, Liu et al. <ref type="bibr" target="#b24">[25]</ref> studies the interaction of prefetching and BW partitioning
ements on real machines provide different observations than past simulation-based work. Wang et al. <ref type="bibr" target="#b46">[47]</ref> proposes a combined cache partition method (SWAP) to take
arks.</p><p>Fig. <ref type="figure">2</ref> shows the IPC speedup from prefetching for SPEC CPU2006 <ref type="bibr" target="#b20">[21]</ref> benchmarks. Several benchmarks, such as 462.libquantum, 41 /ref> and related detection algorithm can be easily explored and updated to achieve better results. <ref type="bibr" target="#b20">[21]</ref> and an micro-benchmark (see below) are used to create mult




ems with a low associativity LLC <ref type="bibr" target="#b18">[19]</ref>. Prior work Selfa et al. <ref type="bibr" target="#b16">[17]</ref> used CAT to improve fairness of multi-programmed workloads ve and friendly, need no more than 2 ways to achieve 90% of their highest performance. Selfa et al. <ref type="bibr" target="#b16">[17]</ref> had a similar observation. This observation-that applicati ad>2) Cache Partitioning (CP):</head><p>There are several proposed CP implementations. Selfa et al. <ref type="bibr" target="#b16">[17]</ref> groups cores into several clusters and each cluster occupi ace each subset into a separate partition. They are compared with the best known algorithm, Dunn in <ref type="bibr" target="#b16">[17]</ref>, in Sec. V-B. Note that CP just needs the first two sampli ningless to throttle the prefetchers. In this case, CMM will use the "Dunn" algorithm in prior work <ref type="bibr" target="#b16">[17]</ref> for cache partitioning. This paper evaluates the first thr t shows Pref-CP and Pref-CP2 significantly improve the system performance over Dunn in Selfa et al. <ref type="bibr" target="#b16">[17]</ref>. Dunn uses the PMU event "STALLS L2 PENDING" to cluster co s stalled due to L2 cache misses <ref type="bibr" target="#b14">[15]</ref>. Prior work Selfa et al. <ref type="bibr" target="#b16">[17]</ref> shows there is a strong positive correlation between its c ference on shared resources such as LLC, memory bandwidth and controller, on-chip interconnects. As <ref type="bibr" target="#b16">[17]</ref> pointed out, as the number of concurrently running applica can cooperatively manage cache sets and ways and provide many fine-grained partitions. Selfa et al. <ref type="bibr" target="#b16">[17]</ref> proposes a clustering-based cache partitioning mechanism t tch friendly.2) Cache Partitioning (CP): There are several proposed CP implementations. Selfa et al.<ref type="bibr" target="#b16">[17]</ref> groups cores into several clusters and each cluster occupi lace each subset into a separate partition. They are compared with the best known algorithm, Dunn in<ref type="bibr" target="#b16">[17]</ref>, in Sec. V-B. Note that CP just needs the first two sampli

ires the knowledge of an application's IPC when running alone. Though prior work Subramanian et al. <ref type="bibr" target="#b22">[23]</ref> tried to estimate the running-alone IPC in multicore proce
information which may be head pose, expression, or landmarks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar
appropriate layers of the network (i.e. a content loss as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>). The precise layers are chosen based on whether we are con
emotional state <ref type="bibr" target="#b15">[16]</ref>, and controlling body movement with music <ref type="bibr" target="#b35">[36]</ref>.</p><p>Our method has the benefits of being self-supervise
ibr" target="#b18">[19]</ref> are split into train/val set, leaving out the 1, 000 test images from <ref type="bibr" target="#b21">[22]</ref> as test set. The results on the test set are reported in T linear pose predictor from the driving vector performs only slightly worse than a supervised method <ref type="bibr" target="#b21">[22]</ref>, which has been trained for this task</p></div> <div xmlns //www.tei-c.org/ns/1.0"><head>Method</head><p>Roll Pitch Yaw MAE X2Face 5.85 7.59 14.62 9.36 KEPLER <ref type="bibr" target="#b21">[22]</ref> (supervised) 8.75 5.85 6.45 7.02</p></div> <div xmlns="htt
.</p><p>The pre-trained network used for these losses is the 11-layer VGG network (configuration A) <ref type="bibr" target="#b36">[37]</ref> trained on the VGG-Face Dataset <ref type="bibr" target="#
dded face. The architecture is based on U-Net <ref type="bibr" target="#b31">[32]</ref> and pix2pix <ref type="bibr" target="#b14">[15]</ref>; the output is a 2-channel image (of the same dimensions a
emotional state <ref type="bibr" target="#b15">[16]</ref>, and controlling body movement with music <ref type="bibr" target="#b35">[36]</ref>.</p><p>Our method has the benefits of being self-supervise
f v→a is required). a is obtained by extracting the 256D audio features from the neural network in <ref type="bibr" target="#b8">[9]</ref> and the 128D v by passing the corresponding frame through th erviews, suggesting that the audio should be especially correlated with the movements of the mouth. <ref type="bibr" target="#b8">[9]</ref>'s model, trained on the BBC-Oxford 'Lip Reading in the Wild' dio signal). A potential source of error is the domain gap between the LRW dataset and VoxCeleb, as <ref type="bibr" target="#b8">[9]</ref>'s model is not fine-tuned on the VoxCeleb dataset which cont
lt on the fitting of 3DMMs by including high level details <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref>, taking into account additional images <ref type="bibr" tar
ated set of works consider how to frontalise a face in a still image using a generic reference face <ref type="bibr" target="#b13">[14]</ref>, transferring expressions of an actor to an avatar <ref ty
2Face is trained on the VoxCeleb video dataset <ref type="bibr" target="#b22">[23]</ref> using dlib <ref type="bibr" target="#b17">[18]</ref> to crop the faces to 256 × 256. The identities are randoml
truction of dynamic program control flow. Collection is built on top of the data source for AutoFDO <ref type="bibr" target="#b4">[5]</ref>, which similarly uses LBRs to reconstruct basic block execut rofiles <ref type="bibr" target="#b24">[25]</ref>, especially with sampling approaches like AutoFDO <ref type="bibr" target="#b4">[5]</ref>.</p><p>The high degree of fragmentation we observed suggests rofiling trace their beginnings to DCPI <ref type="bibr" target="#b0">[1]</ref>. Of these, Aut-oFDO <ref type="bibr" target="#b4">[5]</ref> (built on top of Google-wide-profiling (GWP) <ref type="bibr
<p>Datacenter-wide profiling. Modern systems for always-on profiling trace their beginnings to DCPI <ref type="bibr" target="#b0">[1]</ref>. Of these, Aut-oFDO <ref type="bibr" target="#b4">[5]</ref>
torage, making them difficult to implement in commercial processors.</p><p>More recently, Boomerang <ref type="bibr" target="#b21">[22]</ref> combines fetch-directed instruction prefetching <ref type=
structed in AsmDB) and instruction miss profiles collected with Precise Event-Based Sampling (PEBS) <ref type="bibr" target="#b7">[8]</ref>, which allow us to identify individual instructions that mis
ions. (For instance, in SPEC CPU2006, mcf has an IPC of 0.2 whereas dealII has an IPC of almost 2.0 <ref type="bibr" target="#b26">[27]</ref>.) Our approach addresses this issue by leveraging per-appl
over 100 times larger than the size of a L1 instruction cache (i-cache) and can easily overwhelm it <ref type="bibr" target="#b1">[2]</ref>. Studies show it expanding at rates of over 20% per year <re
cale Computers (WSC) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. The continued growth in cloud-based, digital services has
it <ref type="bibr" target="#b1">[2]</ref>. Studies show it expanding at rates of over 20% per year <ref type="bibr" target="#b15">[16]</ref>. This results in instruction cache miss rates which are or s of different applications, so the kernels that matter most across the fleet (the "datacenter tax" <ref type="bibr" target="#b15">[16]</ref>) may not be significant for a single workload, and are eas . On the contrary, WSC applications are well-known for their long tails and flat execution profiles <ref type="bibr" target="#b15">[16]</ref>, which are best addressed with scalable automatic optimiza frequently. It also corroborates prior findings that low-level library functions ("datacenter tax" <ref type="bibr" target="#b15">[16]</ref>), and specifically memcpy and memcmp (which copy and compa -link time prefetch insertion on the binary level.</p><p>Profiling efforts, both on production WSCs <ref type="bibr" target="#b15">[16]</ref> and on isolated benchmarks <ref type="bibr" target="#b8">[
torage, making them difficult to implement in commercial processors.</p><p>More recently, Boomerang <ref type="bibr" target="#b21">[22]</ref> combines fetch-directed instruction prefetching <ref type=
the L1 or L2 instruction caches.</p><p>Simulation. We use a modified version of the ZSim simulator <ref type="bibr" target="#b30">[31]</ref>. We included a trace-driven execution mode, as well as mod
ions. (For instance, in SPEC CPU2006, mcf has an IPC of 0.2 whereas dealII has an IPC of almost 2.0 <ref type="bibr" target="#b26">[27]</ref>.) Our approach addresses this issue by leveraging per-appl
tchers have reduced the required amount of on-chip storage <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. However, they st
Post-processing a single days' worth of collected data takes ? 8 hours with a 400-machine MapReduce <ref type="bibr" target="#b6">[7]</ref>, and produces ? 600 GiB compressed output. Thus, a years' wo
her than the worst cases in desktop-class applications, commonly represented by SPEC CPU benchmarks <ref type="bibr" target="#b8">[9]</ref>.</p><p>Because the performance of a general-purpose processo forts, both on production WSCs <ref type="bibr" target="#b15">[16]</ref> and on isolated benchmarks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targ
ing served by services hosted in huge datacenters, which are dubbed Warehouse-Scale Computers (WSC) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ
ions. (For instance, in SPEC CPU2006, mcf has an IPC of 0.2 whereas dealII has an IPC of almost 2.0 <ref type="bibr" target="#b26">[27]</ref>.) Our approach addresses this issue by leveraging per-appl
the L1 or L2 instruction caches.</p><p>Simulation. We use a modified version of the ZSim simulator <ref type="bibr" target="#b30">[31]</ref>. We included a trace-driven execution mode, as well as mod
cale Computers (WSC) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. The continued growth in cloud-based, digital services has
it <ref type="bibr" target="#b1">[2]</ref>. Studies show it expanding at rates of over 20% per year <ref type="bibr" target="#b15">[16]</ref>. This results in instruction cache miss rates which are or s of different applications, so the kernels that matter most across the fleet (the "datacenter tax" <ref type="bibr" target="#b15">[16]</ref>) may not be significant for a single workload, and are eas . On the contrary, WSC applications are well-known for their long tails and flat execution profiles <ref type="bibr" target="#b15">[16]</ref>, which are best addressed with scalable automatic optimiza frequently. It also corroborates prior findings that low-level library functions ("datacenter tax" <ref type="bibr" target="#b15">[16]</ref>), and specifically memcpy and memcmp (which copy and compa -link time prefetch insertion on the binary level.</p><p>Profiling efforts, both on production WSCs <ref type="bibr" target="#b15">[16]</ref> and on isolated benchmarks <ref type="bibr" target="#b8">[
ing served by services hosted in huge datacenters, which are dubbed Warehouse-Scale Computers (WSC) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ
, 11, 18-20, 20, 22]</ref> or static control flow analysis <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>. However, in WSC environments it has been commercially infe
optimizations like hot/cold splitting <ref type="bibr" target="#b5">[6]</ref> and partial inlining <ref type="bibr" target="#b31">[32]</ref> aim to address fragmentation by only inlining the hot basi
inimal group miss ratio) and required the assumption that the individual miss ratio curve be convex <ref type="bibr" target="#b8">[9]</ref>. Our algorithm uses dynamic programming to examine the entir he cache between instructions and data, and for multiprogramming by giving each process a partition <ref type="bibr" target="#b8">[9]</ref>.</p><p>In the way of optimizing their algorithm, they proved ss-rate derivative. The allocation is optimal if the miss-rate derivatives are as equal as possible <ref type="bibr" target="#b8">[9]</ref>. The optimality depends on several assumptions. One is that
r" target="#b7">[8]</ref>, and as Sen and Wood recently showed, the performance of non-LRU policies <ref type="bibr" target="#b6">[7]</ref>.</p><p>Locality-performance Correlation: A recent study by W
http://www.tei-c.org/ns/1.0"><head>A. Methodology</head><p>Following the methodology of Wang et al. <ref type="bibr" target="#b11">[12]</ref>, we randomly choose 16 programs from SPEC 2006: perlbench, ame as the implementation by Xiang et al. <ref type="bibr" target="#b15">[16]</ref> and Wang et al. <ref type="bibr" target="#b11">[12]</ref> Xiang et al. reported on average 23 times slowdown from th dation can be found in the use of the footprint theory in optimal program symbiosis in shared cache <ref type="bibr" target="#b11">[12]</ref>, optimal memory allocation in Memcached <ref type="bibr" t miss ratio prediction has a linear relationship between execution time, with a coefficient of 0.938 <ref type="bibr" target="#b11">[12]</ref>. They measure execution times and miss ratios of all 1820 od called adaptive bursty footprint (ABF) profiling, which takes on average 0.09 second per program <ref type="bibr" target="#b11">[12]</ref>. To have reproducible results, our implementation uses the ly-associative LRU cache. Wang et al. tested the analysis on program execution traces for CPU cache <ref type="bibr" target="#b11">[12]</ref>, Hu et al. on keyvalue access traces for Memcached <ref ty d strong correlation (coefficient 0.938) between the predicted miss ratio and measured co-run speed <ref type="bibr" target="#b11">[12]</ref>. The correlation means that if we minimize the miss ratio
istance (CRD), which shows the performance of both partitioned and shared cache for all cache sizes <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" t
ware mechanism to provide programs the protection of partitioning, without the risk of unused space <ref type="bibr" target="#b3">[4]</ref>. and free-for-all sharing can be seen as opposite edge cases
aluated and validated for solo-use cache, including the two initial studies of the footprint theory <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Independent va
r for multiple objectives, e.g., both throughput and fairness in elastic cache utility optimization <ref type="bibr" target="#b17">[18]</ref>.</p><p>Concurrent Reuse Distance: Simulation of shared cac
derive the reuse distance, which can be used to statistically estimate the effect of associativity <ref type="bibr" target="#b7">[8]</ref>, and as Sen and Wood recently showed, the performance of non
alled CASPER to measure the miss ratio curves from cache sizes 16KB to 1024KB in increments of 16KB <ref type="bibr" target="#b0">[1]</ref>. They noted that "miss rate errors would have been unaccepta
ware mechanism to provide programs the protection of partitioning, without the risk of unused space <ref type="bibr" target="#b3">[4]</ref>. and free-for-all sharing can be seen as opposite edge cases
alled CASPER to measure the miss ratio curves from cache sizes 16KB to 1024KB in increments of 16KB <ref type="bibr" target="#b0">[1]</ref>. They noted that "miss rate errors would have been unaccepta
ods (GCN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>, GraphSAGE <ref type="bibr" target="#b12">[13]</ref>), etc.</p><p>Graph-level embedding. The most intuitive way
nt of nodes out of an entire graph, which is too coarse for our task, graph similarity computation. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> assign an attentio
s, most of which are framed as node-level prediction tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>. However, once moving to the graph-level tasks, most existi
a Titan GPU. As for training, we set the batch size to 128, use the Adam algorithm for optimization <ref type="bibr" target="#b20">[21]</ref>, and fix the initial learning rate to 0.001. We set the nu
>[32]</ref>), skip-gram based methods (DeepWalk <ref type="bibr" target="#b30">[31]</ref>, Node2Vec <ref type="bibr" target="#b11">[12]</ref>, LINE <ref type="bibr" target="#b39">[40]</ref>), autoenco
r, the fundamental problem of the exponential time complexity of exact graph similarity computation <ref type="bibr" target="#b27">[28]</ref> remains. The second category tries to reduce the cost of g lynomial or even sub-exponential in the number of nodes in the graphs, such as A*-Beamsearch (Beam) <ref type="bibr" target="#b27">[28]</ref>, Hungarian <ref type="bibr" target="#b34">[35]</ref>, VJ < DB dataset, we take the smallest distance computed by three well-known approximate algorithms, Beam <ref type="bibr" target="#b27">[28]</ref>, Hungarian <ref type="bibr" target="#b22">[23,</ref><ref t tegory of baselines includes three classic algorithms for GED computation. (1) A*-Beamsearch (Beam) <ref type="bibr" target="#b27">[28]</ref>. It is a variant of the A* algorithm in sub-exponential ti " target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. However, these met " target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. We are aware of so
raph is to aggregate the node-level embeddings, either by a simple average or some weighted average <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b49">50]</ref>, named the "sum-base
a Titan GPU. As for training, we set the batch size to 128, use the Adam algorithm for optimization <ref type="bibr" target="#b20">[21]</ref>, and fix the initial learning rate to 0.001. We set the nu
r, the fundamental problem of the exponential time complexity of exact graph similarity computation <ref type="bibr" target="#b27">[28]</ref> remains. The second category tries to reduce the cost of g lynomial or even sub-exponential in the number of nodes in the graphs, such as A*-Beamsearch (Beam) <ref type="bibr" target="#b27">[28]</ref>, Hungarian <ref type="bibr" target="#b34">[35]</ref>, VJ < DB dataset, we take the smallest distance computed by three well-known approximate algorithms, Beam <ref type="bibr" target="#b27">[28]</ref>, Hungarian <ref type="bibr" target="#b22">[23,</ref><ref t tegory of baselines includes three classic algorithms for GED computation. (1) A*-Beamsearch (Beam) <ref type="bibr" target="#b27">[28]</ref>. It is a variant of the A* algorithm in sub-exponential ti " target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. However, these met " target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. We are aware of so
based methods (NetMF <ref type="bibr" target="#b31">[32]</ref>), skip-gram based methods (DeepWalk <ref type="bibr" target="#b30">[31]</ref>, Node2Vec <ref type="bibr" target="#b11">[12]</ref>, LINE
cient (ρ) <ref type="bibr" target="#b38">[39]</ref> and Kendall's Rank Correlation Coefficient (τ ) <ref type="bibr" target="#b19">[20]</ref> measure how well the predicted ranking results match the t
els have been proposed in recent years, including methods inspired by convolutional neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
m the features of their neighbor nodes using a differentiable aggregation function. Hamilton et al. <ref type="bibr" target="#b16">[17]</ref> provides a conceptual review of recent advancements in thi
graph classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40]</ref>, and chemoinformatics <ref type="bibr" target="#b27">[28,</ hitectures to the concatenation of all the node embeddings <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref>, but this requires a specifying (or learning) a canonical o p>Q1 How does DIFFPOOL compare to other pooling methods proposed for GNNs (e.g., using sort pooling <ref type="bibr" target="#b39">[40]</ref> or the SET2SET method <ref type="bibr" target="#b14">[15]< work <ref type="bibr" target="#b14">[15]</ref>. We use GRAPHSAGE as the base GNN model. ? SORTPOOL <ref type="bibr" target="#b39">[40]</ref> applies a GNN architecture and then performs a single laye
ocial network data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref> or graph-based representations of molecules <ref type="bibr get="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>, recurrent neural networks <ref type="bibr" target="#b24">[
graph classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40]</ref>, and chemoinformatics <ref type="bibr" target="#b27">[28,</ hitectures to the concatenation of all the node embeddings <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref>, but this requires a specifying (or learning) a canonical o p>Q1 How does DIFFPOOL compare to other pooling methods proposed for GNNs (e.g., using sort pooling <ref type="bibr" target="#b39">[40]</ref> or the SET2SET method <ref type="bibr" target="#b14">[15]< work <ref type="bibr" target="#b14">[15]</ref>. We use GRAPHSAGE as the base GNN model. ? SORTPOOL <ref type="bibr" target="#b39">[40]</ref> applies a GNN architecture and then performs a single laye
"#b35">36]</ref> or graph-based representations of molecules <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>. The general approa a simple summation or neural network that operates over sets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar tional neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar f type="bibr" target="#b30">[31]</ref>, graph classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40]</ref>, and chemoinformati hes to this problem include simply summing up or averaging all the node embeddings in a final layer <ref type="bibr" target="#b10">[11]</ref>, introducing a "virtual node" that is connected to all the
earch based on the original author's guidelines.</p><p>Kernel-based algorithms. We use the GRAPHLET <ref type="bibr" target="#b33">[34]</ref>, the SHORTEST-PATH <ref type="bibr" target="#b1">[2]</ref>
/ref>. We use protein data sets including ENZYMES, PRO-TEINS <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>, D&amp;D <ref type="bibr" target="#b9">[10]</ref>, the soci
f type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40]</ref>, and chemoinformatics <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" ta
<ref type="bibr" target="#b32">[33]</ref>, and WEISFEILER-LEHMAN OPTIMAL ASSIGNMENT KERNEL (WL-OA) <ref type="bibr" target="#b21">[22]</ref> as kernel baselines. For each kernel, we computed the norm
POOL model where assignment matrices are generated using a deterministic graph clustering algorithm <ref type="bibr" target="#b8">[9]</ref>. ? DIFFPOOL-NOLP is a variant of DIFFPOOL where the link pre layer community structures, which can be captured well with pre-computed graph clustering algorithm <ref type="bibr" target="#b8">[9]</ref>. One observation is that despite significant performance imp
ng from position s. This neural network combines the roles of both policy network and value network <ref type="bibr" target="#b11">12</ref> into a single architecture. The neural network consists of m perfect information. We follow the formalism of alternating Markov games described in previous work <ref type="bibr" target="#b11">12</ref> , noting that algorithms based on value or policy iteration ompare three distinct versions of AlphaGo:</p><p>1. AlphaGo Fan is the previously published program <ref type="bibr" target="#b11">12</ref> that played against Fan Hui in October 2015. This program wa described in this paper. However, it uses the same handcrafted features and rollouts as AlphaGo Lee <ref type="bibr" target="#b11">12</ref> and training was initialised by supervised learning from hum ue component, it was possible to avoid overfitting to the values (a problem described in prior work <ref type="bibr" target="#b11">12</ref> ). After 72 hours the move prediction accuracy exceeded the the KGS test set; the value prediction error was also substantially better than previously reported <ref type="bibr" target="#b11">12</ref> . The validation set was composed of professional games from of AlphaGo Fan, Crazy Stone, Pachi and GnuGo were anchored to the tournament values from prior work <ref type="bibr" target="#b11">12</ref> , and correspond to the players reported in that work. The r lso performed against baseline players with Elo ratings anchored to the previously published values <ref type="bibr" target="#b11">12</ref> .</p><p>We measured the head-to-head performance of AlphaGo hat maximise an upper confidence bound Q(s, a) + U (s, a), where U (s, a) ∝ P (s, a)/(1 + N (s, a)) <ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24</ref> , until a leaf node After 72 hours the move prediction accuracy exceeded the state of the art reported in previous work <ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" ta
he value function from the outcomes of sampled trajectories <ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36</ref> . A simple approach to policy improvement is to select actio
search operators repeatedly in a policy iteration procedure <ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23</ref> : the neural network's parameters are updated to make the mo ry to evaluate each policy and to represent its improvement <ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23</ref> .</p><p>Classification-based reinforcement learning <ref typ
search operators repeatedly in a policy iteration procedure <ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23</ref> : the neural network's parameters are updated to make the mo ry to evaluate each policy and to represent its improvement <ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23</ref> .</p><p>Classification-based reinforcement learning <ref typ
2,</ref><ref type="bibr" target="#b22">23</ref> .</p><p>Classification-based reinforcement learning <ref type="bibr" target="#b36">37</ref> improves the policy using a simple Monte-Carlo search. Many
rget="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref> , and online recommendation systems <ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65</ref> .</p></div> <div xml
="#b54">55</ref> . In addition, the learning process used supervised learning to initialise weights <ref type="bibr" target="#b57">58</ref> , hand-selected weights for piece values <ref type="bibr" ta

f Go 13 . However, very similar algorithms have subsequently proven highly effective in video games <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" targe
ype="bibr" target="#b50">[51]</ref> , checkers <ref type="bibr" target="#b51">52</ref> , backgammon <ref type="bibr" target="#b52">53</ref> , othello <ref type="bibr" target="#b53">54</ref> , Scrabble ="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref> from training data generated by self-play. The trained val ="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref> , a simple Monte-
ay reinforcement learning approaches have achieved high levels of performance in other games: chess <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" t r" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref> or temporaldifference learning <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" t by self-play. The trained value function was used as an evaluation function in an alpha-beta search <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" t initialise weights <ref type="bibr" target="#b57">58</ref> , hand-selected weights for piece values <ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" tar f type="bibr" target="#b55">56</ref> , or used pre-existing computer programs as training opponents <ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50</ref> or to generate game
RNA) <ref type="bibr" target="#b11">[12]</ref>, and the recurrent neural network transducer (RNN-T) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. In particular, th nd-to-end models including attention-based models <ref type="bibr" target="#b6">[7]</ref> and RNN-T <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> trained on ∼12,500 "http://www.tei-c.org/ns/1.0"><head n="2.">RNN-TRANSDUCER</head><p>The RNN-T was proposed by Graves <ref type="bibr" target="#b12">[13]</ref> as an extension to the connectionist temporal classificati ure <ref type="figure">1</ref>, consists of an encoder (referred to as the transcription network in <ref type="bibr" target="#b12">[13]</ref>), a prediction network and a joint network; as described i e="bibr" target="#b13">[14]</ref>. The entire network is trained jointly to optimize the RNN-T loss <ref type="bibr" target="#b12">[13]</ref>, which marginalizes over all alignments of target labels w p><p>During inference, the most likely label sequence is computed using beam search as described in <ref type="bibr" target="#b12">[13]</ref>, with a minor alteration which was found to make the algor nsive without degrading performance: we skip summation over prefixes in pref(y) (see Algorithm 1 in <ref type="bibr" target="#b12">[13]</ref>), unless multiple hypotheses are identical.</p><p>Note tha
language model components. A particular class of architecures known as sequence-to-sequence models <ref type="bibr" target="#b9">[10]</ref> are particularly suited for end-to-end ASR as they include
/ref>, and the recurrent neural network transducer (RNN-T) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. In particular, these architectures allow the output to be d models <ref type="bibr" target="#b6">[7]</ref> and RNN-T <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> trained on ∼12,500 hours of transcribed training data; alth (h enc t , h dec u )<label>(6)</label></formula><p>We use the same form for f joint as described in <ref type="bibr" target="#b13">[14]</ref>. The entire network is trained jointly to optimize the RNN er parameters from a model trained with the CTC loss is beneficial for the phoneme recognition task <ref type="bibr" target="#b13">[14]</ref>. We experimented with initializing encoder networks from m
further improved with word-level edit-based minimum Bayes risk (EMBR) proposed recently by Shannon <ref type="bibr" target="#b22">[23]</ref>. Acoustic models are trained on a set of ∼22 million hand-
P (W ).</p><p>Recently, there has been considerable interest in training end-to-end models for ASR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target onds to the language model.</p><p>One drawback of typical encoder-decoder type architectures (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>) is that the entire in arget="#b14">[15]</ref> we evaluated a number of end-to-end models including attention-based models <ref type="bibr" target="#b6">[7]</ref> and RNN-T <ref type="bibr" target="#b12">[13,</ref><ref type -T model can be compared to other encoder-decoder architectures such as "listen, attend, and spell" <ref type="bibr" target="#b6">[7]</ref>, if we view the combination of the prediction network and th quence length of encoded activations which is similar to the pyramidal sequence length reduction in <ref type="bibr" target="#b6">[7]</ref>. For these models, we used filters covering 3 non-overlappin
M) cells <ref type="bibr" target="#b1">[2]</ref> have recently been shown to be ideal for this task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target
considerable interest in training end-to-end models for ASR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, which directly output ng to a given input frame. CTC has been widely used in previous works to train end-toend ASR models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targ
/ref>, and the recurrent neural network transducer (RNN-T) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. In particular, these architectures allow the output to be d models <ref type="bibr" target="#b6">[7]</ref> and RNN-T <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> trained on ∼12,500 hours of transcribed training data; alth (h enc t , h dec u )<label>(6)</label></formula><p>We use the same form for f joint as described in <ref type="bibr" target="#b13">[14]</ref>. The entire network is trained jointly to optimize the RNN er parameters from a model trained with the CTC loss is beneficial for the phoneme recognition task <ref type="bibr" target="#b13">[14]</ref>. We experimented with initializing encoder networks from m
rks are trained with multiple simultaneous CTC losses which was beneficial for grapheme recognition <ref type="bibr" target="#b21">[22]</ref>. After pre-training all CTC losses and additional weights
/ref>, and the recurrent neural network transducer (RNN-T) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. In particular, these architectures allow the output to be d models <ref type="bibr" target="#b6">[7]</ref> and RNN-T <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> trained on ∼12,500 hours of transcribed training data; alth (h enc t , h dec u )<label>(6)</label></formula><p>We use the same form for f joint as described in <ref type="bibr" target="#b13">[14]</ref>. The entire network is trained jointly to optimize the RNN er parameters from a model trained with the CTC loss is beneficial for the phoneme recognition task <ref type="bibr" target="#b13">[14]</ref>. We experimented with initializing encoder networks from m
P (W ).</p><p>Recently, there has been considerable interest in training end-to-end models for ASR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target onds to the language model.</p><p>One drawback of typical encoder-decoder type architectures (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>) is that the entire in arget="#b14">[15]</ref> we evaluated a number of end-to-end models including attention-based models <ref type="bibr" target="#b6">[7]</ref> and RNN-T <ref type="bibr" target="#b12">[13,</ref><ref type -T model can be compared to other encoder-decoder architectures such as "listen, attend, and spell" <ref type="bibr" target="#b6">[7]</ref>, if we view the combination of the prediction network and th quence length of encoded activations which is similar to the pyramidal sequence length reduction in <ref type="bibr" target="#b6">[7]</ref>. For these models, we used filters covering 3 non-overlappin
he relational graph and apply it to both tasks.</p><p>Our entity classification model, similarly to <ref type="bibr" target="#b16">Kipf and Welling (2017)</ref>, uses softmax classifiers at each node nction or simply a linear transformation g m (h i , h j ) = W h j with a weight matrix W such as in <ref type="bibr" target="#b16">Kipf and Welling (2017)</ref>. This type of transformation has been s tion. While we only consider such a featureless approach in this work, we note that it was shown in <ref type="bibr" target="#b16">Kipf and Welling (2017)</ref> that it is possible for this class of m that operate on local graph neighborhoods <ref type="bibr" target="#b8">(Duvenaud et al. 2015;</ref><ref type="bibr" target="#b16">Kipf and Welling 2017)</ref> to large-scale relational data. These an d et al. 2015;</ref><ref type="bibr" target="#b6">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b16">Kipf and Welling 2017)</ref> for large-scale and highly multi-relatio <ref type="bibr" target="#b8">(Duvenaud et al. 2015)</ref> and graph-based semi-supervised learning <ref type="bibr" target="#b16">(Kipf and Welling 2017)</ref>.</p><p>Motivated by these architectures
s et al. 2015;</ref><ref type="bibr" target="#b7">Dong et al. 2015)</ref> and information retrieval <ref type="bibr" target="#b17">(Kotov and Zhai 2012;</ref><ref type="bibr" target="#b4">Dalton, Diet
s et al. 2015;</ref><ref type="bibr" target="#b7">Dong et al. 2015)</ref> and information retrieval <ref type="bibr" target="#b17">(Kotov and Zhai 2012;</ref><ref type="bibr" target="#b4">Dalton, Diet
ibr" target="#b2">(Bruna et al. 2014;</ref><ref type="bibr" target="#b8">Duvenaud et al. 2015;</ref><ref type="bibr" target="#b6">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" tar
al. 2015)</ref> and information retrieval <ref type="bibr" target="#b17">(Kotov and Zhai 2012;</ref><ref type="bibr" target="#b4">Dalton, Dietz, and Allan 2014;</ref><ref type="bibr" target="#b29">Xio
product with circular correlation. Finally, we include comparisons with two classic algorithms -CP <ref type="bibr" target="#b12">(Hitchcock 1927)</ref> and TransE <ref type="bibr">(Bordes et al. 201
<ref type="bibr">(Yao and Van Durme 2014;</ref><ref type="bibr" target="#b0">Bao et al. 2014;</ref><ref type="bibr" target="#b24">Seyler, Yahya, and Berberich 2015;</ref><ref type="bibr" target="#b13
"#b17">(Kotov and Zhai 2012;</ref><ref type="bibr" target="#b4">Dalton, Dietz, and Allan 2014;</ref><ref type="bibr" target="#b29">Xiong and Callan 2015b;</ref><ref type="bibr" target="#b29">2015a)</r b4">Dalton, Dietz, and Allan 2014;</ref><ref type="bibr" target="#b29">Xiong and Callan 2015b;</ref><ref type="bibr" target="#b29">2015a)</ref>. Even the largest knowledge bases (e.g. DBPedia, Wikidat
target="#tab_1">1</ref>. For a more detailed description of the datasets the reader is referred to <ref type="bibr" target="#b22">Ristoski, de Vries, and Paulheim (2016)</ref>. We remove relations th fferences, we repeated the baselines in a uniform manner, using the canonical test/train split from <ref type="bibr" target="#b22">(Ristoski, de Vries, and Paulheim 2016)</ref>. We performed hyperpara /ref>, and R-GCN (this work). Test performance is reported on the train/test set splits provided by <ref type="bibr" target="#b22">(Ristoski, de Vries, and Paulheim 2016)</ref>.</p></div><figure xmlns pe="foot" target="#foot_3">5</ref> For RDF2Vec, we use an implementation provided by the authors of <ref type="bibr" target="#b22">(Ristoski and Paulheim 2016)</ref> which builds on Mustard. In both c relation.</p><p>Hyperparameters for baselines are chosen according to the best model performance in <ref type="bibr" target="#b22">(Ristoski and Paulheim 2016)</ref>, i.e. WL: 2 (tree depth), 3 (numbe to spatial constraints) are summarized in  <ref type="bibr" target="#b5">Rooij 2015)</ref>, RDF2Vec <ref type="bibr" target="#b22">(Ristoski and Paulheim 2016)</ref>, and R-GCN (this work). Test perfo periments, we compare against recent state-of-the-art classification results from RDF2Vec embeddings<ref type="bibr" target="#b22">(Ristoski and Paulheim 2016)</ref>, Weisfeiler-Lehman kernels (WL) (S rocessing differs from that used in (de Vries and de <ref type="bibr" target="#b5">Rooij 2015;</ref><ref type="bibr" target="#b22">Ristoski and Paulheim 2016)</ref> where for a given target relation ( olov et al. 2013</ref>) model to generate entity embeddings, used for subsequent classification. See<ref type="bibr" target="#b22">Ristoski and Paulheim (2016)</ref> for an in-depth description and di
r edges. Using edge droupout makes our training objective similar to that of denoising autoencoders <ref type="bibr" target="#b28">(Vincent et al. 2008)</ref>. We apply l2 regularization to the decode
rea of neural networks on graphs. It is primarily motivated as an adaption of previous work on GCNs <ref type="bibr" target="#b2">(Bruna et al. 2014;</ref><ref type="bibr" target="#b8">Duvenaud et al.
ecommendations based on collaborative filtering principles <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>, they have not been
s those only using features of students or of courses in capturing variation in student performance <ref type="bibr" target="#b10">[11]</ref>. Part of the benefit of co-enrollment information in predi
ents or advisers when struggle is occurring or is imminent <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>. Early-warning implementations like this can experience uni
ation 1 . They have been shown to generalize using long sequences more effectively than simple RNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref> by using a gated str courses which are unavailable in semester t. (4) Filter out courses the student has already taken. <ref type="bibr" target="#b4">(5)</ref> Filter out the target course. <ref type="bibr" target="#b5">
ning type systems meant to signal to students or advisers when struggle is occurring or is imminent <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>. Early-warning imp
for future predictions.</p><p>We use a popular variant of RNNs called Long Short-Term Memory (LSTM) <ref type="bibr" target="#b17">[18]</ref>, which helps RNNs learn temporal dependencies with the add
elation between course outcomes and timely access to course materials among late enrolling students <ref type="bibr" target="#b0">[1]</ref> serves as a reminder of this.</p></div> <div xmlns="http://w
ents or advisers when struggle is occurring or is imminent <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>. Early-warning implementations like this can experience uni
s those only using features of students or of courses in capturing variation in student performance <ref type="bibr" target="#b10">[11]</ref>. Part of the benefit of co-enrollment information in predi
for future predictions.</p><p>We use a popular variant of RNNs called Long Short-Term Memory (LSTM) <ref type="bibr" target="#b17">[18]</ref>, which helps RNNs learn temporal dependencies with the add
rget="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>. It should be noted that even given a positive validation a


the effect of affirmative action (see e.g., <ref type="bibr" target="#b12">Keith et al., 1985;</ref><ref type="bibr" target="#b11">Kalev et al., 2006)</ref>.</p></div> <div xmlns="http://www.tei-c.org
ed the equality of opportunity constraint and demonstrate limitations of a broad class of criteria. <ref type="bibr" target="#b13">Kleinberg et al. (2017)</ref> and Chouldechova (2016) point out the t
ed the equality of opportunity constraint and demonstrate limitations of a broad class of criteria. <ref type="bibr" target="#b13">Kleinberg et al. (2017)</ref> and Chouldechova (2016) point out the t

. These trade-offs carry over to some extent to the case where we only equalize true positive rates <ref type="bibr" target="#b15">(Pleiss et al., 2017)</ref>.</p><p>A growing literature on fairness i

the effect of affirmative action (see e.g., <ref type="bibr" target="#b12">Keith et al., 1985;</ref><ref type="bibr" target="#b11">Kalev et al., 2006)</ref>.</p></div> <div xmlns="http://www.tei-c.org
e been considered in numerous papers (e.g. <ref type="bibr" target="#b1">Calders et al., 2009;</ref><ref type="bibr" target="#b18">Zafar et al., 2017)</ref>. <ref type="bibr" target="#b8">Hardt et al.
e been considered in numerous papers (e.g. <ref type="bibr" target="#b1">Calders et al., 2009;</ref><ref type="bibr" target="#b18">Zafar et al., 2017)</ref>. <ref type="bibr" target="#b8">Hardt et al.
e been considered in numerous papers (e.g. <ref type="bibr" target="#b1">Calders et al., 2009;</ref><ref type="bibr" target="#b18">Zafar et al., 2017)</ref>. <ref type="bibr" target="#b8">Hardt et al.

approach. Demographic parity and related formulations have been considered in numerous papers (e.g. <ref type="bibr" target="#b1">Calders et al., 2009;</ref><ref type="bibr" target="#b18">Zafar et al.

the effect of affirmative action (see e.g., <ref type="bibr" target="#b12">Keith et al., 1985;</ref><ref type="bibr" target="#b11">Kalev et al., 2006)</ref>.</p></div> <div xmlns="http://www.tei-c.org

has been much work in the social sciences on analyzing the effect of affirmative action (see e.g., <ref type="bibr" target="#b12">Keith et al., 1985;</ref><ref type="bibr" target="#b11">Kalev et al.,
" target="#b1">Calders et al., 2009;</ref><ref type="bibr" target="#b18">Zafar et al., 2017)</ref>. <ref type="bibr" target="#b8">Hardt et al. (2016)</ref> introduced the equality of opportunity const


ed the equality of opportunity constraint and demonstrate limitations of a broad class of criteria. <ref type="bibr" target="#b13">Kleinberg et al. (2017)</ref> and Chouldechova (2016) point out the t
notations, but lack the ability to generalize to other domains. In contrast, embedding-based models <ref type="bibr" target="#b7">(Bordes et al., 2014b;</ref><ref type="bibr" target="#b13">Hao et al., en executing logical queries on incomplete KBs. Our work follows the line of Embedding-based models <ref type="bibr" target="#b7">(Bordes et al., 2014b;</ref><ref type="bibr" target="#b10">Dong et al. ated to these three OOV relations during the test.</p><p>Several baselines are included here: Embed <ref type="bibr" target="#b7">(Bordes et al., 2014b)</ref> deals with factoid QA over KB by matching
ate to handle multi-relation QA due to the lack of reasoning ability.</p><p>Recent reasoning models <ref type="bibr" target="#b16">(Miller et al., 2016;</ref><ref type="bibr" target="#b26">Wang et al. manner during reasoning. MemNN <ref type="bibr" target="#b27">(Weston et al., 2015)</ref>, KVMemN2N <ref type="bibr" target="#b16">(Miller et al., 2016)</ref> and EviNet <ref type="bibr" target="#b21" re the settings are the same as <ref type="bibr" target="#b8">(Bordes et al., 2015)</ref>. KVMemN2N <ref type="bibr" target="#b16">(Miller et al., 2016)</ref> improves the MemN2N for KBQA as it divide
ng ability.</p><p>Recent reasoning models <ref type="bibr" target="#b16">(Miller et al., 2016;</ref><ref type="bibr" target="#b26">Wang et al., 2017)</ref> mainly concentrate on Reading Comprehension bibr" target="#b15">Kumar et al., 2015;</ref><ref type="bibr" target="#b22">Shen et al., 2016;</ref><ref type="bibr" target="#b26">Wang et al., 2017;</ref><ref type="bibr" target="#b9">Celikyilmaz et
"bibr" target="#b8">(Bordes et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b21">Savenkov and Agichtein, 2017)</ref>. In comparison, reasoning over mu et al., 2015)</ref>, KVMemN2N <ref type="bibr" target="#b16">(Miller et al., 2016)</ref> and EviNet <ref type="bibr" target="#b21">(Savenkov and Agichtein, 2017)</ref> transferred the reading comprehe
KB, and this task has been widely studied <ref type="bibr" target="#b8">(Bordes et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b21">Savenkov and Agi br" target="#b7">(Bordes et al., 2014b;</ref><ref type="bibr" target="#b10">Dong et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b13">Hao et al., 2017 ong et al., 2015;</ref><ref type="bibr" target="#b13">Hao et al., 2017)</ref> and external contexts <ref type="bibr" target="#b28">(Xu et al., 2016)</ref> can be used to enrich the representation of a
KB, and this task has been widely studied <ref type="bibr" target="#b8">(Bordes et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b21">Savenkov and Agi br" target="#b7">(Bordes et al., 2014b;</ref><ref type="bibr" target="#b10">Dong et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b13">Hao et al., 2017 ong et al., 2015;</ref><ref type="bibr" target="#b13">Hao et al., 2017)</ref> and external contexts <ref type="bibr" target="#b28">(Xu et al., 2016)</ref> can be used to enrich the representation of a
ld datasets, WebQuestions <ref type="bibr" target="#b3">(Berant et al., 2013)</ref> and WikiAnswers <ref type="bibr" target="#b11">(Fader et al., 2013)</ref>. In this way, the syntactic structure and ts including WebQuestions <ref type="bibr" target="#b3">(Berant et al., 2013)</ref> and WikiAnswers <ref type="bibr" target="#b11">(Fader et al., 2013)</ref> as well as on the Internet. In this manner
ld datasets, WebQuestions <ref type="bibr" target="#b3">(Berant et al., 2013)</ref> and WikiAnswers <ref type="bibr" target="#b11">(Fader et al., 2013)</ref>. In this way, the syntactic structure and ts including WebQuestions <ref type="bibr" target="#b3">(Berant et al., 2013)</ref> and WikiAnswers <ref type="bibr" target="#b11">(Fader et al., 2013)</ref> as well as on the Internet. In this manner
"bibr" target="#b8">(Bordes et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b21">Savenkov and Agichtein, 2017)</ref>. In comparison, reasoning over mu et al., 2015)</ref>, KVMemN2N <ref type="bibr" target="#b16">(Miller et al., 2016)</ref> and EviNet <ref type="bibr" target="#b21">(Savenkov and Agichtein, 2017)</ref> transferred the reading comprehe
ly categorized into two lines: semantic parsing and embedding-based models. Semantic parsing models <ref type="bibr" target="#b30">(Yih et al., 2014;</ref><ref type="bibr" target="#b31">Yih et al., 20
ollows the line of Embedding-based models <ref type="bibr" target="#b7">(Bordes et al., 2014b;</ref><ref type="bibr" target="#b10">Dong et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 201 raphs of an entity in KB <ref type="bibr" target="#b6">(Bordes et al., 2014a)</ref>, answer aspects <ref type="bibr" target="#b10">(Dong et al., 2015;</ref><ref type="bibr" target="#b13">Hao et al., 2
ubject (or head) entity, relation, and the object (or tail) entity. This idea is inspired by TransE <ref type="bibr" target="#b5">(Bordes et al., 2013</ref>), but we adopt M se (see Eq. 6) as a transf
ubject (or head) entity, relation, and the object (or tail) entity. This idea is inspired by TransE <ref type="bibr" target="#b5">(Bordes et al., 2013</ref>), but we adopt M se (see Eq. 6) as a transf
edding-based models. Semantic parsing models <ref type="bibr" target="#b30">(Yih et al., 2014;</ref><ref type="bibr" target="#b31">Yih et al., 2016)</ref> obtain competitive performance at the cost of map questions to logical form queries <ref type="bibr" target="#b20">(Pasupat and Liang, 2015;</ref><ref type="bibr" target="#b31">Yih et al., 2016;</ref><ref type="bibr" target="#b0">Abujabal et al.,
KB, and this task has been widely studied <ref type="bibr" target="#b8">(Bordes et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b21">Savenkov and Agi br" target="#b7">(Bordes et al., 2014b;</ref><ref type="bibr" target="#b10">Dong et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b13">Hao et al., 2017 ong et al., 2015;</ref><ref type="bibr" target="#b13">Hao et al., 2017)</ref> and external contexts <ref type="bibr" target="#b28">(Xu et al., 2016)</ref> can be used to enrich the representation of a
arget="#b24">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b15">Kumar et al., 2015;</ref><ref type="bibr" target="#b22">Shen et al., 2016;</ref><ref type="bibr" target="#b26">Wang et al., 2
KB, and this task has been widely studied <ref type="bibr" target="#b8">(Bordes et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b21">Savenkov and Agi br" target="#b7">(Bordes et al., 2014b;</ref><ref type="bibr" target="#b10">Dong et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b13">Hao et al., 2017 ong et al., 2015;</ref><ref type="bibr" target="#b13">Hao et al., 2017)</ref> and external contexts <ref type="bibr" target="#b28">(Xu et al., 2016)</ref> can be used to enrich the representation of a
lates. The knowledge base for PathQuestion has more than 60,000 triples which are adopted from FB13 <ref type="bibr" target="#b23">(Socher et al., 2013)</ref> with 13 relations and thousands of entiti
lates. The knowledge base for PathQuestion has more than 60,000 triples which are adopted from FB13 <ref type="bibr" target="#b23">(Socher et al., 2013)</ref> with 13 relations and thousands of entiti
ibr" target="#b7">(Bordes et al., 2014b;</ref><ref type="bibr" target="#b13">Hao et al., 2017;</ref><ref type="bibr" target="#b29">Yavuz et al., 2017)</ref> can be trained end-to-end with weak supervi pe="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b13">Hao et al., 2017;</ref><ref type="bibr" target="#b29">Yavuz et al., 2017)</ref> which are recently introduced into the QA c
"bibr" target="#b8">(Bordes et al., 2015;</ref><ref type="bibr" target="#b28">Xu et al., 2016;</ref><ref type="bibr" target="#b21">Savenkov and Agichtein, 2017)</ref>. In comparison, reasoning over mu et al., 2015)</ref>, KVMemN2N <ref type="bibr" target="#b16">(Miller et al., 2016)</ref> and EviNet <ref type="bibr" target="#b21">(Savenkov and Agichtein, 2017)</ref> transferred the reading comprehe
del is challenging. Neural Module Network <ref type="bibr" target="#b1">(Andreas et al., 2015;</ref><ref type="bibr" target="#b2">Andreas et al., 2016)</ref> customized network architectures for diffe
), the Super-Resolution Convolutional Neural Network (SRCNN) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> has demonstrated superior performance to the previous hand-cr m, the Super-Resolution Convolutional Neural Network (SRCNN) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> has drawn considerable attention due to its simple network st Convolutional Neural Network (SRCNN) proposed by Dong et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Motivated by SRCNN, some problems such as face hallucination ><p>We first briefly describe the network structure of SRCNN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, and then we detail how we reformulate the network layer by l Different Upscaling Factors</head><p>Unlike existing methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> that need to train a network from scratch for a different sca lgorithms are mostly learning-based (or patch-based) methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target= space, then followed by a complex mapping to another high-dimensional HR feature space. Dong et al. <ref type="bibr" target="#b1">[2]</ref> show that the mapping accuracy can be substantially improved a wider mapping layer, but at the cost of the running time. For example, the large SRCNN (SRCNN-Ex) <ref type="bibr" target="#b1">[2]</ref> has 57,184 parameters, which are six times larger than that with no pre-processing. 2) The proposed model achieves a speed up of at least 40× than the SRCNN-Ex <ref type="bibr" target="#b1">[2]</ref> while still keeping its exceptional performance. One of its ters in a layer) and depth (i.e., the number of layers) of the mapping layer. As indicated in SRCNN <ref type="bibr" target="#b1">[2]</ref>, a 5 × 5 layer achieves much better results than a 1 × 1 lay s an average PSNR of 32.87 dB, which is already higher than that of SRCNN-Ex (32.75 dB) reported in <ref type="bibr" target="#b1">[2]</ref>. The FSRCNN (48,12,2) contains only 8,832 parameters, then t F) <ref type="bibr" target="#b6">[7]</ref>, SRCNN <ref type="bibr" target="#b0">[1]</ref>, SRCNN-Ex <ref type="bibr" target="#b1">[2]</ref> and the sparse coding based network (SCN) <ref type="bibr" t
r" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> that learn a mapping between the LR and HR image spaces. Amon r" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. If the network was learned directly from the original LR ima sparse-codingbased methods ensures its good performance. Based on the same assumption, Wang et al. <ref type="bibr" target="#b7">[8]</ref> further replace the mapping layer by a set of sparse coding taset to train our networks. To make full use of the dataset, we also adopt data augmentation as in <ref type="bibr" target="#b7">[8]</ref>. We augment the data in two ways. 1) Scaling: each image is ]</ref>, SRCNN-Ex <ref type="bibr" target="#b1">[2]</ref> and the sparse coding based network (SCN) <ref type="bibr" target="#b7">[8]</ref>. The implementations of these methods are all based on their ink the sparse coding sub-network with no loss of mapping accuracy. Furthermore, all these networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targ
#b23">[24]</ref> and Schulter et al. <ref type="bibr" target="#b6">[7]</ref> use the BSD500 dataset <ref type="bibr" target="#b24">[25]</ref>. However, images in the BSD500 are in JPEG format, which a <ref type="bibr" target="#b14">[15]</ref>, Set14 <ref type="bibr" target="#b8">[9]</ref> and BSD200 <ref type="bibr" target="#b24">[25]</ref> dataset for testing. Another 20 images from the validation <ref type="bibr" target="#b29">[30]</ref>, Set14 <ref type="bibr" target="#b8">[9]</ref> and BSD200 <ref type="bibr" target="#b24">[25]</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xm
es of that for the original LR one. This is also the restriction for most learning-based SR methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" targe aining dataset. The 91-image dataset is widely used as the training set in learningbased SR methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" targe ure consists of three parts that are analogous to the main steps of the sparse-coding-based methods <ref type="bibr" target="#b9">[10]</ref>. The patch extraction and representation part refers to the b_0">1</ref>. We also show their performance (average PSNR on Set5) trained on the 91-image dataset <ref type="bibr" target="#b9">[10]</ref>.</p><p>First, we replace the last convolution layer of SRCN

, the FSRCNN-s can run in real-time (&gt; 24 fps) on a generic CPU. The chart is based on the Set14 <ref type="bibr" target="#b8">[9]</ref> results summarized in Tables <ref type="table" target="#tab_ dataset. Following SRCNN and SCN, we use the Set5 <ref type="bibr" target="#b14">[15]</ref>, Set14 <ref type="bibr" target="#b8">[9]</ref> and BSD200 <ref type="bibr" target="#b24">[25]</ref> dataset f type="bibr" target="#b28">[29]</ref> on the Set5 <ref type="bibr" target="#b29">[30]</ref>, Set14 <ref type="bibr" target="#b8">[9]</ref> and BSD200 <ref type="bibr" target="#b24">[25]</ref>   </p><
es of that for the original LR one. This is also the restriction for most learning-based SR methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" targe aining dataset. The 91-image dataset is widely used as the training set in learningbased SR methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" targe ure consists of three parts that are analogous to the main steps of the sparse-coding-based methods <ref type="bibr" target="#b9">[10]</ref>. The patch extraction and representation part refers to the b_0">1</ref>. We also show their performance (average PSNR on Set5) trained on the 91-image dataset <ref type="bibr" target="#b9">[10]</ref>.</p><p>First, we replace the last convolution layer of SRCN
pscaling factors and datasets. We have also done comprehensive comparisons in terms of SSIM and IFC <ref type="bibr" target="#b28">[29]</ref> in Table <ref type="table" target="#tab_5">5 and 6</ref>, y SenseTime Group Limited.  Table <ref type="table">5</ref>. The results of PSNR (dB), SSIM and IFC <ref type="bibr" target="#b28">[29]</ref> on the Set5 <ref type="bibr" target="#b29">[30]</ref>, Set on both 91-image and General-100 dataset. More comparisons with other methods on PSNR, SSIM and IFC<ref type="bibr" target="#b28">[29]</ref> can be found in the supplementary file.</figDesc><table><r
#b23">[24]</ref> and Schulter et al. <ref type="bibr" target="#b6">[7]</ref> use the BSD500 dataset <ref type="bibr" target="#b24">[25]</ref>. However, images in the BSD500 are in JPEG format, which a <ref type="bibr" target="#b14">[15]</ref>, Set14 <ref type="bibr" target="#b8">[9]</ref> and BSD200 <ref type="bibr" target="#b24">[25]</ref> dataset for testing. Another 20 images from the validation <ref type="bibr" target="#b29">[30]</ref>, Set14 <ref type="bibr" target="#b8">[9]</ref> and BSD200 <ref type="bibr" target="#b24">[25]</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xm
r" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> that learn a mapping between the LR and HR image spaces. Amon r" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. If the network was learned directly from the original LR ima sparse-codingbased methods ensures its good performance. Based on the same assumption, Wang et al. <ref type="bibr" target="#b7">[8]</ref> further replace the mapping layer by a set of sparse coding taset to train our networks. To make full use of the dataset, we also adopt data augmentation as in <ref type="bibr" target="#b7">[8]</ref>. We augment the data in two ways. 1) Scaling: each image is ]</ref>, SRCNN-Ex <ref type="bibr" target="#b1">[2]</ref> and the sparse coding based network (SCN) <ref type="bibr" target="#b7">[8]</ref>. The implementations of these methods are all based on their ink the sparse coding sub-network with no loss of mapping accuracy. Furthermore, all these networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targ
es. CNNs acceleration: A number of studies have investigated the acceleration of CNN. Denton et al. <ref type="bibr" target="#b19">[20]</ref> first investigate the redundancy within the CNNs designed the above analysis. This is the main difference between our method and other CNN acceleration works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Nevertheless, wit
nd may refer to graphlets or orbits (graphlet automorphisms) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. The HONE framework expresses a general family of embedding m id search over K ? {1, 2, 3, 4} using 10% of the labeled data. We use all 2-4 node connected orbits <ref type="bibr" target="#b5">[6]</ref> and set D ? = 16 for the local motif embeddings. All methods
e labeled data. Experiments are repeated for 10 random seed initializations. Data was obtained from <ref type="bibr" target="#b6">[7]</ref>.</p><p>We evaluate the HONE variants for link prediction. Gi
[4]</ref>. All other hyperparameters for node2vec <ref type="bibr" target="#b3">[4]</ref>, DeepWalk <ref type="bibr" target="#b4">[5]</ref>, and LINE <ref type="bibr" target="#b8">[9]</ref> correspond
gions of the graph. Intuitively, two nodes belong to the same role if they are structurally similar <ref type="bibr" target="#b7">[8]</ref>. Many network representation learning methods (including ran r proximity (e.g., near one another in the graph). However, such methods are insufficient for roles <ref type="bibr" target="#b7">[8]</ref> as they fail to capture the higher-order connectivity patter
e labeled data. Experiments are repeated for 10 random seed initializations. Data was obtained from <ref type="bibr" target="#b6">[7]</ref>.</p><p>We evaluate the HONE variants for link prediction. Gi
e labeled data. Experiments are repeated for 10 random seed initializations. Data was obtained from <ref type="bibr" target="#b6">[7]</ref>.</p><p>We evaluate the HONE variants for link prediction. Gi

gions of the graph. Intuitively, two nodes belong to the same role if they are structurally similar <ref type="bibr" target="#b7">[8]</ref>. Many network representation learning methods (including ran r proximity (e.g., near one another in the graph). However, such methods are insufficient for roles <ref type="bibr" target="#b7">[8]</ref> as they fail to capture the higher-order connectivity patter

z i ? R D . For node2vec, we perform a grid search over p, q ? {0.25, 0.5, 1, 2, 4} as mentioned in <ref type="bibr" target="#b3">[4]</ref>. All other hyperparameters for node2vec <ref type="bibr" tar 4} as mentioned in <ref type="bibr" target="#b3">[4]</ref>. All other hyperparameters for node2vec <ref type="bibr" target="#b3">[4]</ref>, DeepWalk <ref type="bibr" target="#b4">[5]</ref>, and LINE ="#b4">[5]</ref>, and LINE <ref type="bibr" target="#b8">[9]</ref> correspond to those mentioned in <ref type="bibr" target="#b3">[4]</ref>. In contrast, the HONE variants have only one hyperparameter
[4]</ref>. All other hyperparameters for node2vec <ref type="bibr" target="#b3">[4]</ref>, DeepWalk <ref type="bibr" target="#b4">[5]</ref>, and LINE <ref type="bibr" target="#b8">[9]</ref> correspond
t harm their representational capacity for object recognition.</p><p>The Single Shot Detector (SSD) <ref type="bibr" target="#b21">[22]</ref> is one of the first attempts at using a ConvNet's pyramida tiple layers before computing predictions, which is equivalent to summing transformed features. SSD <ref type="bibr" target="#b21">[22]</ref> and MS-CNN <ref type="bibr" target="#b2">[3]</ref> predict >[35]</ref>, context modeling <ref type="bibr" target="#b15">[16]</ref>, stronger data augmentation <ref type="bibr" target="#b21">[22]</ref>, etc. These improvements are complementary to FPNs and sho
recognition from features computed on a single input scale <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref> (Fig. <ref type="fi o, if exploited, image pyramids are used only at test time <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar n image pyramid has obvious limitations. Inference time increases considerably (e.g., by four times <ref type="bibr" target="#b10">[11]</ref>), making this approach impractical for real applications. maps extracted on a single image scale. Recent and more accurate detection methods like Fast R-CNN <ref type="bibr" target="#b10">[11]</ref> and Faster R-CNN <ref type="bibr" target="#b28">[29]</ref> k, RPN for short) <ref type="bibr" target="#b28">[29]</ref> and region-based detectors (Fast R-CNN) <ref type="bibr" target="#b10">[11]</ref>. We also generalize FPNs to instance segmentation proposal PN <ref type="bibr" target="#b28">[29]</ref> for bounding box proposal generation and in Fast R-CNN <ref type="bibr" target="#b10">[11]</ref> for object detection. To demonstrate the simplicity and ef ://www.tei-c.org/ns/1.0"><head n="4.2.">Feature Pyramid Networks for Fast R-CNN</head><p>Fast R-CNN <ref type="bibr" target="#b10">[11]</ref> is a region-based object detector in which Region-of-Inter :id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Object detection results using Fast R-CNN<ref type="bibr" target="#b10">[11]</ref> on a fixed set of proposals (RPN, {P k }, Table1(c)), eval reates an inconsistency between train/test-time inference. For these reasons, Fast and Faster R-CNN <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref> opt to not use fea r method, called a Feature Pyramid Network (FPN), in various systems for detection and segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" ta ng implementations contribute to the gap: (i) We use an image scale of 800 pixels instead of 600 in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>; (ii) We train wit ref>; (ii) We train with 512 RoIs per image which accelerate convergence, in contrast to 64 RoIs in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>; (iii) We use 5 sc , we make minimal modifications to the original systems of <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b10">11]</ref> when adapting them to our feature pyramid.</p></div> <div x an adapt the assignment strategy of region-based detectors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref> in the case when they are run on image pyramids. Formally,
f type="bibr" target="#b17">[18]</ref>, ParseNet <ref type="bibr" target="#b22">[23]</ref>, and ION <ref type="bibr" target="#b1">[2]</ref>) concatenate features of multiple layers before computing pr 21]</ref>. We train using the union of 80k train images and a 35k subset of val images (trainval35k <ref type="bibr" target="#b1">[2]</ref>), and report ablations on a 5k subset of val images (minival
3]</ref> uses a similar method for object instance segmentation. Several other approaches (HyperNet <ref type="bibr" target="#b17">[18]</ref>, ParseNet <ref type="bibr" target="#b22">[23]</ref>, and I
then interpolating missing levels. Before HOG and SIFT, early work on face detection with ConvNets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b31">32]</ref> computed shallow n
egmentation. Several other approaches (HyperNet <ref type="bibr" target="#b17">[18]</ref>, ParseNet <ref type="bibr" target="#b22">[23]</ref>, and ION <ref type="bibr" target="#b1">[2]</ref>) concaten
p-down and skip connections are popular in recent research <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" targe f> and Sharp-Mask <ref type="bibr" target="#b27">[28]</ref> for segmentation, Recombinator networks <ref type="bibr" target="#b16">[17]</ref> for face detection, and Stacked Hourglass networks <ref ty
eatures have largely been replaced with features computed by deep convolutional networks (ConvNets) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Aside from being nvolutional fashion. This process is independent of the backbone convolutional architectures (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" ta es across scales.</p><p>Deep ConvNet object detectors. With the development of modern deep ConvNets <ref type="bibr" target="#b18">[19]</ref>, object detectors like Over-Feat <ref type="bibr" target="
pyramids (for short we call these featurized image pyramids) form the basis of a standard solution <ref type="bibr" target="#b0">[1]</ref> (Fig. <ref type="figure" target="#fig_0">1(a)</ref>). These
="#b20">[21]</ref> detection challenges use multi-scale testing on featurized image pyramids (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref>). The principle ad only at test time <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>, which creates an i s and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets <ref type="bibr" target="#b15">[16]</ref>. Our method is also easily extended to mask proposals and </ref><ref type="bibr" target="#b15">16]</ref>), and in this paper we present results using ResNets <ref type="bibr" target="#b15">[16]</ref>. The construction of our pyramid involves a bottom-up path the deepest layer of each stage should have the strongest features.</p><p>Specifically, for ResNets <ref type="bibr" target="#b15">[16]</ref> we use the feature activations output by each stage's last hoices. We have experimented with more sophisticated blocks (e.g., using multilayer residual blocks <ref type="bibr" target="#b15">[16]</ref> as the connections) and observed marginally better results an RoI with w × h = 224 2 should be mapped into. Analogous to the ResNet-based Faster R-CNN system <ref type="bibr" target="#b15">[16]</ref> that uses C 4 as the single-scale feature map, we set k 0 s) to all RoIs of all levels. Again, the heads all share parameters, regardless of their levels. In <ref type="bibr" target="#b15">[16]</ref>, a ResNet's conv5 layers (a 9-layer deep subnetwork) are a v4 features, but our method has already harnessed conv5 to construct the feature pyramid. So unlike <ref type="bibr" target="#b15">[16]</ref>, we simply adopt RoI pooling to extract 7×7 features, and ble <ref type="table" target="#tab_0">1</ref>(a, b)) using the single-scale map of C 4 (the same as <ref type="bibr" target="#b15">[16]</ref>) or C 5 , both using the same hyper-parameters as ours, in n Fast R-CNN and RPN, except when specified.</p><p>As a ResNet-based Fast R-CNN baseline, following <ref type="bibr" target="#b15">[16]</ref>, we adopt RoI pooling with an output size of 14×14 and att rget="#tab_2">3</ref>(a) shows our reproduction of the baseline Faster R-CNN system as described in <ref type="bibr" target="#b15">[16]</ref>. Under controlled settings, our FPN (Table <ref type="tabl _2">3</ref>(a) and (b) are baselines that are much stronger than the baseline provided by He et al. <ref type="bibr" target="#b15">[16]</ref> in Table 3(*). We find the following implementations contr 0">[11,</ref><ref type="bibr" target="#b15">16]</ref>; (iii) We use 5 scale anchors instead of 4 in <ref type="bibr" target="#b15">[16]</ref> (adding 32 2 ); (iv) At test time we use 1000 proposals pe b15">[16]</ref> (adding 32 2 ); (iv) At test time we use 1000 proposals per image instead of 300 in <ref type="bibr" target="#b15">[16]</ref>. So comparing with He et al.'s ResNet-50 Faster R-CNN base t="#b8">[9]</ref>, hard negative mining <ref type="bibr" target="#b34">[35]</ref>, context modeling <ref type="bibr" target="#b15">[16]</ref>, stronger data augmentation <ref type="bibr" target="#b21" e trained on the trainval35k set and use ResNet-50.</figDesc><table /><note>† Provided by authors of<ref type="bibr" target="#b15">[16]</ref>.</note></figure> <figure xmlns="http://www.tei-c.org/ns/1. chitectures (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref>), and in this paper we present results using ResNets <ref t (i) We use an image scale of 800 pixels instead of 600 in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>; (ii) We train with 512 RoIs per image which accelerate con ge which accelerate convergence, in contrast to 64 RoIs in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>; (iii) We use 5 scale anchors instead of 4 in <ref type="bi
="#b20">[21]</ref> detection challenges use multi-scale testing on featurized image pyramids (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref>). The principle ad only at test time <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>, which creates an i s and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets <ref type="bibr" target="#b15">[16]</ref>. Our method is also easily extended to mask proposals and </ref><ref type="bibr" target="#b15">16]</ref>), and in this paper we present results using ResNets <ref type="bibr" target="#b15">[16]</ref>. The construction of our pyramid involves a bottom-up path the deepest layer of each stage should have the strongest features.</p><p>Specifically, for ResNets <ref type="bibr" target="#b15">[16]</ref> we use the feature activations output by each stage's last hoices. We have experimented with more sophisticated blocks (e.g., using multilayer residual blocks <ref type="bibr" target="#b15">[16]</ref> as the connections) and observed marginally better results an RoI with w × h = 224 2 should be mapped into. Analogous to the ResNet-based Faster R-CNN system <ref type="bibr" target="#b15">[16]</ref> that uses C 4 as the single-scale feature map, we set k 0 s) to all RoIs of all levels. Again, the heads all share parameters, regardless of their levels. In <ref type="bibr" target="#b15">[16]</ref>, a ResNet's conv5 layers (a 9-layer deep subnetwork) are a v4 features, but our method has already harnessed conv5 to construct the feature pyramid. So unlike <ref type="bibr" target="#b15">[16]</ref>, we simply adopt RoI pooling to extract 7×7 features, and ble <ref type="table" target="#tab_0">1</ref>(a, b)) using the single-scale map of C 4 (the same as <ref type="bibr" target="#b15">[16]</ref>) or C 5 , both using the same hyper-parameters as ours, in n Fast R-CNN and RPN, except when specified.</p><p>As a ResNet-based Fast R-CNN baseline, following <ref type="bibr" target="#b15">[16]</ref>, we adopt RoI pooling with an output size of 14×14 and att rget="#tab_2">3</ref>(a) shows our reproduction of the baseline Faster R-CNN system as described in <ref type="bibr" target="#b15">[16]</ref>. Under controlled settings, our FPN (Table <ref type="tabl _2">3</ref>(a) and (b) are baselines that are much stronger than the baseline provided by He et al. <ref type="bibr" target="#b15">[16]</ref> in Table 3(*). We find the following implementations contr 0">[11,</ref><ref type="bibr" target="#b15">16]</ref>; (iii) We use 5 scale anchors instead of 4 in <ref type="bibr" target="#b15">[16]</ref> (adding 32 2 ); (iv) At test time we use 1000 proposals pe b15">[16]</ref> (adding 32 2 ); (iv) At test time we use 1000 proposals per image instead of 300 in <ref type="bibr" target="#b15">[16]</ref>. So comparing with He et al.'s ResNet-50 Faster R-CNN base t="#b8">[9]</ref>, hard negative mining <ref type="bibr" target="#b34">[35]</ref>, context modeling <ref type="bibr" target="#b15">[16]</ref>, stronger data augmentation <ref type="bibr" target="#b21" e trained on the trainval35k set and use ResNet-50.</figDesc><table /><note>† Provided by authors of<ref type="bibr" target="#b15">[16]</ref>.</note></figure> <figure xmlns="http://www.tei-c.org/ns/1. chitectures (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref>), and in this paper we present results using ResNets <ref t (i) We use an image scale of 800 pixels instead of 600 in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>; (ii) We train with 512 RoIs per image which accelerate con ge which accelerate convergence, in contrast to 64 RoIs in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>; (iii) We use 5 scale anchors instead of 4 in <ref type="bi
ction based models <ref type="bibr" target="#b22">[13,</ref><ref type="bibr" target="#b31">21,</ref><ref type="bibr" target="#b39">29]</ref>. Interaction based models thrive with encoding word-word tr to ranking signals <ref type="bibr" target="#b20">[11,</ref><ref type="bibr" target="#b22">13,</ref><ref type="bibr" target="#b39">29]</ref>. Learned end-to-end from user feedbacks <ref type="bibr" ta t="#b39">29]</ref>. Learned end-to-end from user feedbacks <ref type="bibr" target="#b33">[23,</ref><ref type="bibr" target="#b39">29]</ref>, the word embeddings can encode so matches tailored for rel nce than score-based ones like mean-pooling or max-pooling <ref type="bibr" target="#b22">[13,</ref><ref type="bibr" target="#b39">29]</ref>.</p><p>Kernel-pooling is applied to each M h q ,h d matrix hes are more e ective than weight-summing the similarities <ref type="bibr" target="#b22">[13,</ref><ref type="bibr" target="#b39">29]</ref>-"similarity does not necessarily mean relevance" <ref type= lored for relevance ranking, which has signi cant advantages over traditional feature-based methods <ref type="bibr" target="#b39">[29,</ref><ref type="bibr" target="#b40">30]</ref>. ese initial succe d learningto-rank techniques are then used to combine the n-gram somatches to the nal ranking score <ref type="bibr" target="#b39">[29]</ref>.</p><p>e CNN is the key to modeling n-grams. Typical IR ap [30]</ref>.</p><p>K-NRM uni ed the progress of IR customized embeddings and interaction based model <ref type="bibr" target="#b39">[29]</ref>. It rst embeds words and builds the translation matrix usi log, K-NRM outperforms both neural IR methods and feature-based learning-to-rank by a large margin <ref type="bibr" target="#b39">[29]</ref>.</p><p>ough the so matching of n-grams in information retr -torank layer to calculate the ranking score using the n-gram translations M. is part extends K-NRM <ref type="bibr" target="#b39">[29]</ref> to n-grams. Kernel-pooling is a pooling technique that use ead><p>Conv-KNRM adds the ability of so matching n-grams to the recent state-of-the-art K-NRM model <ref type="bibr" target="#b39">[29]</ref> with convolutional neural networks (CNNs). Without CNNs, C g Conv-KNRM requires large-scale training data, for example, user clicks in a commercial search log <ref type="bibr" target="#b39">[29]</ref> or industryscale annotations <ref type="bibr" target="#b31 raining data. ey are then used in the target domain to generate so -TF features Φ(M). Xiong, et al. <ref type="bibr" target="#b39">[29]</ref> showed that kernel-pooled so -TF features reveal di erent rnel is of low importance in search logs as all candidate documents already contain the query words <ref type="bibr" target="#b39">[29]</ref>; however, synonyms can be a strong signal in a recall-orie Log: Sogou.com is a major Chinese commercial search engine.</p><p>e same se ings as K-NRM were used <ref type="bibr" target="#b39">[29]</ref>. e same sample of Sogou log and training-testing splits ar IR baselines for stronger baseline performance. Body texts of training documents were not available <ref type="bibr" target="#b39">[29]</ref>. e Chinese text was segmented by ICTCLASS <ref type="bibr" ad><p>Training and testing labels on Sogou-Log and Bing-Log were generated following prior research <ref type="bibr" target="#b39">[29]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> ref type="bibr" target="#b33">[23]</ref>, DRMM <ref type="bibr" target="#b22">[13]</ref>, and K-NRM <ref type="bibr" target="#b39">[29]</ref>.</p><p>CDSSM <ref type="bibr" target="#b36">[26]</ref> is ry and document representations on their words' le er-tri-grams (or Chinese characters in Sogou-Log <ref type="bibr" target="#b39">[29]</ref>). e ranking scores are calculated by the similarity betwee erwards.</p><p>K-NRM is a state-of-the-art neural model previously tested on the Sogou-Log dataset <ref type="bibr" target="#b39">[29]</ref>. It uses kernel-pooling instead of DRMM's histogram poolin earch logs, 5-fold cross validation were used to be consistent with the previous study on Sogou-Log <ref type="bibr" target="#b39">[29]</ref>. On ClueWeb09-B, the 10-fold cross validation splits from n Sogou-Log, traditional IR methods used both title and body, and neural IR methods only used title <ref type="bibr" target="#b39">[29]</ref>, as discussed in section 5.1. On Bing-Log, all methods use were all learned end-to-end using the query logs. For Sogou-log, we set embedding dimension L = 300 <ref type="bibr" target="#b39">[29]</ref> . For Bing-Log, we set L = 100 because our pilot study sho ers were: µ 1 = 0.9, µ 2 = 0.7, ..., µ 10 = −0.9.</p><p>e σ of the so match bins were set to be 0.1 <ref type="bibr" target="#b39">[29]</ref>. Model Implementation and E ciency: e model was implemente bout 12 hours on an AWS GPU machine. e training time is similar with prior work using only unigrams <ref type="bibr" target="#b39">[29]</ref>. Most computation time was spent on the embedding layer; t rrent neural IR methods to provide additional improvements <ref type="bibr" target="#b32">[22,</ref><ref type="bibr" target="#b39">29,</ref><ref type="bibr" target="#b40">30]</ref> Comparing the two s
ddings <ref type="bibr" target="#b30">[20]</ref> are introduced to calculate the translation scores <ref type="bibr" target="#b21">[12]</ref>. How to combine the word-level translation scores to gener
available at: h p://boston.lti.cs.cmu.edu/appendices/WSDM2018-ConvKNRM/ word2vec in query expansion <ref type="bibr" target="#b19">[10]</ref>. e relevance feedback based word embeddings are then also
selines were RankSVM <ref type="bibr" target="#b25">[16]</ref> and coordinate ascent ( Coor-Ascent) <ref type="bibr" target="#b29">[19]</ref> ey used 20 features: Boolean AND; Boolean OR; Coordinate m
ddings <ref type="bibr" target="#b30">[20]</ref> are introduced to calculate the translation scores <ref type="bibr" target="#b21">[12]</ref>. How to combine the word-level translation scores to gener
ns/1.0"><head>Technical Presentation</head><p>WSDM'18, February 5-9, 2018, Marina Del Rey, CA, USA  <ref type="bibr" target="#b14">[5]</ref>, and training preference pairs were constructed accordingly h query-document pair as the relevance score. DCTR is a strong baseline in click model competitions <ref type="bibr" target="#b14">[5]</ref>.</p><p>Testing-SAME: is set of testing labels was generated er preferences.</p><p>Testing-RAW: is set of testing labels was motivated by the cascade assumption <ref type="bibr" target="#b14">[5]</ref>. Only the clicked document in a single-click session was co for each dataset. DCTR used the DCTR click model to infer scores that were mapped to 5 Likert scales<ref type="bibr" target="#b14">[5]</ref>. Clicks used the sole click in a session as the binary labe
document o en match at n-grams, such as phrases <ref type="bibr" target="#b28">[18]</ref>, concepts <ref type="bibr" target="#b11">[2]</ref>, and entities <ref type="bibr" target="#b38">[28]</ref>; ho
any learning-to-rank model such as RankSVM <ref type="bibr" target="#b25">[16]</ref> or LambdaMART <ref type="bibr" target="#b37">[27]</ref>. </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
document are combined into one vector, and the match of two vectors is done by deep neural networks <ref type="bibr" target="#b18">[9]</ref>.</p><p>e interaction based methods, on the other hand, dire </p><p>It is unfair for unsupervised <ref type="bibr" target="#b40">[30]</ref> or pseudo-supervised <ref type="bibr" target="#b18">[9]</ref> neural IR methods to compete with Conv-KNRM, which is train
document o en match at n-grams, such as phrases <ref type="bibr" target="#b28">[18]</ref>, concepts <ref type="bibr" target="#b11">[2]</ref>, and entities <ref type="bibr" target="#b38">[28]</ref>; ho
neural methods in information retrieval (neural IR) is the development of interaction based models <ref type="bibr" target="#b22">[13,</ref><ref type="bibr" target="#b31">21,</ref><ref type="bibr" ta pooling methods have shown be er performance than score-based ones like mean-pooling or max-pooling <ref type="bibr" target="#b22">[13,</ref><ref type="bibr" target="#b39">29]</ref>.</p><p>Kernel-pool he domain adaption fashion. MP and CDSSM performed worse than DRMM on TREC data in previous studies <ref type="bibr" target="#b22">[13,</ref><ref type="bibr" target="#b32">22]</ref>.</p><p>It is unfai ng the frequencies of multi-level so matches are more e ective than weight-summing the similarities <ref type="bibr" target="#b22">[13,</ref><ref type="bibr" target="#b39">29]</ref>-"similarity does n be er summarize the word translations into ranking signals <ref type="bibr" target="#b20">[11,</ref><ref type="bibr" target="#b22">13,</ref><ref type="bibr" target="#b39">29]</ref>. Learned end-to-end t neural IR methods can be categorized into two classes: representation based and interaction based <ref type="bibr" target="#b22">[13]</ref>. e earlier a empts of neural IR research were mainly about count' the word-level translation scores at di erent so match levels, instead of to weight-sum them <ref type="bibr" target="#b22">[13]</ref>. e interaction based model and the representation based mo ide so match signals for learning to rank. e kernel-pooling shares the advantage of pyramid pooling <ref type="bibr" target="#b22">[13]</ref> that it 'counts' the so matches at multiple levels, while ="bibr" target="#b36">[26]</ref>, MatchPyramid (MP) <ref type="bibr" target="#b33">[23]</ref>, DRMM <ref type="bibr" target="#b22">[13]</ref>, and K-NRM <ref type="bibr" target="#b39">[29]</ref>.</p>< similarity between the representations.</p><p>MP <ref type="bibr" target="#b33">[23]</ref> and DRMM <ref type="bibr" target="#b22">[13]</ref> are both interaction based models built upon the embedding
ith kernel machines, one typically observes an increase in performance when using soft-DTW over DTW <ref type="bibr" target="#b6">(Cuturi, 2011)</ref> for classification.</p><p>Our contributions. We e
fit discriminators that output histograms <ref type="bibr" target="#b28">(Zhang et al., 2015;</ref><ref type="bibr" target="#b20">Rolet et al., 2016)</ref>. When paired with a flexible learning archi

e alignment between two time series (the optimal alignment itself can also be of interest, see e.g. <ref type="bibr" target="#b10">Garreau et al. 2014)</ref> of respective length n and m by computing

the Dynamic Time Warping (DTW) score <ref type="bibr" target="#b23">(Sakoe &amp; Chiba, 1971;</ref><ref type="bibr" target="#b24">1978)</ref>. DTW computes the best possible alignment between two tim
e addressed by the nearest centroid classifier <ref type="bibr">(Hastie et al., 2001, p.670)</ref>, <ref type="bibr" target="#b26">(Tibshirani et al., 2002)</ref>. This method chooses the class whose

namely backpropagate. A similar idea was recently used to compute the gradient of ANOVA kernels in <ref type="bibr" target="#b3">(Blondel et al., 2016)</ref>.</p></div> <div xmlns="http://www.tei-c.o
p; Jelinek, 1975;</ref><ref type="bibr" target="#b19">Ristad &amp; Yianilos, 1998)</ref> or kernels <ref type="bibr" target="#b21">(Saigo et al., 2004;</ref><ref type="bibr" target="#b8">Cuturi et al. 2009), <ref type="bibr">Cuturi et al. (2007, Theorem 2)</ref> and the highly related formulation of <ref type="bibr" target="#b21">Saigo et al. (2004</ref><ref type="bibr">Saigo et al. ( , p.1685</ref
classifier) to predict a real or a class label output, and engineered to run faster in that context <ref type="bibr" target="#b27">(Yi et al., 1998)</ref>. Recent works by <ref type="bibr" target="#b1
sic idea is very similar to instruction fetch in Multiscalar <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>: Multiscalar divides the sequential instruction stream into i o n i s s i m i l a r t o t h a t u s e d b y Multiscalar <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>. When a fragment is renamed, the hardware determines which fragments. This is similar to the idea of traces <ref type="bibr" target="#b19">[20]</ref> or tasks <ref type="bibr" target="#b20">[21]</ref>, except that fragments are completely general, whereas the
e dynamic program are also consecutive in the static program <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref>. This rearrangement m
><p>Many enhancements to these mechanisms have been proposed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targ
rget="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>, but each of these inherits some problems from the sequenti e mapping created by I1 is available, a parallel renaming mechanism must do one of these two things <ref type="bibr" target="#b21">[22]</ref>: (1) delay renaming I2 until I1 has been renamed, or (2) r
ms have been proposed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar
ms have been proposed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar
ref type="figure">1</ref> illustrates a parallel fetch unit based on the design we proposed earlier <ref type="bibr" target="#b12">[13]</ref>. We refer the reader to the original paper for a complete n, or at the sixteenth instruction. These heuristics are discussed in more detail in our prior work <ref type="bibr" target="#b12">[13]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head cache. Depending on the benchmark, 20-70% of fragments can be reused with just 16 fragment buffers <ref type="bibr" target="#b12">[13]</ref>.</p><p>Thus, the fragment buffers act like a very small tr
e possible, but in this paper we use the trace predictor proposed by Jacobson, Rotenberg, and Smith <ref type="bibr" target="#b10">[11]</ref>. Since the trace predictor predicts trace addresses as wel
e dynamic program are also consecutive in the static program <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref>. This rearrangement m
e possible, but in this paper we use the trace predictor proposed by Jacobson, Rotenberg, and Smith <ref type="bibr" target="#b10">[11]</ref>. Since the trace predictor predicts trace addresses as wel
in the static program <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref>. This rearrangement may be done statically, for example at
tackle the challenges of multicore scaling in the dark silicon age.</p><p>Recently, Raghavan et al. <ref type="bibr" target="#b16">[17]</ref> proposed computational sprinting, in which a chip improves con.</p><p>Instead of shrinking the chip or sacrificing transistor density, computational sprinting <ref type="bibr" target="#b16">[17]</ref> embraces dark silicon by leveraging the extra transistors chip can sustain computational sprinting for one second in the worst case, which is consistent with <ref type="bibr" target="#b16">[17]</ref>. Later we will analyze how NoC-sprinting influences the sp
ystem simulator to setup a sprinting-based multicore architecture with 16 ALPHA CPUs. We use Garnet <ref type="bibr" target="#b0">[1]</ref> to model a 4 ? 4 mesh network and DSENT <ref type="bibr" tar
ated off. Recently researchers have proposed various schemes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targe
while the others remain "dark". There are some existing work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> on adapting system configurations like core count/frequency ng. While the methods to predict the application parallelism <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> is beyond the scope of this paper, we conduct off-line prof
ig_3">3</ref> shows the chip power breakdown when scaling the number of cores based on the Niagara2 <ref type="bibr" target="#b15">[16]</ref> processor. We evaluate the power dissipation with McPAT <r
2 <ref type="bibr" target="#b15">[16]</ref> processor. We evaluate the power dissipation with McPAT <ref type="bibr" target="#b12">[13]</ref> for cores, L2 caches, memory controllers (MC), NoC, and ot
oposed various schemes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref> to mitigate the lat
ig_3">3</ref> shows the chip power breakdown when scaling the number of cores based on the Niagara2 <ref type="bibr" target="#b15">[16]</ref> processor. We evaluate the power dissipation with McPAT <r
>Introduction</head><p>The continuation of technology scaling leads to a utilization wall challenge <ref type="bibr" target="#b20">[21]</ref>: to maintain a constant power envelope, the fraction of a
oposed various schemes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref> to mitigate the lat
thetic Traffic: Furthermore, we construct some synthetic traffic on a network simulator booksim 2.0 <ref type="bibr" target="#b9">[10]</ref> to test NoC-sprinting under different traffic scenarios. Fo
r" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Conventional studies concentrate on the area of multilingual r" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> for a long time, these researches are commonly limited to mak uage dependent softmax layers of SHL-MDNN are optimized jointly by multilingual datasets. SHL-MLSTM <ref type="bibr" target="#b4">[5]</ref> further explores long short-term memory (LSTM) <ref type="bi nder the condition of language information being known during training. A comparison with SHL-MLSTM <ref type="bibr" target="#b4">[5]</ref> with residual learning is investigated on CALL-HOME datasets .tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>The baseline systems come from our previous work <ref type="bibr" target="#b4">[5]</ref> and all results are summarized in Table <ref type="table" ta
et="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr"
n (MA), English (EN), Japanese (JA), Arabic (AR), German (GE) and Spanish (SP). We follow the Kaldi <ref type="bibr" target="#b18">[19]</ref> recipe to process CALLHOME datasets <ref type="foot" targe
is shown in Figure <ref type="figure" target="#fig_0">1</ref>. It stacks multihead attention (MHA) <ref type="bibr" target="#b16">[17]</ref> and position-wise, fully connected layers for both the enc decoder mask out all values corresponding to illegal connections. In addition, positional encodings <ref type="bibr" target="#b16">[17]</ref> are added to the input at the bottoms of these encoder and kens in the sequence.</p><p>The difference between the neural machine translation (NMT) Transformer <ref type="bibr" target="#b16">[17]</ref> and the ASR Transformer is the input of the encoder. We ad [24]</ref>. After trained, the last 20 checkpoints are averaged to make the performance more stable <ref type="bibr" target="#b16">[17]</ref>.</p><p>At the beginning we train the ASR Transformer on En d><p>We perform our experiments on the big model (D1024-H16) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref> of the ASR Transformer. Table <ref type="table" target="#ta
et="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr"
and warmup is used for optimization. During training, label smoothing of value ls = 0.1 is employed <ref type="bibr" target="#b23">[24]</ref>. After trained, the last 20 checkpoints are averaged to ma
ual acoustic modeling by the contextdependent deep neural network hidden Markov models (CD-DNN-HMM) <ref type="bibr" target="#b5">[6]</ref>. The hidden layers of DNN in CD-DNN-HMM can be thought of co
d n="1.">Introduction</head><p>Multilingual speech recognition has been investigated for many years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target s/1.0"><head n="2.">Related work</head><p>Although multilingual speech recognition has been studied <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target linearity, which can be used to extract universal feature transformation from multilingual datasets <ref type="bibr" target="#b0">[1]</ref>. Among the CD-DNN-HMM based approaches, the architecture of ="bibr" target="#b0">[1]</ref>. Among the CD-DNN-HMM based approaches, the architecture of SHL-MDNN <ref type="bibr" target="#b0">[1]</ref>, in which the hidden layers are shared across multiple langu
" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Conventional studies c " target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> for a long time, these
et="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr"
" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Conventional studies c " target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> for a long time, these
ighly utilized. While recent work has proposed speculation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> to cut down the router critical path delay by parallelizing speculation (SP) can be used to cut down the critical path <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. Fig. <ref type="bi e crossbar from each input. Separable VC and switch allocators modeled closely after the designs in <ref type="bibr" target="#b12">[13]</ref> are assumed as they are fast and of low complexity, while
architectural optimizations for energy into our baseline design. First, write-through input buffers <ref type="bibr" target="#b16">[17]</ref> are used, which save buffer read energy whenever a flit is a flit is able to directly bypass to the ST stage. Second, we adopt the cut-through crossbar design <ref type="bibr" target="#b16">[17]</ref>, which sacrifices the full connectivity provided by a matr nt energy savings when the upper metal interconnect is used for such metal links in onchip networks <ref type="bibr" target="#b16">[17]</ref>. EVCs essentially virtualize the physical express links of
by dedicating physical bandwidth to a message flow, suffers in throughput. Similarly, express cubes <ref type="bibr" target="#b10">[11]</ref>, in using physical express channels, trades off throughput , throughput and energy towards the ideal. However, EVCs are inspired by the express cubes topology <ref type="bibr" target="#b10">[11]</ref>. In express cubes, extra physical links that span multiple
the critical path <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. Fig. <ref type="bibr">3(d)</ref> shows the router pipeline
nnect nodes A and B that are k hops away, and prioritizing EVCs over normal virtual channels (NVCs) <ref type="bibr" target="#b8">[9]</ref> at the intermediate nodes. For instance, Fig. <ref type="fig d lines depicting EVCs, where EVCs are not additional physical channels, but virtual channels (VCs) <ref type="bibr" target="#b8">[9]</ref> that share existing physical links. Traveling from node 00 t sume flit-level<ref type="foot" target="#foot_0">1</ref> buffering and credit-based VC flow control <ref type="bibr" target="#b8">[9]</ref> at every router, as opposed to packet-level buffering. A hea two types:</p><p>? NVCs: these are VCs which are allocated just like in traditional VC flow control <ref type="bibr" target="#b8">[9]</ref> and are responsible for carrying a packet through a single h
ilar design with multiple physical express channels originating from every node.</p><p>Ogras et al. <ref type="bibr" target="#b24">[25]</ref> proposed long-range link insertion to connect non-adjacent
icated wires as the de facto interconnection fabric in general-purpose chip multi-processors (CMPs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and application-specif
limitations to wire scaling and increasing bandwidth demands <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, packet-switched on-chip networks are fast replacing shared b
architectural optimizations for energy into our baseline design. First, write-through input buffers <ref type="bibr" target="#b16">[17]</ref> are used, which save buffer read energy whenever a flit is a flit is able to directly bypass to the ST stage. Second, we adopt the cut-through crossbar design <ref type="bibr" target="#b16">[17]</ref>, which sacrifices the full connectivity provided by a matr nt energy savings when the upper metal interconnect is used for such metal links in onchip networks <ref type="bibr" target="#b16">[17]</ref>. EVCs essentially virtualize the physical express links of
ction fabric in general-purpose chip multi-processors (CMPs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and application-specific systems-on-a-chip (SoCs) <ref type="
/write power, power spent in arbitrating for VCs and switch ports, and the crossbar traversal power <ref type="bibr" target="#b18">[19]</ref>. Ideal throughput: Network throughput, which is defined as esigned and pipeline stages sized via detailed critical path analysis (see Section 2). We use Orion <ref type="bibr" target="#b18">[19]</ref>, an architecture-level network energy model, to evaluate t
" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Related to our work, for example, <ref type="bibr" target="# ibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Related to our work, for example, <ref type="bibr" target="#b6">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model rget="#b6">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model in parametric TTS <ref type="bibr" target="#b6">[7]</ref>. These studies consider learning methods within the traditio
s/1.0"><head n="2.">PROPOSED APPROACH</head><p>We use a baseline Tacotron architecture specified in <ref type="bibr" target="#b7">[8]</ref>, where we use a GMM attention <ref type="bibr" target="#b8">
bedding module. The module maps each word to a 128-dimensional vector. We also tried word2vec (W2V) <ref type="bibr" target="#b16">[17]</ref> trained on the same corpus as the word embedding module.</
to produce natural prosody with high audio fidelity using a much simplified voice building pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target ct. Specifically, we propose a simple yet effective semi-supervised framework for training Tacotron <ref type="bibr" target="#b0">[1]</ref>, a recently proposed end-to-end TTS model. We propose to tra
ation of unsupervised and weakly supervised learning for TTS <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target=
type="bibr" target="#b12">13]</ref> or language models that model grammatical and semantic context <ref type="bibr" target="#b13">[14]</ref>. These word vectors can be added as auxiliary to a TTS mod f different lengths.</p><p>For encoder conditioning, we used a neural network language model (NNLM) <ref type="bibr" target="#b13">[14]</ref> trained on English Google News 200B corpus from TensorFlow
dio fidelity using a much simplified voice building pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. However, such models t
valued word vectors that contain the meanings of the words <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> or language models that model grammatical and semantic cont
dio fidelity using a much simplified voice building pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. However, such models t
M attention <ref type="bibr" target="#b8">[9]</ref>, LSTM-based decoder with zoneout regularization <ref type="bibr" target="#b9">[10]</ref> and phoneme inputs derived from normalized text. We use Gri
="bibr" target="#b9">[10]</ref> and phoneme inputs derived from normalized text. We use Griffin-Lim <ref type="bibr" target="#b10">[11]</ref> as the inversion algorithm to convert the predicted spectr
to produce natural prosody with high audio fidelity using a much simplified voice building pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target ct. Specifically, we propose a simple yet effective semi-supervised framework for training Tacotron <ref type="bibr" target="#b0">[1]</ref>, a recently proposed end-to-end TTS model. We propose to tra
ely more effective method is to initialize the entire encoder with a pre-trained bidirectional NNLM <ref type="bibr" target="#b18">[19]</ref>. For decoder pre-training, the model mismatch during pre-t
M attention <ref type="bibr" target="#b8">[9]</ref>, LSTM-based decoder with zoneout regularization <ref type="bibr" target="#b9">[10]</ref> and phoneme inputs derived from normalized text. We use Gri
type="bibr" target="#b12">13]</ref> or language models that model grammatical and semantic context <ref type="bibr" target="#b13">[14]</ref>. These word vectors can be added as auxiliary to a TTS mod f different lengths.</p><p>For encoder conditioning, we used a neural network language model (NNLM) <ref type="bibr" target="#b13">[14]</ref> trained on English Google News 200B corpus from TensorFlow
dio fidelity using a much simplified voice building pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. However, such models t
dio fidelity using a much simplified voice building pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. However, such models t
type="bibr" target="#b12">13]</ref> or language models that model grammatical and semantic context <ref type="bibr" target="#b13">[14]</ref>. These word vectors can be added as auxiliary to a TTS mod f different lengths.</p><p>For encoder conditioning, we used a neural network language model (NNLM) <ref type="bibr" target="#b13">[14]</ref> trained on English Google News 200B corpus from TensorFlow
="bibr" target="#b9">[10]</ref> and phoneme inputs derived from normalized text. We use Griffin-Lim <ref type="bibr" target="#b10">[11]</ref> as the inversion algorithm to convert the predicted spectr
="bibr" target="#b9">[10]</ref> and phoneme inputs derived from normalized text. We use Griffin-Lim <ref type="bibr" target="#b10">[11]</ref> as the inversion algorithm to convert the predicted spectr
="bibr" target="#b9">[10]</ref> and phoneme inputs derived from normalized text. We use Griffin-Lim <ref type="bibr" target="#b10">[11]</ref> as the inversion algorithm to convert the predicted spectr
valued word vectors that contain the meanings of the words <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> or language models that model grammatical and semantic cont
>(Peters et al., 2018)</ref>, GPT <ref type="bibr" target="#b39">(Radford et al., 2018)</ref>, BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, XLM <ref type="bibr" target="#b25">(Lamp with changes in data size or composition.</p><p>We present a replication study of BERT pretraining <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, which includes a careful evaluation of t cently published methods. We release our model, pretraining and fine-tuning code.</p><p>Setup: BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> takes as input a concatenation of two seg ive training formats:</p><p>• SEGMENT-PAIR+NSP: This follows the original input format used in BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, with the NSP loss. Each input has a pair d diverse corpora, such as the ones considered in this work.</p><p>The original BERT implementation <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>  Early experiments revealed only minor di SQUAD RESULTS</head><p>We adopt a much simpler approach for SQuAD compared to past work. While BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> and XLNet <ref type="bibr" target="#b56"> submit RoBERTa to the public SQuAD 2.0 leaderboard. Most of the top systems build upon either BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> or XLNet <ref type="bibr" target="#b56">( )</ref>. This formulation significantly simplifies the task, but is not directly comparable to BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>. Following recent work, we adopt the rank training with large batch sizes.</p><p>We pretrain with sequences of at most T = 512 tokens. Unlike <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>, we do not randomly inject short sequences b4">(Bowman et al., 2015)</ref>, which require predicting relationships between pairs of sentences. <ref type="bibr" target="#b11">Devlin et al. (2019)</ref> observe that removing NSP hurts performanc sults for the four different settings. We first compare the original SEGMENT-PAIR input format from <ref type="bibr" target="#b11">Devlin et al. (2019)</ref> to the SENTENCE-PAIR format; both formats that removing the NSP loss matches or slightly improves downstream task performance, in contrast to <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>. It is possible that the original BERT imp and end-task accuracy for BERT BASE as we increase the batch size, while tuning the learning rate. <ref type="bibr" target="#b11">Devlin et al. (2019)</ref> originally trained BERT BASE for 1M steps alf as many optimization steps, thus seeing four times as many sequences in pretraining compared to <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>.</p><p>To help disentangle the importance ers). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>. We pretrain our model using 1024 V100 GPU et="#b56">Yang et al. (2019)</ref>.</p><p>For SQuAD v1.1 we follow the same finetuning procedure as <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>. For SQuAD v2.0, we additionally classify ead>Table 7 :</head><label>7</label><figDesc>Comparison between the published BERT BASE results from<ref type="bibr" target="#b11">Devlin et al. (2019)</ref> to our reimplementation with either static ranslation <ref type="bibr" target="#b30">(McCann et al., 2017)</ref>, and masked language modeling <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr" target="#b25">Lample &amp et al., 2019)</ref>. Performance is also typically improved by training bigger models on more data <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr" target="#b1">Baevski et a
type="bibr" target="#b10">(De Marneffe et al., 2019)</ref>, Choice of Plausible Alternatives (COPA)<ref type="bibr" target="#b43">(Roemmele et al., 2011)</ref>, Multi-Sentence Reading Comprehension (
e="bibr" target="#b43">(Roemmele et al., 2011)</ref>, Multi-Sentence Reading Comprehension (MultiRC)<ref type="bibr" target="#b22">(Khashabi et al., 2018)</ref>, Reading Comprehension with Commonsense
oint arithmetic on DGX-1 machines, each with 8 × 32GB Nvidia V100 GPUs interconnected by Infiniband <ref type="bibr" target="#b31">(Micikevicius et al., 2018)</ref>.</p></div> <div xmlns="http://www.t
e length during training.</p><p>Architecture: BERT uses the now ubiquitous transformer architecture <ref type="bibr" target="#b50">(Vaswani et al., 2017)</ref>, which we will not review in detail. We
jective was designed to improve performance on downstream tasks, such as Natural Language Inference <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref>, which require predicting relationships be
bibr" target="#b21">(Joshi et al., 2019)</ref>, and multiple variants of autoregressive pretraining <ref type="bibr" target="#b47">(Song et al., 2019;</ref><ref type="bibr" target="#b5">Chan et al., 2
v xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">TEXT ENCODING</head><p>Byte-Pair Encoding (BPE) <ref type="bibr" target="#b45">(Sennrich et al., 2016</ref>) is a hybrid between character-and word-
v xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">TEXT ENCODING</head><p>Byte-Pair Encoding (BPE) <ref type="bibr" target="#b45">(Sennrich et al., 2016</ref>) is a hybrid between character-and word-
="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Self-training methods such as ELMo <ref type="bibr" target="#b35">(Peters et al., 2018)</ref>, GPT <ref type="bibr" target="#b39">(Radf ing objectives, including language modeling <ref type="bibr" target="#b9">(Dai &amp; Le, 2015;</ref><ref type="bibr" target="#b35">Peters et al., 2018;</ref><ref type="bibr" target="#b19">Howard &amp;
="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Self-training methods such as ELMo <ref type="bibr" target="#b35">(Peters et al., 2018)</ref>, GPT <ref type="bibr" target="#b39">(Radf ing objectives, including language modeling <ref type="bibr" target="#b9">(Dai &amp; Le, 2015;</ref><ref type="bibr" target="#b35">Peters et al., 2018;</ref><ref type="bibr" target="#b19">Howard &amp;
hing that often occurs in direct-mapped buffers. A 3-bit performance counter based on Heil's design <ref type="bibr" target="#b16">[17]</ref> tracks the effectiveness of each entry and is used to sele pendence chain. If the generating values are present then ARVI's predictions are near perfect. Heil <ref type="bibr" target="#b16">[17]</ref> proposed another approach that correlates on the differenc
ing the register set tag</head><p>Differentiating paths to a branch can improve prediction accuracy <ref type="bibr" target="#b23">[24]</ref>. ARVI uses the set of registers from the RSE as a path dif Most current dynamic branch predictors use some combination of the branch address, path information <ref type="bibr" target="#b23">[24]</ref>, and the local/global history <ref type="bibr" target="#b2
h on-line data dependence information. These include dynamic scheduling, selective value prediction <ref type="bibr" target="#b5">[6]</ref>, criticality measures and their application <ref type="bibr" y low prediction accuracy (in general) makes it imperative that it be applied wisely. Calder et al. <ref type="bibr" target="#b5">[6]</ref> restrict value prediction to instructions whose early resolu
off pipeline resources based on recent IPC performance in order to save power. Similarly, Folegnani <ref type="bibr" target="#b11">[12]</ref> dynamically adapts the size of the issue queue according t
get="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref>, only small incremental improvements have been realized wit
processors.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>In <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>, limited data depe /ref>, limited data dependence information is used to reduce the wakeup time of the issue queue. In <ref type="bibr" target="#b14">[15]</ref>, for each instruction in the issue queue, a matrix tracks
nce the L1 hybrid is used to filter easily predicted highly biased branches, a confidence estimator <ref type="bibr" target="#b13">[14]</ref> indicates whether the branch is more difficult to predict ch prediction process involve correlating the actual branch register values with the branch outcome <ref type="bibr" target="#b13">[14]</ref> using a conventional value predictor. The authors of the s
redictor designs appear to be reaching the limit relative to the type of input information provided <ref type="bibr" target="#b7">[8]</ref>.</p><p>Related approaches that include additional informatio
alue prediction <ref type="bibr" target="#b5">[6]</ref>, criticality measures and their application <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" ta r" target="#b29">30]</ref> in order to improve load performance. Other researchers, including Bodik <ref type="bibr" target="#b10">[11]</ref>, have proposed techniques for identifying critical instruc
mprovement for the SPEC95 integer benchmarks as compared to the stateof-the-art two-level predictor <ref type="bibr" target="#b25">[26]</ref> proposed for the Alpha EV8.</p><p>The rest of this paper i ch prediction is a hybrid predictor based on the Alpha EV8 branch predictor design called 2Bc-gskew <ref type="bibr" target="#b25">[26]</ref>. There are three predictor tables and one table that contr h address, path information <ref type="bibr" target="#b23">[24]</ref>, and the local/global history <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36]</ref> of branch outcomes
/ref>, as the inherent program characteristic that fundamentally limits ILP gains. Value prediction <ref type="bibr" target="#b21">[22]</ref>, though a promising approach for alleviating data dependen
systems. A more detailed description of the protocol and a proof of its security were presented in <ref type="bibr" target="#b12">[14]</ref>, though the published version of that paper did not presen s modifications to the group in the PVL.</p><p>A proof of the consistency protocol was presented in <ref type="bibr" target="#b12">[14]</ref>, but is beyond the scope of this paper. At a high level, h
stem across people's unreliable desktop machines. Several new distributed hash tables such as Chord <ref type="bibr" target="#b24">[26]</ref> and Pastry <ref type="bibr" target="#b22">[24]</ref> show
cryptographic storage techniques could be applied to achieve some degree of privacy in SUNDR, e.g. <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b0">1]</ref>. Extensive work has y remote disks. Users on multiple clients cannot simultaneously access the same file system. Plutus <ref type="bibr" target="#b10">[12]</ref> is secure file sharing system for untrusted storage which
erver. Unlike previous Byzantine-fault-tolerant file systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">22]</ref> that distribute trust but assume that a threshold fraction the file system state if users have no other evidence of each other's on-line activity.</p><p>Pond <ref type="bibr" target="#b20">[22]</ref> is a prototype of the OceanStore system, a large-scale dat
erver. Unlike previous Byzantine-fault-tolerant file systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">22]</ref> that distribute trust but assume that a threshold fraction the file system state if users have no other evidence of each other's on-line activity.</p><p>Pond <ref type="bibr" target="#b20">[22]</ref> is a prototype of the OceanStore system, a large-scale dat
growing interest in peer-to-peer storage systems comprised of potentially untrusted nodes. Farsite <ref type="bibr" target="#b1">[2]</ref> investigated the possibility of spreading such a file system
alicious parties entirely control the server. Unlike previous Byzantine-fault-tolerant file systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">22]</ref> that distribute trus uspect their system cannot provide consistency.</p><p>The Byzantine fault-tolerant file system, BFS <ref type="bibr" target="#b2">[3]</ref>, uses replication to ensure the integrity of a network file grity without touching the entire file system. Duchamp <ref type="bibr" target="#b5">[6]</ref>, BFS <ref type="bibr" target="#b2">[3]</ref>, SFSRO <ref type="bibr" target="#b7">[9]</ref> and TDB <ref
haring to a CFS-like file system, but trusts the server for the integrity of read-shared data. SNAD <ref type="bibr" target="#b14">[16]</ref> can use digital signatures for integrity, but does not gua
cryptographic storage techniques could be applied to achieve some degree of privacy in SUNDR, e.g. <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b0">1]</ref>. Extensive work has y remote disks. Users on multiple clients cannot simultaneously access the same file system. Plutus <ref type="bibr" target="#b10">[12]</ref> is secure file sharing system for untrusted storage which
BFS <ref type="bibr" target="#b2">[3]</ref>, SFSRO <ref type="bibr" target="#b7">[9]</ref> and TDB <ref type="bibr" target="#b11">[13]</ref> have all made use of hash trees for comparing data or chec
UNDR stores all long-lived data in an append-only block log. Thus, it could use append-only storage <ref type="bibr" target="#b25">[27]</ref> to gain resilience to network attacks. Incremental off-sit
vily dependent on the end-to-end data.</p><p>As our second contribution, we apply a two-stage model <ref type="bibr" target="#b23">(Tu et al., 2017;</ref><ref type="bibr" target="#b10">Kano et al., 20 intermediate representation closely tied to the source text. The architecture has been proposed by <ref type="bibr" target="#b23">Tu et al. (2017)</ref> to realize a reconstruction objective, and a s iliary ASR and MT training data ( §3). This model is similar to the architecture first described by <ref type="bibr" target="#b23">Tu et al. (2017)</ref>. It combines two encoder-decoder models in a c , we apply beam search only for the second stage decoder. We do not use the two-phase beam search of<ref type="bibr" target="#b23">Tu et al. (2017)</ref> because of its prohibitive memory requirements

siderable efforts. We note that other research groups have encountered similar replicability issues <ref type="bibr" target="#b4">(Bansal et al., 2018)</ref>, explanations include the lack of a large

ls <ref type="bibr" target="#b24">(Weiss et al., 2017)</ref>, some work in favor of cascaded models <ref type="bibr" target="#b10">(Kano et al., 2017;</ref><ref type="bibr" target="#b6">Bérard et al., cond contribution, we apply a two-stage model <ref type="bibr" target="#b23">(Tu et al., 2017;</ref><ref type="bibr" target="#b10">Kano et al., 2017)</ref> as an alternative solution to our problem, h > to realize a reconstruction objective, and a similar model was also applied to speech translation <ref type="bibr" target="#b10">(Kano et al., 2017)</ref> to ease trainability, although no experimen lly leading to better data efficiency.</p><p>Note that somewhat related to our multi-task strategy, <ref type="bibr" target="#b10">Kano et al. (2017)</ref> have decomposed their two-stage model in a s books with direct models and report reasonable results, but do not outperform a cascaded baseline. <ref type="bibr" target="#b10">Kano et al. (2017)</ref> have first used a basic two-stage model for question has been investigated previously <ref type="bibr" target="#b24">(Weiss et al., 2017;</ref><ref type="bibr" target="#b10">Kano et al., 2017;</ref><ref type="bibr" target="#b6">Bérard et al.,


oot" n="3" xml:id="foot_2">We also experimented with a final fine-tuning phase on only the main task<ref type="bibr" target="#b16">(Niehues and Cho, 2017)</ref>, but discarded this strategy for lack o


ill serve as our baselines. All models are based on the attentional encoder-decoder architecture of <ref type="bibr" target="#b3">Bahdanau et al. (2015)</ref> with character-level outputs, and use the
fic and latency costs of accessing an off-chip Markov table <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43]</ref> GHB-Based Temporal Prefetchers. Wenisch et al. find that ta
type="bibr" target="#b44">45]</ref>. Their STMS prefetcher <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> instead uses a global history buffer to record a history of
or SPEC benchmarks we use the reference input set. For all single-core benchmarks, we use SimPoints <ref type="bibr" target="#b40">[41]</ref> to find representative regions. Each SimPoint is warmed up
type="bibr" target="#b44">45]</ref>. Their STMS prefetcher <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> instead uses a global history buffer to record a history of
g compiler hints or hardware structures to detect pointers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" tar "#b13">14,</ref><ref type="bibr" target="#b37">38]</ref>. For example, Content Directed Prefetching <ref type="bibr" target="#b11">[12]</ref> searches the content of cache lines for pointer addresses
ry accesses can be prefetched by exploiting spatial locality <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" targe
8,</ref><ref type="bibr" target="#b41">42]</ref> and strided <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar
management scheme that includes a metadata prefetcher, reducing metadata traffic from 482% to 156% <ref type="bibr" target="#b46">[47]</ref>.</p><p>Unfortunately, the presence of off-chip metadata in the metadata, which can include both a metadata cache replacement policy and a metadata prefetcher <ref type="bibr" target="#b46">[47]</ref>.</p><p>In this paper, we present a new temporal data prefe etary simulator for single-core simulations and the ChampSim simulator for multi-core simulations.  <ref type="bibr" target="#b46">[47]</ref>). ? On a 4-core system running CloudSuite server benchmark utilization is quite poor due to the absence of spatial locality in the metadata cache.</p><p>MISB <ref type="bibr" target="#b46">[47]</ref> addresses these issues by divorcing ISB's metadata cache f <ref type="bibr" target="#b44">[45]</ref>, Domino <ref type="bibr" target="#b3">[4]</ref>, and MISB <ref type="bibr" target="#b46">[47]</ref>. STMS, Domino, and MISB represent the state-of-the-art in nst overly optimistic idealized versions of STMS and Domino and against a realistic version of MISB <ref type="bibr" target="#b46">[47]</ref>. We see that Triage outperforms idealized STMS and Domino of the coverage for state-of-the-art temporal prefetchers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47]</ref> comes from a small number of metadata entries, so it is pos in off-chip memory <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>, but the use of off-chip metadata has limited the commercia lization by proposing a new off-chip metadata organization <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47]</ref>. In particular, ISB maps PClocalized correlated address pai data organizations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>, our table-based organization avoids metadata redundancy by
type="figure" target="#fig_0">1</ref>). Second, the marginal utility of the last-level cache (LLC) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref> is typically outwe
predict sequential <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42]</ref> and strided <ref type="bibr" target="#b2">[3,</ref><ref typ
g compiler hints or hardware structures to detect pointers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" tar "#b13">14,</ref><ref type="bibr" target="#b37">38]</ref>. For example, Content Directed Prefetching <ref type="bibr" target="#b11">[12]</ref> searches the content of cache lines for pointer addresses
tion that is at the head of the instruction queue 6cycles after the one that triggered the overflow <ref type="bibr" target="#b11">[12]</ref>. On an Intel Core 2 machine, we observed a similar phenome the other hand, provide instructionbased sampling (IBS) which is similar to the ProfileMe approach <ref type="bibr" target="#b11">[12]</ref>. Unfortunately, this facility only allows sam-pling instru sed profiling. ProfileMe was proposed hardware support to allow accurate instruction-level sampling <ref type="bibr" target="#b11">[12]</ref> for Alpha processors. AMD adopts the ProfileMe approach in
ted profile collection run typically incurs significant overhead (reported to range from 9% to 105% <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, but has been obser
es it consumes. As a result, this phenomenon does not affect the precision of the collected profile <ref type="bibr" target="#b2">[3]</ref>. However, for INST_RETIRED event sampling, the effects of sk by approximating the amount of time that an instruction spends at the head of the instruction queue <ref type="bibr" target="#b2">[3]</ref>. Unfortunately, estimating this quantity on a modern out-ofo
ment policy. Sampling based value profiling is proposed to pursue better efficiency and flexibility <ref type="bibr" target="#b6">[7]</ref>. However, it still incurs an average overhead of around 10%.
stability and accuracy of hardware performance counters <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>. In that work, the authors measured the total number of in
shows the quality of the raw profiles (converted to an edge profile using static profile heuristics <ref type="bibr" target="#b26">[27]</ref>). On comparing the first and second bars, we see that, on
ment policy. Sampling based value profiling is proposed to pursue better efficiency and flexibility <ref type="bibr" target="#b6">[7]</ref>. However, it still incurs an average overhead of around 10%.
cting profiles, recent work has studied the stability and accuracy of hardware performance counters <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>. In that work,
rry out a whole-program analysis, which is usually extremely expensive and not scalable.</p><p>LIPO <ref type="bibr" target="#b17">[18]</ref> is a technique aimed at using a lightweight approach to pe
stability and accuracy of hardware performance counters <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>. In that work, the authors measured the total number of in
oy in actual production environments. Synchronous sampling <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref> using instrumentation has been proposed to reduce overhead
h node features and graph topological structural information to make predictions. Velickovic et al. <ref type="bibr" target="#b28">[27]</ref> adopt attention mechanism into graph learning, and propose ctions of the contents of a neighborhood or sequence. Therefore, they are adaptive to the contents. <ref type="bibr" target="#b28">[27]</ref> adapts an attention mechanism to graph learning and propos thus can help stabilize the process, compared with the previously used row normalization as in GAT <ref type="bibr" target="#b28">[27]</ref>:</p><formula xml:id="formula_4">E ijp = Êijp N j=1 Êijp<la ention based EGNN layer</head><p>We describe the attention based EGNN layer. The original GAT model <ref type="bibr" target="#b28">[27]</ref> is only able to handle one dimensional binary edge feature ula xml:id="formula_9">X l−1 i• , X l−1 j•</formula><p>and E ijp . In existing attention mechanisms <ref type="bibr" target="#b28">[27]</ref>, the attention coefficient depends on X i• and X j• only. yer. Indeed, the essential difference between GCN <ref type="bibr" target="#b18">[18]</ref> and GAT <ref type="bibr" target="#b28">[27]</ref> is whether we use the attention coefficients (i.e., matrix The three citation network datasets are also used in <ref type="bibr" target="#b33">[32]</ref> [18] <ref type="bibr" target="#b28">[27]</ref>. However, they all use a pre-processed version which disca class.</p><p>The baseline methods we used are GCN <ref type="bibr" target="#b18">[18]</ref> and GAT <ref type="bibr" target="#b28">[27]</ref>. To investigate the effectivenesses of each components, we
hebyshev polynomials to the graph Laplacian, spatially localized filtering is obtained. Kipf et al. <ref type="bibr" target="#b18">[18]</ref> approximate the polynomials using a re-normalized first-or rmula_4">E ijp = Êijp N j=1 Êijp<label>(5)</label></formula><p>or symmetric normalization as in GCN <ref type="bibr" target="#b18">[18]</ref>:</p><formula xml:id="formula_5">E ijp = Êijp N i=1 Êijp N e our EGNN(C) layer from the formula of EGNN(A) layer. Indeed, the essential difference between GCN <ref type="bibr" target="#b18">[18]</ref> and GAT <ref type="bibr" target="#b28">[27]</ref> is wheth nd 20% sized subsets, which is called "dense" splitting.</p><p>Following the experiment settings of <ref type="bibr" target="#b18">[18]</ref>[27], we use two layers of EGNN in all of our experiments f nd are penalized more in the loss than a majority class.</p><p>The baseline methods we used are GCN <ref type="bibr" target="#b18">[18]</ref> and GAT <ref type="bibr" target="#b28">[27]</ref>. To inve
k <ref type="bibr" target="#b22">[22]</ref>, node2vec <ref type="bibr" target="#b1">[2]</ref>, LINE <ref type="bibr" target="#b27">[26]</ref>, and HARP <ref type="bibr" target="#b9">[9]</ref>. There a
dom Forest is a traditional learning model which is widely applied to various problems. Weave model <ref type="bibr" target="#b15">[15]</ref> is similar to graph convolution but specifically designed
k <ref type="bibr" target="#b22">[22]</ref>, node2vec <ref type="bibr" target="#b1">[2]</ref>, LINE <ref type="bibr" target="#b27">[26]</ref>, and HARP <ref type="bibr" target="#b9">[9]</ref>. There a
yer before being fed to next layer.</p><p>in image recognition, and recurrent neural networks (RNN) <ref type="bibr" target="#b12">[12]</ref> and Long Short Term Memory (LSTM) <ref type="bibr" target=
"#b4">[4]</ref>, Graph Factorization (GF) algorithm <ref type="bibr" target="#b1">[2]</ref>, GraRep <ref type="bibr" target="#b7">[7]</ref>, and HOPE <ref type="bibr" target="#b21">[21]</ref>. Another
k <ref type="bibr" target="#b22">[22]</ref>, node2vec <ref type="bibr" target="#b1">[2]</ref>, LINE <ref type="bibr" target="#b27">[26]</ref>, and HARP <ref type="bibr" target="#b9">[9]</ref>. There a
t neural networks (RNN) <ref type="bibr" target="#b12">[12]</ref> and Long Short Term Memory (LSTM) <ref type="bibr" target="#b14">[14]</ref> in natural language processing. In real world, many proble
n employing a flexible, stochastic measure of node similarity based on random walks, e.g., DeepWalk <ref type="bibr" target="#b22">[22]</ref>, node2vec <ref type="bibr" target="#b1">[2]</ref>, LINE <r
n employing a flexible, stochastic measure of node similarity based on random walks, e.g., DeepWalk <ref type="bibr" target="#b22">[22]</ref>, node2vec <ref type="bibr" target="#b1">[2]</ref>, LINE <r
"#b4">[4]</ref>, Graph Factorization (GF) algorithm <ref type="bibr" target="#b1">[2]</ref>, GraRep <ref type="bibr" target="#b7">[7]</ref>, and HOPE <ref type="bibr" target="#b21">[21]</ref>. Another
idden layers.</p><p>Our models are compared with two baseline models which are shown in MoleculeNet <ref type="bibr" target="#b32">[31]</ref>: Random Forest and Weave. Random Forest is a traditional l
c <ref type="bibr" target="#b1">[2]</ref>, LINE <ref type="bibr" target="#b27">[26]</ref>, and HARP <ref type="bibr" target="#b9">[9]</ref>. There are several limitations in matrix factorization-based
"#b4">[4]</ref>, Graph Factorization (GF) algorithm <ref type="bibr" target="#b1">[2]</ref>, GraRep <ref type="bibr" target="#b7">[7]</ref>, and HOPE <ref type="bibr" target="#b21">[21]</ref>. Another
extract graph statistics (e.g., degrees) <ref type="bibr" target="#b5">[5]</ref>, kernel functions <ref type="bibr" target="#b29">[28]</ref> <ref type="bibr" target="#b25">[24]</ref> or other hand-cr
from the node similarity matrix or adjacency matrix, e.g., Deep Neural Graph Representations (DNGR) <ref type="bibr" target="#b8">[8]</ref> and Structural Deep Network Embeddings (SDNE) <ref type="bib 11)</label></formula><p>where L is the LeakyReLU activation function; W l is the same mapping as in <ref type="bibr" target="#b8">(8)</ref>.</p><p>The attention coefficients will be used as new edge f
t neural networks (RNN) <ref type="bibr" target="#b12">[12]</ref> and Long Short Term Memory (LSTM) <ref type="bibr" target="#b14">[14]</ref> in natural language processing. In real world, many proble
n employing a flexible, stochastic measure of node similarity based on random walks, e.g., DeepWalk <ref type="bibr" target="#b22">[22]</ref>, node2vec <ref type="bibr" target="#b1">[2]</ref>, LINE <r
ations (DNGR) <ref type="bibr" target="#b8">[8]</ref> and Structural Deep Network Embeddings (SDNE) <ref type="bibr" target="#b31">[30]</ref>. Although autoencoder-based approaches are able to capture
t neural networks (RNN) <ref type="bibr" target="#b12">[12]</ref> and Long Short Term Memory (LSTM) <ref type="bibr" target="#b14">[14]</ref> in natural language processing. In real world, many proble

global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type="bibr" target="#b0">[1]</ref>, and it was used in <ref type="bibr" target="#b1">[2]</ref> nd of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type="bibr" target="#b0">(1)</ref>. Because of memory limitations on our machines, when |S| exc rting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type="bibr" target="#b0">(1)</ref>, and so now we generate results by performing a second pass
ginally introduced for text compression <ref type="bibr" target="#b0">[1]</ref>, and it was used in <ref type="bibr" target="#b1">[2]</ref> for branch prediction. Figure <ref type="figure" target="#fi
a synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type="bibr" target="#b2">[3]</ref>, while YAGS <ref type="bibr" target="#b3">[4]</ref>, which i are not folding a random value, but a global history value derived from the previous history value <ref type="bibr" target="#b2">[3]</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows two
a synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type="bibr" target="#b2">[3]</ref>, while YAGS <ref type="bibr" target="#b3">[4]</ref>, which i are not folding a random value, but a global history value derived from the previous history value <ref type="bibr" target="#b2">[3]</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows two
tion. The leftmost bank on Figure <ref type="figure" target="#fig_0">1</ref> is a bimodal predictor <ref type="bibr" target="#b4">[5]</ref>. We refer to this bank as bank 0. It has 4k entries, and is of groups of consecutive history bits, then it is XORed with the branch PC as in a gshare predictor <ref type="bibr" target="#b4">[5]</ref>. For example, bank 3 is indexed with 40 history bits, and th
re exists a longer matching sequence in T , i.e., s∈T f (s.0 u ). More explanations can be found in <ref type="bibr" target="#b6">[7]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n=
ginally introduced for text compression <ref type="bibr" target="#b0">[1]</ref>, and it was used in <ref type="bibr" target="#b1">[2]</ref> for branch prediction. Figure <ref type="figure" target="#fi
X, the one that provided the final prediction, and only that counter. This is the classical method <ref type="bibr" target="#b5">[6]</ref> : the counter is incremented if the branch is taken, decreme
X, the one that provided the final prediction, and only that counter. This is the classical method <ref type="bibr" target="#b5">[6]</ref> : the counter is incremented if the branch is taken, decreme
ginally introduced for text compression <ref type="bibr" target="#b0">[1]</ref>, and it was used in <ref type="bibr" target="#b1">[2]</ref> for branch prediction. Figure <ref type="figure" target="#fi
re exists a longer matching sequence in T , i.e., s∈T f (s.0 u ). More explanations can be found in <ref type="bibr" target="#b6">[7]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n=
comes unfeasible. A recent relaxation in language modeling <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref> turns the prediction problem on its head. First, instead of
uire a lock to access the model shared parameters, ASGD will achieve an optimal rate of convergence <ref type="bibr" target="#b37">[36]</ref>. While we run experiments on one machine using multiple th
re directly.</p><p>For all models we use a one-vs-rest logistic regression implemented by LibLinear <ref type="bibr" target="#b11">[11]</ref> extended to return the most probable labels as in <ref typ
gh neural networks, these networks have made advancements in diverse fields such as computer vision <ref type="bibr" target="#b23">[22]</ref>, speech recognition <ref type="bibr" target="#b8">[8]</ref
labels represent the interest groups of the users such as 'black and white photos'.</p><p>• YouTube <ref type="bibr" target="#b41">[40]</ref> is a social network between users of the popular video sha atures assumes that modular graph partitions will be useful for classification.</p><p>• EdgeCluster <ref type="bibr" target="#b41">[40]</ref>: This method uses k-means clustering to cluster the adjace t="#b13">[13,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b40">[39]</ref><ref type="bibr" target="#b41">[40]</ref><ref type="bibr" target="#b42">[41]</ref>. In contrast to t on and are offline<ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b40">[39]</ref><ref type="bibr" target="#b41">[40]</ref><ref type="bibr" target="#b42">[41]</ref></note> 			<note x e the exact same datasets and experimental procedure as in <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b41">40]</ref>. Specifically, we randomly sample a portion (TR) of the lab and Micro-F1. When possible we report the original results <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b41">40]</ref> here directly.</p><p>For all models we use a one-vs-rest lo
tional classification algorithms to our work incorporate community information by learning clusters <ref type="bibr" target="#b34">[33]</ref>, by adding edges between nearby nodes <ref type="bibr" tar
rformance in real networks, and has been advocated as a sensible relational classification baseline <ref type="bibr" target="#b27">[26]</ref>.</p><p>• Majority: This naïve method simply chooses the mo
uire a lock to access the model shared parameters, ASGD will achieve an optimal rate of convergence <ref type="bibr" target="#b37">[36]</ref>. While we run experiments on one machine using multiple th
l parameter set is θ = {Φ, Ψ} where the size of each is O(d|V |). Stochastic gradient descent (SGD) <ref type="bibr" target="#b5">[5]</ref> is used to optimize these parameters (Line 4, Algorithm 2).
<p>Distributed representations have been proposed to model structural relationship between concepts <ref type="bibr" target="#b19">[18]</ref>. These representations are trained by the back-propagation
l parameter set is θ = {Φ, Ψ} where the size of each is O(d|V |). Stochastic gradient descent (SGD) <ref type="bibr" target="#b5">[5]</ref> is used to optimize these parameters (Line 4, Algorithm 2).
the activations of deep neural networks. For this, we consider the "natural pre-image" technique of <ref type="bibr" target="#b20">[21]</ref>, whose goal is to characterize the invariants learned by a n untrained deep convolutional generator can be used to replace the surrogate natural prior used in <ref type="bibr" target="#b20">[21]</ref> (the TV norm) with dramatically improved results. Since th antic segmentation) is highly detrimental.</p><p>Natural pre-image. The natural pre-image method of <ref type="bibr" target="#b20">[21]</ref> is a diagnostic tool to study the invariances of a lossy f e obtained by restricting the pre-image to a set X of natural images, called a natural pre-image in <ref type="bibr" target="#b20">[21]</ref>.</p><p>In practice, finding points in the natural pre-imag inding points in the natural pre-image can  Inversion with deep image prior Inversion with TV prior <ref type="bibr" target="#b20">[21]</ref> Pre-trained deep inverting network <ref type="bibr" target on ImageNet ISLVRC) using three different regularizers: the Deep Image prior, the TV norm prior of <ref type="bibr" target="#b20">[21]</ref>, and the network trained to invert representations on a ho ne by regularizing the data term similarly to the other inverse problems seen above. The authors of <ref type="bibr" target="#b20">[21]</ref> prefer to use the TV norm, which is a weak natural image p
onnections between ConvNets and convolutional sparse coding run even deeper and are investigated in <ref type="bibr" target="#b23">[24]</ref> in the context of recognition networks, and more recently
onnections between ConvNets and convolutional sparse coding run even deeper and are investigated in <ref type="bibr" target="#b23">[24]</ref> in the context of recognition networks, and more recently
lone is insufficient to explain the good performance of deep networks. For instance, the authors of <ref type="bibr" target="#b32">[33]</ref> recently showed that the same image classification network
t in inverse image reconstruction problems such as denoising <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> or single-image super-resolution <ref type="bibr" target="# to shallow ConvNets to the reconstructed image <ref type="bibr" target="#b24">[25]</ref>. The work <ref type="bibr" target="#b19">[20]</ref> investigates the model that combines Con-vNet with a self-
econstructing an image from its activations within certain deep networks or from its HOG descriptor <ref type="bibr" target="#b7">[8]</ref>. More generally, ConvNets with similar architectures are now sulting visualizations avoid potential biases arising form the use of powerful learned regularizers <ref type="bibr" target="#b7">[8]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n= nversion with TV prior <ref type="bibr" target="#b20">[21]</ref> Pre-trained deep inverting network <ref type="bibr" target="#b7">[8]</ref> Figure <ref type="figure">9</ref>: AlexNet inversion. Given bibr" target="#b20">[21]</ref>, and the network trained to invert representations on a hold-out set <ref type="bibr" target="#b7">[8]</ref>. The reconstructions obtained with the deep image prior are ef>. The reconstructions obtained with the deep image prior are in many ways at least as natural as <ref type="bibr" target="#b7">[8]</ref>, yet they are not biased by the learning process.</p><p>be d m, which is a weak natural image prior, but is relatively unbiased. On the contrary, papers such as <ref type="bibr" target="#b7">[8]</ref> learn to invert a neural network from examples, resulting in training set. On the other hand, it results in inversions at least as interpretable as the ones of <ref type="bibr" target="#b7">[8]</ref>.</p><p>For evaluation, our method is compared to the ones of For evaluation, our method is compared to the ones of <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b7">[8]</ref>. Figure <ref type="figure">9</ref> shows the results of inve still interpretable. Our approach also produces more informative inversions than a learned prior of <ref type="bibr" target="#b7">[8]</ref>, which have a clear tendency to regress to the mean.</p><p>F
" target="#b19">20]</ref> or single-image super-resolution <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18]</ref>. ConvNets have also state-of-the art learning-based methods SRResNet <ref type="bibr" target="#b18">[19]</ref>, LapSRN <ref type="bibr" target="#b28">[29]</ref> is presented in fig. <ref type="figure" target="#fig_4">5<
inside corrupted image <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>, which are particularly useful when the corruption process i
Lanczos, are differentiable.</p><p>We evaluate super-resolution ability of our approach using Set5 <ref type="bibr" target="#b1">[2]</ref> and Set14 <ref type="bibr" target="#b31">[32]</ref> datasets
ef><ref type="bibr" target="#b31">32]</ref> as well as methods based on convolutional sparse coding <ref type="bibr" target="#b30">[31]</ref>, which can also fit statistical models similar to shallow
on based on pairs of images. In particular, we consider flash-no flash image pair-based restoration <ref type="bibr" target="#b25">[26]</ref>, where the goal is to obtain an image of a scene with the
. , h T ′ ) (T ′ ≤ T ), interleaved with subsampling layers to reduce the computational complexity <ref type="bibr" target="#b25">[26]</ref>. The decoder network generates a probability distribution
with its extremely simplified training and decoding schemes <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. This is very attract a tunable parameter λ (0 ≤ λ ≤ 1) to avoid generating incomplete and repeated hypotheses as follows <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_1">ln PASR(y|x) = (1 − λ) ln PS
peech (TTS) synthesis is investigated in the S2S framework <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Since we are interested in the usage of linguistic context
with its extremely simplified training and decoding schemes <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. This is very attract a tunable parameter λ (0 ≤ λ ≤ 1) to avoid generating incomplete and repeated hypotheses as follows <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_1">ln PASR(y|x) = (1 − λ) ln PS
arget="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> and language feature vectors (LFV) <ref type="bibr" target="#b13">[14]</ref> (cross-lingual adaptation). To obtain a multilingual S2S m
showing promising results in many tasks with its extremely simplified training and decoding schemes <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" targe
systems based on the sequence-to-sequence (S2S) architecture <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> are filling up the gap of performance from the conventional H r summarizing notable parts from the encoder states h. We adopt the location-based scoring function <ref type="bibr" target="#b1">[2]</ref>. To encourage monotonic alignments, the auxiliary Connection
ingual model such as multilingual bottleneck features (BNF) <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t
multilingual model to a new language using transfer learning <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" targe ="#tab_0">1</ref>. Our new systems (line 2) significantly outperformed the old baseline reported on <ref type="bibr" target="#b6">[7]</ref>. The gain mostly came from adding VGG blocks before BLSTM en e <ref type="table" target="#tab_0">1</ref>. When compared to the previous work using the same data <ref type="bibr" target="#b6">[7]</ref>, CF-transfer yielded 21.6% gains relatively on average. Furt
entation of speech data based on text-tospeech (TTS) synthesis is investigated in the S2S framework <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Since we are inte
.org/ns/1.0"><head n="5.1.">Experimental setting</head><p>We used data from the IARPA BABEL project <ref type="bibr" target="#b28">[29]</ref> and selected 10 languages as non-target languages for trai
ug B, and drug C has a similar structure to drug A, there is likely a DDI between drug C and drug B <ref type="bibr" target="#b5">[8]</ref>. Vilar et al. predicted DDIs with a matrix transformation ap a matrix transformation approach using structural similarities of drugs with molecular fingerprints <ref type="bibr" target="#b5">[8]</ref>. In subsequent studies, the authors reported prediction meth ports in literature and databases. Comparing to the prediction results in the study by Vilar et al. <ref type="bibr" target="#b5">[8]</ref>, which also used drug structural similarities, two of the to rednisone, and lovastatin) were predicted to have DDI with simvastatin in the study by Vilar et al. <ref type="bibr" target="#b5">[8]</ref>.</p><p>These two case studies suggested that our approach co
, metabolism, and excretion (ADME). DDI occurs when two drugs share the same mechanism of excretion <ref type="bibr" target="#b1">[4]</ref>. A significant number of studies on PK-based DDI have been d n gastric pH caused by a drug can affect the gastro-intestinal absorption of a co-administered drug <ref type="bibr" target="#b1">[4]</ref>. If two drugs both binding to a same plasma protein are co-a a same plasma protein are co-administered, the concentration of the free drugs in plasma may change <ref type="bibr" target="#b1">[4]</ref>. Also, various drugs are substrates, inhibitors, or inducers
DI knowledge including types, mechanisms, and applications of DDIs using semantic web rule language <ref type="bibr" target="#b12">[15]</ref>. Huang et al. predicted DDI using protein-protein interact
DI knowledge including types, mechanisms, and applications of DDIs using semantic web rule language <ref type="bibr" target="#b12">[15]</ref>. Huang et al. predicted DDI using protein-protein interact
and close monitoring. It is reported that quercetin displaces warfarin bound to human serum albumin <ref type="bibr" target="#b22">[26]</ref> due to competitive binding and that genistein also shares eved from PharmGKB <ref type="bibr" target="#b34">[38]</ref> (3262 associations, downloaded on Sep. <ref type="bibr" target="#b22">26,</ref><ref type="bibr">2014)</ref>. Protein-protein physical inter
population of the transporter protein may depend on the cell type and intracellular membranes type <ref type="bibr" target="#b32">[36]</ref>, and therefore, tissue specific population data of transpo
structural similarities as the input to predict DDIs. In this work, we used PubChem 2D fingerprint <ref type="bibr" target="#b37">[41]</ref> and Tanimoto coefficient to calculate structural similarit
nted on DDInetworks through integrated phenotypic, therapeutic, structural and genomic similarities <ref type="bibr" target="#b10">[13]</ref>. QSAR models for DDI prediction were constructed for CYP1A
single and multiple-dose conditions to predict the DDI for crizotinib with ketoconazole or rifampin <ref type="bibr" target="#b4">[7]</ref>. Structural similarity for DDI prediction has been employed
and 3A4 by using two types of chemical descriptors and the balanced accuracy ranged from 72 to 79% <ref type="bibr" target="#b11">[14]</ref>.</p><p>There are also knowledge-based studies for DDI pred
g-transporter and drug-target were retrieved from DrugBank version 4.1 (downloaded on Sep. 8, 2014) <ref type="bibr" target="#b33">[37]</ref>. This includes, 4002 drugs with fingerprints (drug set M),
chine learning tasks, deep neural networks have been shown to be susceptible to adversarial attacks <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow assification, these perturbations cause the legitimate sample to be misclassified at inference time <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow , adversarial training which augments the training data of the classifier with adversarial examples <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow xamples designed to fool the substitute often end up being misclassified by the targeted classifier <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b19">Papernot e ch to defend against adversarial noise is to augment the training dataset with adversarial examples <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow
ning procedure of the classifier to reduce the magnitude of gradients, e.g., defensive distillation <ref type="bibr" target="#b18">(Papernot et al., 2016d)</ref>, and (3) attempting to remove the adve http://www.tei-c.org/ns/1.0"><head n="2.2.2">DEFENSIVE DISTILLATION</head><p>Defensive distillation <ref type="bibr" target="#b18">(Papernot et al., 2016d)</ref> trains the classifier in two rounds us f gradients around input points, making it difficult for attackers to generate adversarial examples <ref type="bibr" target="#b18">(Papernot et al., 2016d)</ref>. It was, however, shown that, while de


to be susceptible to adversarial attacks <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015)</ref>. These attacks come in the form of adve the classifier with adversarial examples <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015)</ref>, (2) modifying the training procedure o le to be misclassified at inference time <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b16">Papernot aining dataset with adversarial examples <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b14">Moosavi-D R n , resulting in the adversarial example x = x + δ. The ∞ -norm of the perturbation is denoted by <ref type="bibr" target="#b4">(Goodfellow et al., 2015)</ref> and is chosen to be small enough so as work, we focus on untargeted white-box attacks computed using the Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b4">(Goodfellow et al., 2015)</ref>, the Randomized Fast Gradient Sign Met the perturbation δ to:</p><formula xml:id="formula_0">δ = • sign(∇ x J(x, y)).</formula><p>(1) FGSM <ref type="bibr" target="#b4">(Goodfellow et al., 2015)</ref> was designed to be extremely fast rath ormula xml:id="formula_8">(i) 0 } R i=1 .</formula><p>We compare our method to adversarial training <ref type="bibr" target="#b4">(Goodfellow et al., 2015)</ref> and MagNet <ref type="bibr" target="#b
rget="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b16">Papernot et al., 2016b;</ref><ref type="bibr" target="#b11">Liu et al ype="bibr" target="#b9">(Kurakin et al., 2017)</ref>, the Jacobian-based Saliency Map Attack (JSMA) <ref type="bibr" target="#b16">(Papernot et al., 2016b), and</ref><ref type="bibr">Deepfool (Moosavi
/p><p>In our experiments, we use two different image datasets: the MNIST handwritten digits dataset <ref type="bibr" target="#b10">(LeCun et al., 1998)</ref> and the Fashion-MNIST (F-MNIST) clothing a

x attacks. We propose to leverage the representative power of Generative Adversarial Networks (GAN) <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref> to diminish the effect of the adversar samples G(z). D and G are trained in an alternating fashion to minimize the following min-max loss <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref>:</p><formula xml:id="formula_4">min G t was shown that the optimal GAN is obtained when the resulting generator distribution p g = p data <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref>.</p><p>However, GANs turned out to be >As mentioned in Section 2.3, the GAN min-max loss in (5) admits a global optimum when p g = p data <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref>. It can be similarly shown that WGAN a


graphs into grid-like data to enable the use of CNNs directly. This idea was previously explored in <ref type="bibr" target="#b19">[20]</ref>. However, the transformation in <ref type="bibr" target="# as previously explored in <ref type="bibr" target="#b19">[20]</ref>. However, the transformation in <ref type="bibr" target="#b19">[20]</ref> is implemented in the preprocessing process while our meth
ation. We build LGCNs based on the architecture of densely connected convolutional networks (DCNNs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, which achieved st
language processing tasks such as neural machine translation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref>. One common character
ations, such as image classification <ref type="bibr" target="#b4">[5]</ref>, semantic segmentation <ref type="bibr" target="#b1">[2]</ref>, and object detection <ref type="bibr" target="#b9">[10,</re
ations, such as image classification <ref type="bibr" target="#b4">[5]</ref>, semantic segmentation <ref type="bibr" target="#b1">[2]</ref>, and object detection <ref type="bibr" target="#b9">[10,</re
ation. We build LGCNs based on the architecture of densely connected convolutional networks (DCNNs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, which achieved st
language processing tasks such as neural machine translation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref>. One common character
al inputs are usually very high-dimensional feature vectors in some graph dataset, such as the Cora <ref type="bibr" target="#b22">[23]</ref>. The graph embedding layer is essentially a linear transfo benchmark datasets for transductive learning experiments; those are the Cora, Citeseer, and Pubmed <ref type="bibr" target="#b22">[23]</ref>, as summarized in Table <ref type="table" target="#tab_2">
llyconnected layer, we perform a simple sum to aggregate feature vectors of adjacent nodes. Dropout <ref type="bibr" target="#b24">[25]</ref> is applied on both input feature vectors and adjacency mat
demonstrated promising performance in many image-related applications, such as image classification <ref type="bibr" target="#b4">[5]</ref>, semantic segmentation <ref type="bibr" target="#b1">[2]</re
ef type="bibr" target="#b1">[2]</ref>, and object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>. A variety of CNN models have been proposed to continuously
r" target="#b0">2,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b3">5]</ref>. This pipeline system usually suffers from time delay, parame nd generates target words from left to right at each step [1, <ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b3">5]</ref>. This model has also achieved promising results in ASR fields chitecture for all three tasks (ASR, ST and MT). The model architecture is similar with Transformer <ref type="bibr" target="#b3">[5]</ref>, which is the state-of-art model in MT task. Recently, this
type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13]</ref>, hidden representation <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>, or generated sequ
scripts <ref type="foot" target="#foot_2">3</ref> . For Augmented LibriSpeech corpus, we apply BPE <ref type="bibr" target="#b25">[27]</ref> on the combination of English and French text to obtain su
to target language [1, <ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b3">5]</ref>. This pipeline system u This model has also achieved promising results in ASR fields <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b15">17]</ref>. Recent works purpose
type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>, or generated sequence <ref type="bibr" target="#b14">[16]</ref>, and alleviate the performance gap between itself and the " target="#b10">[12]</ref> and natural language processing <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b19">21]</ref>. The teacher and st
type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>, or generated sequence <ref type="bibr" target="#b14">[16]</ref>, and alleviate the performance gap between itself and the " target="#b10">[12]</ref> and natural language processing <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b19">21]</ref>. The teacher and st
ation research studies <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target= ween speech and translated phrase but not to directly predict the final translations. Bérard et al. <ref type="bibr" target="#b6">[8]</ref> give the first proof of the potential for end-to-end speech-
type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>, or generated sequence <ref type="bibr" target="#b14">[16]</ref>, and alleviate the performance gap between itself and the " target="#b10">[12]</ref> and natural language processing <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b19">21]</ref>. The teacher and st
0K, and vocabulary size are 28,912 and 30,000, respectively. We report case-insensitive BLEU scores <ref type="bibr" target="#b26">[28]</ref> by multi-bleu.pl script for the evaluation of ST and MT ta
0K, and vocabulary size are 28,912 and 30,000, respectively. We report case-insensitive BLEU scores <ref type="bibr" target="#b26">[28]</ref> by multi-bleu.pl script for the evaluation of ST and MT ta
type="bibr" target="#b11">13]</ref>, hidden representation <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>, or generated sequence <ref type="bibr" target="#b14">[16]<
raining graph to boost accuracy without affecting the inference graph, including auxiliary training <ref type="bibr" target="#b18">[19]</ref>, multi-task learning <ref type="bibr" target="#b3">[4,</re nvergence of deep networks by adding auxiliary classifiers connected to certain intermediate layers <ref type="bibr" target="#b18">[19]</ref>. However, auxiliary classifiers require specific new desig tei-c.org/ns/1.0"><head n="3.1">Generation of training graph</head><p>Similar to auxiliary training <ref type="bibr" target="#b18">[19]</ref>, we add several new classifier heads into the original net
graph, including auxiliary training <ref type="bibr" target="#b18">[19]</ref>, multi-task learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, and knowledge distill related tasks simultaneously so that knowledge obtained from each task can be reused by the others <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target ef type="figure" target="#fig_1">1 (c</ref>). This structure is very similar to multi-task learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, in which different su
the network-in-training on different epochs to improve the performance of semi-supervised learning <ref type="bibr" target="#b13">[14]</ref>. However, it is hard to scale for a large dataset since te
the rest. Therefore, the inference graph is identical to the original graph g.</p><p>It is shown in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5]</ref> that the training me
get="#b1">2]</ref>. In fact, alternative optimization is popular in generative adversarial networks <ref type="bibr" target="#b7">[8]</ref>, in which a generator and discriminator get alternatively up
ype="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, and knowledge distillation <ref type="bibr" target="#b9">[10]</ref>. Auxiliary training is introduced to improve the convergenc ty model, so that the smaller one obtains better performance than that trained by using labels only <ref type="bibr" target="#b9">[10]</ref>. However, distillation is not an end-to-end solution due to as a distance measure between an average prediction from population and the prediction of each head <ref type="bibr" target="#b9">[10]</ref>. Minimizing this objective aims at transferring the informa ed in Section 4.1.</p><p>Balance between hard and soft loss objectives. We follow the suggestion in <ref type="bibr" target="#b9">[10]</ref> that the backpropagation flow from each soft objective shou
get="#b1">2]</ref>. In fact, alternative optimization is popular in generative adversarial networks <ref type="bibr" target="#b7">[8]</ref>, in which a generator and discriminator get alternatively up
to recalculate a new prediction after updating its parameters. In terms of convergence, recent work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> reveals that simul
igns for their network structures in addition to the target network. Furthermore, it is found later <ref type="bibr" target="#b19">[20]</ref> that auxiliary classifiers do not result in obvious improv smoothed values, and is shown to reduce the vulnerability of noisy or incorrect labels in datasets <ref type="bibr" target="#b19">[20]</ref>. It regularizes the model and relaxes the confidence on th
ype="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, and knowledge distillation <ref type="bibr" target="#b9">[10]</ref>. Auxiliary training is introduced to improve the convergenc ty model, so that the smaller one obtains better performance than that trained by using labels only <ref type="bibr" target="#b9">[10]</ref>. However, distillation is not an end-to-end solution due to as a distance measure between an average prediction from population and the prediction of each head <ref type="bibr" target="#b9">[10]</ref>. Minimizing this objective aims at transferring the informa ed in Section 4.1.</p><p>Balance between hard and soft loss objectives. We follow the suggestion in <ref type="bibr" target="#b9">[10]</ref> that the backpropagation flow from each soft objective shou
es additional regularization.</p><p>ILR sharing is somewhat related to the concept of hint training <ref type="bibr" target="#b17">[18]</ref>, in which a teacher transfers its knowledge to a student n parated classifier heads converge to the exact same one by forcing them to match. It is reported in <ref type="bibr" target="#b17">[18]</ref> that using hints can outperform distillation. To a certain
to recalculate a new prediction after updating its parameters. In terms of convergence, recent work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> reveals that simul
the rest. Therefore, the inference graph is identical to the original graph g.</p><p>It is shown in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5]</ref> that the training me
more accurate classifier typically leads to a better object detection model based on the classifier <ref type="bibr" target="#b11">[12]</ref>.  Model weight distribution and mechanisms of ILR sharing.
to recalculate a new prediction after updating its parameters. In terms of convergence, recent work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> reveals that simul
ef type="bibr" target="#b18">[19]</ref>, multi-task learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, and knowledge distillation <ref type="bibr" target="#b9">[10 ref>). This structure is very similar to multi-task learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, in which different supervised tasks share the same input, as nowledge obtained from each task can be reused by the others <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref>. However, it is not u
le. Two-way distillation. Co-distillation of two instances of the same neural network is studied in <ref type="bibr" target="#b1">[2]</ref> with a focus on training speed-up in a distributed learning with each head one-by-one. This algorithm is used in both <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2]</ref>. In fact, alternative optimization is popular in generative a
sses, respectively. We conduct empirical studies on the CIFAR-10 dataset with ResNet-32, ResNet-110 <ref type="bibr" target="#b8">[9]</ref>, and DenseNet-40-12 <ref type="bibr" target="#b10">[11]</ref ve learning helps improve the performance of ResNet-50 network.</p><p>As following the notations in <ref type="bibr" target="#b8">[9]</ref>, we consider two heads sharing ILRs up to "conv3_x" block fo ><figDesc>Figure 4: Per-layer weight distribution in trained ResNet-50.As following the notations in<ref type="bibr" target="#b8">[9]</ref>, the two split points in the hierarchical sharing with four
to recalculate a new prediction after updating its parameters. In terms of convergence, recent work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> reveals that simul
R-10 dataset with ResNet-32, ResNet-110 <ref type="bibr" target="#b8">[9]</ref>, and DenseNet-40-12 <ref type="bibr" target="#b10">[11]</ref>. ResNets and DenseNets for CIFAR are all designed to have
R-10 dataset with ResNet-32, ResNet-110 <ref type="bibr" target="#b8">[9]</ref>, and DenseNet-40-12 <ref type="bibr" target="#b10">[11]</ref>. ResNets and DenseNets for CIFAR are all designed to have
e reused by the others <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref>. However, it is not useful for a single task use case. Know
UMON-global is still substantial. To reduce the overhead of UMON, we use Dynamic Set Sampling (DSS) <ref type="bibr" target="#b11">[12]</ref>. The key idea behind DSS is that the behavior of the cache e use 32 sets for UMON-DSS. The sampled sets for UMON-DSS are chosen using the simple static policy <ref type="bibr" target="#b11">[12]</ref>, which means set 0 and every 33rd set is selected. For the m (MLP) now fits in the cache which reduces the average MLP and increases the average mlp-based cost<ref type="bibr" target="#b11">[12]</ref> of each miss.</p></note> 			<note xmlns="http://www.tei-c. > 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>DSS was used in<ref type="bibr" target="#b11">[12]</ref> to choose between two replacement policies. Thus, it was u allocate) by using the hit counter information of the sampled sets. Therefore the bounds derived in<ref type="bibr" target="#b11">[12]</ref> are not applicable to our mechanism.</p></note> 			<note x
ctions is obtained for each benchmark using a tool that we developed using the Simpoint methodology <ref type="bibr" target="#b10">[11]</ref>. Two separate benchmarks are combined to form one multipro
tories makes this scheme impractical. Fortunately, the baseline LRU policy obeys the stack property <ref type="bibr" target="#b9">[10]</ref>, which means that an access that hits in a LRU managed cach
line LRU policy is augmented to enable way partitioning <ref type="bibr" target="#b1">[2]</ref>[18] <ref type="bibr" target="#b5">[6]</ref>. To implement way partitioning, we add a bit to the tag-stor .</p><p>Mechanisms to facilitate static and dynamic partitioning of cache is described in detail in <ref type="bibr" target="#b5">[6]</ref>. Recently, Hsu et al. <ref type="bibr" target="#b4">[5]</ref
sources to 1 Demand is determined by the number of unique cache blocks accessed in a given interval <ref type="bibr" target="#b3">[4]</ref>. Consider two applications A and B sharing a fullyassociativ


ven the other application is zero). To address this shortcoming of the greedy algorithm, Suh et. al <ref type="bibr" target="#b17">[18]</ref> propose to also invoke the greedy algorithm for each combi on-convex points of all the competing applications can be very large. To avoid the time complexity, <ref type="bibr" target="#b17">[18]</ref> suggests that the greedy algorithm be invoked only for som partitioning of shared cache was first investigated by Suh et al. <ref type="bibr">[17][18]</ref>. <ref type="bibr" target="#b17">[18]</ref> describes a low-overhead scheme that uses recency position

line LRU policy is augmented to enable way partitioning <ref type="bibr" target="#b1">[2]</ref>[18] <ref type="bibr" target="#b5">[6]</ref>. To implement way partitioning, we add a bit to the tag-stor .</p><p>Mechanisms to facilitate static and dynamic partitioning of cache is described in detail in <ref type="bibr" target="#b5">[6]</ref>. Recently, Hsu et al. <ref type="bibr" target="#b4">[5]</ref
ven the other application is zero). To address this shortcoming of the greedy algorithm, Suh et. al <ref type="bibr" target="#b17">[18]</ref> propose to also invoke the greedy algorithm for each combi on-convex points of all the competing applications can be very large. To avoid the time complexity, <ref type="bibr" target="#b17">[18]</ref> suggests that the greedy algorithm be invoked only for som partitioning of shared cache was first investigated by Suh et al. <ref type="bibr">[17][18]</ref>. <ref type="bibr" target="#b17">[18]</ref> describes a low-overhead scheme that uses recency position
able place to look is human cognition <ref type="bibr" target="#b7">(Davis &amp; Marcus, 2015;</ref><ref type="bibr" target="#b29">Lake et al., 2016;</ref><ref type="bibr" target="#b42">Marcus, 2001;<
els (such that, for example, pictures of cars are labeled as cars). In DeepMind's Atari game system <ref type="bibr" target="#b48">(Mnih et al., 2015)</ref>, neural networks learned mappings between p ke "ball" or "opponent" can lie out of reach.</p><p>Consider for example DeepMind's Atari game work <ref type="bibr" target="#b48">(Mnih et al., 2015)</ref> on deep reinforcement learning, which combi

t-it-keeps-m-1821384511/amp 10 Deep learning's predecessors were vulnerable to similar problems, as <ref type="bibr" target="#b57">Pinker and Prince (1988)</ref>pointed out, in a 11 discussion of neur

els (such that, for example, pictures of cars are labeled as cars). In DeepMind's Atari game system <ref type="bibr" target="#b48">(Mnih et al., 2015)</ref>, neural networks learned mappings between p ke "ball" or "opponent" can lie out of reach.</p><p>Consider for example DeepMind's Atari game work <ref type="bibr" target="#b48">(Mnih et al., 2015)</ref> on deep reinforcement learning, which combi

erunner to today's more sophisticated deep learning based recurrent neural networks, known as RNNs; <ref type="bibr" target="#b11">Elman, 1990)</ref> would have trouble systematically representing and
iously crossed the Atlantic is an embedded clause that specifies which teenager.)</p><p>In the 80's <ref type="bibr" target="#b13">Fodor and Pylyshyn (1988)</ref>expressed similar concerns, with respe


ence (seq2seq) models. Sub-word units were used in seq2seq <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> and RNNT <ref typ
be updated. Recently, there has been work on exploring the use of Unicode bytes to represent text. <ref type="bibr" target="#b21">[22]</ref> presented an LSTM-based multilingual byte-to-span model. T >In this work, we investigate the potential of representing text using byte sequences introduced in <ref type="bibr" target="#b21">[22]</ref> for speech processing. For ASR, we adopt the Listen, Atten
s to Unicode bytes. The encoder network consists of 5 unidirectional Long Short-Term Memory (LSTMs) <ref type="bibr" target="#b24">[25]</ref> layers, with each layer having 1, 400 hidden units. The de
s.</p><p>Graphemes have been used as an alternative modeling unit to phonemes for speech processing <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe
ictionaries. Additionally, the inconsistency in the phonetic systems is also challenging to resolve <ref type="bibr" target="#b2">[3]</ref> when merging different languages.</p><p>Graphemes have been bulary that often has long tail graphemes with very poor coverage.</p><p>To address these problems, <ref type="bibr" target="#b2">[3]</ref> explored the use of features from Unicode character descript
h and dictation traffic. These utterances are further artificially corrupted using a room simulator <ref type="bibr" target="#b30">[31]</ref>, adding varying degrees of noise and reverberation such th
of 80-dimensional log-mel features, computed with a 25ms window and shifted every 10ms. Similar to <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, at each current f
ictionaries. Additionally, the inconsistency in the phonetic systems is also challenging to resolve <ref type="bibr" target="#b2">[3]</ref> when merging different languages.</p><p>Graphemes have been bulary that often has long tail graphemes with very poor coverage.</p><p>To address these problems, <ref type="bibr" target="#b2">[3]</ref> explored the use of features from Unicode character descript
/ref> built a Connectionist Temporal Classification (CTC) model to directly output graphemes, while <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" targ arget="#b21">[22]</ref> for speech processing. For ASR, we adopt the Listen, Attend and Spell (LAS) <ref type="bibr" target="#b8">[9]</ref> model to convert input speech into sequences of Unicode byte del Structure</head><p>The Audio-to-Byte (A2B) model is based on the Listen, Attend and Spell (LAS) <ref type="bibr" target="#b8">[9]</ref> model, with the output target changed from graphemes to Unic d n="2.2.">Output Unit</head><p>End-to-end speech recognition models have typically used characters <ref type="bibr" target="#b8">[9]</ref>, sub-words <ref type="bibr" target="#b11">[12]</ref>, word-p
target="#b16">17]</ref>. Similarly, graphemes are also commonly used to build end-toend TTS systems <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" t
been attracting much interest in both academia and industry <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Conventional phonetically-based speech processing systems re
;</ref><ref type="bibr">Kitaev et al., 2019, i.a.)</ref>.</p><p>In this context, the GLUE benchmark <ref type="bibr" target="#b62">(Wang et al., 2019a)</ref> has become a prominent evaluation framewor s, we use an average of the metrics. More information on the tasks included in GLUE can be found in <ref type="bibr" target="#b62">Wang et al. (2019a)</ref> and in <ref type="bibr">Warstadt et al. (20
the original work, we evaluate using accuracy.</p><p>MultiRC (Multi-Sentence Reading Comprehension, <ref type="bibr" target="#b26">Khashabi et al., 2018</ref>) is a QA task where each example consists iu et al. (2019d)</ref> respectively.</p><p>Human Performance Pilehvar and Camacho-Collados (2019), <ref type="bibr" target="#b26">Khashabi et al. (2018)</ref>, <ref type="bibr">Nangia and</ref><ref t
le supervision can produce representations that effectively transfer to a broad range of NLP tasks  <ref type="bibr" target="#b10">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b14">Dai

de results for COPA, MultiRC, and RTE are from <ref type="bibr" target="#b56">Sap et al. (2019</ref><ref type="bibr" target="#b61">), Trivedi et al. (2019</ref><ref type="bibr" target="#b37">), and Li
ne whether the word is used with the same sense in both sentences. Sentences are drawn from WordNet <ref type="bibr" target="#b43">(Miller, 1995)</ref>, VerbNet <ref type="bibr" target="#b58">(Schuler
the original work, we evaluate using accuracy.</p><p>MultiRC (Multi-Sentence Reading Comprehension, <ref type="bibr" target="#b26">Khashabi et al., 2018</ref>) is a QA task where each example consists iu et al. (2019d)</ref> respectively.</p><p>Human Performance Pilehvar and Camacho-Collados (2019), <ref type="bibr" target="#b26">Khashabi et al. (2018)</ref>, <ref type="bibr">Nangia and</ref><ref t
et al., 2006)</ref>, RTE3 <ref type="bibr" target="#b21">(Giampiccolo et al., 2007)</ref>, and RTE5 <ref type="bibr" target="#b2">(Bentivogli et al., 2009)</ref>. All datasets are combined and convert
g, multi-task learning, and transfer learning in NLP, built around standard tools including PyTorch <ref type="bibr" target="#b46">(Paszke et al., 2017)</ref> and AllenNLP <ref type="bibr" target="#b2 19b</ref>),<ref type="foot" target="#foot_2">4</ref> a modular software toolkit, built with PyTorch <ref type="bibr" target="#b46">(Paszke et al., 2017)</ref>, components from AllenNLP <ref type="bibr

ne whether the word is used with the same sense in both sentences. Sentences are drawn from WordNet <ref type="bibr" target="#b43">(Miller, 1995)</ref>, VerbNet <ref type="bibr" target="#b58">(Schuler
ch precludes fast prediction.</p><p>A recent proposal, the Dead-Block Correlating Prefetcher (DBCP) <ref type="bibr" target="#b11">[12]</ref>, achieves maximal prefetch lookahead through correlation t correlating GHB prefetcher achieves only 31% performance improvement. DBCP with 2MB on-chip storage <ref type="bibr" target="#b11">[12]</ref> achieves only 17% performance improvement. Finally, increa d n="2">Background: DBCP Prefetching</head><p>Lai et al. proposed Dead-Block Correlated Prefetching <ref type="bibr" target="#b11">[12]</ref> to predict the last touch to a cache block prior to that b .1">Why is DBCP Impractical?</head><p>Figure <ref type="figure">3</ref> depicts the anatomy of DBCP <ref type="bibr" target="#b11">[12]</ref>. On every memory access, DBCP computes a last-touch histor /1.0"><head n="3.3">Lookahead for Sequence Retrieval</head><p>Cache misses are frequently clustered <ref type="bibr" target="#b11">[12]</ref>, corresponding to bursts of last touches. During such burs lns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recording Last-Touch Signatures</head><p>As in DBCP <ref type="bibr" target="#b11">[12]</ref>, LT-cords constructs signatures using the history table. E rget="#b14">15]</ref>. The realistic DBCP is implemented with a 2MB on-chip correlation table as in <ref type="bibr" target="#b11">[12]</ref>. For comparison with a larger L2, we quadruple the size of etween a last touch to a block until its eventual eviction. 1 The figure corroborates prior results <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" ta
s must store correlation data across long (e.g., billions of instructions) recurring program phases <ref type="bibr" target="#b17">[18]</ref> with little temporal reuse. Because last-touch signatures
5.8) are obtained by cycle-accurate simulation using SMARTS statistical sampling and checkpointing <ref type="bibr" target="#b24">[25]</ref>. We use trace-driven simulation to create many evenly spac
memory-access performance bottleneck. A number of proposals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar
memory-access performance bottleneck. A number of proposals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar
versely, the designs of Solihin et al. <ref type="bibr" target="#b19">[20]</ref> and Wenisch et al. <ref type="bibr" target="#b23">[24]</ref> record address correlation data in off-chip DRAM. Although signatures.</p><p>To tolerate reordering and stale signatures, previous temporal streaming designs <ref type="bibr" target="#b23">[24]</ref> temporarily buffer data in a small fullyassociative struct the same two misses. We employ a Temporal Correlation Distance metric similar to that introduced in <ref type="bibr" target="#b23">[24]</ref>. Temporal correlation distance between two consecutive mis
perl, for which the "splitmail" input is used. We include results for three pointer-intensive Olden <ref type="bibr" target="#b1">[2]</ref> benchmarks (indicated by * in Table <ref type="table" target
perl, for which the "splitmail" input is used. We include results for three pointer-intensive Olden <ref type="bibr" target="#b1">[2]</ref> benchmarks (indicated by * in Table <ref type="table" target
r proposed class of prefetchers utilizes address correlation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target
5.8) are obtained by cycle-accurate simulation using SMARTS statistical sampling and checkpointing <ref type="bibr" target="#b24">[25]</ref>. We use trace-driven simulation to create many evenly spac
rget="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> advocate microarchi
igh quality for most languages, which can potentially improve the performance of end-to-end systems <ref type="bibr" target="#b6">[7]</ref>.</p><p>Sub-word representations have recently seen their suc d proposed a worddependent silence model to improve ASR accuracy; for use in end-to-end ASR models, <ref type="bibr" target="#b6">[7]</ref> investigated the value of a lexicon in end-to-end ASR. Sub-w
igner. We call this method pronunciation-assisted sub-word modeling (PASM), which adopts fast align <ref type="bibr" target="#b8">[9]</ref> to align a pronunciation lexicon arXiv:1811.04284v2 [cs.CL]

-word dictionary by greedily keep the most frequent co-occurring character sequences. Concurrently, <ref type="bibr" target="#b14">[15]</ref> borrow the practice in voice search <ref type="bibr" targe
" target="#b1">[2]</ref> [3] <ref type="bibr" target="#b3">[4]</ref> and neural machine translation <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref>.</p><p>Due to lack o t recent tide of adopting sub-word representations is largely driven by neural machine translation. <ref type="bibr" target="#b4">[5]</ref> proposed to use byte-pair encoding (BPE) <ref type="bibr" ta e also compare our systems with BPE baselines. The BPE procedure follows the algorithm described in <ref type="bibr" target="#b4">[5]</ref>. All the PASM segmentation schemes are trained using the lex
of tasks, including automatic speech recognition (ASR) <ref type="bibr" target="#b1">[2]</ref> [3] <ref type="bibr" target="#b3">[4]</ref> and neural machine translation <ref type="bibr" target="#b4" s the encoder, location-based attention, and LSTM decoder, with a CTC-weight of 0.5 during training <ref type="bibr" target="#b3">[4]</ref>. To fully see the effect of sub-word methods, we do not perf
chine translation. <ref type="bibr" target="#b4">[5]</ref> proposed to use byte-pair encoding (BPE) <ref type="bibr" target="#b13">[14]</ref> to build a sub-word dictionary by greedily keep the most f

type="bibr" target="#b10">[11]</ref> used sub-words units in particular for detecting unseen words. <ref type="bibr" target="#b11">[12]</ref> used sub-words units in building text-independent speech r

ead><p>The use of a pronunciation dictionary is the standard approach in hybrid speech recognition. <ref type="bibr" target="#b9">[10]</ref> use the phone-level alignment to generate a probabilistic l
-word dictionary by greedily keep the most frequent co-occurring character sequences. Concurrently, <ref type="bibr" target="#b14">[15]</ref> borrow the practice in voice search <ref type="bibr" targe
end ASR. Sub-word methods have a long history of application in a number of language related tasks. <ref type="bibr" target="#b10">[11]</ref> used sub-words units in particular for detecting unseen wo
has proven to be very successful in a number of tasks, including automatic speech recognition (ASR) <ref type="bibr" target="#b1">[2]</ref> [3] <ref type="bibr" target="#b3">[4]</ref> and neural machi
get="#b11">[12]</ref> used sub-words units in building text-independent speech recognition systems. <ref type="bibr" target="#b12">[13]</ref> improved upon sub-word methods in WFST-based speech recogn
of tasks, including automatic speech recognition (ASR) <ref type="bibr" target="#b1">[2]</ref> [3] <ref type="bibr" target="#b3">[4]</ref> and neural machine translation <ref type="bibr" target="#b4" s the encoder, location-based attention, and LSTM decoder, with a CTC-weight of 0.5 during training <ref type="bibr" target="#b3">[4]</ref>. To fully see the effect of sub-word methods, we do not perf
bibr" target="#b3">[4]</ref> and neural machine translation <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref>.</p><p>Due to lack of a pronunciation dictionary, most end-t t="#b15">[16]</ref> to segment words into wordpiece which maximizes the language model probability. <ref type="bibr" target="#b5">[6]</ref> augments the training data with subword segmentation sampled
bibr" target="#b3">[4]</ref> and neural machine translation <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref>.</p><p>Due to lack of a pronunciation dictionary, most end-t t="#b15">[16]</ref> to segment words into wordpiece which maximizes the language model probability. <ref type="bibr" target="#b5">[6]</ref> augments the training data with subword segmentation sampled
ead><p>The use of a pronunciation dictionary is the standard approach in hybrid speech recognition. <ref type="bibr" target="#b9">[10]</ref> use the phone-level alignment to generate a probabilistic l
chine translation. <ref type="bibr" target="#b4">[5]</ref> proposed to use byte-pair encoding (BPE) <ref type="bibr" target="#b13">[14]</ref> to build a sub-word dictionary by greedily keep the most f
chine translation. <ref type="bibr" target="#b4">[5]</ref> proposed to use byte-pair encoding (BPE) <ref type="bibr" target="#b13">[14]</ref> to build a sub-word dictionary by greedily keep the most f
ead><p>The use of a pronunciation dictionary is the standard approach in hybrid speech recognition. <ref type="bibr" target="#b9">[10]</ref> use the phone-level alignment to generate a probabilistic l
kloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in <ref type="bibr" target="#b16">[17]</ref>. We adopt larger input data sets varying from 147 to 187 G both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in <ref type="bibr" target="#b16">[17]</ref>) in the memory system. And for each workload, we collect t performance data of the whole run time after the warm-up instead of a short period (180 seconds in <ref type="bibr" target="#b16">[17]</ref>).</p><p>We find that big data analytics applications share chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper <ref type="bibr" target="#b16">[17]</ref>) workloads. Meanwhile the service workloads in data center s, e.g., HPC-HPL, HPC-DGEMM and chip multiprocessors workloads.</p><p>• Corroborating previous work <ref type="bibr" target="#b16">[17]</ref>, both the big data analytics workloads and service workloa the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite <ref type="bibr" target="#b16">[17]</ref>, SPECweb and TPC-W) in terms of processor pipeline stall b vel cache), respectively. For the service workloads, our observations corroborate the previous work <ref type="bibr" target="#b16">[17]</ref>: the L2 cache is ineffective.</p><p>• For the big data ana rk of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite <ref type="bibr" target="#b16">[17]</ref>. However, CloudSuite paper is biased towards online servic . The input data size varies from 147 to 187 GB. In comparison with that of CloudSuite described in <ref type="bibr" target="#b16">[17]</ref>, our approach are more pragmatic. We adopt a larger data i in both memory and disk systems instead of completely storing data (only 4.5 GB for Naive Bayes in <ref type="bibr" target="#b16">[17]</ref>) in memory. The number of instructions retired of the big , HPCC, PARSEC, TPC-W, SPECweb 2005, and CloudSuite-a scale-out benchmark suite for cloud computing <ref type="bibr" target="#b16">[17]</ref>, and compared them with big data analytics workloads.</p>< as one of the representative big data analytics workloads with a larger data input set (147 GB). In <ref type="bibr" target="#b16">[17]</ref>, the data input size is only 4.5 GB.</p><p>We set up the o n fetch stalls indicates the front end inefficiency. Our observation corroborates the previous work <ref type="bibr" target="#b16">[17]</ref>. The front end inefficiency may caused by high-level langu Figure <ref type="figure" target="#fig_3">5</ref>.</p><p>Implications: Corroborating previous work <ref type="bibr" target="#b16">[17]</ref>, both the big data analytics workloads and the service wor ns, which may be caused by two factors: deep memory hierarchy with long latency in modern processor <ref type="bibr" target="#b16">[17]</ref>, and large binary size complicated by high-level language, sor and save the die area. For the service workloads, our observation corroborate the previous work <ref type="bibr" target="#b16">[17]</ref>: the L2 cache is ineffective.</p><formula xml:id="formula_ s in modern superscalar processors as previous work found e.g., on-chip bandwidth, die area and etc <ref type="bibr" target="#b16">[17]</ref>. According to our correlation analysis in this section, ar ten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref>. Moreover, for mod s prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" ta get="#b18">19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref> and etc. Narayanan et al. <ref type="bibr" target="#b32">[3
ics workloads include business intelligence, machine learning, bio-informatics, and ad hoc analysis <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>The business
="3" xml:id="foot_2">HMM invokes ICTCLASS<ref type="bibr" target="#b3">[4]</ref>; SVM invokes LIBSVM<ref type="bibr" target="#b13">[14]</ref> and Hive-bench invokes Hive.</note> 			<note xmlns="http:/
into an open-source big data benchmark suite-BigDataBench <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref>, which is an open-s
="3" xml:id="foot_2">HMM invokes ICTCLASS<ref type="bibr" target="#b3">[4]</ref>; SVM invokes LIBSVM<ref type="bibr" target="#b13">[14]</ref> and Hive-bench invokes Hive.</note> 			<note xmlns="http:/
into an open-source big data benchmark suite-BigDataBench <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b17">18]</ref>, which is publicly available from <ref type="bibr" target="
ving force behind the design of innovative data center systems including both hardware and software <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" ta
kloads can be classified into two categories: services and data analytics workloads as mentioned in <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b12">[13]</ref>. For the dat
ing data analytics workloads in different aspects, such as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar analytics workloads on single node other then workloads running at data center scale. Huang et al. <ref type="bibr" target="#b18">[19]</ref> characterize the MapReduce framework in system level perfo
t aspects, such as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" tar e the Hadoop framework and do not focus on the micro-architecture's characteristics. Awasthi et al. <ref type="bibr" target="#b11">[12]</ref> also perform a system level characterization of data cente
plications show varying performance, energy behavior and preferable system configuration parameters <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" ta
ion is built.</p><p>Objective Function We optimize the parameters of PNet using REINFORCE algorithm <ref type="bibr" target="#b24">(Williams 1992</ref>) and policy gradient methods <ref type="bibr" ta
target="#b25">(Yang et al. 2016;</ref><ref type="bibr" target="#b27">Zhou, Wan, and Xiao 2016;</ref><ref type="bibr" target="#b14">Lin et al. 2017</ref>) use attention mechanisms to build representati selfattention mechanism and a special regularization term are used to construct sentence embedding <ref type="bibr" target="#b14">(Lin et al. 2017)</ref>.</p><p>The dimension of hidden vectors and th
target="#b25">(Yang et al. 2016;</ref><ref type="bibr" target="#b27">Zhou, Wan, and Xiao 2016;</ref><ref type="bibr" target="#b14">Lin et al. 2017</ref>) use attention mechanisms to build representati selfattention mechanism and a special regularization term are used to construct sentence embedding <ref type="bibr" target="#b14">(Lin et al. 2017)</ref>.</p><p>The dimension of hidden vectors and th
r instance, as we may know, sentiment and negation words are important for sentiment classification <ref type="bibr" target="#b28">(Zhu et al. 2014;</ref><ref type="bibr" target="#b18">Qian et al. 201
<ref type="bibr" target="#b13">Lei, Barzilay, and Jaakkola 2015)</ref> and recurrent neural network <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b4">C used in document-level classification <ref type="bibr" target="#b23">(Tang, Qin, and Liu 2015;</ref><ref type="bibr" target="#b5">Ghosh et al. 2016)</ref> and language modeling <ref type="bibr" target
9">(Kim 2014;</ref><ref type="bibr" target="#b8">Kalchbrenner, Grefenstette, and Blunsom 2014;</ref><ref type="bibr" target="#b13">Lei, Barzilay, and Jaakkola 2015)</ref> and recurrent neural network
types. Bag-of-words representation models ignore the order of words, including deep average network <ref type="bibr" target="#b7">(Iyyer et al. 2015;</ref><ref type="bibr" target="#b8">Joulin et al. 2
">(Iyyer et al. 2015;</ref><ref type="bibr" target="#b8">Joulin et al. 2017</ref>) and autoencoders <ref type="bibr" target="#b15">(Liu et al. 2015)</ref>. Sequence representation models such as convo
red representations. Attention-based methods <ref type="bibr" target="#b25">(Yang et al. 2016;</ref><ref type="bibr" target="#b27">Zhou, Wan, and Xiao 2016;</ref><ref type="bibr" target="#b14">Lin et rpus<ref type="foot" target="#foot_2">3</ref> , a large topic classification dataset constructed by <ref type="bibr" target="#b27">(Zhang, Zhao, and LeCun 2015)</ref>.</p><p>The topic includes World,
t="#b22">(Tai, Socher, and Manning 2015)</ref>, <ref type="bibr" target="#b9">(Kim 2014)</ref>, and <ref type="bibr" target="#b6">(Huang, Qian, and Zhu 2017)</ref>. The rest are obtained by our own im
">(Iyyer et al. 2015;</ref><ref type="bibr" target="#b8">Joulin et al. 2017</ref>) and autoencoders <ref type="bibr" target="#b15">(Liu et al. 2015)</ref>. Sequence representation models such as convo
addition and subtraction algorithms.</p><p>Other recent algorithms for modular arithmetic appear in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Fix N &gt; 1
//www.tei-c.org/ns/1.0"><p>1. Description. Some algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" t
="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> require extensive modular arithmetic. We propose a represent
//www.tei-c.org/ns/1.0"><p>1. Description. Some algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" t
="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> require extensive modular arithmetic. We propose a represent
xml:lang="en"> 		<body> <div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Description. Some algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" t
. Some algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> require extensive m
p>Other recent algorithms for modular arithmetic appear in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Fix N &gt; 1. Define an N-residue to be a residue cla
//www.tei-c.org/ns/1.0"><p>1. Description. Some algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" t
p>Other recent algorithms for modular arithmetic appear in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Fix N &gt; 1. Define an N-residue to be a residue cla
//www.tei-c.org/ns/1.0"><p>1. Description. Some algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" t
to handle the variety of models of parallelism that appear in HPC programs. Toward this end, PEBIL <ref type="bibr" target="#b0">[1]</ref> has recently added support for handling multithreaded x86 64 afety</head><p>PEBIL generates and inserts code into the program which has two principle functions: <ref type="bibr" target="#b0">(1)</ref> to add functionality to a program, functionality which is us
/Linux binary instrumentation platforms -Pin <ref type="bibr" target="#b1">[2]</ref> and DyninstAPI <ref type="bibr" target="#b2">[3]</ref>. We then go on to compare PEBIL to Pin experimentally in the x86/Linux binary instrumentation projects -Pin <ref type="bibr" target="#b1">[2]</ref>, DyninstAPI <ref type="bibr" target="#b2">[3]</ref> and Valgrind <ref type="bibr" target="#b7">[8]</ref> being t
/Linux binary instrumentation platforms -Pin <ref type="bibr" target="#b1">[2]</ref> and DyninstAPI <ref type="bibr" target="#b2">[3]</ref>. We then go on to compare PEBIL to Pin experimentally in the x86/Linux binary instrumentation projects -Pin <ref type="bibr" target="#b1">[2]</ref>, DyninstAPI <ref type="bibr" target="#b2">[3]</ref> and Valgrind <ref type="bibr" target="#b7">[8]</ref> being t
eful tool for dealing with those aspects of compiler and program development. Models of performance <ref type="bibr" target="#b5">[6]</ref> and energy <ref type="bibr" target="#b6">[7]</ref> can also
velop and refine memory subsystem designs. Program behavior in terms of performance and correctness <ref type="bibr" target="#b4">[5]</ref> often depends heavily on memory behavior, making memory addr
model is explored and compared to two other popular x86/Linux binary instrumentation platforms -Pin <ref type="bibr" target="#b1">[2]</ref> and DyninstAPI <ref type="bibr" target="#b2">[3]</ref>. We t inary instrumentation projects. There are many other x86/Linux binary instrumentation projects -Pin <ref type="bibr" target="#b1">[2]</ref>, DyninstAPI <ref type="bibr" target="#b2">[3]</ref> and Valg
/Linux binary instrumentation platforms -Pin <ref type="bibr" target="#b1">[2]</ref> and DyninstAPI <ref type="bibr" target="#b2">[3]</ref>. We then go on to compare PEBIL to Pin experimentally in the x86/Linux binary instrumentation projects -Pin <ref type="bibr" target="#b1">[2]</ref>, DyninstAPI <ref type="bibr" target="#b2">[3]</ref> and Valgrind <ref type="bibr" target="#b7">[8]</ref> being t
eral sampling scenarios for the OpenMP-multithreaded implementations of the NAS Parallel Benchmarks <ref type="bibr" target="#b3">[4]</ref>. Collecting a memory address trace is a useful way of stress ools to collect memory address traces for the OpenMP implementations of the NAS Parallel Benchmarks <ref type="bibr" target="#b3">[4]</ref> A list of all benchmarks along with breif descriptions of th
type="bibr" target="#b1">[2]</ref>, DyninstAPI <ref type="bibr" target="#b2">[3]</ref> and Valgrind <ref type="bibr" target="#b7">[8]</ref> being the most popular. This section focuses on Pin and Dyni
type="bibr" target="#b1">[2]</ref>, DyninstAPI <ref type="bibr" target="#b2">[3]</ref> and Valgrind <ref type="bibr" target="#b7">[8]</ref> being the most popular. This section focuses on Pin and Dyni
r and program development. Models of performance <ref type="bibr" target="#b5">[6]</ref> and energy <ref type="bibr" target="#b6">[7]</ref> can also depend on the ability to understand how programs ut


ult, with findings from convex problems failing to account for empirical facts about generalization <ref type="bibr" target="#b17">Zhang et al. (2017)</ref>. As such, this result is remarkable for bei
ult, with findings from convex problems failing to account for empirical facts about generalization <ref type="bibr" target="#b17">Zhang et al. (2017)</ref>. As such, this result is remarkable for bei
contrastive estimation <ref type="bibr" target="#b7">(Mnih &amp; Teh, 2012)</ref>, sampled softmax <ref type="bibr" target="#b4">(Jean et al., 2015)</ref>, or one-vs-each approximation <ref type="bib


ttp://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We use publicly available datasets from <ref type="bibr" target="#b18">Zhang et al. (2015)</ref> to evaluate our models (http: //goo.gl/JyCn criminative LSTM model is competitive with other discriminative models based on logistic regression <ref type="bibr" target="#b18">(Zhang et al., 2015;</ref><ref type="bibr" target="#b5">Joulin et al. 015;</ref><ref type="bibr" target="#b5">Joulin et al., 2016)</ref> or convolutional neural networks <ref type="bibr" target="#b18">(Zhang et al., 2015;</ref><ref type="bibr" target="#b16">Xiao &amp; C
ve model, the dimension of the class embedding is also set to 100. We train our model using AdaGrad <ref type="bibr" target="#b1">(Duchi et al., 2012)</ref> and tune the learning rate on development s
mpled softmax <ref type="bibr" target="#b4">(Jean et al., 2015)</ref>, or one-vs-each approximation <ref type="bibr" target="#b15">(Titsias, 2017)</ref>. However, even with these approximations, discr
mpled softmax <ref type="bibr" target="#b4">(Jean et al., 2015)</ref>, or one-vs-each approximation <ref type="bibr" target="#b15">(Titsias, 2017)</ref>. However, even with these approximations, discr
oustic model with a "bottleneck" layer using a frame based criterion on a large multilingual corpus <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target ilingual models can be adapted to the specific language to improve performance further. The work by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> presented bottleneck ific softmax layers, which are trained using cross-entropy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>. This architecture ca ) if X ∈ XL1 softmax(WL2e + bL2) if X ∈ XL2 . . . softmax(WLne + bLn) if X ∈ XLn</formula><p>Unlike <ref type="bibr" target="#b4">[5]</ref>, we do not have any bottleneck layer, and the whole model is
using cross-entropy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>. This architecture can also be used as a "bottleneck" featu
tures across languages which can be mapped to the same space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>. Authors in <ref type="bibr" target="#b11">[12]</ref> looke
e space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>. Authors in <ref type="bibr" target="#b11">[12]</ref> looked at unsupervised pretraining on different languages
nd to be significantly easier to train than those that have been trained using hidden Markov models <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. <ref type="bibr" "><head>P (z|X) =</head><p>p∈CTC Path(z)</p><formula xml:id="formula_0">P (p|X)</formula><p>Like in <ref type="bibr" target="#b20">[20]</ref> we use this loss along with stacked Bidirectional LSTM lay t" n="2" xml:id="foot_1">The code to train the multi-lingual model will be released as part of EESEN<ref type="bibr" target="#b20">[20]</ref>.</note> 		</body> 		<back> 			<div type="references">
nd to be significantly easier to train than those that have been trained using hidden Markov models <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. <ref type="bibr" "><head>P (z|X) =</head><p>p∈CTC Path(z)</p><formula xml:id="formula_0">P (p|X)</formula><p>Like in <ref type="bibr" target="#b20">[20]</ref> we use this loss along with stacked Bidirectional LSTM lay t" n="2" xml:id="foot_1">The code to train the multi-lingual model will be released as part of EESEN<ref type="bibr" target="#b20">[20]</ref>.</note> 		</body> 		<back> 			<div type="references">

atures by sharing hidden layers across languages.</p><p>Connectionist Temporal Classification (CTC, <ref type="bibr" target="#b19">[19]</ref>) lends itself to low-resource multi-lingual experiments, b
" features are extracted, on top of which a target-language acoustic model can be built. Authors in <ref type="bibr" target="#b14">[15]</ref> showed that these multilingual models can be adapted to th
pecific language to improve performance further. The work by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> presented bottleneck features for multi-lingual systems whe
ing separated into multiple language-specific softmax layers, which are trained using cross-entropy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" targ

ble, cannot be transcribed, or are otherwise hard to come by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>The standard approach is to train a context dependent
and gave competitive results when compared to systems with mono-lingual features. Other approaches <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref> constructed a shar
using a frame based criterion on a large multilingual corpus <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. The network up to the
ed to systems with mono-lingual features. Other approaches <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref> constructed a shared language independent phone set, which
unts of training data may not be available, cannot be transcribed, or are otherwise hard to come by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>The standard ap s of the models shifted to learning features across languages which can be mapped to the same space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>. Authors in <ref typ
using a frame based criterion on a large multilingual corpus <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. The network up to the
using a frame based criterion on a large multilingual corpus <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. The network up to the
unts of training data may not be available, cannot be transcribed, or are otherwise hard to come by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>The standard ap s of the models shifted to learning features across languages which can be mapped to the same space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>. Authors in <ref typ
ing separated into multiple language-specific softmax layers, which are trained using cross-entropy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" targ
f> used subspace Gaussian mixture model to map phonemes of different languages together. Authors in <ref type="bibr" target="#b9">[10]</ref> introduce the use of a shared phone set to build HMM based
ed to systems with mono-lingual features. Other approaches <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref> constructed a shared language independent phone set, which
pecific language to improve performance further. The work by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> presented bottleneck features for multi-lingual systems whe
ual speech recognition involved the use of language independent features like articulatory features <ref type="bibr" target="#b7">[8]</ref> to train HMM based systems. Authors in <ref type="bibr" targ
ual speech recognition involved the use of language independent features like articulatory features <ref type="bibr" target="#b7">[8]</ref> to train HMM based systems. Authors in <ref type="bibr" targ
ble, cannot be transcribed, or are otherwise hard to come by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>The standard approach is to train a context dependent
pecific language to improve performance further. The work by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> presented bottleneck features for multi-lingual systems whe
atures by sharing hidden layers across languages.</p><p>Connectionist Temporal Classification (CTC, <ref type="bibr" target="#b19">[19]</ref>) lends itself to low-resource multi-lingual experiments, b
f> used subspace Gaussian mixture model to map phonemes of different languages together. Authors in <ref type="bibr" target="#b9">[10]</ref> introduce the use of a shared phone set to build HMM based
ble, cannot be transcribed, or are otherwise hard to come by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>The standard approach is to train a context dependent

en Markov models <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. <ref type="bibr" target="#b22">[22]</ref> shows that multi-lingual CTC systems with shared phones ca
ad coverage of the protein universe, as found in the 17929 families of the recent Pfam 32.0 release <ref type="bibr" target="#b10">[11]</ref>. Recent work that applies deep learning is either restrict database is carefully curated, at least 25% of sequences have no experimentally validation function <ref type="bibr" target="#b10">[11]</ref>, and additional experimental functional characterization o of the art models including profile HMMs we use the highly curated Protein families (Pfam) database <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>. The 17929 familie otKB have at least one Pfam family annotation, including 74.5% of proteins from reference proteomes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. Many domains have
dels, while the embedding network varies.</p><p>Our ProtCNN networks use residual networks (ResNets <ref type="bibr" target="#b31">[32]</ref>, a variant of convolutional neural networks that train fas riant of convolutional neural networks that train faster and are more stable, even with many layers <ref type="bibr" target="#b31">[32]</ref>). Fig. <ref type="figure" target="#fig_3">5C</ref> depicts
es <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, while DeepFam <ref type="bibr" target="#b13">[14]</ref> considers 2892 COG families of more than 100 sequences eac from averaging.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Both  <ref type="bibr" target="#b13">[14]</ref>. Our ProtENN trained across Pfam full yields a three-fold
es <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, while DeepFam <ref type="bibr" target="#b13">[14]</ref> considers 2892 COG families of more than 100 sequences eac from averaging.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Both  <ref type="bibr" target="#b13">[14]</ref>. Our ProtENN trained across Pfam full yields a three-fold
st one-third of microbial proteins cannot be annotated through alignment to characterized sequences <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Moreover, the run tim
es <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, while DeepFam <ref type="bibr" target="#b13">[14]</ref> considers 2892 COG families of more than 100 sequences eac from averaging.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Both  <ref type="bibr" target="#b13">[14]</ref>. Our ProtENN trained across Pfam full yields a three-fold
embedding for this putative new family and so forth, inspired by current methods such as PSI-BLAST <ref type="bibr" target="#b2">[3]</ref>, jackhmmer <ref type="bibr" target="#b26">[27,</ref><ref typ
I-BLAST <ref type="bibr" target="#b2">[3]</ref>, jackhmmer <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> and hhblits <ref type="bibr" target="#b3">[4]</ref>. This r ke the set of unaligned training sequences as a sequence database, and using phmmer from HMMER 3.1b <ref type="bibr" target="#b27">[28]</ref> we query each test sequence against this database to find
="#tab_0">1</ref>).</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>BLASTp</head><p>BLASTp <ref type="bibr" target="#b37">[38]</ref> is one of the most well known algorithms for searching for
he CNN models we also trained a recurrent neural network (RNN) with single-layer bidirectional LSTM <ref type="bibr" target="#b33">[34]</ref>, which achieved accuracy of 0.982 on the Pfam seed dataset
dels, while the embedding network varies.</p><p>Our ProtCNN networks use residual networks (ResNets <ref type="bibr" target="#b31">[32]</ref>, a variant of convolutional neural networks that train fas riant of convolutional neural networks that train faster and are more stable, even with many layers <ref type="bibr" target="#b31">[32]</ref>). Fig. <ref type="figure" target="#fig_3">5C</ref> depicts
irectly. Two popular neural network sequence models are Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b10">[10]</ref> and recurrent models for sequence generation <ref type="bi
rks (CNNs) <ref type="bibr" target="#b0">[1]</ref> have achieved great success in acoustic modeling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target " target="#b6">6]</ref>, like regular Deep Neural Networks (DNNs), which results in a hybrid system <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target the required non-linear modeling capabilities.</p><p>Unlike the time windows applied in DNN systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target ://www.tei-c.org/ns/1.0"><head n="2.">Convolutional Neural Networks</head><p>Most of the CNN models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target n be very slow due to the iterative multiplications over time when the input sequence is very long; <ref type="bibr" target="#b1">(2)</ref> The training process is sometimes tricky due to the well-kno
ful recurrent neural network architecture used in this context is the Long Short-Term Memory (LSTM) <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" ta ple layers of bi-directional LSTMs and CTC on top which is pre-trained with the transducer networks <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref> obtained the state
speech recognition <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref> is the maxout funct
ful recurrent neural network architecture used in this context is the Long Short-Term Memory (LSTM) <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" ta ple layers of bi-directional LSTMs and CTC on top which is pre-trained with the transducer networks <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref> obtained the state
to the well-known problem of gradient vanishing/exploding <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. Although various approaches have been proposed to address
d n="2.2.2.">Parametric Rectifier Linear Unit</head><p>The Parametric Rectifier Linear Unit (PReLU) <ref type="bibr" target="#b23">[23]</ref> is an extension of the ReLU in which the output of the mod
to improve the results for the task of speech recognition <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" tar
to the well-known problem of gradient vanishing/exploding <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. Although various approaches have been proposed to address
s in acoustic modeling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. In the context of Automatic Speech Recognition, CNNs are usu lts in a hybrid system <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. In the typical hybrid system, the neural net is trained to p applied in DNN systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, the temporal modeling is deployed within convolutional layer Most of the CNN models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> in the speech domain have large filters and use limited weigh connected layers and, finally, CTC is added on the top of the model. Following the suggestion from <ref type="bibr" target="#b3">[4]</ref>, we only perform pooling along the frequency band on the fir <ref type="bibr" target="#b28">[28]</ref>, while pooling in time has been shown to be less helpful <ref type="bibr" target="#b3">[4]</ref>. Specifically, suppose that the i th feature map before and ling. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, we follow the suggestions from <ref type="bibr" target="#b3">[4]</ref> that the max pooling is performed only once after the first
d n="2.2.2.">Parametric Rectifier Linear Unit</head><p>The Parametric Rectifier Linear Unit (PReLU) <ref type="bibr" target="#b23">[23]</ref> is an extension of the ReLU in which the output of the mod
to improve the results for the task of speech recognition <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" tar
speech recognition <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref> is the maxout funct
to improve the results for the task of speech recognition <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" tar
to the well-known problem of gradient vanishing/exploding <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. Although various approaches have been proposed to address
">[1]</ref> have achieved great success in acoustic modeling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. In the context of Auto eep Neural Networks (DNNs), which results in a hybrid system <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. In the typical hybrid lities.</p><p>Unlike the time windows applied in DNN systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, the temporal modeling onvolutional Neural Networks</head><p>Most of the CNN models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> in the speech domain ha
speech recognition <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref> is the maxout funct
TC to perform large vocabulary continuous speech recognition <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16]</ref>. It seems that RNNs have become somewhat of a default metho activation function which has been shown to improve the results for the task of speech recognition <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" ta
ues, such as data/model parallelization across multiple GPUs <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b19">19]</ref> and careful initializations for recurrent connections <ref
b10">[10]</ref> and recurrent models for sequence generation <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b11">11]</ref>.</p><p>To the best of our knowledge, all end-to-end neural
et="#b16">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref> is the maxout function <ref type="bibr" target="#b27">[27]<
Term Memory (LSTM) <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15]</ref>. For example, a mod
on operation is set to 2, then all the potential words can easily fuse into corresponding positions <ref type="bibr" target="#b4">[Kim, 2014]</ref>. As shown in Figure <ref type="figure" target="#fig_ tional efficiency. In general, end-to-end CNNs in NLP have mainly been used for text classification <ref type="bibr" target="#b4">[Kim, 2014]</ref>. For sequence labeling tasks, CNNs have been mainly state ← − h w i , which are concatenated for the NER prediction.</p><p>CNN. We apply a standard CNN <ref type="bibr" target="#b4">[Kim, 2014]</ref> structure on the character or word sequence to obtai 0"><head n="4.3">Hyper-Parameter Settings</head><p>For all four of the datasets, we used the Adamax <ref type="bibr" target="#b4">[Kingma and Ba, 2014]</ref> optimization to train our networks. The in
dentify due to their uncertain boundaries, complex compositions, and NE definitions within the nest <ref type="bibr" target="#b3">[Duan and Zheng, 2011]</ref>. Hence, one intuitive way to perform Chin
tures. In particular, these models can achieve results that are competitive to the state-of-the-art <ref type="bibr" target="#b1">[Che et al., 2013;</ref><ref type="bibr" target="#b6">Wang et al., 201
tures. In particular, these models can achieve results that are competitive to the state-of-the-art <ref type="bibr" target="#b1">[Che et al., 2013;</ref><ref type="bibr" target="#b6">Wang et al., 201
uities cannot be settled without reference to the whole-sentence context and high-level information <ref type="bibr" target="#b5">[Ma et al., 2014]</ref>.</p><p>In this paper, we present a novel convo fter seeing the whole sentence. By adding a feedback layer and feeding back the high-level features <ref type="bibr" target="#b5">[Ma et al., 2014]</ref>, this rethinking mechanism can leverage the hi e use of high-level features to tackle the ambiguity among the potential words is still a key issue <ref type="bibr" target="#b5">[Ma et al., 2014]</ref>.</p><p>In this work, we treat the features at ds can be processed in parallel. The second issue is addressed by the use of a rethinking mechanism <ref type="bibr" target="#b5">[Li et al., 2018]</ref>. Most existing Chinese NER models learn featur m in neural networks have been made in image classification to tackle issues of occlusion and noise <ref type="bibr" target="#b5">[Li et al., 2018]</ref>. <ref type="bibr" target="#b5">Li et al. [2018 sm. Chinese NER with lexicon. To date, using neural networks for NER has been the dominant approach <ref type="bibr" target="#b5">[Ma and Hovy, 2016]</ref>. For Chinese NER, there have been explicit d 14]</ref>. For sequence labeling tasks, CNNs have been mainly used for low-level feature extraction <ref type="bibr" target="#b5">[Ma and Hovy, 2016]</ref> as input for alternative architectures. <ref tion to tackle issues of occlusion and noise <ref type="bibr" target="#b5">[Li et al., 2018]</ref>. <ref type="bibr" target="#b5">Li et al. [2018]</ref> used the output posterior possibilities of a CN method on four datasets, including OntoNotes <ref type="bibr">[Weischedel et al., 2011]</ref>, MSRA <ref type="bibr" target="#b5">[Levow, 2006]</ref>  <ref type="table" target="#tab_1">1</ref>.</p><p> r embeddings and lexicon embeddings<ref type="foot" target="#foot_1">1</ref> trained using word2vec <ref type="bibr" target="#b5">[Mikolov et al., 2013]</ref>, over the automatically segmented Chinese
, . . . , and L-gram X L m ). In this work, we propose the use of the multi-scale feature attention <ref type="bibr" target="#b6">[Wang et al., 2018]</ref> to adaptively selects the features of differ that are competitive to the state-of-the-art <ref type="bibr" target="#b1">[Che et al., 2013;</ref><ref type="bibr" target="#b6">Wang et al., 2013]</ref>. However, due to the influence of the word se
traction <ref type="bibr" target="#b0">[Bunescu and Mooney, 2005]</ref>, question answering systems <ref type="bibr" target="#b2">[Diefenbach et al., 2018]</ref>, and other applications. Compared with "#fig_0">2</ref>). To incorporate the lexicon feature effectively, we use the vectorbased attention <ref type="bibr" target="#b2">[Chen et al., 2018]</ref> to combine the l gram feature with the word
, . . . , and L-gram X L m ). In this work, we propose the use of the multi-scale feature attention <ref type="bibr" target="#b6">[Wang et al., 2018]</ref> to adaptively selects the features of differ that are competitive to the state-of-the-art <ref type="bibr" target="#b1">[Che et al., 2013;</ref><ref type="bibr" target="#b6">Wang et al., 2013]</ref>. However, due to the influence of the word se
uities cannot be settled without reference to the whole-sentence context and high-level information <ref type="bibr" target="#b5">[Ma et al., 2014]</ref>.</p><p>In this paper, we present a novel convo fter seeing the whole sentence. By adding a feedback layer and feeding back the high-level features <ref type="bibr" target="#b5">[Ma et al., 2014]</ref>, this rethinking mechanism can leverage the hi e use of high-level features to tackle the ambiguity among the potential words is still a key issue <ref type="bibr" target="#b5">[Ma et al., 2014]</ref>.</p><p>In this work, we treat the features at ds can be processed in parallel. The second issue is addressed by the use of a rethinking mechanism <ref type="bibr" target="#b5">[Li et al., 2018]</ref>. Most existing Chinese NER models learn featur m in neural networks have been made in image classification to tackle issues of occlusion and noise <ref type="bibr" target="#b5">[Li et al., 2018]</ref>. <ref type="bibr" target="#b5">Li et al. [2018 sm. Chinese NER with lexicon. To date, using neural networks for NER has been the dominant approach <ref type="bibr" target="#b5">[Ma and Hovy, 2016]</ref>. For Chinese NER, there have been explicit d 14]</ref>. For sequence labeling tasks, CNNs have been mainly used for low-level feature extraction <ref type="bibr" target="#b5">[Ma and Hovy, 2016]</ref> as input for alternative architectures. <ref tion to tackle issues of occlusion and noise <ref type="bibr" target="#b5">[Li et al., 2018]</ref>. <ref type="bibr" target="#b5">Li et al. [2018]</ref> used the output posterior possibilities of a CN method on four datasets, including OntoNotes <ref type="bibr">[Weischedel et al., 2011]</ref>, MSRA <ref type="bibr" target="#b5">[Levow, 2006]</ref>  <ref type="table" target="#tab_1">1</ref>.</p><p> r embeddings and lexicon embeddings<ref type="foot" target="#foot_1">1</ref> trained using word2vec <ref type="bibr" target="#b5">[Mikolov et al., 2013]</ref>, over the automatically segmented Chinese
traction <ref type="bibr" target="#b0">[Bunescu and Mooney, 2005]</ref>, question answering systems <ref type="bibr" target="#b2">[Diefenbach et al., 2018]</ref>, and other applications. Compared with "#fig_0">2</ref>). To incorporate the lexicon feature effectively, we use the vectorbased attention <ref type="bibr" target="#b2">[Chen et al., 2018]</ref> to combine the l gram feature with the word
tures. In particular, these models can achieve results that are competitive to the state-of-the-art <ref type="bibr" target="#b1">[Che et al., 2013;</ref><ref type="bibr" target="#b6">Wang et al., 201
ns and argue their shortcomings when used in real systems.</p><p>OS-level page coloring Lin's study <ref type="bibr" target="#b26">[29]</ref> aims to offer cache partitioning by using OS-level page co
r of ways reserved). IPC is generally accepted as a good indicator of relative workload performance <ref type="bibr" target="#b23">[26]</ref> and it is adversely affected by increase of cache misses.<
allocation mechanisms according to the workloads behaviors <ref type="bibr" target="#b19">[22,</ref><ref type="bibr" target="#b20">23,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" tar
rmance interference, then throttles coscheduled workloads to reduce interference. Similarly, Quasar <ref type="bibr" target="#b11">[14]</ref> is a cluster scheduler that profiles workloads and uses th
I/O Performance interference and isolation in I/O subsystem has been studied extensively. SecondNet <ref type="bibr" target="#b15">[18]</ref> and Oktopus <ref type="bibr" target="#b4">[7]</ref> static
ange detection methods <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b35">38]</ref> that can be used and they are pluggable into our work.</p><
sts. There are other workload phase change detection methods <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b35">38]</ref> that can be used and
get="#b33">36,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b39">42]</ref>. These approaches are explored in simulation as opposed to
much larger than the available cache on the socket, MLOAD-60MB generates cyclic data access pattern <ref type="bibr" target="#b32">[35]</ref> so that can not benefit from additional ways. Its IPC does ation requires a comparison between different cache sizes. Some workloads with cyclic access pattern<ref type="bibr" target="#b32">[35]</ref> have this behavior.</figDesc><table><row><cell>low cache r
work in the same research area.</p><p>Performation isolation in memory hiearchy Subramanian et al. <ref type="bibr" target="#b36">[39]</ref> developed an Application Slowdown Model that accurately es
ket. Recognizing the need for cache isolation, Intel introduced a Cache Allocation Technology (CAT) <ref type="bibr" target="#b17">[20]</ref> that allows runtime partitioning of the L3 cache among dif
"the deeper the better" might not be the case in SR. Inspired by the success of very deep networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" targ p networks could suffer from the performance degradation problem, as observed in visual recognition <ref type="bibr" target="#b7">[8]</ref> and image restoration <ref type="bibr" target="#b16">[17]</r nce Sec. 1 overviews DL-based SISR, this section focuses on three most related work to ours: ResNet <ref type="bibr" target="#b7">[8]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref> and DRCN <re iv> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">ResNet</head><p>The main idea of ResNet <ref type="bibr" target="#b7">[8]</ref> is to use a residual learning framework to ease the training iple weight layers in the residual unit) Table <ref type="table">1</ref>. Strategies used in ResNet <ref type="bibr" target="#b7">[8]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref t bel>(1)</label></formula><p>where x is the output of the residual unit, h(x) is an identity mapping <ref type="bibr" target="#b7">[8]</ref> : h(x) = x, W is a set of weights (the biases are omitted to ng the recursive block structure, in which several residual units are stacked. Noted that in ResNet <ref type="bibr" target="#b7">[8]</ref>, different residual units use different inputs for the ident </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Residual Unit</head><p>In ResNet <ref type="bibr" target="#b7">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the acti fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Simplified structures of (a) ResNet<ref type="bibr" target="#b7">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref ty
hieves better performance than the state-of-theart methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. After increasing the depth without adding any parameters, eNet <ref type="bibr" target="#b19">[21]</ref>, Kim et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> propose two very deep convolutional networks for SR, both s ameters. Both the DL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and non-DL <ref type="bibr" target="#b9">[10,</ref><ref typ testing sets, by citing the results of prior methods from <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. The two DRRN models outperforms all existing methods in al chieves better performance than the state-of-theart methods<ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. After increasing the depth without adding any parameters, the performance and significantly outperforms VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref> and RED30 <ref type="bibr" target="#b16">[17]</ref> by 0.3 the other hand, to control the model parameters, the Deeply-Recursive Convolutional Network (DRCN) <ref type="bibr" target="#b13">[14]</ref> introduces a very deep recursive layer via a chain structu ><p>(2) Recursive learning of residual units is proposed in DRRN to keep our model compact. In DRCN <ref type="bibr" target="#b13">[14]</ref>, a deep recursive layer (up to 16 convolutional recursions et <ref type="bibr" target="#b7">[8]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref> and DRCN <ref type="bibr" target="#b13">[14]</ref>. Fig. <ref type="figure" target="#fig_1">2</ref> illustrat type="bibr" target="#b12">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN <ref type="bibr" target="#b13">[14]</ref>. The blue dashed box refers to a recursive layer, among wh esNet <ref type="bibr" target="#b7">[8]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref> and DRRN. U, d, T, and B are the numbers of residual units cursive block.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">DRCN</head><p>DRCN <ref type="bibr" target="#b13">[14]</ref> is motivated by the observation that adding more weight la s a given image x as feature maps H 0 . The inference net f 2 (H 0 ) stacks T recursions (T = 16 in <ref type="bibr" target="#b13">[14]</ref>) in a recursive layer, with shared weights among these rec ">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref>, RED <ref type="bibr" target="#b16">[17]</ref> and DRRN wi es the performance and significantly outperforms VDSR<ref type="bibr" target="#b12">[13]</ref>, DRCN<ref type="bibr" target="#b13">[14]</ref> and RED30<ref type="bibr" target="#b16">[17]</ref> by 0.37 type="bibr" target="#b12">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN<ref type="bibr" target="#b13">[14]</ref>. The blue dashed box refers to a recursive layer, among wh nt CNN models for SR <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar mparison, similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">23]</ref>, we crop pixels nea
th but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. After increasing pe="bibr" target="#b26">28]</ref> on ImageNet <ref type="bibr" target="#b19">[21]</ref>, Kim et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> propose two very d /head></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>By following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">23]</ref>, we use a training mmarizes quantitative results on the four testing sets, by citing the results of prior methods from <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. The two DRRN mode pth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods<ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. After increasing meters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref> and RED30 respectively. On the one hand, to accelerate the convergence speed of very deep networks, the VDSR <ref type="bibr" target="#b12">[13]</ref> is trained with a very high learning rate (10 −1 , instead on focuses on three most related work to ours: ResNet <ref type="bibr" target="#b7">[8]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref> and DRCN <ref type="bibr" target="#b13">[14]</ref>. Fig. < on. Denoting the input as x and the underlying mapping as H(x), the residual mapping is defined as  <ref type="bibr" target="#b12">[13]</ref>. The purple line refers to a global identity mapping. (c) <ref type="table">1</ref>. Strategies used in ResNet <ref type="bibr" target="#b7">[8]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref> and DRRN. ">VDSR</head><p>Differing from ResNet that uses residual learning in every few stacked layers, VDSR <ref type="bibr" target="#b12">[13]</ref> introduces GRL, i.e., residual learning between the input structure of DRRN is illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>. Actually, VDSR <ref type="bibr" target="#b12">[13]</ref> can be viewed as a special case of DRRN, i.e., when U = 0, r that, for each original image, we have 7 additional augmented versions. Besides, inspired by VDSR <ref type="bibr" target="#b12">[13]</ref>, we also use scale augmentation to train our model, and im epochs. Since a large learning rate is used in our work, we adopt the adjustable gradient clipping <ref type="bibr" target="#b12">[13]</ref> to boost the convergence rate while suppressing exploding VDSR re-implementation also uses BN and ReLU as the activation functions, unlike the original VDSR <ref type="bibr" target="#b12">[13]</ref> that does not use BN. These results are faithful since our ese results are faithful since our VDSR re-implementation achieves similar benchmark performance as <ref type="bibr" target="#b12">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref RCNN <ref type="bibr" target="#b1">[2]</ref>, SelfEx <ref type="bibr" target="#b9">[10]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref> and DRRN are illustrated in Fig. <ref type="figure" target /1.0"><head n="4.5.">Discussions</head><p>Since global residual learning has been well discussed in <ref type="bibr" target="#b12">[13]</ref>, in this section, we mainly focus on local residual learni ucture. Local Residual Learning To demonstrate the effectiveness of LRL, DRRN is compared with VDSR <ref type="bibr" target="#b12">[13]</ref>, which has no LRL. For fair comparison, the depth and numb 5]</ref> and FSRCNN <ref type="bibr" target="#b2">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref>, RED <ref ameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR<ref type="bibr" target="#b12">[13]</ref>, DRCN<ref type="bibr" target="#b13">[14]</ref> and RED30<r ResNet<ref type="bibr" target="#b7">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref type="bibr" target="#b12">[13]</ref>. The purple line refers to a global identity mapping. (c) Ratio (PSNR) performance of several recent CNN models for SR <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" tar me depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and non-DL <ref typ images are of the same size. For fair comparison, similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" tar
">31]</ref>. As the pioneer CNN model for SR, Super-Resolution Convolutional Neural Network (SRCNN) <ref type="bibr" target="#b1">[2]</ref> predicts the nonlinear LR-HR mapping via a fully convolution monality among the above CNN models is that their networks contain fewer than 5 layers, e.g., SRCNN <ref type="bibr" target="#b1">[2]</ref> uses 3 convolutional layers. Their deeper structures with 4 ween the input ILR image and the output HR image. There are three notes for VDSR: (1) Un-like SRCNN <ref type="bibr" target="#b1">[2]</ref> that only uses 3 layers, VDSR stacks 20 weight layers (3 × 3 arget="#b32">[34]</ref>. The results are presented in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type="bibr" target="#b1">[2]</ref> SelfEx <ref type="bibr" target="#b9">[10]</ref> RFL <ref typ e as <ref type="bibr" target="#b12">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref type="bibr" target="#b1">[2]</ref>, SelfEx <ref type="bibr" target="#b9">[10]</ref>, VDSR <ref PSyCo [20] and IA <ref type="bibr" target="#b28">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type="bibr" target="#b1">[2]</ref>, DJSR <ref type="bibr" target="#b31">[33]</ref>, CSCN <ref t rsity of images. Shi et al. <ref type="bibr" target="#b23">[25]</ref> observe that the prior models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">32]</ref> increase LR image's 1</ref> shows the Peak Signal-to-Noise Ratio (PSNR) performance of several recent CNN models for SR <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ , k = 297K) structure, which has the same depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ only. Therefore, the input and output images are of the same size. For fair comparison, similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ type="bibr" target="#b9">[10]</ref> RFL <ref type="bibr" target="#b21">[23]</ref>   the results of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">20,</
ive function is optimized via the mini-batch stochastic gradient descent (SGD) with backpropagation <ref type="bibr" target="#b14">[15]</ref>. We implement DRRN via Caffe <ref type="bibr" target="#b11
tion Details</head><p>Data augmentation is performed on the 291-image training dataset. Inspired by <ref type="bibr" target="#b28">[30]</ref>, the flipped and rotated versions of the training images a "bibr" target="#b21">[23]</ref>, NBSRF <ref type="bibr" target="#b20">[22]</ref>, PSyCo [20] and IA <ref type="bibr" target="#b28">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type="bibr" ta
restores the high-frequency information, it is widely used in applications such as medical imaging <ref type="bibr" target="#b24">[26]</ref>, satellite imaging <ref type="bibr" target="#b27">[29]</re
restores the high-frequency information, it is widely used in applications such as medical imaging <ref type="bibr" target="#b24">[26]</ref>, satellite imaging <ref type="bibr" target="#b27">[29]</re
oxes in Fig. <ref type="figure" target="#fig_1">2(d)</ref>), which further facilitates the learning <ref type="bibr" target="#b15">[16]</ref>. We highlight the differences of the network structures be ed to the chain mode, this multipath mode facilitates the learning and is less prone to overfitting <ref type="bibr" target="#b15">[16]</ref>. Therefore, we formulate our residual unit as</p><formula structure of our recursive blocks in Fig. <ref type="figure" target="#fig_3">4</ref>. Motivated by <ref type="bibr" target="#b15">[16]</ref>, we introduce a convolutional layer at the beginning of th the recursive learning is indeed effective under the same structure, and less prone to overfitting <ref type="bibr" target="#b15">[16]</ref>.</p><p>Multi-Path Structure To demonstrate the effectivene
28">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type="bibr" target="#b1">[2]</ref>, DJSR <ref type="bibr" target="#b31">[33]</ref>, CSCN <ref type="bibr" target="#b30">[32]</ref>, ESPCN <re
e activation functions, batch normalization (BN) <ref type="bibr" target="#b10">[11]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref>, are omitted for clarity.</p></div> <div xmlns="http://www ulated as Eq. 1 and the activation functions (BN <ref type="bibr" target="#b10">[11]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref>) are performed after the weight layers. In contrast to suc
ef type="bibr" target="#b30">[32]</ref>, ESPCN <ref type="bibr" target="#b23">[25]</ref> and FSRCNN <ref type="bibr" target="#b2">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type="bibr" tar
<ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref> and RED30 <ref type="bibr" target="#b16">[17]</ref> by 0.37, 0.21 and 0.21 dB respectively.</p><p>posed invers ction, and adopt an ensemble strategy to further improve the performance. Very recently, Mao et al. <ref type="bibr" target="#b16">[17]</ref> propose a 30-layer convolutional auto-encoder network name em, as observed in visual recognition <ref type="bibr" target="#b7">[8]</ref> and image restoration <ref type="bibr" target="#b16">[17]</ref>. The reason may be a significant amount of image details a VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref>, RED <ref type="bibr" target="#b16">[17]</ref> and DRRN with d = 20 and 52. Fig. <ref type="figure" targe SR<ref type="bibr" target="#b12">[13]</ref>, DRCN<ref type="bibr" target="#b13">[14]</ref> and RED30<ref type="bibr" target="#b16">[17]</ref> by 0.37, 0.21 and 0.21 dB respectively.</figDesc></figure> rget="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" tar
<ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref> and RED30 <ref type="bibr" target="#b16">[17]</ref> by 0.37, 0.21 and 0.21 dB respectively.</p><p>posed invers ction, and adopt an ensemble strategy to further improve the performance. Very recently, Mao et al. <ref type="bibr" target="#b16">[17]</ref> propose a 30-layer convolutional auto-encoder network name em, as observed in visual recognition <ref type="bibr" target="#b7">[8]</ref> and image restoration <ref type="bibr" target="#b16">[17]</ref>. The reason may be a significant amount of image details a VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref>, RED <ref type="bibr" target="#b16">[17]</ref> and DRRN with d = 20 and 52. Fig. <ref type="figure" targe SR<ref type="bibr" target="#b12">[13]</ref>, DRCN<ref type="bibr" target="#b13">[14]</ref> and RED30<ref type="bibr" target="#b16">[17]</ref> by 0.37, 0.21 and 0.21 dB respectively.</figDesc></figure> rget="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" tar
type="bibr" target="#b34">[36]</ref>, BSD100 <ref type="bibr" target="#b17">[18]</ref> and Urban100 <ref type="bibr" target="#b9">[10]</ref>, which have 5, 14, 100 and 100 images respectively.</p></di M) <ref type="foot" target="#foot_0">1</ref> . Especially on the recent difficult Ur-ban100 dataset <ref type="bibr" target="#b9">[10]</ref>, DRRN significantly advances the state of the art, with the ted in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type="bibr" target="#b1">[2]</ref> SelfEx <ref type="bibr" target="#b9">[10]</ref> RFL <ref type="bibr" target="#b21">[23]</ref>   the results eported in Tab. Qualitative comparisons among SRCNN <ref type="bibr" target="#b1">[2]</ref>, SelfEx <ref type="bibr" target="#b9">[10]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref> and DRRN ar et5 and Set14. Shallow (non-DL) models include A+ <ref type="bibr" target="#b29">[31]</ref>, SelfEx <ref type="bibr" target="#b9">[10]</ref>, RFL <ref type="bibr" target="#b21">[23]</ref>, NBSRF <ref 2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and non-DL <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b21">23] L <ref type="bibr" target="#b21">[23]</ref>   the results of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b21">23]<
ive function is optimized via the mini-batch stochastic gradient descent (SGD) with backpropagation <ref type="bibr" target="#b14">[15]</ref>. We implement DRRN via Caffe <ref type="bibr" target="#b11
he case in SR. Inspired by the success of very deep networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b26">28]</ref> on ImageNet <ref ty
pe="bibr">[20,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b29">31]</ref>. As the pioneer CNN model for SR, Super-Resolution Convolut that report PSNR for scale factor ×3 on datasets Set5 and Set14. Shallow (non-DL) models include A+ <ref type="bibr" target="#b29">[31]</ref>, SelfEx <ref type="bibr" target="#b9">[10]</ref>, RFL <ref
128 filters of the size 3 × 3.</p><p>For weight initialization, we use the same method as He et al. <ref type="bibr" target="#b6">[7]</ref>, which is shown to be suitable for networks utilizing ReLU.
ructures with only 6 convolutional layers, where the activation functions, batch normalization (BN) <ref type="bibr" target="#b10">[11]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref>, are om et="#b7">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the activation functions (BN <ref type="bibr" target="#b10">[11]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref>) are pe
oblem of Super Resolution (SR), and have demonstrated superiority over reconstruction-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">35]</ref> or other learning pa
oblem of Super Resolution (SR), and have demonstrated superiority over reconstruction-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">35]</ref> or other learning pa
ef type="bibr" target="#b33">[35]</ref> and other 200 images are from Berkeley Segmentation Dataset <ref type="bibr" target="#b17">[18]</ref>. For testing, we utilize four widely used benchmark datase t5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b34">[36]</ref>, BSD100 <ref type="bibr" target="#b17">[18]</ref> and Urban100 <ref type="bibr" target="#b9">[10]</ref>, whi
28">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type="bibr" target="#b1">[2]</ref>, DJSR <ref type="bibr" target="#b31">[33]</ref>, CSCN <ref type="bibr" target="#b30">[32]</ref>, ESPCN <re
(CNN), are widely used to address the ill- . PSNR of recent CNN models for scale factor ×3 on Set5 <ref type="bibr" target="#b0">[1]</ref>. Red points are our models. △, ✩, and • are models with less ="bibr" target="#b17">[18]</ref>. For testing, we utilize four widely used benchmark datasets, Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b34">[36]</ref>, BSD100 <re gure 1</head><label>1</label><figDesc>Figure1. PSNR of recent CNN models for scale factor ×3 on Set5<ref type="bibr" target="#b0">[1]</ref>. Red points are our models. △, ✩, and • are models with less
>[26]</ref>, satellite imaging <ref type="bibr" target="#b27">[29]</ref>, security and surveillance <ref type="bibr" target="#b35">[37]</ref>, where high-frequency details are greatly desired.</p><p>I
e demonstrated superiority over reconstruction-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">35]</ref> or other learning paradigms <ref type="bibr">[20,</ref><ref get="#b21">23]</ref>, we use a training dataset of 291 images, where 91 images are from Yang et al. <ref type="bibr" target="#b33">[35]</ref> and other 200 images are from Berkeley Segmentation Datase
ef type="bibr" target="#b30">[32]</ref>, ESPCN <ref type="bibr" target="#b23">[25]</ref> and FSRCNN <ref type="bibr" target="#b2">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type="bibr" tar
amples unique to the input. Inspired by the learning iterative shrinkage and thresholding algorithm <ref type="bibr" target="#b4">[5]</ref>, Cascaded Sparse Coding Network (CSCN) <ref type="bibr" targ
amples unique to the input. Inspired by the learning iterative shrinkage and thresholding algorithm <ref type="bibr" target="#b4">[5]</ref>, Cascaded Sparse Coding Network (CSCN) <ref type="bibr" targ
"#b30">[32]</ref> is trained end-to-end to fully exploit the natural sparsity of images. Shi et al. <ref type="bibr" target="#b23">[25]</ref> observe that the prior models <ref type="bibr" target="#b1 SR <ref type="bibr" target="#b31">[33]</ref>, CSCN <ref type="bibr" target="#b30">[32]</ref>, ESPCN <ref type="bibr" target="#b23">[25]</ref> and FSRCNN <ref type="bibr" target="#b2">[3]</ref>. Very d get="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b30">32]</ref> versus the number o
oxes in Fig. <ref type="figure" target="#fig_1">2(d)</ref>), which further facilitates the learning <ref type="bibr" target="#b15">[16]</ref>. We highlight the differences of the network structures be ed to the chain mode, this multipath mode facilitates the learning and is less prone to overfitting <ref type="bibr" target="#b15">[16]</ref>. Therefore, we formulate our residual unit as</p><formula structure of our recursive blocks in Fig. <ref type="figure" target="#fig_3">4</ref>. Motivated by <ref type="bibr" target="#b15">[16]</ref>, we introduce a convolutional layer at the beginning of th the recursive learning is indeed effective under the same structure, and less prone to overfitting <ref type="bibr" target="#b15">[16]</ref>.</p><p>Multi-Path Structure To demonstrate the effectivene
n applications such as medical imaging <ref type="bibr" target="#b24">[26]</ref>, satellite imaging <ref type="bibr" target="#b27">[29]</ref>, security and surveillance <ref type="bibr" target="#b35">
e activation functions, batch normalization (BN) <ref type="bibr" target="#b10">[11]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref>, are omitted for clarity.</p></div> <div xmlns="http://www ulated as Eq. 1 and the activation functions (BN <ref type="bibr" target="#b10">[11]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref>) are performed after the weight layers. In contrast to suc
oblem of Super Resolution (SR), and have demonstrated superiority over reconstruction-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">35]</ref> or other learning pa
e activation functions, batch normalization (BN) <ref type="bibr" target="#b10">[11]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref>, are omitted for clarity.</p></div> <div xmlns="http://www ulated as Eq. 1 and the activation functions (BN <ref type="bibr" target="#b10">[11]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref>) are performed after the weight layers. In contrast to suc
(CNN), are widely used to address the ill- . PSNR of recent CNN models for scale factor ×3 on Set5 <ref type="bibr" target="#b0">[1]</ref>. Red points are our models. △, ✩, and • are models with less ="bibr" target="#b17">[18]</ref>. For testing, we utilize four widely used benchmark datasets, Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b34">[36]</ref>, BSD100 <re gure 1</head><label>1</label><figDesc>Figure1. PSNR of recent CNN models for scale factor ×3 on Set5<ref type="bibr" target="#b0">[1]</ref>. Red points are our models. △, ✩, and • are models with less
n applications such as medical imaging <ref type="bibr" target="#b24">[26]</ref>, satellite imaging <ref type="bibr" target="#b27">[29]</ref>, security and surveillance <ref type="bibr" target="#b35">
pe="bibr">[20,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b29">31]</ref>. As the pioneer CNN model for SR, Super-Resolution Convolut that report PSNR for scale factor ×3 on datasets Set5 and Set14. Shallow (non-DL) models include A+ <ref type="bibr" target="#b29">[31]</ref>, SelfEx <ref type="bibr" target="#b9">[10]</ref>, RFL <ref
oblem of Super Resolution (SR), and have demonstrated superiority over reconstruction-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">35]</ref> or other learning pa
he case in SR. Inspired by the success of very deep networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b26">28]</ref> on ImageNet <ref ty
ef type="bibr" target="#b30">[32]</ref>, ESPCN <ref type="bibr" target="#b23">[25]</ref> and FSRCNN <ref type="bibr" target="#b2">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type="bibr" tar
nd ×4 respectively.</p><p>Further, we also use another metric: Information Fidelity Criterion (IFC) <ref type="bibr" target="#b22">[24]</ref> for comparison, which claims to have the highest correlati
(CNN), are widely used to address the ill- . PSNR of recent CNN models for scale factor ×3 on Set5 <ref type="bibr" target="#b0">[1]</ref>. Red points are our models. △, ✩, and • are models with less ="bibr" target="#b17">[18]</ref>. For testing, we utilize four widely used benchmark datasets, Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b34">[36]</ref>, BSD100 <re gure 1</head><label>1</label><figDesc>Figure1. PSNR of recent CNN models for scale factor ×3 on Set5<ref type="bibr" target="#b0">[1]</ref>. Red points are our models. △, ✩, and • are models with less
t="#b29">[30]</ref> and the trace cache architecture as proposed by Rotenberg, Bennett and Smith in <ref type="bibr" target="#b31">[32]</ref>.</p><p>In Section 3 we describe our proposed stream fetch f> shows a block diagram of the trace cache mechanism as proposed by Rotenberg, Benett and Smith in <ref type="bibr" target="#b31">[32]</ref>. The trace cache captures the dynamic instruction stream, <ref type="bibr" target="#b33">[34]</ref>, and the trace cache architecture using a trace predictor <ref type="bibr" target="#b31">[32]</ref> and selective trace storage <ref type="bibr" target="#b28" rget="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> is one such high fetch width mechanism, recently implemente
including state of the art fetch architectures like the FTB proposed by Reinman, Austin and Calder <ref type="bibr" target="#b29">[30]</ref> and the trace cache architecture as proposed by Rotenberg, h prediction mechanism and the instruction cache access, as proposed by Reinman, Austin, and Calder <ref type="bibr" target="#b29">[30]</ref>. The branch prediction mechanism is a fully autonomous eng ion cache is then driven by the requests stored in the FTQ.</p><p>Another important contribution of <ref type="bibr" target="#b29">[30]</ref> is the Fetch Target Buffer (FTB). It extends the BTB by al ssibly containing multiple basic blocks.</p><p>The use of an FTQ is not novel, it was introduced in <ref type="bibr" target="#b29">[30]</ref>. It decouples the branch prediction from the memory access .0"><head n="3.3.">Fetch target queue</head><p>Following the proposal of Reinman, Austin and Calder <ref type="bibr" target="#b29">[30]</ref> we have decoupled the branch prediction stage from the ins ream fetch architecture with three other state-of-the-art fetch architectures: the FTB architecture <ref type="bibr" target="#b29">[30]</ref> using a perceptron branch predictor <ref type="bibr" targe
blockahead predictor <ref type="bibr" target="#b34">[35]</ref>, or the tree-like subgraph predictor <ref type="bibr" target="#b7">[8]</ref> can also be used to implement a fetch engine capable of prov
ance fetch mechanisms, with varying degrees of complexity and performance. The branch address cache <ref type="bibr" target="#b37">[38]</ref>, and the collapsing buffer <ref type="bibr" target="#b6">[
to the decode stage.</p><p>Other multiple branch predictors like the multiple blockahead predictor <ref type="bibr" target="#b34">[35]</ref>, or the tree-like subgraph predictor <ref type="bibr" targ
ance fetch mechanisms, with varying degrees of complexity and performance. The branch address cache <ref type="bibr" target="#b37">[38]</ref>, and the collapsing buffer <ref type="bibr" target="#b6">[
blockahead predictor <ref type="bibr" target="#b34">[35]</ref>, or the tree-like subgraph predictor <ref type="bibr" target="#b7">[8]</ref> can also be used to implement a fetch engine capable of prov
the FTB architecture <ref type="bibr" target="#b29">[30]</ref> using a perceptron branch predictor <ref type="bibr" target="#b17">[18]</ref>, the Alpha EV8 architecture using a 2bcgskew predictor <re
ics to replace the profile data <ref type="bibr" target="#b1">[2]</ref>, or dynamic code optimizers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>While our str
scalar processor, fetching multiple basic blocks per cycle becomes necessary.</p><p>The trace cache <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" targ the Pentium4 <ref type="bibr" target="#b13">[14]</ref>, or enabling dynamic optimization of traces <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. Our instruction str mechanism. Techniques such as path associativity, partial matching, and inactive instruction issue <ref type="bibr" target="#b8">[9]</ref> target an improvement in the trace cache hit rate. Branch pr
ance. The branch address cache <ref type="bibr" target="#b37">[38]</ref>, and the collapsing buffer <ref type="bibr" target="#b6">[7]</ref> represent earlier attempts at a fetch architecture capable o -banked instruction cache, so that we can always guarantee a full width of instructions, as done in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. Our solution requir
addresses). The hash function uses a DOLC scheme similar to what was used in multiscalar processors <ref type="bibr" target="#b15">[16]</ref>.   The predictor maintains two separate path history regis
p><p>The trace cache <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> is one such high fe asic blocks up to the first encountered taken branch, much in the way the SEQ.3 engine described in <ref type="bibr" target="#b30">[31]</ref>.</p><p>The rePLay microarchitecture <ref type="bibr" targe sing layout optimized codes. The sequential engine used in that work is the SEQ.3 unit described in <ref type="bibr" target="#b30">[31]</ref>. However, such engine proves very complex to implement, an an always guarantee a full width of instructions, as done in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. Our solution requires a wider read port to the instruction
cycle, as long as they reside sequentially in the same cache line.</p><p>The Alpha EV8 architecture <ref type="bibr" target="#b33">[34]</ref> uses an interleaved BTB and a multiple branch predictor to or <ref type="bibr" target="#b17">[18]</ref>, the Alpha EV8 architecture using a 2bcgskew predictor <ref type="bibr" target="#b33">[34]</ref>, and the trace cache architecture using a trace predictor
, and several branches, regardless of them being taken or not taken.</p><p>The next trace predictor <ref type="bibr" target="#b16">[17]</ref> provides trace level sequencing. That is, the fetch engine
y improve instruction memory performance, but also have an impact on the branch prediction accuracy <ref type="bibr" target="#b26">[27]</ref>, and the effective fetch width of the front-end <ref type=
e="bibr" target="#b8">[9]</ref> target an improvement in the trace cache hit rate. Branch promotion <ref type="bibr" target="#b20">[21]</ref> targets an increase in the average trace length, allowing
blockahead predictor <ref type="bibr" target="#b34">[35]</ref>, or the tree-like subgraph predictor <ref type="bibr" target="#b7">[8]</ref> can also be used to implement a fetch engine capable of prov
et="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>, and to align branc
get="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>, and to align branches to benefit the underlying fetch arch
fetch width, Yeh and Patt introduced a novel fetch architecture based on dynamic branch prediction <ref type="bibr" target="#b38">[39]</ref>. Figure <ref type="figure" target="#fig_9">2</ref>  The fe a cache line, and select the valid instructions from it.</p><p>The original design by Yeh and Patt <ref type="bibr" target="#b38">[39]</ref> had a single table for the BTB and branch predictor. Calde
scalar processor, fetching multiple basic blocks per cycle becomes necessary.</p><p>The trace cache <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" targ the Pentium4 <ref type="bibr" target="#b13">[14]</ref>, or enabling dynamic optimization of traces <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. Our instruction str mechanism. Techniques such as path associativity, partial matching, and inactive instruction issue <ref type="bibr" target="#b8">[9]</ref> target an improvement in the trace cache hit rate. Branch pr
wer the expected stochastic gradient variance. As shown in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, the reduction of variance can lead to faster convergence. ques, such as stratified sampling <ref type="bibr" target="#b34">[35]</ref> and importance sampling <ref type="bibr" target="#b35">[36]</ref> are proposed to achieve the variance reduction. Different
ommendation algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30]</ref>, and propose a more general framework that combines both co stateof-the-art models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30]</ref> are special cases of our framework (e.g. using MSE-loss/Log rve that the current recommendation models based on MSE-loss <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref> can be improved by others such as SG-loss and pairwise loss bsumes existing models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Stochastic Gradient Descent <ref type="bibr" target= id="formula_1">-(u,v )∈D log σ (f T u g v ) + λE v ′ ∼Pn log σ (−f T u g v ′ )</formula><p>MSE-loss <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_2">(u,v )∈D ( r + uv − f T u have been applied to such problems in many existing work. In <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>, mean square loss ( ng and neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. <ref type="bibr" t
implicit. This is usually referred as "implicit feedback" <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div> <div xml ny real-world scenarios and studied by many papers as well <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. Although collabora
ate side information <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, or item content information <ref type="bibr" target="#b10" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref>, as well as content
added using both weight decay on user embedding and dropout on item embedding. For RNN, we use LSTM <ref type="bibr" target="#b11">[12]</ref> with 50 hidden units. For both models, the dimensions of u
bibr" target="#b27">28]</ref>, or item content information <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref> into the recommendation algorithm. Although these methods c xt recommendation task <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> as an illustrative content information <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Deep Neural Networ ed for the experiments. The first data set CiteULike, collected from CiteU-Like.org, is provided in <ref type="bibr" target="#b30">[31]</ref>. The CiteULike data set contains users bookmarking papers,
in social and knowledge graph mining <ref type="bibr" target="#b1">[2]</ref>, image caption ranking <ref type="bibr" target="#b19">[20]</ref>, and so on. For those scenarios, we believe better samplin
ences, while negative signals are usually implicit. This is usually referred as "implicit feedback" <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta diction, implicit feedback is found in many real-world scenarios and studied by many papers as well <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta
ress the issue and improve performance, hybrid methods are proposed to incorporate side information <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target mpared to traditional feature detectors, such as SIFT and n-grams, DNNs and other embedding methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target
implicit feedback" <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>T any papers as well <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. Although collaborative filtering techniques are powerful, r + uv − f T u g v ) 2 + λE v ′ ∼Pn ( r − uv ′ − f T u g v ′ ) 2</formula><p>Pairwise loss Log-loss <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_3">-(u,v )∈D E v ′ ∼Pn log σ ly seen in implicit feedback setting, some have argued that the pairwise loss would be advantageous <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref>, as pairwise loss e <ref type="table">1</ref> also gives two instances of such loss functions used in existing papers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref> (with γ being the
en increasing, the number of data points in the mini-batch. Sampling techniques are also studied in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39]</ref> to distribute the co
m for the chest database creation or expansion, performing NER training and modeling using NeuroNER <ref type="bibr" target="#b9">[12]</ref> and then generating the current or the new model, and hence
attribute-value structures by the simplified mammographic ontology preset or selected, Taira et al. <ref type="bibr" target="#b5">[6]</ref> develop a natural language processor and a graphical user in
nerated consisting of findings and modifiers automatically <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b2">[3]</ref>. In particularly, based on the structured electronic medical
d Dependency Parser and BIRSDS-Ontology recited from the prior research proposed by Liberman et al. <ref type="bibr" target="#b8">[9]</ref>.</p><p>Actually, Gupta's research is not suitable for realis
nerated consisting of findings and modifiers automatically <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b2">[3]</ref>. In particularly, based on the structured electronic medical
attribute-value structures by the simplified mammographic ontology preset or selected, Taira et al. <ref type="bibr" target="#b5">[6]</ref> develop a natural language processor and a graphical user in
structured medical records data and the inefficiency.</p><p>There are various aspects and researchs <ref type="bibr" target="#b3">[4]</ref> trying to solve the problem mentioned by deploying the skill
no reporting style changes in creating their input radiology free-text document, Hassanpour et al. <ref type="bibr" target="#b6">[7]</ref> describe a machine learning system to annotate radiology rep
no reporting style changes in creating their input radiology free-text document, Hassanpour et al. <ref type="bibr" target="#b6">[7]</ref> describe a machine learning system to annotate radiology rep
structured medical records data and the inefficiency.</p><p>There are various aspects and researchs <ref type="bibr" target="#b3">[4]</ref> trying to solve the problem mentioned by deploying the skill
h as the deep recursive neural network (deep RNN) constructed by stacking multiple recursive layers <ref type="bibr" target="#b0">[1]</ref> from the unstructured medical records, and apply for obtaini
aints. As a result, researchers have explored software-based techniques to tolerate hardware faults <ref type="bibr" target="#b2">[3]</ref>. Softwarebased techniques do not require any modification in
s. Although the time cost was reduced, it brought a large number of false positives.</p><p>The work <ref type="bibr" target="#b15">[16]</ref> proposes a configurable protection technique for SDC-causi s, machine learning based methods are introduced to identify the SDC-causing instructions. The work <ref type="bibr" target="#b15">[16]</ref> proposes a machine learning algorithm based model, namely pproach in detail. We first define some terms used in this paper, some of which are drawn from work <ref type="bibr" target="#b15">[16]</ref>.</p><p>Dynamic Dependency Graph: A Dynamic Dependency Grap ased on prior work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" t are based on duplicating the backward slices of the instructions to protect, similar to prior work <ref type="bibr" target="#b15">[16]</ref>.</p><p>We insert a check immediately after the instruction tion efficiency and SDC impact are imperative parameters for evaluating our approach. In literature <ref type="bibr" target="#b15">[16]</ref>, the SDC detection efficiency (DE) is defined as the ratio

structions have no effect on the program results. In this paper, we use the staticslicing technique <ref type="bibr" target="#b23">[24]</ref> to transform a program to an identical but smaller and sim
structions have no effect on the program results. In this paper, we use the staticslicing technique <ref type="bibr" target="#b23">[24]</ref> to transform a program to an identical but smaller and sim
raining samples (i.e., SDC vulnerability). We build the trees following the random forest framework <ref type="bibr" target="#b24">[25]</ref>. For each tree in the random forest, a subset of samples i
structions have no effect on the program results. In this paper, we use the staticslicing technique <ref type="bibr" target="#b23">[24]</ref> to transform a program to an identical but smaller and sim
ef> ran fault injections for the selected dynamic instruction sequences called "pilots". SymPLIFIED <ref type="bibr" target="#b11">[12]</ref> identified SDC-causing instructions by symbolic execution, study by either predicting their outcomes or showing them equivalent to other faults. SmartInjector <ref type="bibr" target="#b11">[12]</ref> firstly lists all possible faults in an application, and t of instruction. Features are extracted according to the above analysis and also based on prior work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" ta
e shown that SDCs are caused by errors in a relatively small proportion of programs' data variables <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and by selectively pr
applications.</p><p>Various efforts have been made to refine the injection framework. CriticalFault <ref type="bibr" target="#b9">[10]</ref> applied vulnerability analysis to avoid the injections that l fault injection (SFI) to model the soft error rate (SER) of targeted systems.</p><p>CriticalFault <ref type="bibr" target="#b9">[10]</ref> proposes a biased injection framework that employs vulnerab
in a relatively small proportion of programs' data variables <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and by selectively protecting these SDC-prone variables, one ref type="bibr" target="#b18">[19]</ref>. LLFI works at the LLVM compiler's intermediate code level <ref type="bibr" target="#b8">[9]</ref>, and allows fault injections to be performed at specific pro
Cs in real executions. However, it is even more time-consuming than fault injection.</p><p>The work <ref type="bibr" target="#b14">[15]</ref> proposes a software-based method to identify and harden th rently masked in the system. Different instructions of a program have different error-derating rate <ref type="bibr" target="#b14">[15]</ref>. In other words, different instructions of a program have
b20">[21]</ref>, Parboil benchmarks <ref type="bibr" target="#b21">[22]</ref> and PARSEC benchmarks <ref type="bibr" target="#b22">[23]</ref>. We divide the 15 applications into two groups; one group
t="#b12">13,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. In total, 62 features are extracted. We categorize these
e shown that SDCs are caused by errors in a relatively small proportion of programs' data variables <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and by selectively pr
structions have no effect on the program results. In this paper, we use the staticslicing technique <ref type="bibr" target="#b23">[24]</ref> to transform a program to an identical but smaller and sim
SDCs in real executions. However, it was even more time-consuming than fault injection. Shoestring <ref type="bibr" target="#b12">[13]</ref> assumed that instructions, which impact global memory or p pplications.</p><p>Another SDC identifying method is statistical vulnerability analysis. Shoestring <ref type="bibr" target="#b12">[13]</ref> uses a static analysis approach to identify the instructio cording to the above analysis and also based on prior work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" ta
e performance overhead of 15.8%, the simple heuristic it uses only covers 33.9% of SDCs. SymPLIFIED <ref type="bibr" target="#b13">[14]</ref> identifies SDC-causing instructions by symbolic execution,
which are drawn from SPEC benchmarks <ref type="bibr" target="#b19">[20]</ref>, Stanford benchmarks <ref type="bibr" target="#b20">[21]</ref>, Parboil benchmarks <ref type="bibr" target="#b21">[22]</r
SDCs in real executions. However, it was even more time-consuming than fault injection. Shoestring <ref type="bibr" target="#b12">[13]</ref> assumed that instructions, which impact global memory or p pplications.</p><p>Another SDC identifying method is statistical vulnerability analysis. Shoestring <ref type="bibr" target="#b12">[13]</ref> uses a static analysis approach to identify the instructio cording to the above analysis and also based on prior work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" ta
ches or memories, or glitches in combinational logics that can propagate and be captured in latches <ref type="bibr" target="#b0">[1]</ref>. SEU could result in silent data corruption (SDC), which mea
in a relatively small proportion of programs' data variables <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and by selectively protecting these SDC-prone variables, one ref type="bibr" target="#b18">[19]</ref>. LLFI works at the LLVM compiler's intermediate code level <ref type="bibr" target="#b8">[9]</ref>, and allows fault injections to be performed at specific pro
ches or memories, or glitches in combinational logics that can propagate and be captured in latches <ref type="bibr" target="#b0">[1]</ref>. SEU could result in silent data corruption (SDC), which mea
b20">[21]</ref>, Parboil benchmarks <ref type="bibr" target="#b21">[22]</ref> and PARSEC benchmarks <ref type="bibr" target="#b22">[23]</ref>. We divide the 15 applications into two groups; one group
ches or memories, or glitches in combinational logics that can propagate and be captured in latches <ref type="bibr" target="#b0">[1]</ref>. SEU could result in silent data corruption (SDC), which mea
ip and the reduction of chip sizes, the transient fault rate of software will grow with Moore's Law <ref type="bibr" target="#b1">[2]</ref>. Therefore, it is necessary to protect these devices against
applications.</p><p>Various efforts have been made to refine the injection framework. CriticalFault <ref type="bibr" target="#b9">[10]</ref> applied vulnerability analysis to avoid the injections that l fault injection (SFI) to model the soft error rate (SER) of targeted systems.</p><p>CriticalFault <ref type="bibr" target="#b9">[10]</ref> proposes a biased injection framework that employs vulnerab
b20">[21]</ref>, Parboil benchmarks <ref type="bibr" target="#b21">[22]</ref> and PARSEC benchmarks <ref type="bibr" target="#b22">[23]</ref>. We divide the 15 applications into two groups; one group
in a relatively small proportion of programs' data variables <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and by selectively protecting these SDC-prone variables, one ref type="bibr" target="#b18">[19]</ref>. LLFI works at the LLVM compiler's intermediate code level <ref type="bibr" target="#b8">[9]</ref>, and allows fault injections to be performed at specific pro
ese approaches have already been used in mission critical systems for satellites and space missions <ref type="bibr" target="#b3">[4]</ref>.</p><p>Although software-based approaches such as full dupli
e shown that SDCs are caused by errors in a relatively small proportion of programs' data variables <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and by selectively pr
ip and the reduction of chip sizes, the transient fault rate of software will grow with Moore's Law <ref type="bibr" target="#b1">[2]</ref>. Therefore, it is necessary to protect these devices against
Cs in real executions. However, it is even more time-consuming than fault injection.</p><p>The work <ref type="bibr" target="#b14">[15]</ref> proposes a software-based method to identify and harden th rently masked in the system. Different instructions of a program have different error-derating rate <ref type="bibr" target="#b14">[15]</ref>. In other words, different instructions of a program have
r setting, all our ConvNet layer configurations are designed using the same principles, inspired by <ref type="bibr" target="#b3">Ciresan et al. (2011)</ref>; <ref type="bibr" target="#b21">Krizhevsky t="#b23">Lin et al. (2014)</ref>.</p><p>Small-size convolution filters have been previously used by <ref type="bibr" target="#b3">Ciresan et al. (2011)</ref>, but their nets are significantly less dee



s, computed using two networks, which is performed by stacking their respective image descriptors.  <ref type="bibr" target="#b7">(Everingham et al., 2015)</ref>. These datasets contain 10K and 22.5K tation (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task <ref type="bibr" target="#b7">(Everingham et al., 2015)</ref>, which consists in predicting an actio
he whole image and over multiple scales <ref type="bibr" target="#b29">(Sermanet et al., 2014;</ref><ref type="bibr" target="#b16">Howard, 2014)</ref>. In this paper, we address another important aspe
ations of large-scale image classification systems, from high-dimensional shallow feature encodings <ref type="bibr" target="#b26">(Perronnin et al., 2010)</ref> (the winner of ILSVRC-2011) to deep Co



tion-assisted classification pipeline. In this section we evaluate very deep features on Caltech-101<ref type="bibr" target="#b8">(Fei-Fei et al., 2004)</ref> and Caltech-256<ref type="bibr" target="#
18">[19,</ref><ref type="bibr" target="#b15">16]</ref> and transductive experimental design methods <ref type="bibr" target="#b26">[27]</ref>. These kinds of active learning algorithms are referred to ion matrix A. The selected samples are therefore considered to be the most representative.</p><p>In <ref type="bibr" target="#b26">[27]</ref>, an early active learning via a Transduction Experimental problem to solve, thus an approximate solution by a sequential optimization problem is proposed in <ref type="bibr" target="#b26">[27]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head nformative and representative examples for labeling using the min-max margin-based approach. 4. TED <ref type="bibr" target="#b26">[27]</ref> Active learning via Transduction Experimental Design is an
. Active learning is widely studied to solve this kind of sample selection problem. As discussed in <ref type="bibr" target="#b17">[18]</ref>, active learning methods can be divided into two categorie ertain number of labeled samples to evaluate the uncertainty of the unlabeled data or sampling bias <ref type="bibr" target="#b17">[18]</ref> will result. It is therefore recommended that such methods ive learning algorithms are referred to as early active learning or early stage experimental design <ref type="bibr" target="#b17">[18]</ref>. We illustrate the procedures of and example of the tradit a><p>Finding the optimal subset V ⊂ X in Eq. ( <ref type="formula">7</ref>) is NP-hard. Inspired by <ref type="bibr" target="#b17">[18]</ref>, we relax the problem to the following problem by introduc ime, the least squared loss used in Eq. ( <ref type="formula">8</ref>) is sensitive to the outliers <ref type="bibr" target="#b17">[18]</ref>, which makes the algorithm not robust.</p><p>We note that type="bibr" target="#b25">26]</ref>, the 2,1 -norm is used instead of the 2,0 -norm. It is shown in <ref type="bibr" target="#b17">[18]</ref> that the 2,1 -norm is the minimum convex hull of the 2,0 - 1.0"><head n="2.">K-means</head><p>We use the K-means algorithm as another baseline algorithm as in <ref type="bibr" target="#b17">[18]</ref>. In each experiment, samples are ranked by their distances It formulates a regularized linear regression problem which minimizes reconstruction error. 5. RRSS <ref type="bibr" target="#b17">[18]</ref> Early active learning via Robust Representation and Struct ance of the linear methods with our algorithm. This is consistent with the mathematical analysis in <ref type="bibr" target="#b17">[18]</ref> that kernelization produces more discriminative representa ithm not robust.</p><p>We note that in previous researches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref>, the 2,1 -norm is u s, minimization of A 2,1 will achieve the same result as A 2,0 when A is row-sparse. As analyzed in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>, the 2,1 -norm can
s topic has attracted considerable attention in recent years <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target to identify the same person in different camera views among a potentially huge number of imposters <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. At the same time, p Laplacian matrix and D is the degree matrix with each element D ii = j S V (i, j). As discussed in <ref type="bibr" target="#b8">[9]</ref>, minimizing the pairwise constraint will force the similar r images of a person have a high probability of sharing the similar representation features in re-id <ref type="bibr" target="#b8">[9]</ref>, this will make early active learning schema more suitable f
ow dimensional subspace by cross-view quadratic discriminant analysis for metric learning. 4. kLFDA <ref type="bibr" target="#b23">[24]</ref> Kernelized Local Fisher Discriminant Classifier is a close
target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>. In the field of co
nty sampling methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref> query by committee me
body pose, view angle, occlusion and illumination conditions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Supervised r nt camera views among a potentially huge number of imposters <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. At the same time, pairwise labeled data is required for ea sentation <ref type="bibr" target="#b11">[12]</ref>. As in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>, all person images are scaled to 128 × 48 pixels. We then u oday's world. For example, there might be more than over a hundred in one underground train station <ref type="bibr" target="#b19">[20]</ref>.</p><p>To save labor costs, it is essential to design an e bsets as training and test sets, with no overlapping of person identities. Following the setting in <ref type="bibr" target="#b19">[20]</ref>, we divide the probe and gallery sets for re-id as follows
ention in recent years <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targ ula_17">)</formula><p>We denote our Early Active Learning with Pairwise Constraint algorithm in Eq. <ref type="bibr" target="#b9">(10)</ref> as EALPC and the kenerlized version of our algorithm in Eq. gorithm for optimizing the proposed objective function.</p><p>Taking the derivative w.r.t. A in Eq. <ref type="bibr" target="#b9">(10)</ref> and setting it to zero, we obtain 1 :</p><formula xml:id="f algorithm:</p><p>Theorem 1. Algorithm 1 monotonically decreases the objective function value of Eq. <ref type="bibr" target="#b9">(10)</ref> in each iteration. 1 In practice, when xi − Xai = 0, pii ca <ref type="bibr" target="#b11">(12)</ref>.</p><p>Algorithm 1: Algorithm for solving problem in Eq. <ref type="bibr" target="#b9">(10)</ref> Input: The data matrix X ∈ R d×n , parameters α and β. 1 In
g for re-id on four widely referred benchmark datasets for person re-identification.</p><p>1. VIPeR <ref type="bibr" target="#b3">[4]</ref> The VIPeR dataset contains 1,264 images of 632 persons from
g for re-id on four widely referred benchmark datasets for person re-identification.</p><p>1. VIPeR <ref type="bibr" target="#b3">[4]</ref> The VIPeR dataset contains 1,264 images of 632 persons from
same result as A 2,0 when A is row-sparse. As analyzed in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>, the 2,1 -norm can suppress the effect of outlying samples. mera B records 749 persons and Camera A records 385 persons, 200 of whom are same persons. 3. i-LID <ref type="bibr" target="#b29">[30]</ref> The i-LID dataset records 119 individuals captured by thre
handle large dimensional feature vectors while maximizing a Fischer optimization criterion. 5. MFA <ref type="bibr" target="#b24">[25]</ref> Marginal Fisher Analysis method is introduced for dimensio
umination conditions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Supervised re-id methods can achieve promising resul
ype="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref> query by committee methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref>. Most of these activ
early active learning, there are clustering-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref> and transductive experimental design methods <ref type="bib
erformance analysis of the proposed early active learning algorithms on person re-id.</p><p>1. NFST <ref type="bibr" target="#b27">[28]</ref> Null Foley-Sammon Transform space learning is a re-id algo
rent camera. Variations in viewpoint and illumination conditions occur frequently in VIPeR. 2. PRID <ref type="bibr" target="#b4">[5]</ref> The PRID dataset contains images of 385 individuals from two
ype="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref> query by committee methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref>. Most of these activ
target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>. In the field of co
="#b17">[18]</ref>, which makes the algorithm not robust.</p><p>We note that in previous researches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta .tei-c.org/ns/1.0"><head n="4">Convergence Analysis</head><p>We first introduce a lemma proposed in <ref type="bibr" target="#b16">[17]</ref>:</p><p>Lemma 1. For any arbitrary vector m and n there is<
umination conditions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Supervised re-id methods can achieve promising resul
ype="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref> query by committee methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref>. Most of these activ
umination conditions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Supervised re-id methods can achieve promising resul
for labeling when there are already some labeled samples. They include uncertainty sampling methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" targ
rent camera. Variations in viewpoint and illumination conditions occur frequently in VIPeR. 2. PRID <ref type="bibr" target="#b4">[5]</ref> The PRID dataset contains images of 385 individuals from two
="bibr" target="#b21">22]</ref> query by committee methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref>. Most of these active learning methods prefer to select uncer
rent camera. Variations in viewpoint and illumination conditions occur frequently in VIPeR. 2. PRID <ref type="bibr" target="#b4">[5]</ref> The PRID dataset contains images of 385 individuals from two
umination conditions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Supervised re-id methods can achieve promising resul
al. It contains 476 images with large occlusions caused by luggage and viewpoint changes. 4. CAVIAR <ref type="bibr" target="#b1">[2]</ref> The CAVIAR dataset contains 72 individuals captured by two c
same result as A 2,0 when A is row-sparse. As analyzed in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>, the 2,1 -norm can suppress the effect of outlying samples. mera B records 749 persons and Camera A records 385 persons, 200 of whom are same persons. 3. i-LID <ref type="bibr" target="#b29">[30]</ref> The i-LID dataset records 119 individuals captured by thre
handle large dimensional feature vectors while maximizing a Fischer optimization criterion. 5. MFA <ref type="bibr" target="#b24">[25]</ref> Marginal Fisher Analysis method is introduced for dimensio
for labeling when there are already some labeled samples. They include uncertainty sampling methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" targ
revious researches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref>, the 2,1 -norm is used instead of the 2,0 -norm. It is show
ctly from characters have made rapid progress in recent years, and achieved very high voice quality <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" targe s the latent variable does in VAE. Therefore, in this paper we intend to introduce VAE to Tacotron2 <ref type="bibr" target="#b0">[1]</ref>, a state-of-the-art end-to-end speech synthesis model, to le ate before add operation. The attention module and decoder have the same architecture as Tacotron 2 <ref type="bibr" target="#b0">[1]</ref>. Then, WaveNet <ref type="bibr" target="#b18">[19]</ref> voc usually neutral speaking style, is approaching the extreme quality close to human expert recording <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, the interests in expr
f type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and speech generation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> tasks. VAE has man ws the good performance of this method.</p><p>We have become aware of recent work by Akuzawa et al. <ref type="bibr" target="#b11">[12]</ref> which combines an autoregressive speech synthesis model wi
style within or cross speakers based on end-toend TTS model <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>.</p><p>Deep generativ
="bibr" target="#b5">[6]</ref>.</p><p>Deep generative models, such as Variational Autoencoder (VAE) <ref type="bibr" target="#b6">[7]</ref> and Generative Adversarial Network (GAN) <ref type="bibr" ta ="2.1.">Variational Autoencoder</head><p>Variational Autoencoder was first defined by Kingma et al. <ref type="bibr" target="#b6">[7]</ref> which constructs a relationship between unobserved continuou
onsists of three 1-D convolutional layers with 5 width and 512 channels followed by a bidirectional <ref type="bibr" target="#b14">[15]</ref> LSTM <ref type="bibr" target="#b15">[16]</ref> layer using
der have the same architecture as Tacotron 2 <ref type="bibr" target="#b0">[1]</ref>. Then, WaveNet <ref type="bibr" target="#b18">[19]</ref> vocoder is utilized to reconstruct waveform.</p><p>The tot
ch as transferring prosody and speaking style within or cross speakers based on end-toend TTS model <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe ected layers. We use the same architecture and hyperparameters for reference encoder as Wang et al. <ref type="bibr" target="#b3">[4]</ref> which consists of six 2-D convolutional layers followed by a ensional mel spectrograms were extracted with frame shift 12.5 ms and frame length 50 ms. GST model <ref type="bibr" target="#b3">[4]</ref> with character inputs was used as our baseline model. The hy ref> with character inputs was used as our baseline model. The hyperparameters are set according to <ref type="bibr" target="#b3">[4]</ref>. As for our proposed model, the dimension of latent variable
style within or cross speakers based on end-toend TTS model <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>.</p><p>Deep generativ
t encoder state is simply added by z and then is consumed by a location-sensitive attention network <ref type="bibr" target="#b17">[18]</ref> which converts encoded sequence to a fixed-length context
der have the same architecture as Tacotron 2 <ref type="bibr" target="#b0">[1]</ref>. Then, WaveNet <ref type="bibr" target="#b18">[19]</ref> vocoder is utilized to reconstruct waveform.</p><p>The tot
t encoder state is simply added by z and then is consumed by a location-sensitive attention network <ref type="bibr" target="#b17">[18]</ref> which converts encoded sequence to a fixed-length context
="bibr" target="#b5">[6]</ref>.</p><p>Deep generative models, such as Variational Autoencoder (VAE) <ref type="bibr" target="#b6">[7]</ref> and Generative Adversarial Network (GAN) <ref type="bibr" ta ="2.1.">Variational Autoencoder</head><p>Variational Autoencoder was first defined by Kingma et al. <ref type="bibr" target="#b6">[7]</ref> which constructs a relationship between unobserved continuou
ch as transferring prosody and speaking style within or cross speakers based on end-toend TTS model <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe ected layers. We use the same architecture and hyperparameters for reference encoder as Wang et al. <ref type="bibr" target="#b3">[4]</ref> which consists of six 2-D convolutional layers followed by a ensional mel spectrograms were extracted with frame shift 12.5 ms and frame length 50 ms. GST model <ref type="bibr" target="#b3">[4]</ref> with character inputs was used as our baseline model. The hy ref> with character inputs was used as our baseline model. The hyperparameters are set according to <ref type="bibr" target="#b3">[4]</ref>. As for our proposed model, the dimension of latent variable
5 width and 512 channels followed by a bidirectional <ref type="bibr" target="#b14">[15]</ref> LSTM <ref type="bibr" target="#b15">[16]</ref> layer using zoneout <ref type="bibr" target="#b16">[17]</r
t encoder state is simply added by z and then is consumed by a location-sensitive attention network <ref type="bibr" target="#b17">[18]</ref> which converts encoded sequence to a fixed-length context
t encoder state is simply added by z and then is consumed by a location-sensitive attention network <ref type="bibr" target="#b17">[18]</ref> which converts encoded sequence to a fixed-length context
der have the same architecture as Tacotron 2 <ref type="bibr" target="#b0">[1]</ref>. Then, WaveNet <ref type="bibr" target="#b18">[19]</ref> vocoder is utilized to reconstruct waveform.</p><p>The tot
Autoencoder (VAE) <ref type="bibr" target="#b6">[7]</ref> and Generative Adversarial Network (GAN) <ref type="bibr" target="#b7">[8]</ref>, are powerful architectures which can learn complicated dist
ype="bibr">IEEE ICASSP 2019</ref> Work done during internship at Microsoft STC Asia text generation <ref type="bibr" target="#b8">[9]</ref>, image generation <ref type="bibr" target="#b9">[10,</ref><r g or continuously sampling between latent representations which can obtain interpretable homotopies <ref type="bibr" target="#b8">[9]</ref>.</p><p>Intuitively, in speech generation, the latent state o rops to nearly zero and never rises again, which means the encoder doesn't work. Thus, KL annealing <ref type="bibr" target="#b8">[9]</ref> is introduced to our task to solve this problem. That is, du //www.tei-c.org/ns/1.0"><head n="3.2.1.">Interpolation of latent variables</head><p>As mentioned in <ref type="bibr" target="#b8">[9]</ref>, VAE supports smoothly interpolation and continuous sampling
der have the same architecture as Tacotron 2 <ref type="bibr" target="#b0">[1]</ref>. Then, WaveNet <ref type="bibr" target="#b18">[19]</ref> vocoder is utilized to reconstruct waveform.</p><p>The tot
="bibr" target="#b14">[15]</ref> LSTM <ref type="bibr" target="#b15">[16]</ref> layer using zoneout <ref type="bibr" target="#b16">[17]</ref> with probability 0.1. The output text encoder state is sim
="bibr" target="#b5">[6]</ref>.</p><p>Deep generative models, such as Variational Autoencoder (VAE) <ref type="bibr" target="#b6">[7]</ref> and Generative Adversarial Network (GAN) <ref type="bibr" ta ="2.1.">Variational Autoencoder</head><p>Variational Autoencoder was first defined by Kingma et al. <ref type="bibr" target="#b6">[7]</ref> which constructs a relationship between unobserved continuou
e their remarkable performance, recent studies show that GCNs are vulnerable to adversarial attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, i.e. carefully desi bibr" target="#b37">38]</ref> try to attack the model by changing training data and evasion attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref> try to generate fake after (evasion attacks) the training phase of GCNs. • Targeted or Non-targeted. In targeted attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, the attacker focus eted attacks can be further divided into two categories based on attack settings. In direct attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, the attacker can di r example, the attacker tends to connect nodes from different communities to confuse the classifier <ref type="bibr" target="#b6">[7]</ref>. While plain vectors cannot adapt to such changes, Gaussian into the graph. We regard this method as an illustrating example of non-targeted attacks. • RL-S2V <ref type="bibr" target="#b6">[7]</ref> <ref type="foot" target="#foot_2">3</ref> : This method gene
ble research attention <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18]</ref>. State-of-the-art GCNs usually follow a "message-passing" f ion of the Laplacian matrix is avoided, thus reducing the overall time complexity. Kipf and Welling <ref type="bibr" target="#b17">[18]</ref> further propose to simplify the graph convolution using on GCN methods have been proposed, here we focus on a representative one proposed by Kipf and Welling <ref type="bibr" target="#b17">[18]</ref>. Here, the (l + 1) t h convolutional layer is defined as:< istributions by using our Gaussian-based Graph Convolutions.</p><p>Following the original GCN model <ref type="bibr" target="#b17">[18]</ref>, we also impose L 2 regularization on parameters of the fi To evaluate the robustness of RGCN, we compare it with two state-of-the-art GCN models:</p><p>• GCN <ref type="bibr" target="#b17">[18]</ref>: As introduced in Section 3.2 , this is the original GCN m methods is evaluated on a separate test set of 1000 labels. We adopt the same dataset splits as in <ref type="bibr" target="#b17">[18]</ref> and report the average results of 10 runs. In experiments, ectiveness of our proposed method, we adopt three citation networks commonly used in previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>: Cora, Citeseer an "table" target="#tab_0">1</ref>.</p><p>We closely follow the experimental setting in previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. Specifically, we . In experiments, we set the number of layers as two for all methods as suggested by previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. For GCN and RGCN,
to improve efficiency <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref> considering edge at directly applied, e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div> <div xml
n the past few years. Next, we briefly review some representative GCNs, and readers are referred to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref> for some comprehensi
target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref> considering edge attributes <ref type="bibr" target="#b14"> target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n
studies show that GCNs are vulnerable to adversarial attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, i.e. carefully designed small perturbations in graph struc tack the model by changing training data and evasion attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref> try to generate fake samples for a trained model, i.e. the ase of GCNs. • Targeted or Non-targeted. In targeted attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, the attacker focus on misclassifying some target nodes whi o two categories based on attack settings. In direct attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, the attacker can directly manipulate the edges or features tegories <ref type="bibr" target="#b26">[27]</ref>:</p><p>• Poisoning or Evasion. Poisoning attacks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> try to attack the he attacker can directly manipulate the edges or features of the target nodes. In influence attacks <ref type="bibr" target="#b36">[37]</ref>, the attacker can only manipulate other nodes except the t t is designed for evasion and targeted attacks and can only perform direct attacks.</p><p>• NETTACK <ref type="bibr" target="#b36">[37]</ref>: This method also generates adversarial perturbations targ
t="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>, adding residual and jumping connections <ref type="bibr" target="#b31">[32]</ref>, sampling to improve efficiency <ref type="bibr" target="#
matrices. The non-linear activation is ELU (•) <ref type="bibr" target="#b5">[6]</ref> and ReLU (•) <ref type="bibr" target="#b20">[21]</ref> for means and variances respectively since variances are r
type="bibr" target="#b16">[17]</ref>. MPNNs <ref type="bibr" target="#b9">[10]</ref> and GraphSAGE <ref type="bibr" target="#b11">[12]</ref> unify these approaches using the "message-passing" framewo
assign different weights in aggregating node neighborhoods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar riginal GCN model which defines graph convolution as aggregating features from neighborhoods. • GAT <ref type="bibr" target="#b29">[30]</ref>: This model enhances GCN by introducing multihead self-att pt three citation networks commonly used in previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>: Cora, Citeseer and Pubmed <ref type="bibr" target="#b23">[ closely follow the experimental setting in previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. Specifically, we use all node features and 20 labels per c yers as two for all methods as suggested by previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. For GCN and RGCN, we set the number of hidden units as 32.
ng edge attributes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>, disentangling node representations <ref type="bibr" target
t="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>, adding residual and jumping connections <ref type="bibr" target="#b31">[32]</ref>, sampling to improve efficiency <ref type="bibr" target="#
ects of such unexpected adversarial changes in the variances <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36]</ref>. As a result, using Gaussian distributions can enhance the
target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>, disentangling node representations <ref type="bibr" target="#b19">[20]</ref> and automatically selecting hyper-parameters <ref type="bi
resentations <ref type="bibr" target="#b19">[20]</ref> and automatically selecting hyper-parameters <ref type="bibr" target="#b28">[29]</ref>. However, all of these works do not consider the robustnes
r" target="#b2">[3]</ref> first introduce convolutions for graph data using graph signal processing <ref type="bibr" target="#b24">[25]</ref>. By using the eigen-decomposition of the graph Laplacian m
matrices. The non-linear activation is ELU (•) <ref type="bibr" target="#b5">[6]</ref> and ReLU (•) <ref type="bibr" target="#b20">[21]</ref> for means and variances respectively since variances are r
r" target="#b2">[3]</ref> first introduce convolutions for graph data using graph signal processing <ref type="bibr" target="#b24">[25]</ref>. By using the eigen-decomposition of the graph Laplacian m
ng edge attributes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>, disentangling node representations <ref type="bibr" target
t="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>, adding residual and jumping connections <ref type="bibr" target="#b31">[32]</ref>, sampling to improve efficiency <ref type="bibr" target="#
type="bibr" target="#b10">[11]</ref> for all weight matrices. The non-linear activation is ELU (•) <ref type="bibr" target="#b5">[6]</ref> and ReLU (•) <ref type="bibr" target="#b20">[21]</ref> for m
target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref> considering edge attributes <ref type="bibr" target="#b14"> target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n
period under study, which was just over 14% and we were long 653 out of 1077 days.)</p><p>As Sharpe <ref type="bibr" target="#b5">(6)</ref> points out, instead of buying and selling short the DJIA, we

r example, one Granville indicator is the mutual fund cash fund to asset ratio. This is analyzed in <ref type="bibr" target="#b4">(5)</ref>, which concludes this indicator has no predictive power. Ano
r example, one Granville indicator is the mutual fund cash fund to asset ratio. This is analyzed in <ref type="bibr" target="#b4">(5)</ref>, which concludes this indicator has no predictive power. Ano
r example, one Granville indicator is the mutual fund cash fund to asset ratio. This is analyzed in <ref type="bibr" target="#b4">(5)</ref>, which concludes this indicator has no predictive power. Ano
r example, one Granville indicator is the mutual fund cash fund to asset ratio. This is analyzed in <ref type="bibr" target="#b4">(5)</ref>, which concludes this indicator has no predictive power. Ano



r example, one Granville indicator is the mutual fund cash fund to asset ratio. This is analyzed in <ref type="bibr" target="#b4">(5)</ref>, which concludes this indicator has no predictive power. Ano

d by the number of stacked layers (depth). Recent evidence <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref> reveals that network depth is of crucial importance, and th rk depth is of crucial importance, and the leading results <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar to the output <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b47">48]</ref>. In <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b23">24]</ref>, a few intermediat entering layer responses, gradients, and propagated errors, implemented by shortcut connections. In <ref type="bibr" target="#b42">[43]</ref>, an "inception" layer is composed of a shortcut branch and
ction baseline results on PASCAL VOC 2007 and 2012 <ref type="bibr" target="#b4">[5]</ref> and COCO <ref type="bibr" target="#b25">[26]</ref>. We adopt Faster R-CNN <ref type="bibr" target="#b31">[32]
d the "levels" of features can be enriched by the number of stacked layers (depth). Recent evidence <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref> reveals that netwo arget="#b42">43]</ref> reveals that network depth is of crucial importance, and the leading results <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" ta type="bibr" target="#b20">[21]</ref>. For best results, we adopt the fullyconvolutional form as in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b11">12]</ref>, and average the s the challenging ImageNet dataset <ref type="bibr" target="#b34">[35]</ref> all exploit "very deep" <ref type="bibr" target="#b39">[40]</ref> models, with a depth of sixteen <ref type="bibr" target="# > all exploit "very deep" <ref type="bibr" target="#b39">[40]</ref> models, with a depth of sixteen <ref type="bibr" target="#b39">[40]</ref> to thirty <ref type="bibr" target="#b15">[16]</ref>. Many s the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets <ref type="bibr" target="#b39">[40]</ref>. Our ensemble has 3.57% top-5 error on the ImageNet test s selines (Fig. <ref type="figure">3</ref>, middle) are mainly inspired by the philosophy of VGG nets <ref type="bibr" target="#b39">[40]</ref> (Fig. <ref type="figure">3</ref>, left). The convolutional .</head><p>It is worth noticing that our model has fewer filters and lower complexity than VGG nets <ref type="bibr" target="#b39">[40]</ref> (Fig. <ref type="figure">3</ref> Residual Network. Based o >. The image is resized with its shorter side randomly sampled in [256, 480] for scale augmentation <ref type="bibr" target="#b39">[40]</ref>. A 224×224 crop is randomly sampled from an image or its h gains from increased depth.</p><p>Second, compared to its plain counterpart, the 34-layer 6.66 VGG <ref type="bibr" target="#b39">[40]</ref> (v5) 6.8 PReLU-net <ref type="bibr" target="#b11">[12]</re >[32]</ref> as the detection method. Here we are interested in the improvements of replacing VGG-16 <ref type="bibr" target="#b39">[40]</ref> with ResNet-101. The detection implementation (see appendi <p>Our implementation for ImageNet follows the practice in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref>. The image is resized with its shorter side randomly sample
"http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> have led to a seri with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation <ref type="bibr" target="#b21">[22]</ref>.</p><p>When deeper networks are able to start converging,
he leading results <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> on the challenging rget="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b11">12]</ref> and intermediate normalization layers <ref type="bibr" targ r best results, we adopt the fullyconvolutional form as in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b11">12]</ref>, and average the scores at multiple scales (images are resi re activation, following <ref type="bibr" target="#b15">[16]</ref>. We initialize the weights as in <ref type="bibr" target="#b11">[12]</ref> and train all plain/residual nets from scratch. We use SGD ain counterpart, the 34-layer 6.66 VGG <ref type="bibr" target="#b39">[40]</ref> (v5) 6.8 PReLU-net <ref type="bibr" target="#b11">[12]</ref> 4.94 BN-inception <ref type="bibr" target="#b15">[16]</ref rparts. We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in <ref type="bibr" target="#b11">[12]</ref> and BN <ref type="bibr" target="#b15">[16]</ref> but with
et="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> on the challenging ImageNet dataset <ref type="bibr" target b39">[40]</ref> models, with a depth of sixteen <ref type="bibr" target="#b39">[40]</ref> to thirty <ref type="bibr" target="#b15">[16]</ref>. Many other nontrivial visual recognition tasks <ref type= rget="#b35">36,</ref><ref type="bibr" target="#b11">12]</ref> and intermediate normalization layers <ref type="bibr" target="#b15">[16]</ref>, which enable networks with tens of layers to start conver ugmentation in <ref type="bibr" target="#b20">[21]</ref> is used. We adopt batch normalization (BN) <ref type="bibr" target="#b15">[16]</ref> right after each convolution and before activation, follow type="bibr" target="#b15">[16]</ref> right after each convolution and before activation, following <ref type="bibr" target="#b15">[16]</ref>. We initialize the weights as in <ref type="bibr" target=" of 0.9. We do not use dropout <ref type="bibr" target="#b12">[13]</ref>, following the practice in <ref type="bibr" target="#b15">[16]</ref>.</p><p>In testing, for comparison studies we adopt the sta ifficulty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN <ref type="bibr" target="#b15">[16]</ref>, which ensures forward propagated signals to have non-zero et="#b39">[40]</ref> (v5) 6.8 PReLU-net <ref type="bibr" target="#b11">[12]</ref> 4.94 BN-inception <ref type="bibr" target="#b15">[16]</ref> 4.82 ResNet (ILSVRC <ref type="bibr">'15)</ref> 3.57</p><p tum of 0.9, and adopt the weight initialization in <ref type="bibr" target="#b11">[12]</ref> and BN <ref type="bibr" target="#b15">[16]</ref> but with no dropout. These models are trained with a minib
et="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> on the challenging ImageNet dataset <ref type="bibr" target b39">[40]</ref> models, with a depth of sixteen <ref type="bibr" target="#b39">[40]</ref> to thirty <ref type="bibr" target="#b15">[16]</ref>. Many other nontrivial visual recognition tasks <ref type= rget="#b35">36,</ref><ref type="bibr" target="#b11">12]</ref> and intermediate normalization layers <ref type="bibr" target="#b15">[16]</ref>, which enable networks with tens of layers to start conver ugmentation in <ref type="bibr" target="#b20">[21]</ref> is used. We adopt batch normalization (BN) <ref type="bibr" target="#b15">[16]</ref> right after each convolution and before activation, follow type="bibr" target="#b15">[16]</ref> right after each convolution and before activation, following <ref type="bibr" target="#b15">[16]</ref>. We initialize the weights as in <ref type="bibr" target=" of 0.9. We do not use dropout <ref type="bibr" target="#b12">[13]</ref>, following the practice in <ref type="bibr" target="#b15">[16]</ref>.</p><p>In testing, for comparison studies we adopt the sta ifficulty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN <ref type="bibr" target="#b15">[16]</ref>, which ensures forward propagated signals to have non-zero et="#b39">[40]</ref> (v5) 6.8 PReLU-net <ref type="bibr" target="#b11">[12]</ref> 4.94 BN-inception <ref type="bibr" target="#b15">[16]</ref> 4.82 ResNet (ILSVRC <ref type="bibr">'15)</ref> 3.57</p><p tum of 0.9, and adopt the weight initialization in <ref type="bibr" target="#b11">[12]</ref> and BN <ref type="bibr" target="#b15">[16]</ref> but with no dropout. These models are trained with a minib
1,</ref><ref type="bibr" target="#b41">42]</ref> present shortcut connections with gating functions <ref type="bibr" target="#b14">[15]</ref>. These gates are data-dependent and have parameters, in co
">Introduction</head><p>Deep convolutional neural networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> have led to a series of breakthroughs for image classificat type="bibr" target="#b20">21]</ref> have led to a series of breakthroughs for image classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" ta 1.0"><head n="3.4.">Implementation</head><p>Our implementation for ImageNet follows the practice in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref>. The image is resi 4 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted <ref type="bibr" target="#b20">[21]</ref>. The standard color augmentation in <ref type="bibr" targe pixel mean subtracted <ref type="bibr" target="#b20">[21]</ref>. The standard color augmentation in <ref type="bibr" target="#b20">[21]</ref> is used. We adopt batch normalization (BN) <ref type="bibr t="#b15">[16]</ref>.</p><p>In testing, for comparison studies we adopt the standard 10-crop testing <ref type="bibr" target="#b20">[21]</ref>. For best results, we adopt the fullyconvolutional form as
1,</ref><ref type="bibr" target="#b41">42]</ref> present shortcut connections with gating functions <ref type="bibr" target="#b14">[15]</ref>. These gates are data-dependent and have parameters, in co
arget="#b10">11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27]</ref> have also 1 http://image-net.org/challenges/LSVRC/2015/ and
. Value prediction was proposed to address this limitation <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b18">20]</ref>. By predicting the value(s) produced by an instruction (pro iction</head><p>Since the introduction of value prediction <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b18">20]</ref>, there has been a plethora of work on this subject. In gene that belong to the first sequence, a conventional value predictor (e.g., Last-Value-Predictor, LVP <ref type="bibr" target="#b18">[20]</ref>) might mispredict the second load's value because the valu
mplications on the area and energyper-access for the PRF, yielding this option unattractive as well <ref type="bibr" target="#b41">[42]</ref>.</p><p>Our profiling shows that, at any given point in tim
Single-thread (ST) performance is critical for both single-threaded and multi-threaded applications <ref type="bibr" target="#b14">[16]</ref>. Current flagship processors excel at extracting instructi
ss field is 32-bit or 49-bit, and it stores a predicted memory address. ? Confidence is a 2-bit FPC <ref type="bibr" target="#b29">[31]</ref>. An FPC is different than a conventional counter in that e
that prevents such violations.</p><p>Memory Renaming (MR) <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b40">41]</ref> is another application that leverages MDP to predict loads
g a function to the value(s) produced by previous instance(s) of the instruction. Stride predictors <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b12">12]</ref> are good examples s Prediction</head><p>Address predictors can also be categorized into: Computation-based Predictors <ref type="bibr" target="#b10">[10]</ref>, and Context-based Predictors <ref type="bibr" target="#b2 " target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b32">34]</ref>, and to prefetch th
type="bibr" target="#b36">[38]</ref>, SPEC2K6 <ref type="bibr" target="#b37">[39]</ref>, and EEMBC <ref type="bibr" target="#b28">[30]</ref>. Moreover, we enriched our benchmark pool with other popul
e exception: dependent loads are not allowed to be reordered <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b25">27]</ref>. Value prediction can violate this rule. To avoid violating
icroarchitecture, similar to the work of Perais and Seznec <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b27">29]</ref>.</p><p>The recently proposed state-of-art value predictor V p>The recently proposed state-of-art value predictor VTAGE <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b27">29]</ref> addresses some of the key practical challenges facing value proposed state-of-art value predictors VTAGE <ref type="bibr" target="#b26">[28]</ref> and D-VTAGE <ref type="bibr" target="#b27">[29]</ref> are context-based predictors. VTAGE uses several tagged pr
g a function to the value(s) produced by previous instance(s) of the instruction. Stride predictors <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b12">12]</ref> are good examples s Prediction</head><p>Address predictors can also be categorized into: Computation-based Predictors <ref type="bibr" target="#b10">[10]</ref>, and Context-based Predictors <ref type="bibr" target="#b2 " target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b32">34]</ref>, and to prefetch th
prediction accuracy with less stringent confidence requirements. In contrast to branch-path history <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b35">37]</ref>, load-path history e.g., local or global branch history <ref type="bibr" target="#b35">[37]</ref>, branch-path history <ref type="bibr" target="#b24">[26]</ref> ...etc.) Our investigation revealed a new, unique context
n technique for achieving truthfulness in online auctions is based on the concept of a supply curve <ref type="bibr" target="#b21">[22]</ref>, as applied by Zhang et al. <ref type="bibr" target="#b3">
e primal problem (3) is a special case of the multidimensional multiple-choice 0-1 knapsack problem <ref type="bibr" target="#b30">[31]</ref>, which is both NP-hard and, more strongly, has no fully po
arriving workloads and solve optimization problems for load balancing and VM scheduling. Xu et al. <ref type="bibr" target="#b24">[25]</ref> summarize the recent attempts in managing performance over
s in managing performance overhead in clouds. Lin et al. <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> study the energy efficiency in VM provisioning. VM migrati
chanisms have been at the focal point of recent literature on cloud resource allocation and pricing <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Spot Instance <ref the CPU, RAM, and Disk resource pools into typed VM instances, are no longer made randomly a priori <ref type="bibr" target="#b2">[3]</ref>, but made dynamically upon receiving user bids. Dynamic reso Instead of the actual valuation of each bundle, is used in the one-round resource allocation as in <ref type="bibr" target="#b2">(3)</ref>, such that the bid from a user with a smaller remaining budg llocation: Algorithm 2 is our primal-dual approximation algorithm to the NP-hard allocation problem <ref type="bibr" target="#b2">(3)</ref>. is the maximum amount of type-resource at datacenter requir et="#b1">(2)</ref>. Proof of (3): Constraints (1a), (1c), (1d) are guaranteed by the constraints in <ref type="bibr" target="#b2">(3)</ref>. In order to analyze the property about constraint (1b), we we prove <ref type="bibr" target="#b7">(8)</ref>. Now we utilize the inequality (8) to prove claim <ref type="bibr" target="#b2">(3)</ref>. For some user , suppose is the first time . Then by <ref ty
which is both NP-hard and, more strongly, has no fully polynomial-time approximation schemes unless <ref type="bibr" target="#b31">[32]</ref>. What we will pursue in is an auction mechanism, which not
[23]</ref> consider network load when allocating VMs in a distributed cloud system. Maguluri et al. <ref type="bibr" target="#b23">[24]</ref> tackle the randomness of arriving workloads and solve opti
n a preallocated budget for a given time period (e.g., a year or a month like that in an ad-auction <ref type="bibr" target="#b9">[10]</ref>). Thus, a customer's purchase desire drastically declines o f the online algorithm framework when additional information on users' budget spending is available <ref type="bibr" target="#b9">[10]</ref>. Suppose that each user's total spending over rounds is at
algorithms have been proposed in cloud computing scenarios with different focuses. Alicherry et al. <ref type="bibr" target="#b22">[23]</ref> consider network load when allocating VMs in a distributed
of recent work further study auction mechanism design in cloud markets from different perspectives <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. Unfortunately, all ables in LP <ref type="bibr" target="#b5">(6)</ref>. We therefore resort to its dual, formulated in <ref type="bibr" target="#b6">(7)</ref>, where dual variables and associate with primal constraints ociate with primal constraints (6a) and (6b), respectively maximize</p><p>(7b) Even though the dual <ref type="bibr" target="#b6">(7)</ref> has an exponential number of constraints, the ellipsoid meth . 3: Solve the pair of primal-dual decomposition LPs in <ref type="bibr" target="#b5">(6)</ref> and <ref type="bibr" target="#b6">(7)</ref> using the ellipsoid method, using Algorithm 2 as a separatio e describe the separation oracle. In each iteration of the ellipsoid method, a possible solution of <ref type="bibr" target="#b6">(7)</ref> is generated, and is given as the input of the seperation or erates and calls . The correctness of this method has been discussed. Therefore, this oracle solves <ref type="bibr" target="#b6">(7)</ref> as we expect.</p></div> <div xmlns="http://www.tei-c.org/ns/
ithms are applied to solving the underlying allocation problem, VCG loses its truthfulness property <ref type="bibr" target="#b16">[17]</ref>. One usually needs to custom design a payment rule to work
utions.</formula><p>as a means for, e.g., model debugging or architecture selection. A recent paper <ref type="bibr" target="#b3">(Jain and Wallace, 2019)</ref> points to possible pitfalls that may ca ntion Might be Explanation</head><p>In this section, we briefly describe the experimental design of <ref type="bibr" target="#b3">Jain and Wallace (2019)</ref> and look at the results they provide to
t="#b11">Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b8">Mullenbach et al., 2018;</ref><ref type="bibr" target="#b16">Thorne et al., 2019;</ref><ref type="bibr" target="#b14">Serrano and


t="#b11">Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b8">Mullenbach et al., 2018;</ref><ref type="bibr" target="#b16">Thorne et al., 2019;</ref><ref type="bibr" target="#b14">Serrano and
and their suitability for providing explanations for model predictions is a topic of high interest <ref type="bibr" target="#b17">(Xu et al., 2015;</ref><ref type="bibr" target="#b11">Rocktäschel et
s/1.0" place="foot" n="6" xml:id="foot_5">We do not include the Twitter Adverse Drug Reactions (ADR)<ref type="bibr" target="#b9">(Nikfarjam et al., 2015)</ref> dataset as the source tweets are no lon
and their suitability for providing explanations for model predictions is a topic of high interest <ref type="bibr" target="#b17">(Xu et al., 2015;</ref><ref type="bibr" target="#b11">Rocktäschel et
, depending on the amount of human involvement and the difficulty of the task.</p><p>In prior work, <ref type="bibr" target="#b5">Lei et al. (2016)</ref> train a model to simultaneously generate ratio rationales to evaluate their model. Generally, many accept the notion of extractive methods such as <ref type="bibr" target="#b5">Lei et al. (2016)</ref>, in which explanations come directly from the
tion), as plausible. Works such as <ref type="bibr" target="#b8">Mullenbach et al. (2018)</ref> and <ref type="bibr" target="#b1">Ehsan et al. (2019)</ref> use human evaluation to evaluate explanation
nting with the binary classification subset of their tasks, and on models with an LSTM architecture <ref type="bibr" target="#b2">(Hochreiter and Schmidhuber, 1997)</ref>, the only one the authors mak
ures of the common covariates. These factors limit the benefit of applying stratification routinely <ref type="bibr" target="#b8">(8)</ref>.</p><p>Our new method (i) accomplishes the goals of stratifi
cology approach was presented to identify genes associated with adverse cardiovascular drug effects <ref type="bibr" target="#b31">(31)</ref>. Integra-tion of these methods with TWOSIDES may lead to f
e gamma Poisson shrinker (GPS) or Information Component (IC) <ref type="bibr" target="#b6">(6,</ref><ref type="bibr" target="#b7">7)</ref>. These methods estimate confidence intervals (CIs) for the di drug-event associations with little evidence to support them <ref type="bibr" target="#b6">(6,</ref><ref type="bibr" target="#b7">7)</ref>. Stratification is designed to address reporting biases by di

to SIDER) at predicting drug targets and drug indications <ref type="bibr" target="#b13">(13,</ref><ref type="bibr" target="#b14">14)</ref>. Furthermore, we use the new methods to identify adverse dr ecent studies have also shown that drug indications can be predicted using side-effect similarities <ref type="bibr" target="#b14">(14)</ref>. We use the same side-effect similarity scores computed fo
ts when compared to the results of randomized control trials <ref type="bibr" target="#b9">(9)</ref><ref type="bibr" target="#b10">(10)</ref><ref type="bibr" target="#b11">(11)</ref>. However, like ot oup versus the nonexposed group as a function of the available covariates using logistic regression <ref type="bibr" target="#b10">(10)</ref>. Each exposed patient (that is, report) is matched to a no
f>. In an analogous manner, protein structural similarity can explain and predict drug side effects <ref type="bibr" target="#b18">(18)</ref>. More recently, network and chemical properties have been
of thiazides and SSRIs. However, each drug class is individually implicated in causing hyponatremia <ref type="bibr" target="#b32">(32)</ref><ref type="bibr" target="#b33">(33)</ref><ref type="bibr" t
ts when compared to the results of randomized control trials <ref type="bibr" target="#b9">(9)</ref><ref type="bibr" target="#b10">(10)</ref><ref type="bibr" target="#b11">(11)</ref>. However, like ot oup versus the nonexposed group as a function of the available covariates using logistic regression <ref type="bibr" target="#b10">(10)</ref>. Each exposed patient (that is, report) is matched to a no
ized control trials <ref type="bibr" target="#b9">(9)</ref><ref type="bibr" target="#b10">(10)</ref><ref type="bibr" target="#b11">(11)</ref>. However, like other confounder controlling methods, PSM r
network and chemical properties have been combined together into predictive models of drug effects <ref type="bibr" target="#b19">(19)</ref>; these approaches all rely on a comprehensive database of
cedure. We learn residuals only and use extremely high learning rates (10 4 times higher than SRCNN <ref type="bibr" target="#b8">[6]</ref>) enabled by adjustable gradient clipping. Our proposed metho ely, random forest <ref type="bibr" target="#b20">[18]</ref> and convolutional neural network (CNN) <ref type="bibr" target="#b8">[6]</ref> have also been used with large improvements in accuracy.</p> 8">[6]</ref> have also been used with large improvements in accuracy.</p><p>Among them, Dong et al. <ref type="bibr" target="#b8">[6]</ref> has demonstrated that a CNN can be used to learn a mapping f are highly correlated. Moreover, our initial learning rate is 10 4 times higher than that of SRCNN <ref type="bibr" target="#b8">[6]</ref>. This is enabled by residual-learning and gradient clipping. d reconstruction. Filters of spatial sizes 9 × 9, 1 × 1, and 5 × 5 were used respectively.</p><p>In <ref type="bibr" target="#b8">[6]</ref>, Dong et al. attempted to prepare deeper models, but failed ce. We successfully use 20 weight layers (3 × 3 for each layer). Our network is very deep (20 vs. 3 <ref type="bibr" target="#b8">[6]</ref>) and information used for reconstruction (receptive field) i for Very Deep Networks Training deep models can fail to converge in realistic limit of time. SRCNN <ref type="bibr" target="#b8">[6]</ref> fails to show superior performance with more than three weig a network to converge within a week on a common GPU. Looking at Fig. <ref type="figure">9</ref> of <ref type="bibr" target="#b8">[6]</ref>, it is not easy to say their deeper networks have converged n of 200 images from Berkeley Segmentation Dataset <ref type="bibr" target="#b18">[16]</ref>. SRCNN <ref type="bibr" target="#b8">[6]</ref> uses a very large ImageNet dataset.</p><p>We use 291 images The public code of SRCNN based on a CPU implementation is slower than the code used by Dong et. al <ref type="bibr" target="#b8">[6]</ref> in their paper based on a GPU implementation.</p><p>In Figur
/ref><ref type="bibr" target="#b17">15]</ref> methods interpolate the patch subspace. Sparse coding <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" ta ermed SRCNN, does not require any engineered features that are typically necessary in other methods <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" ta bibr" target="#b20">[18]</ref> has two methods, where the first one uses 91 images from Yang et al. <ref type="bibr" target="#b27">[25]</ref> and the second one uses 291 images with the addition of 20
entation Dataset used in Timofte et al. <ref type="bibr" target="#b24">[22]</ref> and Yang and Yang <ref type="bibr" target="#b26">[24]</ref> for benchmark, is also employed.</p></div> <div xmlns="htt
get="#b7">5]</ref>. Dataset 'Urban100', a dataset of urban images recently provided by Huang et al. <ref type="bibr" target="#b13">[11]</ref>, is very interesting as it contains many challenging image A+ <ref type="bibr" target="#b24">[22]</ref>, RFL <ref type="bibr" target="#b20">[18]</ref>, SelfEx <ref type="bibr" target="#b13">[11]</ref> and SRCNN <ref type="bibr" target="#b7">[5]</ref>. In Tabl
to model a mapping from LR to HR patches. Neighbor embedding <ref type="bibr" target="#b6">[4,</ref><ref type="bibr" target="#b17">15]</ref> methods interpolate the patch subspace. Sparse coding <ref resolution methods <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b5">3]</ref> and we find that CNN- n SR comparisons are considered. Possible pairs (s train ,s test ) are tried for the dataset 'Set5' <ref type="bibr" target="#b17">[15]</ref>. Experimental results are summarized in Table <ref type="t can be slightly different.</p><p>Test dataset For benchmark, we use four datasets. Datasets 'Set5' <ref type="bibr" target="#b17">[15]</ref> and 'Set14' <ref type="bibr" target="#b28">[26]</ref> are
d the second one uses 291 images with the addition of 200 images from Berkeley Segmentation Dataset <ref type="bibr" target="#b18">[16]</ref>. SRCNN <ref type="bibr" target="#b8">[6]</ref> uses a very
olution (SISR) <ref type="bibr" target="#b14">[12]</ref>, <ref type="bibr" target="#b10">[8]</ref>, <ref type="bibr" target="#b11">[9]</ref>. SISR is widely used in computer vision applications rangin bibr" target="#b22">[20,</ref><ref type="bibr" target="#b15">13]</ref> or internal patch recurrence <ref type="bibr" target="#b11">[9]</ref>.</p><p>Currently, learning methods are widely used to model ticated models to luminance components as in other methods <ref type="bibr" target="#b6">[4]</ref>, <ref type="bibr" target="#b11">[9]</ref>, <ref type="bibr" target="#b28">[26]</ref>. This is because
get="#b7">5]</ref>. Dataset 'Urban100', a dataset of urban images recently provided by Huang et al. <ref type="bibr" target="#b13">[11]</ref>, is very interesting as it contains many challenging image A+ <ref type="bibr" target="#b24">[22]</ref>, RFL <ref type="bibr" target="#b20">[18]</ref>, SelfEx <ref type="bibr" target="#b13">[11]</ref> and SRCNN <ref type="bibr" target="#b7">[5]</ref>. In Tabl
ng <ref type="bibr" target="#b9">[7]</ref> more powerful methods utilizing statistical image priors <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b15">13]</ref> or internal patch
d the second one uses 291 images with the addition of 200 images from Berkeley Segmentation Dataset <ref type="bibr" target="#b18">[16]</ref>. SRCNN <ref type="bibr" target="#b8">[6]</ref> uses a very
community. Early methods include interpolation such as bicubic interpolation and Lanczos resampling <ref type="bibr" target="#b9">[7]</ref> more powerful methods utilizing statistical image priors <re
ranslation using the recent Ten-sor2Tensor framework and the Transformer sequence-to-sequence model <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref>. We examine some of the critical paramet "6.">Conclusion</head><p>We presented a broad range of basic experiments with the Transformer model <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> for English-to-Czech neural machine tran training steps is given but no indication on "how much converged" the model was at that point, e.g. <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref>. Most probably, the training was run unti r faking the global_step stored in the checkpoint) to make sure the learning rate is not too small. <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref> suggest to average the last 20 checkpoint
observation goes against the common knowledge in other NMT frameworks and deep learning in general <ref type="bibr" target="#b12">(Keskar et al., 2017)</ref> that smaller batches proceed slower (trai
xternal subword units, it comes with its own built-in method similar to the word-piece algorithm by <ref type="bibr" target="#b23">Wu et al. (2016)</ref> and does not expect the input to be even token
) is the new state of the art in machine translation, see e.g. the most recent evaluation campaigns <ref type="bibr" target="#b3">(Bojar et al., 2017a;</ref><ref type="bibr" target="#b7">Cettolo et al


new checkpoints to appear. 10 http://ufal.mff.cuni.cz/czeng/czeng17, which is a subset of CzEng 1.6 <ref type="bibr" target="#b2">(Bojar et al., 2016)</ref>.</p></div> <div xmlns="http://www.tei-c.org

xternal subword units, it comes with its own built-in method similar to the word-piece algorithm by <ref type="bibr" target="#b23">Wu et al. (2016)</ref> and does not expect the input to be even token

e need to scale the learning rate linearly when increasing the effective batch size.</p><p>However, <ref type="bibr" target="#b9">Hoffer et al. (2017)</ref> suggest to use √ k scaling instead of the l atch, we actually increase the actual learning rate √ k times, in accordance with the suggestion of <ref type="bibr" target="#b9">Hoffer et al. (2017)</ref>.<ref type="foot" target="#foot_25">33</ref> the update once after N batches (and summing the gradients). This is similar to the ghost batches of<ref type="bibr" target="#b9">Hoffer et al. (2017)</ref>, but using ghost batch size higher than the s/1.0" place="foot" n="33" xml:id="foot_25">In addition to suggesting the √ k learning-rate scaling,<ref type="bibr" target="#b9">Hoffer et al. (2017)</ref> show that to fully close the "generalizatio
i-c.org/ns/1.0"><head n="2">Related Work</head><p>CNN Compression and Acceleration. Extensive works <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" ta time. We observe a correlation between the pre-fine-tune accuracy and the post fine-tuning accuracy <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b22">22]</ref>. As shown in Table For channel pruning, we use max response selection (pruning the weights according to the magnitude <ref type="bibr" target="#b20">[20]</ref>), and preserve Batch Normalization <ref type="bibr" target /ref>. However, it requires iterative prune &amp; fine-tune procedure to achieve decent performance <ref type="bibr" target="#b20">[20]</ref>, and single-shot pruning without retraining will greatly h
get="#b12">[13]</ref>. We use the compressed VGG-16 (from Sec 4.2) as the backbone for Faster R-CNN <ref type="bibr" target="#b43">[43]</ref>. In Table <ref type="table">5</ref>, AMC achieves 0.7% bet
engine on multiple neural networks, including VGG <ref type="bibr" target="#b45">[45]</ref>, ResNet <ref type="bibr" target="#b21">[21]</ref>, and MobileNet-V1 <ref type="bibr" target="#b23">[23]</ref sing reward R err to accurately find the sparsity ratios for pruning 50% for Plain-20 and ResNet-56 <ref type="bibr" target="#b21">[21]</ref> and compare it with empirical policies. We outperform empi rom hand-crafted heuristic (Figure <ref type="figure">2</ref>). It learns a bottleneck architecture <ref type="bibr" target="#b21">[21]</ref>.</p><p>Accuracy-Guaranteed Compression. By using the R Par respectively. Policy given by AMC looks like sawtooth, which resembles the bottleneck architecture <ref type="bibr" target="#b21">[21]</ref>. The accuracy given by AMC outperforms hand-crafted polici tab_1"><head>Table 2 .</head><label>2</label><figDesc>Pruning policy comparison of Plain-20, ResNets<ref type="bibr" target="#b21">[21]</ref> on CIFAR-10<ref type="bibr" target="#b28">[28]</ref>. RErr performance surpasses many manually designed architectures <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b8">9]</ref>. Cai et al . <ref typ
> decomposes weights into light-weight pieces, for example <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref> proposed to acceler
ssion. Quantization <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">41]</ref> and special convolution implementations <ref type="bibr" ta
model compression <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22]</ref>. The core of model compression technique is to determine th n previous studies <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">22]</ref>. Take convolutional layer as an example. The shape of a wei ing. We compare our approach with three empirical policies <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b22">22]</ref> illustrated in Figure <ref type="figure">2</ref>: uniform s e pre-fine-tune accuracy and the post fine-tuning accuracy <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b22">22]</ref>. As shown in Table <ref type="table" target="#tab_1">2</ref b33">[33]</ref> -3.58 SPP (handcraft) <ref type="bibr" target="#b49">[49]</ref> -2.3 CP (handcraft) <ref type="bibr" target="#b22">[22]</ref> -1.  <ref type="figure" target="#fig_0">4</ref>. We can fi ssion ratio on object detection task. mAP (%) mAP [0.5, 0.95] (%) baseline 68.7 36.7 2× handcrafted <ref type="bibr" target="#b22">[22]</ref> 68.3 (-0.4) 36.7 (-0.0) 4× handcrafted <ref type="bibr" ta 6.7 2× handcrafted <ref type="bibr" target="#b22">[22]</ref> 68.3 (-0.4) 36.7 (-0.0) 4× handcrafted <ref type="bibr" target="#b22">[22]</ref> 66.9 (-1.8)</p><p>35.1 (-1.6) 4× handcrafted <ref type="bi
engine on multiple neural networks, including VGG <ref type="bibr" target="#b45">[45]</ref>, ResNet <ref type="bibr" target="#b21">[21]</ref>, and MobileNet-V1 <ref type="bibr" target="#b23">[23]</ref sing reward R err to accurately find the sparsity ratios for pruning 50% for Plain-20 and ResNet-56 <ref type="bibr" target="#b21">[21]</ref> and compare it with empirical policies. We outperform empi rom hand-crafted heuristic (Figure <ref type="figure">2</ref>). It learns a bottleneck architecture <ref type="bibr" target="#b21">[21]</ref>.</p><p>Accuracy-Guaranteed Compression. By using the R Par respectively. Policy given by AMC looks like sawtooth, which resembles the bottleneck architecture <ref type="bibr" target="#b21">[21]</ref>. The accuracy given by AMC outperforms hand-crafted polici tab_1"><head>Table 2 .</head><label>2</label><figDesc>Pruning policy comparison of Plain-20, ResNets<ref type="bibr" target="#b21">[21]</ref> on CIFAR-10<ref type="bibr" target="#b28">[28]</ref>. RErr performance surpasses many manually designed architectures <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b8">9]</ref>. Cai et al . <ref typ
the transferable network blocks, and its performance surpasses many manually designed architectures <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" ta
Ps ∆acc % VGG-16 FP (handcraft) <ref type="bibr" target="#b31">[31]</ref> 20% -14.6 RNP (handcraft) <ref type="bibr" target="#b33">[33]</ref> -3.58 SPP (handcraft) <ref type="bibr" target="#b49">[49]< sting state-of-the-art channel reduction methods: FP <ref type="bibr" target="#b31">[31]</ref>, RNP <ref type="bibr" target="#b33">[33]</ref> and SPP <ref type="bibr" target="#b49">[49]</ref>. All the ent pruned layers can be summed up linearly, which does not stand according to our experiments. RNP <ref type="bibr" target="#b33">[33]</ref> groups all convolutional channels into 4 sets and trains a
d hardware such as <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b39">39]</ref>. However, it requires iterative prune &amp; fine-tune proce
ssion. Quantization <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">41]</ref> and special convolution implementations <ref type="bibr" ta
with the success of deep neural networks (DNNs), some researches have applied DNNs to precipitation <ref type="bibr" target="#b20">[21]</ref> and radar echo <ref type="bibr" target="#b31">[32]</ref> n ation and intensity of rain and snow. To solve the problem of spatiotemporal dependency, Shi et al. <ref type="bibr" target="#b20">[21]</ref> developed the conventional LSTM and propose convolutional , with a goal to overcome the drawbacks of FC-LSTM in handling spatial-temporal data such as videos <ref type="bibr" target="#b20">[21]</ref>. Specifically, in the ConvLSTM network, the fully-connecte
<ref type="bibr" target="#b28">[29]</ref>, PredCNN <ref type="bibr" target="#b26">[27]</ref>, DCRNN <ref type="bibr" target="#b25">[26]</ref> and StepDeep <ref type="bibr" target="#b19">[20]</ref>. To CNN based architecture. Traffic forecasting is one canonical example of spatiotemporal tasks. DCRNN <ref type="bibr" target="#b25">[26]</ref> models traffic flow as a diffusion process on a directed g
ning Prediction. Generally speaking, modern lightning prediction methods fall into three categories <ref type="bibr" target="#b30">[31]</ref>: numerical diagnosis prediction based on synoptic backgrou
6">[7]</ref>. Preliminary studies focus on understanding the electrification mechanism of lightning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ
den state of the last moment, respectively. The detailed calculation steps of (5) were presented in <ref type="bibr" target="#b1">(2)</ref>. Observation Encoder. The observation encoder is in charge o r loading the initial values, the prediction decoder will run following the calculation as shown in <ref type="bibr" target="#b1">(2)</ref>. In particular, the input and the prediction at a moment t & nfiguration of the prediction decoder. In addition, inspired by the method from machine translation <ref type="bibr" target="#b1">[2]</ref>, the prediction for the moment t will be sent as the input o
l prediction methods in meteorology, and others are data-driven models.</p><p>PR92. The PR92 method <ref type="bibr" target="#b29">[30]</ref> is the most well documented commonly used lightning parame
earches have applied DNNs to precipitation <ref type="bibr" target="#b20">[21]</ref> and radar echo <ref type="bibr" target="#b31">[32]</ref> nowcast. These methods can nowcast weather distribution fo utional neural networks to detect extreme weather in large historical climate datasets. Wang et al. <ref type="bibr" target="#b31">[32]</ref> presented a predictive recurrent neural network (PredRNN)
pe="bibr" target="#b9">[10]</ref> method to adapt learning rates. Adam essentially combines RMSProp <ref type="bibr" target="#b27">[28]</ref> with momentum. The initial learning rate is set to 0.0001
chanism of lightning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref>. These researches provide basic theory and routes for the p
s wildfires, power failures, air traffic disruption, and damage to telecommunication infrastructure <ref type="bibr" target="#b16">[17]</ref>.</p><p>The undoubted importance of lightning has driven si
s wildfires, power failures, air traffic disruption, and damage to telecommunication infrastructure <ref type="bibr" target="#b16">[17]</ref>.</p><p>The undoubted importance of lightning has driven si
motifs or graphlets are defined as small induced subgraphs occurring in a bigger network structure <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" targ tanding the higher-order organizational patterns in networks <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4]</ref>. On the algorithmic side, a large amount of research has been
f disciplines, ranging from social and communication networks to molecular biology and neuroscience <ref type="bibr" target="#b20">[20]</ref>. Typically, these systems are modeled as static graphs tha
on user v's wall at time t. WIKITALK. This dataset represents edits on user talk pages on Wikipedia <ref type="bibr" target="#b16">[16]</ref>. An edge (u, v, t) signifies that user u edited user v's t
f-like patterns have been used to create evolution rules that govern the ways that networks develop <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b24">24]</ref>. The way we consider
ay connected forever <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b17">17]</ref> or aggregate temporal information into a sequence of snapsh EMAIL-EU. This dataset is a collection of emails between members of a European research institution <ref type="bibr" target="#b17">[17]</ref>. An edge (u, v, t) signifies that person u sent person v a
hermore, motifs are critical for understanding the higher-order organizational patterns in networks <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4]</ref>. On the algorithmic si
type="bibr" target="#b17">17]</ref> or aggregate temporal information into a sequence of snapshots <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target
f disciplines, ranging from social and communication networks to molecular biology and neuroscience <ref type="bibr" target="#b20">[20]</ref>. Typically, these systems are modeled as static graphs tha
ll induced subgraphs occurring in a bigger network structure <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b29">29]</ref>. We extend static m hs, where these models have proved crucial to under-standing the mechanisms driving complex systems <ref type="bibr" target="#b19">[19]</ref> and to characterizing classes of static networks <ref type f counts can also be measured with respect to a null model <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b19">19]</ref>. Such analysis may yield additional discoveries. Importantl
ay connected forever <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b17">17]</ref> or aggregate temporal information into a sequence of snapsh EMAIL-EU. This dataset is a collection of emails between members of a European research institution <ref type="bibr" target="#b17">[17]</ref>. An edge (u, v, t) signifies that person u sent person v a
aggregate temporal information into a sequence of snapshots <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b23">23]</ref>. These techniques fai
nowledge can be captured for recovering the high-frequency details in HR images.</p><p>Recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> have successfully tional neural network <ref type="bibr" target="#b1">[2]</ref>. Among them, the CNN-based approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> have recently set ork, making it easy to train. In addition, in previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, only high-level features at top layers were used in the re formance. Instead of using interpolation for upscaling as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, recent studies <ref type="bibr" target="#b2">[3,</ref><ref ere studied and compared in our work. As in previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, only the feature maps at the top layer are used as input f to learn an end-to-end mapping for SR. Subsequently, a deep network with 20 layers was proposed in <ref type="bibr" target="#b10">[11]</ref> to improve the reconstruction accuracy of CNN. The residua on accuracy of CNN. The residuals between the HR images and the interpolated LR images were used in <ref type="bibr" target="#b10">[11]</ref> to speedup the converging speed in training and also to im yers</head><p>In previous SR methods such as SRCNN <ref type="bibr" target="#b1">[2]</ref> and VDSR <ref type="bibr" target="#b10">[11]</ref>, bicubic interpolation is used to upscale LR images to the plus <ref type="bibr" target="#b23">[24]</ref>, SRCNN <ref type="bibr" target="#b1">[2]</ref>, VDSR <ref type="bibr" target="#b10">[11]</ref> and DRCN <ref type="bibr" target="#b11">[12]</ref>. The im <ref type="bibr" target="#b1">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR <ref type="bibr" target="#b10">[11]</ref> with 20-layer CNN. It should be mentioned that the most si Aplus <ref type="bibr" target="#b23">[24]</ref> SRCNN <ref type="bibr" target="#b1">[2]</ref> VDSR <ref type="bibr" target="#b10">[11]</ref> DRCN <ref type="bibr" target="#b11">[12]</ref>   In additi
form single image super-resolution (SISR), and significant improvements over shallow CNN structures <ref type="bibr" target="#b1">[2]</ref> have been observed. One benefit from using deeper networks i [4]</ref>, random forest <ref type="bibr" target="#b19">[20]</ref> and convolutional neural network <ref type="bibr" target="#b1">[2]</ref>. Among them, the CNN-based approaches <ref type="bibr" targe ef> have recently set state of the art for SISR. A network with three layers was first developed in <ref type="bibr" target="#b1">[2]</ref> to learn an end-to-end mapping for SR. Subsequently, a deep tei-c.org/ns/1.0"><head n="3.2.">Deconvolution layers</head><p>In previous SR methods such as SRCNN <ref type="bibr" target="#b1">[2]</ref> and VDSR <ref type="bibr" target="#b10">[11]</ref>, bicubic using other SISR methods, including bicubic, Aplus <ref type="bibr" target="#b23">[24]</ref>, SRCNN <ref type="bibr" target="#b1">[2]</ref>, VDSR <ref type="bibr" target="#b10">[11]</ref> and DRCN <re datasets. On average, an increase of about 1.0 dB using the proposed method was achieved over SRCNN <ref type="bibr" target="#b1">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR < llowing us to train very deep Dataset Bicubic Aplus <ref type="bibr" target="#b23">[24]</ref> SRCNN <ref type="bibr" target="#b1">[2]</ref> VDSR <ref type="bibr" target="#b10">[11]</ref> DRCN <ref typ formation and gradient through the network, making it easy to train. In addition, in previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, only high-level fea also to improve the reconstruction performance. Instead of using interpolation for upscaling as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, recent studies <ref different types of network structures were studied and compared in our work. As in previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, only the feature ma
or the training. During testing, the dataset Set5 <ref type="bibr" target="#b0">[1]</ref> and Set14 <ref type="bibr" target="#b28">[29]</ref> are often used for SR benchmark. The B100 from the Berkele
f skip connections. Many symmetric skip connections were introduced in an encoding-decoding network <ref type="bibr" target="#b16">[17]</ref> for image restoration tasks. However, the improvement of t ormation for solving the SR problem. Therefore, recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref> have employed deconvolution layers to learn the upscaling f
he training of networks. Many recent approaches have been proposed to address this problem. ResNets <ref type="bibr" target="#b5">[6]</ref> and Highway Networks <ref type="bibr" target="#b21">[22]</re as first proposed in <ref type="bibr" target="#b6">[7]</ref>. Different from ResNets as proposed in <ref type="bibr" target="#b5">[6]</ref>, the feature maps are concatenated in DenseNet rather than d
f skip connections. Many symmetric skip connections were introduced in an encoding-decoding network <ref type="bibr" target="#b16">[17]</ref> for image restoration tasks. However, the improvement of t ormation for solving the SR problem. Therefore, recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref> have employed deconvolution layers to learn the upscaling f
s do not necessarily represent visually pleasing results. Recently, perceptual loss was proposed in <ref type="bibr" target="#b9">[10]</ref> for SR to replace the low-level pixel-wise loss. Further, a
d representation power and often generate blurry high resolution outputs. Sparsity-based techniques <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref> have recently deve
re used for testing. In addition, the proposed method was also evaluated using the Urban100 dataset <ref type="bibr" target="#b8">[9]</ref> which includes 100 challenging images. All experiments were e borders of HR images were cropped so that all the results had the same region. The public code in <ref type="bibr" target="#b8">[9]</ref> was used for calculating the evaluation metrics. Table <ref
d representation power and often generate blurry high resolution outputs. Sparsity-based techniques <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref> have recently deve
s do not necessarily represent visually pleasing results. Recently, perceptual loss was proposed in <ref type="bibr" target="#b9">[10]</ref> for SR to replace the low-level pixel-wise loss. Further, a
LR to HR space, including neighbor embedding <ref type="bibr" target="#b3">[4]</ref>, random forest <ref type="bibr" target="#b19">[20]</ref> and convolutional neural network <ref type="bibr" target="
posed to address this problem. ResNets <ref type="bibr" target="#b5">[6]</ref> and Highway Networks <ref type="bibr" target="#b21">[22]</ref> use bypassing path between layers to effectively train net
LR to HR space, including neighbor embedding <ref type="bibr" target="#b3">[4]</ref>, random forest <ref type="bibr" target="#b19">[20]</ref> and convolutional neural network <ref type="bibr" target="
posed to address this problem. ResNets <ref type="bibr" target="#b5">[6]</ref> and Highway Networks <ref type="bibr" target="#b21">[22]</ref> use bypassing path between layers to effectively train net
ayer has a short path  to the loss in the proposed network, leading to an implicit deep supervision <ref type="bibr" target="#b15">[16]</ref>. This can help the training of very deep networks and impr
ave been developed in computer vision community. A detailed review of these methods can be found in <ref type="bibr" target="#b25">[26]</ref>. Among them, interpolation methods are easy to implement a
tionary of atoms. The dictionary can be formed by a database of patches or learnt from the database <ref type="bibr" target="#b26">[27]</ref>. Such dictionary-based methods <ref type="bibr" target="#b
="#b28">[29]</ref> are often used for SR benchmark. The B100 from the Berkeley segmentation dataset <ref type="bibr" target="#b17">[18]</ref> consisting of 100 natural images were used for testing. In
,000 images were randomly selected from ImageNet for the training. During testing, the dataset Set5 <ref type="bibr" target="#b0">[1]</ref> and Set14 <ref type="bibr" target="#b28">[29]</ref> are ofte
y benefit from the collective knowledge of features at different levels. Moreover, previous studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> have shown that redund ssing path between layers to effectively train networks with more than 100 layers. Stochastic depth <ref type="bibr" target="#b7">[8]</ref> randomly drops layers to improve the training of deep residu
mpactness and to improve the computational efficiency. It has been demonstrated in previous studies <ref type="bibr" target="#b22">[23]</ref> that a convolution layer with 1 × 1 kernel can be used as
ed somewhat implicit until residual networks <ref type="bibr" target="#b5">(He et al. (2015)</ref>; <ref type="bibr" target="#b6">He et al. (2016)</ref>) explicitly introduced a reparameterization of ead><p>Since the advent of residual networks <ref type="bibr" target="#b5">(He et al. (2015)</ref>; <ref type="bibr" target="#b6">He et al. (2016)</ref>), most state-of-the-art networks for image clas d of a sequence of such residual blocks. In comparison with the full pre-activation architecture in <ref type="bibr" target="#b6">He et al. (2016)</ref>, we remove two batch normalization layers and o
er remains elusive. Experiments in <ref type="bibr" target="#b4">Goodfellow et al. (2014)</ref> and <ref type="bibr" target="#b3">Dauphin et al. (2014)</ref> suggest that the training objectives have
er remains elusive. Experiments in <ref type="bibr" target="#b4">Goodfellow et al. (2014)</ref> and <ref type="bibr" target="#b3">Dauphin et al. (2014)</ref> suggest that the training objectives have
e training objectives have a limited number of bad local minima with large function values. Work by <ref type="bibr" target="#b2">Choromanska et al. (2015)</ref> draws an analogy between the optimizat
e training objectives have a limited number of bad local minima with large function values. Work by <ref type="bibr" target="#b2">Choromanska et al. (2015)</ref> draws an analogy between the optimizat

UCTION</head><p>Traditional convolutional neural networks for image classification, such as AlexNet <ref type="bibr" target="#b12">(Krizhevsky et al. (2012)</ref>), are parameterized in such a way tha ile no longer state-of-the-art, this performance is significantly better than the 40.7% reported by <ref type="bibr" target="#b12">Krizhevsky et al. (2012)</ref>, as well as the best all-convolutional

alogy between the optimization landscape of neural nets and that of the spin glass model in physics <ref type="bibr" target="#b0">(Auffinger et al. (2013)</ref>). <ref type="bibr" target="#b13">Soudry


target="#b23">(Peters et al., 2018;</ref><ref type="bibr" target="#b24">Radford et al., 2018;</ref><ref type="bibr" target="#b5">Devlin et al., 2019)</ref>, which has improved performances on various ngth of n by the same WordPiece tokenizer <ref type="bibr" target="#b35">(Wu et al., 2016)</ref> in <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>. Next, as shown in Fig. <ref type="figure"> s generated by the cross-modality encoder. For the cross-modality output, following the practice in <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>, we append a special token [CLS] (denoted a and each of them only focuses on a single modality (i.e., language or vision). Different from BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, which applies the transformer encoder onl om branch of Fig. <ref type="figure" target="#fig_0">2</ref>, the task setup is almost same to BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>: words are randomly masked with a probabil n image and a sentence match each other. This task is similar to 'Next Sentence Prediction' in BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>.</p><p>Image Question Answering (QA) In or by the WordPiece tokenizer <ref type="bibr" target="#b35">(Wu et al., 2016)</ref> provided in BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> image to maximize the pre-training compute s. We take Adam (Kingma and Ba, 2014) as the optimizer with a linear-decayed learning-rate schedule <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> and a peak learning rate at 1e − 4. We tra .</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">BERT versus LXMERT</head><p>BERT <ref type="bibr" target="#b5">(Devlin et al., 2019</ref>) is a pre-trained language encoder which im
21, 2019 submission deadline. Since then, there have been some recently updated papers such as MCAN <ref type="bibr" target="#b38">(Yu et al., 2019b)</ref>, MUAN <ref type="bibr" target="#b37">(Yu et

type="bibr">Kim et al. (2018)</ref>, which achieves the best accuracy among other recent works: MFH <ref type="bibr" target="#b14">(Yu et al., 2018)</ref>, Pythia <ref type="bibr" target="#b14">(Jiang curacy among other recent works: MFH <ref type="bibr" target="#b14">(Yu et al., 2018)</ref>, Pythia <ref type="bibr" target="#b14">(Jiang et al., 2018)</ref>, DFAF <ref type="bibr" target="#b6">(Gao e
age Embeddings Instead of using the feature map output by a convolutional neural network, we follow <ref type="bibr" target="#b0">Anderson et al. (2018)</ref> in taking the features of detected object feature f j . Instead of directly using the RoI feature f j without considering its position p j in <ref type="bibr" target="#b0">Anderson et al. (2018)</ref>, we learn a position-aware embedding v j sets w.r.t. all metrics.</figDesc><table><row><cell>. The ob-</cell></row></table><note>(provided by<ref type="bibr" target="#b0">Anderson et al. (2018)</ref>). We do not fine-tune the Faster R-CNN de tector and freeze it as a feature extractor. Different from detecting variable numbers of objects in<ref type="bibr" target="#b0">Anderson et al. (2018)</ref>, we consistently keep 36 objects for each T pretraining is around 22% absolute lower.</p><p>BERT+BUTD Bottom-Up and Top-Down (BUTD) attention <ref type="bibr" target="#b0">(Anderson et al., 2018</ref>) method encodes questions with GRU <ref t utilize it as our single-modality encoders and design our cross-modality encoder based on it. BUTD <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref>  LXMERT is also the only method which ra
or representations of a single image and its descriptive sentence. It consists of three Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> encoders: an object relationship encoder s following the recent progress in designing natural language processing models (e.g., transformers <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>). As shown in Fig. <ref type="figure">1< set of context vectors {y j }. Specifically, we use the multi-head attention following Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>.</p><p>Single-Modality Encoders After th </p><p>Single-Modality Encoders After the embedding layers, we first apply two transformer encoders <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>, i.e., a language en-coder and an object ="#b27">(Seo et al., 2017)</ref> adds modeling layers in solving reading comprehension. Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> is first used in machine translation, we malization (annotated by the '+' sign in Fig. <ref type="figure">1</ref>) after each sublayer as in <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>.</p><p>Cross-Modality Encoder Each cross-
="bibr" target="#b10">He et al., 2016)</ref> and shown their effectiveness on large vision datasets <ref type="bibr" target="#b4">(Deng et al., 2009;</ref><ref type="bibr" target="#b19">Lin et al., 20
type="bibr">Kim et al. (2018)</ref>, which achieves the best accuracy among other recent works: MFH <ref type="bibr" target="#b14">(Yu et al., 2018)</ref>, Pythia <ref type="bibr" target="#b14">(Jiang curacy among other recent works: MFH <ref type="bibr" target="#b14">(Yu et al., 2018)</ref>, Pythia <ref type="bibr" target="#b14">(Jiang et al., 2018)</ref>, DFAF <ref type="bibr" target="#b6">(Gao e
soning dataset where some existing approaches <ref type="bibr" target="#b12">(Hu et al., 2017;</ref><ref type="bibr" target="#b22">Perez et al., 2018)</ref> fail, and the SotA method is 'MaxEnt' in <r
14">(Yu et al., 2018)</ref>, Pythia <ref type="bibr" target="#b14">(Jiang et al., 2018)</ref>, DFAF <ref type="bibr" target="#b6">(Gao et al., 2019a)</ref>, and Cycle-Consistency <ref type="bibr" targ
> applies bi-directional attention to the vision-and-language tasks while its concurrent work BiDAF <ref type="bibr" target="#b27">(Seo et al., 2017)</ref> adds modeling layers in solving reading comp
0"><head>B. FIFO Queue Benchmark</head><p>A simple FIFO queue implementation was written based upon <ref type="bibr" target="#b7">[8]</ref>. The queue is based around a ring buffer, with read and writ re operations are needed (i.e. we do not need CAS). For MIPS64 this has been implemented exactly as <ref type="bibr" target="#b7">[8]</ref>, for Mamba a simple modification was made. The FIFO ring buf
reads. Much effort has been made to optimize coherency protocols to reduce needless communication ( <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>). Furthermore, in o
ed under EPSRC grant EP/F018649/1 not maintained, and a more relaxed consistency model used instead <ref type="bibr" target="#b3">[4]</ref>. This breaks that idea that a shared memory system with cach
f> building on earlier work done by the J-Machine <ref type="bibr" target="#b10">[11]</ref> and HEP <ref type="bibr" target="#b11">[12]</ref>, utilizes hardware multithreading and has a presence bit p
he Cray XMT <ref type="bibr" target="#b9">[10]</ref> building on earlier work done by the J-Machine <ref type="bibr" target="#b10">[11]</ref> and HEP <ref type="bibr" target="#b11">[12]</ref>, utilize
s. Amdahl's law shows that this is limited by the time spent synchronizing and in critical sections <ref type="bibr" target="#b5">[6]</ref>. So for a program to scale well with core count it needs to
ed under EPSRC grant EP/F018649/1 not maintained, and a more relaxed consistency model used instead <ref type="bibr" target="#b3">[4]</ref>. This breaks that idea that a shared memory system with cach
s. Amdahl's law shows that this is limited by the time spent synchronizing and in critical sections <ref type="bibr" target="#b5">[6]</ref>. So for a program to scale well with core count it needs to
Separate hardware mechanisms are used to implemented finegrained locking and barriers. The Cray XMT <ref type="bibr" target="#b9">[10]</ref> building on earlier work done by the J-Machine <ref type="b
Separate hardware mechanisms are used to implemented finegrained locking and barriers. The Cray XMT <ref type="bibr" target="#b9">[10]</ref> building on earlier work done by the J-Machine <ref type="b
ed under EPSRC grant EP/F018649/1 not maintained, and a more relaxed consistency model used instead <ref type="bibr" target="#b3">[4]</ref>. This breaks that idea that a shared memory system with cach
0"><head n="1">Introduction</head><p>Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> has become enormously popular and proven t RT-wwm), we suggest taking another pre-training steps on the task data, which was also suggested by <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>.</p><p>• As there are so many possibilitie settings and data statistics in different task. † represents the dataset was also evaluated by BERT<ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. ‡ represents the dataset was also evaluat , which is beneficial for the researcher to design more powerful models based on them.</p><p>Before <ref type="bibr" target="#b8">Devlin et al. (2019)</ref> releasing BERT with whole word masking, <re <ref type="foot" target="#foot_1">3</ref> , and pre-processed with WikiExtractor.py as suggested by <ref type="bibr" target="#b8">Devlin et al. (2019)</ref>  <ref type="bibr" target="#b8">Devlin et al sed with WikiExtractor.py as suggested by <ref type="bibr" target="#b8">Devlin et al. (2019)</ref>  <ref type="bibr" target="#b8">Devlin et al. (2019)</ref>, for computation efficiency and learning lo
ssing tasks. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD <ref type="bibr" target="#b14">(Rajpurkar et al., 2018)</ref>, CoQA <ref type="bibr" target="#b17">(
tence Pair Matching (SPM): LCQMC <ref type="bibr" target="#b13">(Liu et al., 2018)</ref>, BQ Corpus <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> • Document Classification (DC): THUC-News <r
testing their performance in a wider range.</p><p>• Machine Reading Comprehension (MRC): CMRC 2018 <ref type="bibr" target="#b6">(Cui et al., 2019)</ref>, DRCD <ref type="bibr" target="#b18">(Shao et
18</ref><ref type="bibr">), CJRC (Duan et al., 2019</ref>) • Natural Language Inference (NLI): XNLI <ref type="bibr" target="#b4">(Conneau et al., 2018)</ref> • Sentiment Classification (SC): ChnSenti
tence Pair Matching (SPM): LCQMC <ref type="bibr" target="#b13">(Liu et al., 2018)</ref>, BQ Corpus <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> • Document Classification (DC): THUC-News <r
-wwm-ext, and RoBERTa-wwm-ext-large. 1 2 Tsinghua university has also released a model called ERNIE <ref type="bibr" target="#b23">(Zhang et al., 2019b)</ref> but was not trained on Chinese. In this p
tence Pair Matching (SPM): LCQMC <ref type="bibr" target="#b13">(Liu et al., 2018)</ref>, BQ Corpus <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> • Document Classification (DC): THUC-News <r

18</ref><ref type="bibr">), CJRC (Duan et al., 2019</ref>) • Natural Language Inference (NLI): XNLI <ref type="bibr" target="#b4">(Conneau et al., 2018)</ref> • Sentiment Classification (SC): ChnSenti
Rajpurkar et al., 2018)</ref>, CoQA <ref type="bibr" target="#b17">(Reddy et al., 2019)</ref>, QuAC <ref type="bibr" target="#b3">(Choi et al., 2018)</ref>, NaturalQuestions <ref type="bibr" target="#
type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b23">Zhang et al., 2015a;</ref><ref type="bibr" target="#b20">Yang et al., 2016;</ref><ref type="bibr" target="#b18">Wang et al., 2 out which words are useful and which words are useless. Therefore, we apply an attention mechanism <ref type="bibr" target="#b20">(Yang et al., 2016)</ref> to get those important words and assemble t
obtained from meta-training tasks for a newly seen few-shot task such as intention classification, <ref type="bibr" target="#b5">Han et al. (2018)</ref> present a relation classification dataset -Few one layer convolutional neural networks (CNN). For ease of comparison, its details are the same as <ref type="bibr" target="#b5">Han et al. (2018)</ref> proposed. Hierarchical Attention In order to g with the CNN encoder. For the neural networks based baselines, we use the same hyper parameters as <ref type="bibr" target="#b5">Han et al. (2018)</ref> proposed.</p><p>For our hierarchical attention or 5 way 5 shot and 10 way 5 shot settings on FewRel test set.</figDesc><table /><note>* reported by<ref type="bibr" target="#b5">Han et al. (2018)</ref> and ◇ reported by<ref type="bibr" target="#b3" as achieved excellent performance in few-shot image classification and few-shot text classification <ref type="bibr" target="#b5">(Han et al., 2018;</ref><ref type="bibr" target="#b3">Gao et al., 2019 ttp://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>FewRel Few-Shot Relation Classification <ref type="bibr" target="#b5">(Han et al., 2018)</ref>  </p></div> <div xmlns="http://www.tei-c.org/
ification dataset -FewRel, and adapt most recent state-of-the-art few-shot learning methods for it, <ref type="bibr" target="#b3">Gao et al. (2019)</ref> propose a hybrid attention-based prototypical and useless at the same time.</p><p>So we apply a CNN-based feature attention mechanism similar to <ref type="bibr" target="#b3">Gao et al. (2019)</ref> proposed as a class feature extractor. It depe 17)</ref> which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by <ref type="bibr" target="#b3">Gao et al. (2019)</ref> which includes Proto and PHATT. For a fair com ><table /><note>* reported by<ref type="bibr" target="#b5">Han et al. (2018)</ref> and ◇ reported by<ref type="bibr" target="#b3">Gao et al. (2019)</ref>.</note></figure> 			<note xmlns="http://www.te assification and few-shot text classification <ref type="bibr" target="#b5">(Han et al., 2018;</ref><ref type="bibr" target="#b3">Gao et al., 2019)</ref> tasks respectively, so our model is based on p shra et al., 2018)</ref>, Proto <ref type="bibr" target="#b13">(Snell et al., 2017)</ref> and PHATT <ref type="bibr" target="#b3">(Gao et al., 2019)</ref> respectively.</p></div> <div xmlns="http://ww
few data but achieve good performance. A typical example of this approach is prototypical networks <ref type="bibr" target="#b13">(Snell et al., 2017)</ref>, which averages the vector of few support http://www.tei-c.org/ns/1.0"><head n="4.3">Prototypical Networks</head><p>The prototypical networks <ref type="bibr" target="#b13">(Snell et al., 2017)</ref> has achieved excellent performance in few- cia and Bruna, 2018)</ref>, SNAIL <ref type="bibr" target="#b10">(Mishra et al., 2018)</ref>, Proto <ref type="bibr" target="#b13">(Snell et al., 2017)</ref> and PHATT <ref type="bibr" target="#b3">(G its label, and obviate the need for fine-tuning to adapt to new class types. Prototypical networks <ref type="bibr" target="#b13">(Snell et al., 2017</ref>) learns a metric space in which the model c (y = l i q) = exp(−d(g θ (q), c i ) Σ L l=1 exp(−d(g θ (q), c l )<label>(9)</label></formula><p>As <ref type="bibr" target="#b13">Snell et al. (2017)</ref> mentioned, squared Euclidean distance is a he implementation details are as follows.</p><p>For FewRel dataset, we cite the results reported by <ref type="bibr" target="#b13">Snell et al. (2017)</ref> which includes Finetune, kNN, MetaN, GNN, a
neural networks, they are MetaN <ref type="bibr" target="#b11">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type="bibr" target="#b4">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type="bibr" target="#b10">(

ype="bibr" target="#b0">Bengio (2011)</ref> adopt the target task from the pre-trained models. Then <ref type="bibr" target="#b8">Koch et al. (2015)</ref> explore a method for learning siamese neural
r model with five state-of-the-art fewshot learning models based on neural networks, they are MetaN <ref type="bibr" target="#b11">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type="bibr" target="#b4">(G
class, and it can apply to unseen classes. The early works aim to use transfer learning approaches, <ref type="bibr" target="#b1">Caruana (1994)</ref> and <ref type="bibr" target="#b0">Bengio (2011)</
computer vision field, the research and applications in NLP field are extremely limited. Recently, <ref type="bibr" target="#b21">Yu et al. (2018)</ref> propose an adaptive metric learning approach t
arning siamese neural networks which employs an unique structure to rank similarity between inputs. <ref type="bibr" target="#b17">Vinyals et al. (2016)</ref> use matching networks to map a small labe best hyper parameters, and D test to evaluate the model.</p><p>The "episode" training strategy that <ref type="bibr" target="#b17">Vinyals et al. (2016)</ref> proposed has proved to be effective. For
rks aim to use transfer learning approaches, <ref type="bibr" target="#b1">Caruana (1994)</ref> and <ref type="bibr" target="#b0">Bengio (2011)</ref> adopt the target task from the pre-trained models.
class, and it can apply to unseen classes. The early works aim to use transfer learning approaches, <ref type="bibr" target="#b1">Caruana (1994)</ref> and <ref type="bibr" target="#b0">Bengio (2011)</
dataset, we implement all above seven baseline models and our models. we use the Baidu Encyclopedia <ref type="bibr" target="#b9">(Li et al., 2018)</ref> as our initialized word representation, it inc
dominant text classification models in deep learning <ref type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b23">Zhang et al., 2015a;</ref><ref type="bibr" target="#b20">Yang et al.,
nd prototype representations of each class and classify the query to the nearest prototype's class. <ref type="bibr" target="#b15">Sung et al. (2018)</ref> propose a two-branch relation networks, whic

dominant text classification models in deep learning <ref type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b23">Zhang et al., 2015a;</ref><ref type="bibr" target="#b20">Yang et al.,
ype="bibr" target="#b0">Bengio (2011)</ref> adopt the target task from the pre-trained models. Then <ref type="bibr" target="#b8">Koch et al. (2015)</ref> explore a method for learning siamese neural
r model with five state-of-the-art fewshot learning models based on neural networks, they are MetaN <ref type="bibr" target="#b11">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type="bibr" target="#b4">(G
uch as bagof-words or n-grams <ref type="bibr" target="#b19">(Wang and Manning, 2012)</ref> or SVMs <ref type="bibr" target="#b16">(Tang et al., 2015)</ref>. The neural network based methods like <ref

hdalai and Yu, 2017)</ref>, GNN <ref type="bibr" target="#b4">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type="bibr" target="#b10">(Mishra et al., 2018)</ref>, Proto <ref type="bibr" target="#b13">(Sn
nd prototype representations of each class and classify the query to the nearest prototype's class. <ref type="bibr" target="#b15">Sung et al. (2018)</ref> propose a two-branch relation networks, whic

r model with five state-of-the-art fewshot learning models based on neural networks, they are MetaN <ref type="bibr" target="#b11">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type="bibr" target="#b4">(G
d embeddings and hyperparameters of instance encoder as PHATT proposed. In detail, we use the Glove <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref> consisting of 6B tokens and 400K voca
dominant text classification models in deep learning <ref type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b23">Zhang et al., 2015a;</ref><ref type="bibr" target="#b20">Yang et al.,
nd prototype representations of each class and classify the query to the nearest prototype's class. <ref type="bibr" target="#b15">Sung et al. (2018)</ref> propose a two-branch relation networks, whic
ype="bibr" target="#b0">Bengio (2011)</ref> adopt the target task from the pre-trained models. Then <ref type="bibr" target="#b8">Koch et al. (2015)</ref> explore a method for learning siamese neural
hdalai and Yu, 2017)</ref>, GNN <ref type="bibr" target="#b4">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type="bibr" target="#b10">(Mishra et al., 2018)</ref>, Proto <ref type="bibr" target="#b13">(Sn
ibr" target="#b23">Zhang et al., 2015a;</ref><ref type="bibr" target="#b20">Yang et al., 2016;</ref><ref type="bibr" target="#b18">Wang et al., 2018)</ref> require a considerable amount of labeled dat
hdalai and Yu, 2017)</ref>, GNN <ref type="bibr" target="#b4">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type="bibr" target="#b10">(Mishra et al., 2018)</ref>, Proto <ref type="bibr" target="#b13">(Sn
dominant text classification models in deep learning <ref type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b23">Zhang et al., 2015a;</ref><ref type="bibr" target="#b20">Yang et al.,
rks aim to use transfer learning approaches, <ref type="bibr" target="#b1">Caruana (1994)</ref> and <ref type="bibr" target="#b0">Bengio (2011)</ref> adopt the target task from the pre-trained models.
dominant text classification models in deep learning <ref type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b23">Zhang et al., 2015a;</ref><ref type="bibr" target="#b20">Yang et al.,
neural networks, they are MetaN <ref type="bibr" target="#b11">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type="bibr" target="#b4">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type="bibr" target="#b10">(
lve it. The traditional methods mainly focus on feature engineerings such as bagof-words or n-grams <ref type="bibr" target="#b19">(Wang and Manning, 2012)</ref> or SVMs <ref type="bibr" target="#b16"
uch as bagof-words or n-grams <ref type="bibr" target="#b19">(Wang and Manning, 2012)</ref> or SVMs <ref type="bibr" target="#b16">(Tang et al., 2015)</ref>. The neural network based methods like <ref
dominant text classification models in deep learning <ref type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b23">Zhang et al., 2015a;</ref><ref type="bibr" target="#b20">Yang et al.,
ibr" target="#b23">Zhang et al., 2015a;</ref><ref type="bibr" target="#b20">Yang et al., 2016;</ref><ref type="bibr" target="#b18">Wang et al., 2018)</ref> require a considerable amount of labeled dat
class, and it can apply to unseen classes. The early works aim to use transfer learning approaches, <ref type="bibr" target="#b1">Caruana (1994)</ref> and <ref type="bibr" target="#b0">Bengio (2011)</
d embeddings and hyperparameters of instance encoder as PHATT proposed. In detail, we use the Glove <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref> consisting of 6B tokens and 400K voca
ype="bibr" target="#b0">Bengio (2011)</ref> adopt the target task from the pre-trained models. Then <ref type="bibr" target="#b8">Koch et al. (2015)</ref> explore a method for learning siamese neural
hallenges above, we propose to model the attributed networks with graph convolutional network (GCN) <ref type="bibr" target="#b15">[16]</ref>. GCN, which takes the topological structure and nodal attr se a new type of attributed network encoder inspired by the graph convolutional network (GCN) model <ref type="bibr" target="#b15">[16]</ref>. Specifically, GCN considers the high-order node proximity a particular layer, the convolution operation is D − 1 2 A D − 1 2 XW, and its complexity is O(mdh) <ref type="bibr" target="#b15">[16]</ref> as AX can be efficiently implemented using sparse-dense ma rning performance by considering neighbors of nodes that are multiple hops away. In particular, GCN <ref type="bibr" target="#b15">[16]</ref> takes the structure and attribute information as input, an autoencoder architecture. Meanwhile, recent research advances on graph convolutional network (GCN) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" targ
atures for each node <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. They are increasingly used to model a wide range of comple
="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary -Deep Autoencoder</head><p>As suggested by <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta
ience have shown that data often exhibits correlation among the attributes of connected individuals <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29]</ref>, and such insights
ode interactions are observed, attributed networks also encode a rich set of features for each node <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ tems, such as social media networks, critical infrastructure networks, and gene regulatory networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>. For example, in soc
ed networks also encode a rich set of features for each node <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. They are increasin
]</ref> detects anomalies at the contextual level and only considers nodal attributes.</p><p>• SCAN <ref type="bibr" target="#b33">[34]</ref> is a structure based detection method</p><p>• Radar <ref t
unsupervised. Among them, one family of methods study the problem at the mesoscope with ego-network <ref type="bibr" target="#b23">[24]</ref> or community analysis <ref type="bibr" target="#b9">[10]</ unity anomalies within a unified probabilistic model <ref type="bibr" target="#b9">[10]</ref>. AMEN <ref type="bibr" target="#b23">[24]</ref> considers the ego-network information for each node and di mlns="http://www.tei-c.org/ns/1.0"><p>which detects anomalies at the structural level.</p><p>• AMEN <ref type="bibr" target="#b23">[24]</ref> uses both attribute and network structure information to d
n and attempt to find anomalies in a node feature subspace <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar used on spotting abnormal nodes in a node feature subspace <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21]</ref>. For example, GOutR
substructure in which a small set of nodes are much more closely linked to each other than average <ref type="bibr" target="#b29">[30]</ref>. Therefore, after we specify the clique size as m, we rand
m at the mesoscope with ego-network <ref type="bibr" target="#b23">[24]</ref> or community analysis <ref type="bibr" target="#b9">[10]</ref> and then identify anomalies by measuring the abnormality of taneously find communities as well as spot community anomalies within a unified probabilistic model <ref type="bibr" target="#b9">[10]</ref>. AMEN <ref type="bibr" target="#b23">[24]</ref> considers t
pe="bibr" target="#b20">[21]</ref> and the introduction of the TAGE predictor by Seznec and Michaud <ref type="bibr" target="#b25">[26]</ref>. In practice, on the traces distributed for the first two e predictors targeting special categories of branches with a state-of-the-art main predictor (TAGE, <ref type="bibr" target="#b25">[26]</ref>, OGEHL <ref type="bibr" target="#b20">[21]</ref>, Piecewis L <ref type="bibr" target="#b7">[8]</ref>), e.g. a loop predictor with the TAGE predictor in L-TAGE <ref type="bibr" target="#b25">[26]</ref> or the address-branch correlator in <ref type="bibr" targe ns/1.0"><head n="3.">Background on the TAGE Predictor</head><p>The TAGE predictor was introduced in <ref type="bibr" target="#b25">[26]</ref> and is the core predictor of the L-TAGE predictor that won h a single 4-bit counter USE_ALT_ON_NA was found to allow to (slightly) improve prediction accuracy <ref type="bibr" target="#b25">[26]</ref>. The prediction computation algorithm is as follows:</p><p ts showed that one can use wider tag for long histories for a better tradeoff. Previous experiments <ref type="bibr" target="#b25">[26]</ref> have shown that the TAGE predictor performs efficiently on
et="#b27">[28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" targe
the Championship Branch Prediction (CBP) in 2004 and 2006 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar ype="bibr" target="#b0">1]</ref>, the introduction of GEometric History Length predictors by Seznec <ref type="bibr" target="#b20">[21]</ref> and the introduction of the TAGE predictor by Seznec and M ches with a state-of-the-art main predictor (TAGE, <ref type="bibr" target="#b25">[26]</ref>, OGEHL <ref type="bibr" target="#b20">[21]</ref>, Piecewise Linear <ref type="bibr" target="#b10">[11]</ref orm a geometric series, i.e, L(i) = (int)(α i−1 * L(1) + 0.5) as introduced for the OGEHL predictor <ref type="bibr" target="#b20">[21]</ref>. On a TAGE predictor, most of the storage is used in table 6]</ref> predictor as a representative of first generation predictor and a 520 Kbits GEHL predictor <ref type="bibr" target="#b20">[21]</ref> as a representative of neural inspired predictors 2 . A GE efficient implementation of the Statistical Corrector predictor was derived from the GEHL predictor <ref type="bibr" target="#b20">[21]</ref>. It features 4 logical tables indexed with the 4 shortest milar to the technique proposed for dynamically adapting the update threshold of the GEHL predictor <ref type="bibr" target="#b20">[21]</ref>.</p><p>When adding the Statistical Corrector predictor on update on global history predictors is relatively marginal <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. However these studies are assuming that the predictor tabl arginal on global history predictors featuring long history <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. However these studies were assuming that the predictor tab
by a small community that participated to the Championship Branch Prediction (CBP) in 2004 and 2006 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta ctors led by Jimenez <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1]</ref>, the introduction of ="bibr" target="#b25">[26]</ref>, OGEHL <ref type="bibr" target="#b20">[21]</ref>, Piecewise Linear <ref type="bibr" target="#b10">[11]</ref>, FTL <ref type="bibr" target="#b7">[8]</ref>), e.g. a loop ed on neural based techniques, respectively GEHL combined with LGEHL for FTL++ and piecewise linear <ref type="bibr" target="#b10">[11]</ref> combined with dynamic weight adaptation <ref type="bibr" t
arget="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" targe
there has been a very active research in branch prediction <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar
ive. This technique was already mentioned by Baniasadi and Moshovos in the context of energy saving <ref type="bibr" target="#b1">[2]</ref>.</p><p>We confirmed this assumption by simulations of 3 very
Ship Branch Prediction, the FTL++ predictor <ref type="bibr" target="#b6">[7]</ref> and the OH-SNAP <ref type="bibr" target="#b13">[14]</ref> were respectively ranked 2nd and 3rd with respectively 581
econd, building on the side predictors recently presented at the 3rd Championship Branch Prediction <ref type="bibr" target="#b23">[24]</ref> for the TAGE predictor, we show that TAGE can be combined predictor with the ISL-TAGE predictor, that was presented at the 3rd Championship Branch Prediction <ref type="bibr" target="#b23">[24]</ref>.</p><p>For the TAGE predictor, in order to limit the mispr cting. At similar storage budget, the TAGE-LSC (+ IUM) predictor outperforms the ISL-TAGE predictor <ref type="bibr" target="#b23">[24]</ref> 1 using a smaller number of components and a smaller numbe detail the side predictors that were proposed for TAGE at the 3rd Championship on Branch Prediction <ref type="bibr" target="#b23">[24]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head dict this class of statistically biased branches, the Statistical Corrector predictor is introduced <ref type="bibr" target="#b23">[24]</ref>. The correc-tion aims at detecting the unlikely prediction n of the performance loss due to imperfect TAGE+IUM+loop predictor. Unrealistic tricks were used in <ref type="bibr" target="#b23">[24]</ref> to bring the performance to 568 MPPKI.</p></div> <div xmln of the TAGE+IUM predictor by more than 8 %.</p><p>In order to get fair comparison with the ISL-TAGE <ref type="bibr" target="#b23">[24]</ref>, we adjusted the size of the TAGE-LSC predictor to 512 Kbi side predictors were presented at the 3rd Championship Branch Prediction for the ISL-TAGE predictor <ref type="bibr" target="#b23">[24]</ref>. We have further extended this approach showing that the S
) in 2004 and 2006 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" targe e core predictor of the L-TAGE predictor that won the second Championship Branch Prediction in 2006 <ref type="bibr" target="#b22">[23]</ref>. Figure <ref type="figure">1</ref> illustrates a TAGE pred /p><p>Depending on the storage budget one will use small or large number of tables. It was shown in <ref type="bibr" target="#b22">[23]</ref> that for small budgets (e.g. 32K-64Kbits) using 5 to 8 tab ws to recover most of the mispredictions due to delayed updates. As already proposed for the L-TAGE <ref type="bibr" target="#b22">[23]</ref>, TAGE can be augmented with a loop predictor to correctly
the Championship Branch Prediction (CBP) in 2004 and 2006 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar ype="bibr" target="#b0">1]</ref>, the introduction of GEometric History Length predictors by Seznec <ref type="bibr" target="#b20">[21]</ref> and the introduction of the TAGE predictor by Seznec and M ches with a state-of-the-art main predictor (TAGE, <ref type="bibr" target="#b25">[26]</ref>, OGEHL <ref type="bibr" target="#b20">[21]</ref>, Piecewise Linear <ref type="bibr" target="#b10">[11]</ref orm a geometric series, i.e, L(i) = (int)(α i−1 * L(1) + 0.5) as introduced for the OGEHL predictor <ref type="bibr" target="#b20">[21]</ref>. On a TAGE predictor, most of the storage is used in table 6]</ref> predictor as a representative of first generation predictor and a 520 Kbits GEHL predictor <ref type="bibr" target="#b20">[21]</ref> as a representative of neural inspired predictors 2 . A GE efficient implementation of the Statistical Corrector predictor was derived from the GEHL predictor <ref type="bibr" target="#b20">[21]</ref>. It features 4 logical tables indexed with the 4 shortest milar to the technique proposed for dynamically adapting the update threshold of the GEHL predictor <ref type="bibr" target="#b20">[21]</ref>.</p><p>When adding the Statistical Corrector predictor on update on global history predictors is relatively marginal <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. However these studies are assuming that the predictor tabl arginal on global history predictors featuring long history <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. However these studies were assuming that the predictor tab
ref>  Register traffic characteristics. We collect a number of characteristics concerning registers <ref type="bibr" target="#b5">[6]</ref>. Our first characteristic is the average number of input ope
Data stream strides. The data stream is characterized with respect to local and global data strides <ref type="bibr" target="#b9">[10]</ref>. A global stride is defined as the difference in the data m
e level such as operation mix, number of function calls, number of address computations, etc. Conte <ref type="bibr" target="#b2">[3]</ref> uses kiviat views to qualitatively compare program behavior
composition techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref>. These techniques first measure a number of program charact
tecture-dependent characteristics such as cache miss rates, branch mispredict rates, etc. Yi et al. <ref type="bibr" target="#b16">[17]</ref> use a Plackett-Burman design for classifying benchmarks ba
tecture-dependent characteristics such as cache miss rates, branch mispredict rates, etc. Yi et al. <ref type="bibr" target="#b16">[17]</ref> use a Plackett-Burman design for classifying benchmarks ba
based on microarchitecture-independent characteristics such as the stack distance, see for example <ref type="bibr" target="#b17">[18]</ref>. However, we are unaware of any work that proposes a super
y. Several researchers have proposed methods for quantifying program similarity. Saavedra and Smith <ref type="bibr" target="#b12">[13]</ref> use the squared Euclidean distance computed in a benchmark
dependent manner we used the Prediction by Partial Matching (PPM) predictor proposed by Chen et al. <ref type="bibr" target="#b1">[2]</ref>, which is a universal compression/prediction technique.</p><
based on microarchitecture-independent characteristics such as the stack distance, see for example <ref type="bibr" target="#b17">[18]</ref>. However, we are unaware of any work that proposes a super
r.</p><p>Based on this prior work, researchers have proposed benchmark suite composition techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target
en microarchitecture-independent program characteristics and processor performance, see for example <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target
m characteristics and processor performance, see for example <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>. However, these techn

e principal components to unit variance. This gives equal weight to all of the principal components <ref type="bibr" target="#b4">[5]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n= archers have proposed benchmark suite composition techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref>. These techniques fir
y. Several researchers have proposed methods for quantifying program similarity. Saavedra and Smith <ref type="bibr" target="#b12">[13]</ref> use the squared Euclidean distance computed in a benchmark
dependent manner we used the Prediction by Partial Matching (PPM) predictor proposed by Chen et al. <ref type="bibr" target="#b1">[2]</ref>, which is a universal compression/prediction technique.</p><
e level such as operation mix, number of function calls, number of address computations, etc. Conte <ref type="bibr" target="#b2">[3]</ref> uses kiviat views to qualitatively compare program behavior
tecture-dependent characteristics such as cache miss rates, branch mispredict rates, etc. Yi et al. <ref type="bibr" target="#b16">[17]</ref> use a Plackett-Burman design for classifying benchmarks ba
e level such as operation mix, number of function calls, number of address computations, etc. Conte <ref type="bibr" target="#b2">[3]</ref> uses kiviat views to qualitatively compare program behavior
mance, see for example <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>. However, these techniques do not predict performance for a
composition techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref>. These techniques first measure a number of program charact
2.">MODEL</head><p>Our sequence-to-sequence model is an encoder-decoder architecture with attention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target
ence model is an encoder-decoder architecture with attention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Let X = [X 1 , . . . , P (y u | X, y &lt;u ) = h(S u , Q u ).<label>(4)</label></formula><p>The function g(•) is a GRU RNN <ref type="bibr" target="#b5">[6]</ref> which encodes the previous token and query vector Q u−1 to p
ata</head><p>All experiments are performed on the publicly available Lib-riSpeech audio book corpus <ref type="bibr" target="#b10">[11]</ref>. We use the "train-clean-100" set as the paired data set, blic domain books. The books were selected such that there is no overlap with the dev and test sets <ref type="bibr" target="#b10">[11]</ref>. On the other hand, the training data set transcriptions a for the apostrophe in contractions (we replace hyphens with a space). Unlike the original LM corpus <ref type="bibr" target="#b10">[11]</ref> we take no steps to replace non-standard words with a cano
r semi-supervised ASR is to use the "trainclean-100" subset of LibriSpeech as the labelled data set <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Table <ref type=" their model on "train-clean-100" as the baseline for a back-translation style approach. Liu et al. <ref type="bibr" target="#b19">[20]</ref> augment a sequence-to-sequence model with the CTC loss. Co
abels from 460 hours of clean speech.</p><p>We also evaluate two methods for pseudo-label filtering <ref type="bibr" target="#b3">[4]</ref> tailored to the mistakes often encountered with sequence-to-
cher approach without an external LM gives an improvement when trained on unreleased unpaired audio <ref type="bibr" target="#b33">[34]</ref>. In both of these cases, however, the use of non-public da
ng including word-sense disambiguation <ref type="bibr" target="#b1">[2]</ref>, noun identification <ref type="bibr" target="#b22">[23]</ref> and parsing <ref type="bibr" target="#b2">[3]</ref>, in ad
rove pseudo-label quality, e.g. confidence-based filtering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and agreementbased
lting text corpus for LM training. First, we detect sentence boundaries using the "punkt" tokenizer <ref type="bibr" target="#b11">[12]</ref> implemented in NLTK <ref type="bibr" target="#b12">[13]</r
rove pseudo-label quality, e.g. confidence-based filtering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and agreementbased
abels from 460 hours of clean speech.</p><p>We also evaluate two methods for pseudo-label filtering <ref type="bibr" target="#b3">[4]</ref> tailored to the mistakes often encountered with sequence-to-
and the data selection process can take place at different levels ranging from frames to utterances <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Using pseud
or poorly tuned supervised baseline model, a common issue with semi-supervised learning in general <ref type="bibr" target="#b36">[37]</ref>. In contrast, we compare our self-trained models to a well
generated from a model trained on a much smaller labelled data set.</p><p>We revisit self-training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> in the context of sequ aining has been applied to tasks in natural language processing including word-sense disambiguation <ref type="bibr" target="#b1">[2]</ref>, noun identification <ref type="bibr" target="#b22">[23]</re
ng including word-sense disambiguation <ref type="bibr" target="#b1">[2]</ref>, noun identification <ref type="bibr" target="#b22">[23]</ref> and parsing <ref type="bibr" target="#b2">[3]</ref>, in ad
ype="bibr" target="#b2">[3]</ref>, in addition to tasks in computer vision such as object detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and image classifi
ch smaller labelled data set.</p><p>We revisit self-training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> in the context of sequenceto-sequence models with attention. " target="#b1">[2]</ref>, noun identification <ref type="bibr" target="#b22">[23]</ref> and parsing <ref type="bibr" target="#b2">[3]</ref>, in addition to tasks in computer vision such as object dete
get="#b7">[8]</ref>. Other than 20% dropout, we use 1% uniform target sampling, 10% label smoothing <ref type="bibr" target="#b14">[15]</ref> and 1% word piece sampling <ref type="bibr" target="#b15">
ch smaller labelled data set.</p><p>We revisit self-training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> in the context of sequenceto-sequence models with attention. " target="#b1">[2]</ref>, noun identification <ref type="bibr" target="#b22">[23]</ref> and parsing <ref type="bibr" target="#b2">[3]</ref>, in addition to tasks in computer vision such as object dete
ype="bibr" target="#b2">[3]</ref>, in addition to tasks in computer vision such as object detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and image classifi
rove pseudo-label quality, e.g. confidence-based filtering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and agreementbased
nclean-100" subset of LibriSpeech as the labelled data set <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows the an LM.</p><p>"train-clean-100" as well as several other results from the literature. Hayashi et al. <ref type="bibr" target="#b20">[21]</ref> use a sequence-to-sequence model with a BiLSTM-based encod a synthetic data set, but target hidden state representations instead of acoustic features directly <ref type="bibr" target="#b20">[21]</ref>. Alternatively, both unpaired audio and text can be used b
d parameter (Equation <ref type="formula">7</ref>). We train a word piece convolutional LM (ConvLM) <ref type="bibr" target="#b17">[18]</ref> on the text data set described in Section 4.1 using the sa
rove pseudo-label quality, e.g. confidence-based filtering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and agreementbased
cher approach without an external LM gives an improvement when trained on unreleased unpaired audio <ref type="bibr" target="#b33">[34]</ref>. In both of these cases, however, the use of non-public da
"bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and agreementbased selection <ref type="bibr" target="#b29">[30]</ref> that also takes advantage of outputs from multiple systems
rove pseudo-label quality, e.g. confidence-based filtering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and agreementbased
lting text corpus for LM training. First, we detect sentence boundaries using the "punkt" tokenizer <ref type="bibr" target="#b11">[12]</ref> implemented in NLTK <ref type="bibr" target="#b12">[13]</r
n different ways of data filtering to improve pseudo-label quality, e.g. confidence-based filtering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" ta
d parameter (Equation <ref type="formula">7</ref>). We train a word piece convolutional LM (ConvLM) <ref type="bibr" target="#b17">[18]</ref> on the text data set described in Section 4.1 using the sa
nclean-100" subset of LibriSpeech as the labelled data set <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows the an LM.</p><p>"train-clean-100" as well as several other results from the literature. Hayashi et al. <ref type="bibr" target="#b20">[21]</ref> use a sequence-to-sequence model with a BiLSTM-based encod a synthetic data set, but target hidden state representations instead of acoustic features directly <ref type="bibr" target="#b20">[21]</ref>. Alternatively, both unpaired audio and text can be used b
r semi-supervised ASR is to use the "trainclean-100" subset of LibriSpeech as the labelled data set <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Table <ref type=" their model on "train-clean-100" as the baseline for a back-translation style approach. Liu et al. <ref type="bibr" target="#b19">[20]</ref> augment a sequence-to-sequence model with the CTC loss. Co
get="#b7">[8]</ref>. Other than 20% dropout, we use 1% uniform target sampling, 10% label smoothing <ref type="bibr" target="#b14">[15]</ref> and 1% word piece sampling <ref type="bibr" target="#b15">
usion are personalized PageRank (PPR) <ref type="bibr" target="#b55">[56]</ref> and the heat kernel <ref type="bibr" target="#b35">[36]</ref>. PPR corresponds to choosing T = T rw and θ PPR k = α(1 − cal graph learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, especially for clu
Most importantly, there are fast approximations for both PPR <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b75">76]</ref> and the heat kernel <ref type="bibr" target="#b33">[34]</re
<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b75">76]</ref> and the heat kernel <ref type="bibr" target="#b33">[34]</ref>, with which GDC achieves a linear runtime O(N ). Furthermo e="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, especially for clustering <ref type="bibr" target="#b33">[34]</ref>, semi-supervised classification <ref type="bibr" target="#

nce it is localized and has a limited number of parameters <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>:</p><formula xml:id="formula_3">g ξ (L) = J j=0 ξ j L j = U

by concatenating features aggregated using the transition matrices of k-hop random walks, GraphHeat <ref type="bibr" target="#b77">[78]</ref> uses the heat kernel and PAN <ref type="bibr" target="#b43

/ref> and Baskin et al. <ref type="bibr" target="#b5">[6]</ref>, and the name GNN first appeared in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b63">64]</ref>. However, they onl
/ref> and Baskin et al. <ref type="bibr" target="#b5">[6]</ref>, and the name GNN first appeared in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b63">64]</ref>. However, they onl
type="bibr" target="#b65">[66]</ref>, and the co-purchase graphs AMAZON COMPUTERS and AMAZON PHOTO <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b65">66]</ref>. We only use their
hat CORA and CITESEER are both small graphs with low average degree. Graphs become denser with size <ref type="bibr" target="#b38">[39]</ref> and in practice we expect GDC to typically reduce the aver
f type="bibr" target="#b31">[32]</ref>, also referred to as Message Passing Neural Networks (MPNNs) <ref type="bibr" target="#b22">[23]</ref> are the prevalent approach in this field but they only pas </ref>, which are based on the eigendecomposition of the graph Laplacian, and spatial-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" ta
sian distributions instead of vectors <ref type="bibr" target="#b9">[10]</ref>, use an auto-encoder <ref type="bibr" target="#b30">[31]</ref>, or train an encoder by maximizing the mutual information
>Limitations. GDC is based on the assumption of homophily, i.e. "birds of a feather flock together" <ref type="bibr" target="#b47">[48]</ref>. Many methods share this assumption and most common datase
ef type="bibr" target="#b64">[65]</ref>, CORA <ref type="bibr" target="#b46">[47]</ref>, and PUBMED <ref type="bibr" target="#b51">[52]</ref>, the co-author graph COAUTHOR CS <ref type="bibr" target="

mal values fall within a narrow range that is consistent across datasets and models. setting on PPI <ref type="bibr" target="#b48">[49]</ref>, which is rather unsurprising since the underlying data us



e GDC on six datasets: The citation graphs CITESEER <ref type="bibr" target="#b64">[65]</ref>, CORA <ref type="bibr" target="#b46">[47]</ref>, and PUBMED <ref type="bibr" target="#b51">[52]</ref>, the

>Limitations. GDC is based on the assumption of homophily, i.e. "birds of a feather flock together" <ref type="bibr" target="#b47">[48]</ref>. Many methods share this assumption and most common datase
nce it is localized and has a limited number of parameters <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>:</p><formula xml:id="formula_3">g ξ (L) = J j=0 ξ j L j = U
es not preserve the eigenvectors and its effect therefore cannot be calculated precisely. Wu et al. <ref type="bibr" target="#b76">[77]</ref> empirically found that adding self-loops shrinks the graph
position of the graph Laplacian, and spatial-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" tar
f type="bibr" target="#b31">[32]</ref>, also referred to as Message Passing Neural Networks (MPNNs) <ref type="bibr" target="#b22">[23]</ref> are the prevalent approach in this field but they only pas </ref>, which are based on the eigendecomposition of the graph Laplacian, and spatial-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" ta
ef type="bibr" target="#b64">[65]</ref>, CORA <ref type="bibr" target="#b46">[47]</ref>, and PUBMED <ref type="bibr" target="#b51">[52]</ref>, the co-author graph COAUTHOR CS <ref type="bibr" target="
alues in S represent the influence between all pairs of nodes, which typically are highly localized <ref type="bibr" target="#b52">[53]</ref>. This is a major advantage over spectral-based models sinc
>[30]</ref>, spectral clustering (using L sym ) <ref type="bibr" target="#b53">[54]</ref>, DeepWalk <ref type="bibr" target="#b59">[60]</ref>, and Deep Graph Infomax (DGI) <ref type="bibr" target="#b7
the underlying probability matrix of a sampled stochastic block model (SBM) graph. Kloumann et al. <ref type="bibr" target="#b34">[35]</ref> and Ragain <ref type="bibr" target="#b62">[63]</ref> showe
position of the graph Laplacian, and spatial-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" tar
ccuracies, limitations and pitfalls of the related technique known as negotiated-congestion routing <ref type="bibr" target="#b27">[28]</ref>. In Copyright (c) 2008 IEEE. Personal use of this material 5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bib ested regions, often at the cost of increased wirelength.</p><p>Negotiated-congestion Routing (NCR) <ref type="bibr" target="#b27">[28]</ref> was introduced in the mid-1990s for global routing in FPGA (b e ), added cost reflecting congestion history (h e ), and penalty for current congestion (p e ) <ref type="bibr" target="#b27">[28]</ref>. NCR seeks to minimize e c e .</p><p>To begin negotiated-c re-route is the same for each iteration, but can be chosen arbitrarily, according to the authors of <ref type="bibr" target="#b27">[28]</ref>, because the gradual cost increase in congested areas remo ="formula_7">c e = b e + h e • p e<label>(8)</label></formula><p>which is different than Equation 1 <ref type="bibr" target="#b27">[28]</ref>, but also is more intuitive since it preserves the base co
RSMT tool FLUTE <ref type="bibr" target="#b8">[9]</ref> is used by BoxRouter 1.0 [6], BoxRouter 2.0 <ref type="bibr" target="#b6">[7]</ref> and FastRoute <ref type="bibr" target="#b31">[32]</ref>, <re

alternative routes when they are available. The cost c e of routing edge e c e = (b e + h e ) • p e <ref type="bibr" target="#b0">(1)</ref> is a function of the base cost (b e ), added cost reflecting
regions. Major differences between various implementations <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bib

regions. Major differences between various implementations <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bib
r modification to Dijkstra's algorithm that significantly improves speed during 2-d and 3-d routing <ref type="bibr" target="#b15">[16]</ref>. In A*-search, a lower bound of the distance to the target ng instances such as multi-commodity flow based techniques <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>, described below. Essential to the coarsening stage is the n proposed, such as the use of multi-commodity flows (MCF) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref> and integer linear programming (ILP) <ref type="bibr" targ
d development as evidenced by a growing body of literature <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" et, and selects one of them. It is particularly amenable to Integer Linear Programming formulations <ref type="bibr" target="#b5">[6]</ref>, as described later in the section.</p><p>Multi-pin nets. Mo einer Minimal Tree (RSMT) construction algorithms has become increasingly popular in the literature <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" th a maze router to use lesser congested regions. Major differences between various implementations <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" get="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref> and integer linear programming (ILP) <ref type="bibr" target="#b5">[6]</ref>. Both of these techniques attempt to route nets simultaneous ion is used to solve the LP. ILP-based BoxRouter 1.0 has been compared to a recent MCF-based router <ref type="bibr" target="#b5">[6]</ref> and found to be superior in speed and solution quality. Addi h A*search relies. Hence, the slower Dijkstra's algorithm must be used instead.</p><p>BoxRouter 1.0 <ref type="bibr" target="#b5">[6]</ref> avoids fine-grain net ordering in congested regions through enchmarks</head><p>Table II describes the ISPD '98 IBM benchmarks and compares FGR to BoxRouter 1.0 <ref type="bibr" target="#b5">[6]</ref> in terms of runtime. Table <ref type="table" target="#tab_3" iner Minimal Tree (RSMT) construction algorithms have become increasingly popular in the literature <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" HE ISPD '98 IBM BENCHMARK SUITE<ref type="bibr" target="#b16">[17]</ref>. RUNTIMES FOR BOXROUTER 1.0<ref type="bibr" target="#b5">[6]</ref> AND FGR 1.0 ARE GIVEN IN SECONDS. FGR IS FASTER THAN BOXROUT
chnique is surprisingly useful in ASIC routing and justified by via minimization. Empirical studies <ref type="bibr" target="#b42">[43]</ref> show that in a fullyrouted design a majority of all 2-pin
ues work similarly to those in partitioning <ref type="bibr" target="#b19">[20]</ref> and placement <ref type="bibr" target="#b4">[5]</ref>. The original routing problem is effectively made simpler th
es in resource utilization and on-chip buffer requirements <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>To overcome these challenges, we present Tangram, a t-product operations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>A notable technique to further improve NN efficiency ied with the output vectors. They can be formulated as CONV and FC layers with different dimensions <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref>. Because the outpu pagation of CONV and FC layers can be formulated as new CONV or FC layers with different dimensions <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref>. The BSD optimizat he data forwarding needed for inter-layer pipelining is already available in tiled NN architectures <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref>. In Tangram, ALLO rget="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. As shown in Figure get="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. Such inter-layer p (all layers) can be mapped onto a single or multiple chips <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. This approach does tiles.</p><p>Inter-layer pipelining: ISAAC <ref type="bibr" target="#b35">[36]</ref> and PipeLayer <ref type="bibr" target="#b39">[40]</ref> used inter-layer pipelining in ReRAM-based accelerators, b

type="bibr" target="#b32">33]</ref>, or statically compressed the NN structures into sparse formats <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar
ructures provide higher accuracy on more challenging tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42]</ref>, there is strong in umbers of layers but each individual layer is rather small <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref>. With inter-layer p
ent capacities, and to calculate the characteristics of the PE array bus wires at different lengths <ref type="bibr" target="#b25">[26]</ref>. The NoC power is estimated to be 0.61 pJ/bit per hop <ref ths <ref type="bibr" target="#b25">[26]</ref>. The NoC power is estimated to be 0.61 pJ/bit per hop <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43]</ref>. The default confi
nergy overheads for the data multicast needed when each time we start processing a new set of fmaps <ref type="bibr" target="#b7">[8]</ref> (see Figure <ref type="figure" target="#fig_9">11</ref>). Th
e most effective solution for many challenging classification, recognition, and prediction problems <ref type="bibr" target="#b23">[24]</ref>. Hence, there is significant interest in finding scalable
example, recent NNs use up to a few hundreds of layers, with each layer sized at several megabytes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta with more layers and more complex DAG structures provide higher accuracy on more challenging tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta case for many recent NNs that use large numbers of layers but each individual layer is rather small <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta s/1.0"><head n="3.3">Inter-Layer Pipelining for Complex NN DAGs</head><p>Recent NNs, such as ResNet <ref type="bibr" target="#b16">[17]</ref> and various LSTMs <ref type="bibr" target="#b40">[41,</ref
a local SRAM buffer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>. A network-on-chip multiple NN engines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" tar s tiled accelerators <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>. Recent work has al a-layer parallelism) leads to significant data duplication <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, and pipelining Session: Machine Learning II ASPLOS <ref ty ns of a single layer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. Table <ref type="table" target="#tab_1">1</ref> shows diff er controller, which notifies the PE array to start the computation when all data have been fetched <ref type="bibr" target="#b21">[22]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head to each of the 3D channels. Neurocube proposed a simple heuristic for NN partitioning across tiles <ref type="bibr" target="#b21">[22]</ref> and TETRIS extended it to hybrid partitioning schemes <ref
ructures provide higher accuracy on more challenging tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42]</ref>, there is strong in umbers of layers but each individual layer is rather small <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref>. With inter-layer p
uffer and do not need to be accessed remotely. Hence the buffers operate like an optimal NUCA cache <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>. Data rotation happe our baseline system. Our BSD proposal shares similar insights with non-uniform cache access (NUCA) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>. It leverages applic
ons of deep clustering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, deep attractor networks <ref type="bibr" target="#b3">[4,</r et="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>. However, this usually only leads to small improvements, even proach for supervised speech separation is via T-F masking <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3]</ref>. The proposed approach is expected to produce even better sep reconstruction, it is necessary to first obtain a good enough magnitude estimate. Our recent study <ref type="bibr" target="#b2">[3]</ref> proposed a novel multi-task learning approach combining the gs:</p><formula xml:id="formula_0">LDC,classic = V V T − Y Y T 2 F (1)</formula><p>Our recent study <ref type="bibr" target="#b2">[3]</ref> suggests that an alternative loss function, which whitens th be discussed in Section 3.4. Following <ref type="bibr" target="#b21">[22]</ref>, our recent study <ref type="bibr" target="#b2">[3]</ref> proposed a chimera++ network combining the two approaches vi " target="#b13">[14]</ref> only performs iterative reconstruction for each source independently. In <ref type="bibr" target="#b2">[3]</ref>, we therefore proposed to utilize the MISI algorithm <ref ty es remain fixed during iterations, while the phase of each source are iteratively reconstructed. In <ref type="bibr" target="#b2">[3]</ref>, the phase reconstruction was only added as a post-processin tained when applying five iterations of Griffin-Lim on each source independently, as is reported in <ref type="bibr" target="#b2">[3]</ref>. Performing end-to-end optimization using LWA improves the r ates directly in the time domain. Our result is 1.1 dB better than the previous state-of-the-art by <ref type="bibr" target="#b2">[3]</ref> in terms of both SI-SDR and SDR.</p></div> <div xmlns="http:
d by deep learning based speech enhancement and separation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" tar as in <ref type="bibr" target="#b20">[21]</ref>, proposed in the same conference. A follow-up work <ref type="bibr" target="#b18">[19]</ref> of <ref type="bibr" target="#b22">[23]</ref> supplies clea
ent and separation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>. However, this usuall
processing step on the magnitudes produced by deep learning based speech enhancement and separation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta
-Lim algorithm <ref type="bibr" target="#b13">[14]</ref>, multiple input spectrogram inverse (MISI) <ref type="bibr" target="#b14">[15]</ref>, ISSIR <ref type="bibr" target="#b15">[16]</ref>, and cons ly. In <ref type="bibr" target="#b2">[3]</ref>, we therefore proposed to utilize the MISI algorithm <ref type="bibr" target="#b14">[15]</ref> (see Algorithm 1) to reconstruct the clean phase of each s
demonstrating overwhelming advantages over previous methods including graphical modeling approaches <ref type="bibr" target="#b7">[8]</ref>, spectral clustering approaches <ref type="bibr" target="#b8
a GPU and through which backpropagation can be performed.</p><p>A recent study by Williamson et al. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> proposed a complex
#b30">31]</ref>, WaveNet <ref type="bibr" target="#b31">[32]</ref>, generative adversarial networks <ref type="bibr" target="#b32">[33]</ref>, or encoder-decoder architectures <ref type="bibr" target=
use phase is difficult to estimate. It is well-known that this incurs a phase inconsistency problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta
" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> have dramatically improved the performance of single-channel a is to train a maskinference network to minimize the minimum loss over all permutations. Following <ref type="bibr" target="#b6">[7]</ref>, the phase-sensitive mask (PSM) <ref type="bibr" target="#b2
he cocktail party problem. The inventions of deep clustering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, deep attractor network e="bibr" target="#b4">5]</ref> and permutation free training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target= We report the performance using scale-invariant SDR (SI-SDR) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target= ks first in <ref type="bibr" target="#b0">[1]</ref>, and was later found to be working very well in <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b5">[6]</ref>. The idea is to
ltiple activation functions that can work with γ &gt; 1 will be discussed in Section 3.4. Following <ref type="bibr" target="#b21">[22]</ref>, our recent study <ref type="bibr" target="#b2">[3]</ref>
/head><p>Sigmoidal units are dominantly used in the output layer of deep learning based T-F masking <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, partly because th arget="#b36">[37]</ref>, such as the IRM <ref type="bibr" target="#b37">[38]</ref> and its variants <ref type="bibr" target="#b35">[36]</ref>. Restricting the possible values of the T-F mask to lie in
br" target="#b38">39]</ref>, as well as the SDR metric computed using the bss eval sources software <ref type="bibr" target="#b39">[40]</ref> because it is used by other groups. We believe SI-SDR is a
well-known that this incurs a phase inconsistency problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, especially for spe
del well data with bi-modal distribution <ref type="bibr" target="#b36">[37]</ref>, such as the IRM <ref type="bibr" target="#b37">[38]</ref> and its variants <ref type="bibr" target="#b35">[36]</ref> /p><p>To obtain clean magnitudes, the oracle mask should be |Sc|/|X| (also known as the FFT mask in <ref type="bibr" target="#b37">[38]</ref> or the ideal amplitude mask in <ref type="bibr" target="#b
del well data with bi-modal distribution <ref type="bibr" target="#b36">[37]</ref>, such as the IRM <ref type="bibr" target="#b37">[38]</ref> and its variants <ref type="bibr" target="#b35">[36]</ref> /p><p>To obtain clean magnitudes, the oracle mask should be |Sc|/|X| (also known as the FFT mask in <ref type="bibr" target="#b37">[38]</ref> or the ideal amplitude mask in <ref type="bibr" target="#b
processing step on the magnitudes produced by deep learning based speech enhancement and separation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta
use phase is difficult to estimate. It is well-known that this incurs a phase inconsistency problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta
ing. However, this makes their approach equivalent to conventional magnitude spectrum approximation <ref type="bibr" target="#b23">[24]</ref>, which does not perform as well as the phase-sensitive mas
f type="bibr" target="#b2">3]</ref>, deep attractor networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and permutation free training <ref type="bibr" target="#b0">[ invariant SDR (SI-SDR) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39]</ref>, as well as the SDR m
" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39]</ref>, as well as the SDR metric computed using the bss eval sour y other groups. We believe SI-SDR is a more proper measure for singlechannel instantaneous mixtures <ref type="bibr" target="#b38">[39]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
ef type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, deep attractor networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and permutation free t
"#b14">[15]</ref>, ISSIR <ref type="bibr" target="#b15">[16]</ref>, and consistent Wiener filtering <ref type="bibr" target="#b16">[17]</ref>, which can recover the clean phase to some extent starting
br" target="#b38">39]</ref>, as well as the SDR metric computed using the bss eval sources software <ref type="bibr" target="#b39">[40]</ref> because it is used by other groups. We believe SI-SDR is a
aphical modeling approaches <ref type="bibr" target="#b7">[8]</ref>, spectral clustering approaches <ref type="bibr" target="#b8">[9]</ref>, and CASA methods <ref type="bibr" target="#b9">[10]</ref>.<
del well data with bi-modal distribution <ref type="bibr" target="#b36">[37]</ref>, such as the IRM <ref type="bibr" target="#b37">[38]</ref> and its variants <ref type="bibr" target="#b35">[36]</ref> /p><p>To obtain clean magnitudes, the oracle mask should be |Sc|/|X| (also known as the FFT mask in <ref type="bibr" target="#b37">[38]</ref> or the ideal amplitude mask in <ref type="bibr" target="#b
sed exciting advances towards solving the cocktail party problem. The inventions of deep clustering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target e="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and permutation free training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target reconstruction unless specified.</p><p>We report the performance using scale-invariant SDR (SI-SDR) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target yielding significant improvements over the individual models.</p><p>The key idea of deep clustering <ref type="bibr" target="#b0">[1]</ref> is to learn a highdimensional embedding vector for each T-F y.</p><p>Another permutation-free training scheme was proposed for mask-inference networks first in <ref type="bibr" target="#b0">[1]</ref>, and was later found to be working very well in <ref type="b ental Setup</head><p>We validate the proposed algorithms on the publicly-available wsj0-2mix corpus <ref type="bibr" target="#b0">[1]</ref>, which is widely used in many speakerindependent speech sepa
all permutations. Following <ref type="bibr" target="#b6">[7]</ref>, the phase-sensitive mask (PSM) <ref type="bibr" target="#b20">[21]</ref> is used as the training target. It is common in phase-sens e mask for minimum squared error in the complex spectrum domain, when using the noisy phases, as in <ref type="bibr" target="#b20">[21]</ref>, proposed in the same conference. A follow-up work <ref ty source c for example. The real component is equivalent to the earlier proposed phase-sensitive mask <ref type="bibr" target="#b20">[21]</ref>, which contains patterns clearly predictable from energy-b o known as the FFT mask in <ref type="bibr" target="#b37">[38]</ref> or the ideal amplitude mask in <ref type="bibr" target="#b20">[21]</ref>). Clearly, this mask can go beyond one, because the underl r" target="#b20">[21]</ref>, which contains patterns clearly predictable from energy-based features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>. However, recent s
use phase is difficult to estimate. It is well-known that this incurs a phase inconsistency problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta
y, one stream of research is focused on iterative methods such as the classic Griffin-Lim algorithm <ref type="bibr" target="#b13">[14]</ref>, multiple input spectrogram inverse (MISI) <ref type="bibr are multiple target sources to be separated in each mixture in our study. The Griffin-Lim algorithm <ref type="bibr" target="#b13">[14]</ref> only performs iterative reconstruction for each source ind
use phase is difficult to estimate. It is well-known that this incurs a phase inconsistency problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta
#b30">31]</ref>, WaveNet <ref type="bibr" target="#b31">[32]</ref>, generative adversarial networks <ref type="bibr" target="#b32">[33]</ref>, or encoder-decoder architectures <ref type="bibr" target=
2014</ref><ref type="bibr" target="#b20">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type="bibr" target="#b22">(Zhang et al., 2015;</ref><ref type="bibr" target="#b23">Zhou et al., ef> is a revision of CNN which uses piecewise max-pooling to extract more relation features. BiLSTM <ref type="bibr" target="#b22">(Zhang et al., 2015)</ref> is also commonly used for RE with the help
2015) )</ref> and recurrent neural network <ref type="bibr" target="#b22">(Zhang et al., 2015;</ref><ref type="bibr" target="#b23">Zhou et al., 2016)</ref>. To automatically obtain a large training da ying relations, we apply an attention mechanism over a BiLSTM Encoder, which is first introduced in <ref type="bibr" target="#b23">(Zhou et al., 2016)</ref> for RC. The model architecture is illustrat g et al., 2015)</ref> is also commonly used for RE with the help of position embeddings. BiLSTM+ATT <ref type="bibr" target="#b23">(Zhou et al., 2016)</ref> adds an attention mechanism into BiLSTM to
><p>In order to cheaply obtain a large amount of labeled RC training data, Distant Supervision (DS) <ref type="bibr" target="#b9">(Mintz et al., 2009)</ref> was proposed to automatically generate trai 3">Zhou et al., 2016)</ref>. To automatically obtain a large training dataset, DS has been proposed <ref type="bibr" target="#b9">(Mintz et al., 2009)</ref>. However, DS also introduces noisy data, ma relation. See the second sentence in Figure <ref type="figure" target="#fig_0">1</ref> for example. <ref type="bibr" target="#b9">Mintz et al. (2009)</ref> reports that distant supervision may lead to
type "place of birth" for "Bill Lockyer" and "California". Such indicating words is called patterns <ref type="bibr" target="#b5">(Hearst, 1992;</ref><ref type="bibr" target="#b3">Hamon and Nazarenko, thod relies on relation patterns. Pattern-based extraction is widely used in information extraction <ref type="bibr" target="#b5">(Hearst, 1992;</ref><ref type="bibr" target="#b3">Hamon and Nazarenko, ">Han et al., 2018)</ref> to select trustable instances. The third research line relies on patterns <ref type="bibr" target="#b5">(Hearst, 1992;</ref><ref type="bibr" target="#b3">Hamon and Nazarenko,
arries out classification on bag-level and often fails to perform well on sentence-level prediction <ref type="bibr" target="#b2">(Feng et al., 2018b)</ref>. Secondly, in order to reduce noise for sen ction is more friendly with comprehend sentence tasks, like question answering and semantic parsing <ref type="bibr" target="#b2">(Feng et al., 2018b)</ref>. Different from commonly-used bag-level eva he classification performance on dev set dose not increase. Generally, the bootstrap procedure end  <ref type="bibr" target="#b2">(Feng et al., 2018b)</ref> starts with a pre-trained CNN model using i researchers then resort to reinforcement learning or adversarial training to select trustable data <ref type="bibr" target="#b2">(Feng et al., 2018b;</ref><ref type="bibr" target="#b10">Qin et al., 2 nstances and is not suitable for sentence-level prediction. The second kind of approach utilizes RL <ref type="bibr" target="#b2">(Feng et al., 2018b;</ref><ref type="bibr" target="#b18">Xiangrong et ention mechanism over all sentences in a bag and thus can reduce the weight of noise data. CNN+RL 2 <ref type="bibr" target="#b2">(Feng et al., 2018b</ref>) is a novel reinforcement learning (RL) base 2018b</ref>) also introduces RL to heuristically recognize false positive instances. Different from <ref type="bibr" target="#b2">Feng et al. (2018b)</ref>, they redis-tribute false positives into neg T is a bag-level method and is not suitable for sentence-level evaluation, which is consistent with <ref type="bibr" target="#b2">Feng et al. (2018b)</ref>. 5 Analysis and Discussion</p></div> <div xm
g open information extraction and event extraction, and also overlapping relation extraction models <ref type="bibr" target="#b0">(Dai et al., 2019)</ref>.</p></div><figure xmlns="http://www.tei-c.org
takenly label a relation. Data programming <ref type="bibr" target="#b13">(Ratner et al., 2016</ref><ref type="bibr" target="#b12">(Ratner et al., , 2017) )</ref> fuses DS-based labels and manual rela eling process of DS to find noisy patterns. <ref type="bibr" target="#b13">Ratner et al. (2016</ref><ref type="bibr" target="#b12">Ratner et al. ( , 2017) )</ref> proposes to fuse DS-based labels and
><p>In order to cheaply obtain a large amount of labeled RC training data, Distant Supervision (DS) <ref type="bibr" target="#b9">(Mintz et al., 2009)</ref> was proposed to automatically generate trai 3">Zhou et al., 2016)</ref>. To automatically obtain a large training dataset, DS has been proposed <ref type="bibr" target="#b9">(Mintz et al., 2009)</ref>. However, DS also introduces noisy data, ma relation. See the second sentence in Figure <ref type="figure" target="#fig_0">1</ref> for example. <ref type="bibr" target="#b9">Mintz et al. (2009)</ref> reports that distant supervision may lead to
><p>In order to cheaply obtain a large amount of labeled RC training data, Distant Supervision (DS) <ref type="bibr" target="#b9">(Mintz et al., 2009)</ref> was proposed to automatically generate trai 3">Zhou et al., 2016)</ref>. To automatically obtain a large training dataset, DS has been proposed <ref type="bibr" target="#b9">(Mintz et al., 2009)</ref>. However, DS also introduces noisy data, ma relation. See the second sentence in Figure <ref type="figure" target="#fig_0">1</ref> for example. <ref type="bibr" target="#b9">Mintz et al. (2009)</ref> reports that distant supervision may lead to

bibr" target="#b15">(Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al bibr" target="#b15">(Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al
bibr" target="#b15">(Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al bibr" target="#b15">(Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al
ef><ref type="bibr" target="#b3">Hamon and Nazarenko, 2001)</ref>. Among them, the generative model <ref type="bibr" target="#b17">(Takamatsu et al., 2012)</ref> directly models the labeling process o r" target="#b5">(Hearst, 1992;</ref><ref type="bibr" target="#b3">Hamon and Nazarenko, 2001)</ref>. <ref type="bibr" target="#b17">Takamatsu et al. (2012)</ref> directly models the labeling process of
arries out classification on bag-level and often fails to perform well on sentence-level prediction <ref type="bibr" target="#b2">(Feng et al., 2018b)</ref>. Secondly, in order to reduce noise for sen ction is more friendly with comprehend sentence tasks, like question answering and semantic parsing <ref type="bibr" target="#b2">(Feng et al., 2018b)</ref>. Different from commonly-used bag-level eva he classification performance on dev set dose not increase. Generally, the bootstrap procedure end  <ref type="bibr" target="#b2">(Feng et al., 2018b)</ref> starts with a pre-trained CNN model using i researchers then resort to reinforcement learning or adversarial training to select trustable data <ref type="bibr" target="#b2">(Feng et al., 2018b;</ref><ref type="bibr" target="#b10">Qin et al., 2 nstances and is not suitable for sentence-level prediction. The second kind of approach utilizes RL <ref type="bibr" target="#b2">(Feng et al., 2018b;</ref><ref type="bibr" target="#b18">Xiangrong et ention mechanism over all sentences in a bag and thus can reduce the weight of noise data. CNN+RL 2 <ref type="bibr" target="#b2">(Feng et al., 2018b</ref>) is a novel reinforcement learning (RL) base 2018b</ref>) also introduces RL to heuristically recognize false positive instances. Different from <ref type="bibr" target="#b2">Feng et al. (2018b)</ref>, they redis-tribute false positives into neg T is a bag-level method and is not suitable for sentence-level evaluation, which is consistent with <ref type="bibr" target="#b2">Feng et al. (2018b)</ref>. 5 Analysis and Discussion</p></div> <div xm
re recently proposed based on different neural architectures, such as convolutional neural networks <ref type="bibr" target="#b21">(Zeng et al., 2014</ref><ref type="bibr" target="#b20">(Zeng et al., d embedding, position embedding, and entity type embedding. Position embedding is first proposed by <ref type="bibr" target="#b21">Zeng et al. (2014)</ref> to incorporate position information of input ffectiveness of RC after denoising, several non-denoising methods are also used for comparison. CNN <ref type="bibr" target="#b21">(Zeng et al., 2014)</ref> is a widely-used architecture for RE. It in
ibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al., 2015)</ref> relaxes the DS assumption as at-least-one. I ibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al., 2015)</ref>. However, it models noise problem on a bag o tures, such as convolutional neural networks <ref type="bibr" target="#b21">(Zeng et al., 2014</ref><ref type="bibr" target="#b20">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type= e for RE. It introduces position embeddings to represent the location of an input entity pair. PCNN <ref type="bibr" target="#b20">(Zeng et al., 2015)</ref> is a revision of CNN which uses piecewise m
bibr" target="#b15">(Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al bibr" target="#b15">(Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al
bibr" target="#b15">(Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al bibr" target="#b15">(Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al
ibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al., 2015)</ref> relaxes the DS assumption as at-least-one. I ibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al., 2015)</ref>. However, it models noise problem on a bag o tures, such as convolutional neural networks <ref type="bibr" target="#b21">(Zeng et al., 2014</ref><ref type="bibr" target="#b20">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type= e for RE. It introduces position embeddings to represent the location of an input entity pair. PCNN <ref type="bibr" target="#b20">(Zeng et al., 2015)</ref> is a revision of CNN which uses piecewise m
that this might not be the best choice. Instead, we use a recently released sentence-level test set <ref type="bibr" target="#b14">(Ren et al., 2017)</ref> for evaluation. However, there also exist se
ibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al., 2015)</ref> relaxes the DS assumption as at-least-one. I ibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al., 2015)</ref>. However, it models noise problem on a bag o tures, such as convolutional neural networks <ref type="bibr" target="#b21">(Zeng et al., 2014</ref><ref type="bibr" target="#b20">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type= e for RE. It introduces position embeddings to represent the location of an input entity pair. PCNN <ref type="bibr" target="#b20">(Zeng et al., 2015)</ref> is a revision of CNN which uses piecewise m
takenly label a relation. Data programming <ref type="bibr" target="#b13">(Ratner et al., 2016</ref><ref type="bibr" target="#b12">(Ratner et al., , 2017) )</ref> fuses DS-based labels and manual rela eling process of DS to find noisy patterns. <ref type="bibr" target="#b13">Ratner et al. (2016</ref><ref type="bibr" target="#b12">Ratner et al. ( , 2017) )</ref> proposes to fuse DS-based labels and
mentioning branch predictor warmup is by Haskins and Conte <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref> in which they propose memory reference reuse latency (MRRL) mup length per sampling unit. (Note that the MRRL approach <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref> corresponds to a zero BHM history length.) We further obser
"#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, self-monitored adaptive cache warmup (SMA) <ref type="bibr" target="#b18">[19]</ref>, memory hierarchy state (MHS) <ref type="bibr" target="#b4
ckpointed cache warmup <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> and (iii) compressed br reuse latency (MRRL) <ref type="bibr" target="#b16">[17]</ref>, boundary line reuse latency (BLRL) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, self-monitored adap
e warmup. Various approaches have been proposed such as no warmup, stale state (also called stitch) <ref type="bibr" target="#b12">[13]</ref>, fixed warmup <ref type="bibr" target="#b0">[1]</ref>, cac
g way of using BHM in practice for sampled processor simulation is to use (i) checkpointed sampling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> maintaining reduced ch checkpoints of architecture state (registers and memory) along with (ii) checkpointed cache warmup <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target simulated in a correct way. This can be achieved through fastforwarding or (reduced) checkpointing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>. Checkpointing is espe diction rates are computed.</p><p>A number of papers have proposed checkpointed sampling techniques <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target adaptive cache warmup (SMA) <ref type="bibr" target="#b18">[19]</ref>, memory hierarchy state (MHS) <ref type="bibr" target="#b4">[5]</ref>, memory timestamp record (MRT) <ref type="bibr" target="#b6" b6">[7]</ref>, live-points <ref type="bibr" target="#b8">[9]</ref> and memory hierarchy state (MHS) <ref type="bibr" target="#b4">[5]</ref>. They suggest to store the branch predictor state as part of </ref> (sampling unit size of 10K instructions) and SimPoint <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21]</ref> (sampling unit sizes
s <ref type="bibr" target="#b13">[14]</ref>, no-state-loss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>, minimal subset evaluation (MSE) <ref type="bibr" target="#
e. All measurements presented in this paper are obtained using the binary instrumentation tool ATOM <ref type="bibr" target="#b21">[22]</ref>. The branch predictors considered in this paper are shown
ces. Therefore, researchers have proposed sampled simulation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target= r" target="#b2">[3]</ref> and targeted sampling based on program phase behavior as done in SimPoint <ref type="bibr" target="#b1">[2]</ref>.</p><p>The second issue is how to get to those sampling unit ,</ref><ref type="bibr" target="#b8">9]</ref> (sampling unit size of 10K instructions) and SimPoint <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target
g way of using BHM in practice for sampled processor simulation is to use (i) checkpointed sampling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> maintaining reduced ch checkpoints of architecture state (registers and memory) along with (ii) checkpointed cache warmup <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target simulated in a correct way. This can be achieved through fastforwarding or (reduced) checkpointing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>. Checkpointing is espe diction rates are computed.</p><p>A number of papers have proposed checkpointed sampling techniques <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target adaptive cache warmup (SMA) <ref type="bibr" target="#b18">[19]</ref>, memory hierarchy state (MHS) <ref type="bibr" target="#b4">[5]</ref>, memory timestamp record (MRT) <ref type="bibr" target="#b6" b6">[7]</ref>, live-points <ref type="bibr" target="#b8">[9]</ref> and memory hierarchy state (MHS) <ref type="bibr" target="#b4">[5]</ref>. They suggest to store the branch predictor state as part of </ref> (sampling unit size of 10K instructions) and SimPoint <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21]</ref> (sampling unit sizes
e warmup. Various approaches have been proposed such as no warmup, stale state (also called stitch) <ref type="bibr" target="#b12">[13]</ref>, fixed warmup <ref type="bibr" target="#b0">[1]</ref>, cac
s <ref type="bibr" target="#b13">[14]</ref>, no-state-loss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>, minimal subset evaluation (MSE) <ref type="bibr" target="#
r et al. for making replacement decisions in a software-managed UCA called the Indirect Index Cache <ref type="bibr" target="#b8">[9]</ref>. In our scheme, when a hit occurs to a cache line, it is swa hes built with discrete components <ref type="bibr" target="#b16">[17]</ref>. Hallnor and Reinhardt <ref type="bibr" target="#b8">[9]</ref> studied a fully associative software-managed design for larg
CONCLUSIONS</head><p>Non-uniform accesses have started to appear in high performance cache designs <ref type="bibr" target="#b22">[23]</ref>. In this paper, we proposed several new designs that treat
ted for modeling these long transmission channels, since it uses the Rubenstein RC wire delay model <ref type="bibr" target="#b13">[14]</ref> and assumes bit-line capacitative loading on each wire. We
cent work has shown that 8 FO4 delays is close to the optimal clock for superscalar microprocessors <ref type="bibr" target="#b11">[12]</ref>. We assume an unloaded 132cycle access to main memory, obt
CONCLUSIONS</head><p>Non-uniform accesses have started to appear in high performance cache designs <ref type="bibr" target="#b22">[23]</ref>. In this paper, we proposed several new designs that treat

ectmapped on-chip caches by virtually binding regions of the address space to portions of the cache <ref type="bibr" target="#b4">[5]</ref>. They also studied, as did Johnson and Hwu <ref type="bibr"
en for their high L1 miss rates. The 16 applications include six SPEC2000 floating-point benchmarks <ref type="bibr" target="#b29">[30]</ref>, six SPEC2000 integer benchmarks, three scientific applica
minimize access time. Cache modeling tools, such as Cacti <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>, enable fast exploration of the cache design space by autom
e organizations with parameters derived from Cacti. sire-alpha models an Alpha 21264 core in detail <ref type="bibr" target="#b17">[18]</ref>. We assumed that all microarchitectural parameters other t
ted for modeling these long transmission channels, since it uses the Rubenstein RC wire delay model <ref type="bibr" target="#b13">[14]</ref> and assumes bit-line capacitative loading on each wire. We
e linked together. <ref type="bibr" target="#b36">[37]</ref> proposes a method using self-attention <ref type="bibr" target="#b35">[36]</ref> and bi-affine scoring algorithm to predict biological rela 14">[15]</ref> and Convolutional neural networks (CNNs).</p><p>Self-Attention. We adapt Transformer <ref type="bibr" target="#b35">[36]</ref> to encode word sequences in a paragraph, where we calculat across sentences. We base on recent Transformer architecture <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref> to build this module, due to its better performance in enco
the abbreviation, as a short form of text, is prone to ambiguity. Word sense disambiguation methods <ref type="bibr" target="#b22">[23]</ref> have been studied to disambiguate word senses, however, de be expensive and the collected datasets are normally small in size.</p><p>Word sense disambiguation <ref type="bibr" target="#b22">[23]</ref> is a type of technique used to distinguish ambiguous word </p><p>Inspired by word sense disambiguation methods that label super sense types for word clusters <ref type="bibr" target="#b22">[23]</ref>, we jointly predict the types for abbreviation candidates
rk based methods have achieved great success in relation extraction, including CNN-based approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> and LSTMbased appr ions in a paragraph. To this end, our model consists of a single-sentence module with Piecewise CNN <ref type="bibr" target="#b39">[40]</ref>, and a cross-sentence module which leverages self-attentio based methods <ref type="bibr" target="#b19">[20]</ref>, and supervised relation extraction methods <ref type="bibr" target="#b39">[40]</ref>. The pattern-based method <ref type="bibr" target="#b13">[ . In the following we introduce evaluated methods in detail.</p><p>PCNN_single: Piecewise CNN model <ref type="bibr" target="#b39">[40]</ref>, which is one of the state of art single-sentence relation
n single sentence relation extraction with an exception of <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37]</ref>, which focus on general documents while not targeting on a e extracted dependency parse tree, where the tree roots of different sentences are linked together. <ref type="bibr" target="#b36">[37]</ref> proposes a method using self-attention <ref type="bibr" ta the self-attention of the words, and use a convolutional layer in self-attention blocks similar to <ref type="bibr" target="#b36">[37]</ref> to alleviate the burden on the model to attend to local fe
domain, which either extract facts from Wikipedia info-boxes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref> or harvest knowledge with specific linguistic patterns <ref
scarcity raises a challenge for supervised and distantly supervised entity extraction methods like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> or weakly supervis of annotated corpus and well-covered in-domain knowledge bases, general entity recognition methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> do not fit our can
e others apply distant supervision to link entity mentions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> in the text to the knowledge base entities <ref type="bibr"
ess in relation extraction, including CNN-based approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> and LSTMbased approaches <ref type="bibr" target="#b30">[31
onomy construction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42]</ref>, which extracts con on isA relation. They either rely on pattern-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> which extract hierarchical relation leveraging linguistic f
/ref> or harvest knowledge with specific linguistic patterns <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>. Taxonomy can be viewed as a tree-structure knowledge graph
not. Evaluated methods can be divided to unsupervised methods including co-occurrence based methods <ref type="bibr" target="#b9">[10]</ref>, word-similarity based methods <ref type="bibr" target="#b1 also used.</p><p>Sent_cooccur: A method similar to co-occurrence method used in hypernym detection <ref type="bibr" target="#b9">[10]</ref>. Sent_cooccur calculates the co-occurrence frequency of can
pe="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> and LSTMbased approaches <ref type="bibr" target="#b30">[31]</ref>. These approaches all consider relations lying in a single
nowledge base can also influence the availability of relation labels when using distant supervision <ref type="bibr" target="#b21">[22]</ref>.</p><p>Entity: General entity recognition does not directl tion sets are given in the datasets, while others apply distant supervision to link entity mentions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> in the text to the
co-occurrence based methods <ref type="bibr" target="#b9">[10]</ref>, word-similarity based methods <ref type="bibr" target="#b19">[20]</ref>, and supervised relation extraction methods <ref type="bib core based on word embedding similarity, where the embedding is pretrained with the Skip-Gram model <ref type="bibr" target="#b19">[20]</ref>    <ref type="table" target="#tab_4">4</ref> shows the man
pe="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> and LSTMbased approaches <ref type="bibr" target="#b30">[31]</ref>. These approaches all consider relations lying in a single
is set to 1, since we did not observe performance gain in increasing layers. We use Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with a learning rate 0.001. In training, the batch size is
has attracted much attention from the community, while most of the works focus on news and web data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref>. Recent neural netwo
apers and iterate this process.</p><p>One step assisting with this process is taxonomy construction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" ta ef> which extract hierarchical relation leveraging linguistic features, or clustering-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>, which cluster con
onomy construction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42]</ref>, which extracts con on isA relation. They either rely on pattern-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> which extract hierarchical relation leveraging linguistic f
e others apply distant supervision to link entity mentions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> in the text to the knowledge base entities <ref type="bibr"
r performance in encoding long-distance context compared to Long Short Term Memory Networks (LSTMs) <ref type="bibr" target="#b14">[15]</ref> and Convolutional neural networks (CNNs).</p><p>Self-Atten
e others apply distant supervision to link entity mentions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> in the text to the knowledge base entities <ref type="bibr"
/ref> to alleviate the burden on the model to attend to local features. We add residual connections <ref type="bibr" target="#b11">[12]</ref> to both multihead attention and convolutional layers. The
te positive training examples from the table without human effort.We first used a table parsing tool<ref type="bibr" target="#b5">[6]</ref> 3 to extract tables from raw pdf files of papers. Then we pr
step assisting with this process is taxonomy construction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar xonomy construction methods mostly focus on isA relation. They either rely on pattern-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> which extract hier ns may convey a meaning related to comparative relation. Unsupervised pattern-based methods such as <ref type="bibr" target="#b13">[14]</ref> focus on isA relation, which are not suitable for finding ds have been proposed to extract these hierarchical relation, either leveraging linguistic patterns <ref type="bibr" target="#b13">[14]</ref> or hierarchical clustering of concepts which implicitly ca sed relation extraction methods <ref type="bibr" target="#b39">[40]</ref>. The pattern-based method <ref type="bibr" target="#b13">[14]</ref> is not compared due to its low recall in our task.</p><p>T
apers and iterate this process.</p><p>One step assisting with this process is taxonomy construction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" ta ef> which extract hierarchical relation leveraging linguistic features, or clustering-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>, which cluster con
ommunity, while most of the works focus on news and web data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref>. Recent neural network based methods have achieved great su Predicting whether an algorithm candidate pair is compared forms a multi-instance learning problem <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>. For each pair, a
co-occurrence based methods <ref type="bibr" target="#b9">[10]</ref>, word-similarity based methods <ref type="bibr" target="#b19">[20]</ref>, and supervised relation extraction methods <ref type="bib core based on word embedding similarity, where the embedding is pretrained with the Skip-Gram model <ref type="bibr" target="#b19">[20]</ref>    <ref type="table" target="#tab_4">4</ref> shows the man
te positive training examples from the table without human effort.We first used a table parsing tool<ref type="bibr" target="#b5">[6]</ref> 3 to extract tables from raw pdf files of papers. Then we pr
/ref> or harvest knowledge with specific linguistic patterns <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>. Taxonomy can be viewed as a tree-structure knowledge graph
step assisting with this process is taxonomy construction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar xonomy construction methods mostly focus on isA relation. They either rely on pattern-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> which extract hier ns may convey a meaning related to comparative relation. Unsupervised pattern-based methods such as <ref type="bibr" target="#b13">[14]</ref> focus on isA relation, which are not suitable for finding ds have been proposed to extract these hierarchical relation, either leveraging linguistic patterns <ref type="bibr" target="#b13">[14]</ref> or hierarchical clustering of concepts which implicitly ca sed relation extraction methods <ref type="bibr" target="#b39">[40]</ref>. The pattern-based method <ref type="bibr" target="#b13">[14]</ref> is not compared due to its low recall in our task.</p><p>T
step assisting with this process is taxonomy construction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar xonomy construction methods mostly focus on isA relation. They either rely on pattern-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> which extract hier ns may convey a meaning related to comparative relation. Unsupervised pattern-based methods such as <ref type="bibr" target="#b13">[14]</ref> focus on isA relation, which are not suitable for finding ds have been proposed to extract these hierarchical relation, either leveraging linguistic patterns <ref type="bibr" target="#b13">[14]</ref> or hierarchical clustering of concepts which implicitly ca sed relation extraction methods <ref type="bibr" target="#b39">[40]</ref>. The pattern-based method <ref type="bibr" target="#b13">[14]</ref> is not compared due to its low recall in our task.</p><p>T
ucted knowledge bases focus on general domain, which either extract facts from Wikipedia info-boxes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref> or harvest knowledge
ref><ref type="bibr" target="#b33">34]</ref> or harvest knowledge with specific linguistic patterns <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>. Taxonomy can be vie
, <ref type="bibr" target="#b3">[4]</ref> collected a dataset for scientific taxonomy construction, <ref type="bibr" target="#b12">[13]</ref> studied the evolution of scientific topics through dynamic
e others apply distant supervision to link entity mentions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> in the text to the knowledge base entities <ref type="bibr"
ef type="figure">1</ref>, for example, a roadmap for algorithm Generative Adversarial Network (GAN) <ref type="bibr" target="#b8">[9]</ref>, describes its successors and competitors in the scientific in the computer science domain, there are algorithms such as GAN (Generative Adversarial Networks) <ref type="bibr" target="#b8">[9]</ref> and DCGAN (Deep Convolutional Generative Adversarial Network ly published in this conference. "GAN" (Generative Adversarial Networks) is a deep generative model <ref type="bibr" target="#b8">[9]</ref>, which has been extensively cited since proposed. Researcher
apers and iterate this process.</p><p>One step assisting with this process is taxonomy construction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" ta ef> which extract hierarchical relation leveraging linguistic features, or clustering-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>, which cluster con
domain, which either extract facts from Wikipedia info-boxes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref> or harvest knowledge with specific linguistic patterns <ref
n <ref type="bibr" target="#b1">[2]</ref> to each component of transformer block, and adopt dropout <ref type="bibr" target="#b32">[33]</ref> to the input layer, piece-wise max-pooling and Transformer
ed and distantly supervised entity extraction methods like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> or weakly supervised phrase extraction approaches relying o domain knowledge bases, general entity recognition methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> do not fit our candidate mention extraction. With low occur #b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> in the text to the knowledge base entities <ref type="bibr" target="#b18">[19]</ref> and acquire relation labels. Their weaknesses lie in the f del with general phrase mining algorithms <ref type="bibr" target="#b29">[30]</ref>, entity linking <ref type="bibr" target="#b18">[19]</ref>, and general cross-sentence relations with corresponding s
te positive training examples from the table without human effort.We first used a table parsing tool<ref type="bibr" target="#b5">[6]</ref> 3 to extract tables from raw pdf files of papers. Then we pr
ed and distantly supervised entity extraction methods like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> or weakly supervised phrase extraction approaches relying o domain knowledge bases, general entity recognition methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> do not fit our candidate mention extraction. With low occur #b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> in the text to the knowledge base entities <ref type="bibr" target="#b18">[19]</ref> and acquire relation labels. Their weaknesses lie in the f del with general phrase mining algorithms <ref type="bibr" target="#b29">[30]</ref>, entity linking <ref type="bibr" target="#b18">[19]</ref>, and general cross-sentence relations with corresponding s
nowledge base can also influence the availability of relation labels when using distant supervision <ref type="bibr" target="#b21">[22]</ref>.</p><p>Entity: General entity recognition does not directl tion sets are given in the datasets, while others apply distant supervision to link entity mentions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> in the text to the
(FDP) <ref type="bibr" target="#b20">[21]</ref> and Address Map Pattern Matching Prefetching (AMPM) <ref type="bibr" target="#b11">[12]</ref>. We now describe both of these techniques in some detail.<
rget="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" targe
us studies that have proposed novel prefetching algorithms <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targe
rget="#b17">18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" targe
arget="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targe veness of prefetchers. Dahlgren et al. proposed adaptive sequential prefetching for multiprocessors <ref type="bibr" target="#b4">[5]</ref>. Their proposal dynamically modulated prefetcher distance by
3">Related Work</head><p>There are numerous studies that have proposed novel prefetching algorithms <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" targ . Jiminez et al. present a real life dynamic implementation of a prefetcher in the POWER7 processor <ref type="bibr" target="#b12">[13]</ref>. The POWER7 processor supports a number of prefetcher conf
arget="#b19">20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targ
rget="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" targe
solution.</p><p>In this work, we compare Sandbox Prefetching to Feedback Directed Prefetching (FDP) <ref type="bibr" target="#b20">[21]</ref> and Address Map Pattern Matching Prefetching (AMPM) <ref t
target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar
us studies that have proposed novel prefetching algorithms <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targe
ustness for classical neural networks/robust training (e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>), we tackle various ertifiable robustness <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref> providing guarantee ginal sample measured by, e.g., the infinity-norm or L2-norm <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>, often e.g. ϵ &lt; /p><p>For this work, specifically the class of methods based on convex relaxations are of relevance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. They construct a the remaining layers, since the input to them is no longer binary, we adapt the bounds proposed in <ref type="bibr" target="#b17">[18]</ref>. Generalized to the GNN we therefore obtain:</p><formula x
5</ref>) as robust cross entropy loss. One common issue with deep learning models is overconfidence <ref type="bibr" target="#b13">[14]</ref>, i.e. the models predicting effectively a probability of 1
ional networks, have gained much attention and improved the state of the art in node classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target
ional networks, have gained much attention and improved the state of the art in node classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target
ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> and node embeddings <ref type="bibr" target="#b0">[1]</ref>. All of these works focus on generating adversarial examples
method on the widely used and publicly available datasets Cora-ML (N=2,995, E=8,416, D=2,879, K=7) <ref type="bibr" target="#b15">[16]</ref>, Citeseer (N=3,312, E=4,715, D=3,703, K=6) <ref type="bibr
bust training (e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>), we tackle various additional challenges: Being the first arget="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref> providing guarantees that no perturbation w.r.t. a specific ss of methods based on convex relaxations are of relevance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. They construct a convex relaxation for computing a lower b nity-norm or L2-norm <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>, often e.g. ϵ &lt; 0.1 This is clearly not practical in our can often been done efficiently, and by exploiting duality it enables to even train a robust model <ref type="bibr" target="#b19">[20]</ref>. As already mentioned, our work differs significantly from the ReLU activation function. While there are many ways to achieve this, we follow the approach of <ref type="bibr" target="#b19">[20]</ref> in this work. The core idea is (i) to treat the matrices H makes this approach rather slow. As an alternative, we can consider the dual of the linear program <ref type="bibr" target="#b19">[20]</ref>. There, any dual-feasible solution is a lower bound on the appendix. Note that parts of the dual problem in Theorem 4.3 have a similar form to the problem in <ref type="bibr" target="#b19">[20]</ref>. For instance, we can interpret this dual problem as a bac akes the computation of robustness certificates extremely fast. For example, adopting the result of <ref type="bibr" target="#b19">[20]</ref>, instead of optimizing over Ω we can set it to</p><formula o this). While there exist more computationally involved algorithms to compute more accurate bounds <ref type="bibr" target="#b19">[20]</ref>, we leave adaptation of such bounds to the graph domain fo odes in the graph. y * t denotes the (known) class label of node t.</p><p>To improve robustness, in <ref type="bibr" target="#b19">[20]</ref> (for classical neural networks) it has been proposed to in oblem of the above linear program is max</p><p>for l = 2, . . . L, (n, j) ∈ I (l )</p><p>As done in <ref type="bibr" target="#b19">[20]</ref> we can exploit complementarity of the ReLU constraints cor
(e.g. the Web) adversaries are omnipresent, e.g., manipulating online reviews and product websites <ref type="bibr" target="#b10">[11]</ref>. One of the core challenges is that in a GNN a node's pred und on the worst-case loss achievable. Note that we can omit optimizing over Ω by setting it to Eq. <ref type="bibr" target="#b10">(11)</ref>. We refer to the loss function in Eq. ( <ref type="formula
D=2,879, K=7) <ref type="bibr" target="#b15">[16]</ref>, Citeseer (N=3,312, E=4,715, D=3,703, K=6) <ref type="bibr" target="#b18">[19]</ref>, and PubMed (N=19,717, E=44,324, D=500, K=3) <ref type="bi =3,703, K=6) <ref type="bibr" target="#b18">[19]</ref>, and PubMed (N=19,717, E=44,324, D=500, K=3) <ref type="bibr" target="#b18">[19]</ref>. For every dataset, we allow local (i.e. per-node) changes
graphs <ref type="bibr" target="#b8">[9]</ref>, prediction of customer types in e-commerce networks <ref type="bibr" target="#b5">[6]</ref>, or the assignment of scientific papers from a citation netw
(e.g. the Web) adversaries are omnipresent, e.g., manipulating online reviews and product websites <ref type="bibr" target="#b10">[11]</ref>. One of the core challenges is that in a GNN a node's pred und on the worst-case loss achievable. Note that we can omit optimizing over Ω by setting it to Eq. <ref type="bibr" target="#b10">(11)</ref>. We refer to the loss function in Eq. ( <ref type="formula
d of these three operations is the critical path of the IQ, and is a critical path of the processor <ref type="bibr" target="#b17">[21]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head target="#b26">30]</ref>.</p><p>The most widely known circuit used as select logic is a tree arbiter <ref type="bibr" target="#b17">[21]</ref>. This circuit uses a tree structure to connect multiple sm t priority in order from the bottom of the stacked arbiters <ref type="bibr" target="#b8">[12,</ref><ref type="bibr" target="#b17">21]</ref>.</p><p>Note that the priority in arbitration is not flexibl e instructions in descending order in terms of the priority <ref type="bibr" target="#b8">[12,</ref><ref type="bibr" target="#b17">21]</ref>; that is, ?rant 0 is the grant signal of the instruction wi
ntly reduced. <ref type="bibr">Kora et al.</ref> proposed configuring the IQ to exploit ILP and MLP <ref type="bibr" target="#b13">[17]</ref>. To exploit ILP, the IQ is reduced, whereas to exploit MLP
ge area.</p><p>Stark et al. proposed breaking the wakeup-select loop into different pipeline stages <ref type="bibr" target="#b23">[27]</ref>. Brown et al. proposed issuing instructions without select ied in[7,  </p></note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p><ref type="bibr" target="#b23">27]</ref> and is discussed in Section 5.</p></note> 			<note xmlns="h
ce. CIRC was often assumed in previous studies on IQs (e.g., <ref type="bibr" target="#b2">[6,</ref><ref type="bibr" target="#b11">15]</ref>), but is not used in current processors.</p><p>The last typ sed the problem caused by wrap-around (i.e., incorrect issue priority) in CIRC at the circuit level <ref type="bibr" target="#b11">[15]</ref>. However, in their proposed circuits, the wires traverse v
which selects the single oldest ready instruction, is used together with RAND in current processors <ref type="bibr" target="#b7">[11,</ref><ref type="bibr" target="#b18">22,</ref><ref type="bibr" tar processor vendors do not publish their IQ organizations, AGE is generally used in modern processors <ref type="bibr" target="#b7">[11,</ref><ref type="bibr" target="#b18">22,</ref><ref type="bibr" tar hese are both used in commercial processors. For example, the CAM type is used in the AMD Bulldozer <ref type="bibr" target="#b7">[11]</ref>, whereas the RAM type is used in the IBM POWER8 <ref type=" used. We refer to this type of IQ as AGE. The age matrix is used in parallel with the select logic <ref type="bibr" target="#b7">[11]</ref>, and selects only the single oldest ready instruction. The
/p><p>Sakai et al. proposed an IQ scheme that assigns high priority to multiple oldest instructions <ref type="bibr" target="#b19">[23]</ref> (not the single oldest instruction as in the age matrix).
s of the IQ, including random selection and selection based on the number of dependent instructions <ref type="bibr" target="#b4">[8]</ref>. Their evaluation results showed that the select policies th
e priority, further degrading performance. CIRC was often assumed in previous studies on IQs (e.g., <ref type="bibr" target="#b2">[6,</ref><ref type="bibr" target="#b11">15]</ref>), but is not used in a hierarchical scheduling window that divides the IQ into a large slow queue and a small fast queue <ref type="bibr" target="#b2">[6]</ref>. The larger queue requires multiple cycles for scheduling wh
he IQ was extensively studied around 2000, and a comprehensive survey was performed by Abella et al <ref type="bibr" target="#b0">[4]</ref>.</p><p>One processor that implemented SHIFT was the DEC Alph

/p><p>Sakai et al. proposed an IQ scheme that assigns high priority to multiple oldest instructions <ref type="bibr" target="#b19">[23]</ref> (not the single oldest instruction as in the age matrix).
pe="bibr" target="#b0">[1]</ref>, Giffin, et al., <ref type="bibr" target="#b22">[23]</ref>, Spivey <ref type="bibr" target="#b23">[24]</ref>, Bond and McKinley <ref type="bibr" target="#b24">[25]</re
4]</ref>, fuzzy mathematics model <ref type="bibr" target="#b14">[15]</ref>, subjective logic model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, software behavior
" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref> .</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref> .</p></div> <div xml
ticability. The research on system call is indispensable. Remarkable works include Forrest, et al., <ref type="bibr" target="#b0">[1]</ref>, Giffin, et al., <ref type="bibr" target="#b22">[23]</ref>,
rol flow and data flow gives us an angle of attacker' view <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. M. Abadi, et al., has present two important papers. The on cessing control for memory regions. They explore the benefits for CFI and present an implementation <ref type="bibr" target="#b20">[21]</ref>.</p><p>Reverse technology is often to facilitate understan
nd hardware technology <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target
r" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>, and finite state automaton model <ref type="bibr" target="#b18">[19]</ref>.</p><p>The research area of subverting control flow and da
nd hardware technology <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target
pe="bibr" target="#b14">[15]</ref>, subjective logic model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, software behavior model <ref type="bibr" target="#b1">[2,<
bibr" target="#b22">[23]</ref>, Spivey <ref type="bibr" target="#b23">[24]</ref>, Bond and McKinley <ref type="bibr" target="#b24">[25]</ref>. Forrest firstly proposed distinguishing program self usin
es with few contributions in a prediction. Inspired by the theory of hierarchical abstract machines <ref type="bibr" target="#b15">(Parr and Russell 1998)</ref>, we cast the task of profile reviser as
pe="bibr" target="#b12">(Koren, Bell, and Volinsky 2009)</ref>, bayesian personalized ranking (BPR) <ref type="bibr" target="#b17">(Rendle et al. 2009</ref>) and factorization machine (FM) <ref type=" r" target="#b8">(He et al. 2018</ref>). Baseline Methods. The comparison methods include:</p><p>BPR <ref type="bibr" target="#b17">(Rendle et al. 2009</ref>): optimizes a pairwise ranking loss for the forms, such as learning behavior analysis <ref type="bibr" target="#b0">(Anderson et al. 2014;</ref><ref type="bibr" target="#b17">Qiu et al. 2016;</ref><ref type="bibr" target="#b16">Qi et al. 2018</
at decomposing complex tasks into multiple small tasks to reduce the complexity of decision making <ref type="bibr" target="#b1">(Barto and Mahadevan 2003)</ref>, where different HRLs such as option-
ide the low-level task to speed up its local learning and does not propagate to the high-level task <ref type="bibr" target="#b4">(Ghavamzadeh and Mahadevan 2003)</ref>. Specifically, we first calcula
ifferent HRLs such as option-based HRL that formulates the abstract knowledge and action as options <ref type="bibr" target="#b23">(Sutton, Precup, and Singh 1999)</ref> and the hierarchical abstract erarchical abstract machines (HAMs) that decomposes high-level activities into low-level activities <ref type="bibr" target="#b23">(Sutton, Precup, and Singh 1999)</ref> are proposed. We formalize our
is a precision-based metrics that accounts for the predicted position of the ground truth instance <ref type="bibr" target="#b10">(Huang et al. 2018;</ref><ref type="bibr" target="#b8">He et al. 2018
at decomposing complex tasks into multiple small tasks to reduce the complexity of decision making <ref type="bibr" target="#b1">(Barto and Mahadevan 2003)</ref>, where different HRLs such as option-
reinforce-ment learning algorithm to solve many kinds of problems, such as relation classification <ref type="bibr" target="#b3">(Feng et al. 2018</ref>), text classification <ref type="bibr" target= ly revise the user profiles; finally, we jointly train the models together. Same as the settings of <ref type="bibr" target="#b3">(Feng et al. 2018)</ref>, to have a stable update, each parameter is u
is a precision-based metrics that accounts for the predicted position of the ground truth instance <ref type="bibr" target="#b10">(Huang et al. 2018;</ref><ref type="bibr" target="#b8">He et al. 2018
is a precision-based metrics that accounts for the predicted position of the ground truth instance <ref type="bibr" target="#b10">(Huang et al. 2018;</ref><ref type="bibr" target="#b8">He et al. 2018
gnoring the order of the historical courses, we can adopt the factored item similarity model (FISM) <ref type="bibr" target="#b11">(Kabbur, Ning, and Karypis 2013)</ref> to represent each course as an stic features. But for fair comparison, we only use the embeddings of users and courses.</p><p>FISM <ref type="bibr" target="#b11">(Kabbur, Ning, and Karypis 2013)</ref>: is an itemto-item collaborati
the output of a pitch tracker to generate reference annotations for melody and multi-f0 estimation <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref>.</p><p>In this pap n of 230 monophonic stems taken from MedleyDB and re-synthesized using the methodology presented in <ref type="bibr" target="#b18">[18]</ref>, which uses an analysis/synthesis approach to generate a s
as proposed by YIN <ref type="bibr" target="#b11">[11]</ref>. More recent approaches include SWIPE <ref type="bibr" target="#b12">[12]</ref>, which performs template matching with the spectrum of a s ophonic pitch tracking, represented by the pYIN <ref type="bibr" target="#b13">[13]</ref> and SWIPE <ref type="bibr" target="#b12">[12]</ref> algorithms. To examine the noise robustness of each algori
ion (ACF) <ref type="bibr" target="#b7">[7]</ref>, the average magnitude difference function (AMDF) <ref type="bibr" target="#b8">[8]</ref>, the normalized cross-correlation function (NCCF) as propose
e reference annotations for melody and multi-f0 estimation <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref>.</p><p>In this paper, we present a novel, data-driven metho in corresponding to the ground truth fundamental frequency is given a magnitude of one.</p><p>As in <ref type="bibr" target="#b19">[19]</ref>, in order to soften the penalty for near-correct predictio
1]</ref> or as a core component of melody extraction systems <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. Pitch estimation is also important for speech analysis, wher
domly selected from the training set. Each convolutional layer is preceded with batch normalization <ref type="bibr" target="#b21">[21]</ref> and followed by a dropout layer <ref type="bibr" target="#
ies, the state of the art is achieved by YIN-based methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref>, with pYIN being the best performing method to date <ref ty
as proposed by YIN <ref type="bibr" target="#b11">[11]</ref>. More recent approaches include SWIPE <ref type="bibr" target="#b12">[12]</ref>, which performs template matching with the spectrum of a s ophonic pitch tracking, represented by the pYIN <ref type="bibr" target="#b13">[13]</ref> and SWIPE <ref type="bibr" target="#b12">[12]</ref> algorithms. To examine the noise robustness of each algori
i and ?i are real numbers between 0 and 1. This loss function is optimized using the ADAM optimizer <ref type="bibr" target="#b20">[20]</ref>, with the learning rate 0.0002. The best performing model
ns include the cepstrum <ref type="bibr" target="#b6">[6]</ref>, the autocorrelation function (ACF) <ref type="bibr" target="#b7">[7]</ref>, the average magnitude difference function (AMDF) <ref type=
"bibr" target="#b8">[8]</ref>, the normalized cross-correlation function (NCCF) as proposed by RAPT <ref type="bibr" target="#b9">[9]</ref> and PRAAT <ref type="bibr" target="#b10">[10]</ref>, and the
eb documents, blog posts, news articles, and system logs) into appropriate HTML format. Prior works <ref type="bibr" target="#b66">[68]</ref> have realized the potential of hardware specialization for tasks include string finding, matching, replacing, trimming, comparing, etc. Previous work, such as <ref type="bibr" target="#b66">[68]</ref>, propose methods for string matching in hardware. However,
ry to prior works deploying a hash table that supports only GET requests in a memcached environment <ref type="bibr" target="#b53">[55]</ref>. Furthermore, supporting such a hash table in the PHP envi erversside Javascript applications <ref type="bibr" target="#b72">[73]</ref> or memcached workloads <ref type="bibr" target="#b53">[55]</ref>). Note that we simulate an aggressive memory system with p e store the keys in the hash table itself, unlike the hash table designed for memcached deployments <ref type="bibr" target="#b53">[55]</ref>. Storing the keys directly in the hash table eases the tra table that supports only GET operation has been deployed in hardware before for memcached workloads <ref type="bibr" target="#b53">[55]</ref>. Furthermore, <ref type="bibr" target="#b28">[30]</ref> de in contrast to the most large-scale memcached deployments <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b53">55]</ref> where the GET requests vastly outnumber the SET and other r get="#b34">36,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" tar

le recent efforts to address instruction cache bottlenecks <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b48">50]</ref> in datacenter workloads.</p><p>However, the software commun

merous research efforts have been devoted to optimizing warehouse-scale(WSC) and big data workloads <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" ta
type checking <ref type="bibr" target="#b20">[22]</ref>, hash table accesses for user-defined types <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32]</ref>, etc.) or the asso ynamic key names. These accesses cannot be converted to regular offset accesses by software methods <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" ta . Next, we describe briefly those optimizations from prior research proposals.</p><p>Inline Caching <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32]</ref> and Hash Map Inlin mic key names; such accesses cannot be converted to efficient offset references by software methods <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" ta ware and software optimization techniques from prior works <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" tar ce counting.</p><p>While there are many research proposals <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" tar
get="#b45">47,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b68">69]</ref> developed in C++-li
et="#b22">[24,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" tar
al from prior work that introduces minimal changes to the cache subsystem to mitigate this overhead <ref type="bibr" target="#b44">[46]</ref>.</p><p>In addition to applying the above four optimization
1 The branch prediction accuracy observed on Intel server processors and TAGE are in the same range <ref type="bibr" target="#b58">[60]</ref> compact enough that can be effectively cached in the L1. B
20">[22]</ref>, hash table accesses for user-defined types <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32]</ref>, etc.) or the associated overhead of garbage collected lang ations from prior research proposals.</p><p>Inline Caching <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32]</ref> and Hash Map Inlining <ref type="bibr" target="#b38">[40]</ s from prior works <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b38">40]</ref> together to these P e converted to regular offset accesses by software methods <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b38">40]</ref>. Programmability an research proposals <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b38">40]</ref> from the academic c nverted to efficient offset references by software methods <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b38">40]</ref>. Typically, these a
type checking <ref type="bibr" target="#b20">[22]</ref>, hash table accesses for user-defined types <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32]</ref>, etc.) or the asso ynamic key names. These accesses cannot be converted to regular offset accesses by software methods <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" ta . Next, we describe briefly those optimizations from prior research proposals.</p><p>Inline Caching <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32]</ref> and Hash Map Inlin mic key names; such accesses cannot be converted to efficient offset references by software methods <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" ta ware and software optimization techniques from prior works <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" tar ce counting.</p><p>While there are many research proposals <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" tar
re to process during most of their execution time. Prior work on predicting data-dependent branches <ref type="bibr" target="#b33">[35]</ref> may improve the MPKI of the PHP applications.</p><p>Branch

big data workloads <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" tar ="#b54">56,</ref><ref type="bibr" target="#b68">69]</ref> developed in C++-like compiled languages. <ref type="bibr" target="#b45">[47]</ref> has demonstrated in-depth microarchitectural characterizat applications.</p><p>Dynamic heap management in its entirety is non-trivial to implement in hardware <ref type="bibr" target="#b45">[47]</ref>. Our heap manager on the other hand relies on hardware onl

ance bottlenecks in real-world Javascript applications. Prior works propose instruction prefetching <ref type="bibr" target="#b26">[28]</ref>, preexecution techniques <ref type="bibr" target="#b27">[2
accelerator requires a maximum of 3 cycles to process up to 64 character blocks. We use CACTI 6.5+ <ref type="bibr" target="#b50">[52]</ref> to estimate the access latency, energy and area of the rem o satisfy a request from a hardware free list. The content reuse table has 32 entries. We use McPAT <ref type="bibr" target="#b50">[52]</ref> to collect core power and energy.</p></div> <div xmlns="ht
server-side web development, PHP is the most commonly used <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b71">72]</ref>, representing about 82.3% <ref type="bibr" target="#b11">[1 load inspired the design of the HipHop JIT compiler (HHVM) <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b71">72]</ref>, which translates PHP to native code. HHVM demonstrates a s avascript applications. However, PHP is most commonly used <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b71">72]</ref>, representing 82.3% <ref type="bibr" target="#b11">[13]</re
rget="#b6">[8,</ref><ref type="bibr" target="#b55">57]</ref> or recent hardware regexp accelerators <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" ta
ro-benchmark suites have been the primary focus for most architectural optimizations of web servers <ref type="bibr" target="#b18">[20]</ref>. Furthermore, these micro-benchmarks spend most of their t ploring architectural support for PHP applications and they mainly experiment with micro-benchmarks <ref type="bibr" target="#b18">[20]</ref>. <ref type="bibr" target="#b18">[20]</ref> improves the en cations and they mainly experiment with micro-benchmarks <ref type="bibr" target="#b18">[20]</ref>. <ref type="bibr" target="#b18">[20]</ref> improves the energy efficiency of SPECWeb2005 workloads by
d Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type="bibr" target="#b11">[14]</ref> . It could obtain transformation rules automatically durin

ype="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> , Hidden Markov Model (HMM) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> , Condition Random Fie
the precision and the recall of the NEs which occur in the training corpus rarely would be degraded <ref type="bibr" target="#b12">[15]</ref> . Second, if the training corpus is limited, it is feasibl
the precision and the recall of the NEs which occur in the training corpus rarely would be degraded <ref type="bibr" target="#b12">[15]</ref> . Second, if the training corpus is limited, it is feasibl


the article. Named Entity was proposed by Message Understanding Conference (MUC) for the first time <ref type="bibr" target="#b0">[1]</ref> . Named Entity Recognition (NER) is a difficult and challeng

ype="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> , Hidden Markov Model (HMM) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> , Condition Random Fie
re used in Japanese NER, such as Maximum Entropy Model (MEM) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> , Hidden Markov Model (HMM) <ref type="bibr" target="#b3">[4,

ating feature maps, effectively learning an equivariant representation. This work was later extended<ref type="bibr" target="#b8">(Dieleman et al., 2016)</ref> and evaluated on various computer vision

This supports the idea that equivariance is a good inductive bias for deep convolutional networks. <ref type="bibr" target="#b0">Agrawal et al. (2015)</ref> show that useful representations can be le
ant representations. Invariance can be achieved by pose normalization using an equivariant detector <ref type="bibr" target="#b28">(Lowe, 2004;</ref><ref type="bibr" target="#b16">Jaderberg et al., 20
fast computation of planar convolutions <ref type="bibr" target="#b30">(Mathieu et al., 2014;</ref><ref type="bibr" target="#b39">Vasilache et al., 2015;</ref><ref type="bibr" target="#b22">Lavin &am



t uses sparse, high-dimensional feature maps to deal with highdimensional groups of transformations.<ref type="bibr" target="#b7">Dieleman et al. (2015)</ref> showed that rotation symmetry can be expl
t uses sparse, high-dimensional feature maps to deal with highdimensional groups of transformations.<ref type="bibr" target="#b7">Dieleman et al. (2015)</ref> showed that rotation symmetry can be expl
work is carried out in the Forte verification framework, originally built on top of the Voss system <ref type="bibr" target="#b7">[8]</ref>. The interface language to Forte is reFLect, a lazy, strongl
ve. The techniques used to establish this are discussed in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. The global control invariant for the Core i7 EXE cluster c
verification of the implication between input and output constraints, we use the tool discussed in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Finally, the reflection mechanism allows terms in t it initialization sequence and is inductive. The techniques used to establish this are discussed in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. The global contro
ures: low-level protocols, register renaming, arithmetic units, microarchitecture descriptions etc. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>. In an industrial pr
ures: low-level protocols, register renaming, arithmetic units, microarchitecture descriptions etc. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>. In an industrial pr
ing, arithmetic units, microarchitecture descriptions etc. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>. In an industrial product development setting, formal verific only by inspection and reviews. Recent forays into such early microarchitecture validation in Intel <ref type="bibr" target="#b3">[4]</ref> have been very encouraging.</p><p>As discussed above, much o
ached sufficient maturity that they have now been routinely applied for a series of design projects <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" targ iest concerted floating-point verification effort in Intel was carried out on the Pentium Pro design<ref type="bibr" target="#b16">[17]</ref>. The goal of the effort was full formal verification of al
ing, arithmetic units, microarchitecture descriptions etc. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>. In an industrial product development setting, formal verific only by inspection and reviews. Recent forays into such early microarchitecture validation in Intel <ref type="bibr" target="#b3">[4]</ref> have been very encouraging.</p><p>As discussed above, much o
arget="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6]</ref>, and expanded to cover the full datapath functionality of the ing-point and other high-complexity, high-risk datapaths to almost all datapaths in the EXE cluster <ref type="bibr" target="#b5">[6]</ref>. This project was also the first to combine formal verificat
arget="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6]</ref>, and expanded to cover the full datapath functionality of the ing-point and other high-complexity, high-risk datapaths to almost all datapaths in the EXE cluster <ref type="bibr" target="#b5">[6]</ref>. This project was also the first to combine formal verificat
ached sufficient maturity that they have now been routinely applied for a series of design projects <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" targ iest concerted floating-point verification effort in Intel was carried out on the Pentium Pro design<ref type="bibr" target="#b16">[17]</ref>. The goal of the effort was full formal verification of al
erial images and LiDAR data. The developed network is based on a modified residual learning network <ref type="bibr" target="#b13">(He et al., 2016)</ref> that extracts robust low/mid/high-level featu res in the input images <ref type="bibr" target="#b50">(Zhang et al., 2016)</ref>. Previous studies <ref type="bibr" target="#b13">(He et al., 2016)</ref> have found that increasing the depth of neura e="figure" target="#fig_2">1</ref>).</p><p>A more detailed description of ResNet-50 can be found in <ref type="bibr" target="#b13">(He et al., 2016)</ref> and here ResNet-50 is modified as follows to cy may degrade after a saturation. This phenomenon is often referred to as the degradation problem. <ref type="bibr" target="#b13">He et al. (2016)</ref> recently proposed a Residual Network (ResNet)
raction such as buildings, roads, and trees <ref type="bibr" target="#b5">(Cheng et al., 2017;</ref><ref type="bibr" target="#b19">Kaiser et al., 2017;</ref><ref type="bibr" target="#b23">Liu et al.,
e results in pixel-wise labeling <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref>. <ref type="bibr" target="#b25">Long et al. (2015)</ref> proposed a fully convolutional neural networ rg/ns/1.0"><head n="2.1.2.">Encoder-decoder network architecture</head><p>FCN models, such as FCN8s <ref type="bibr" target="#b25">(Long et al., 2015)</ref>, SegNet <ref type="bibr" target="#b3">(Badr tract image features and effectively solve the end-to-end learning problem of semantic segmentation <ref type="bibr" target="#b25">(Long et al., 2015)</ref>. The encoder part could be a deep CNN (e.g.
emote sensing images makes it complex to extract the spectral and geometrical features of buildings <ref type="bibr" target="#b0">(Alshehhi et al., 2017)</ref>.</p><p>Based on the used data, the build
raction such as buildings, roads, and trees <ref type="bibr" target="#b5">(Cheng et al., 2017;</ref><ref type="bibr" target="#b19">Kaiser et al., 2017;</ref><ref type="bibr" target="#b23">Liu et al.,
emote sensing images makes it complex to extract the spectral and geometrical features of buildings <ref type="bibr" target="#b0">(Alshehhi et al., 2017)</ref>.</p><p>Based on the used data, the build
get="#b27">(Meng et al., 2012;</ref><ref type="bibr" target="#b35">Rottensteiner et al., 2005;</ref><ref type="bibr" target="#b48">Zarea and Mohammadzadeh, 2016)</ref>. High-resolution aerial and/ or
CNNs and FCNs have been widely used in urban objects extraction such as buildings, roads, and trees <ref type="bibr" target="#b5">(Cheng et al., 2017;</ref><ref type="bibr" target="#b19">Kaiser et al.
at uses both deep features and hand-crafted features to perform semantic labeling of aerial images. <ref type="bibr" target="#b47">Yuan (2017)</ref> designed a deep convolutional network that integrat
rban areas, have varied sizes and band reflectance and are often obscured by trees and their shadow <ref type="bibr" target="#b23">(Liu et al., 2017a)</ref>, and (2) the high intra-class and low inter br" target="#b5">(Cheng et al., 2017;</ref><ref type="bibr" target="#b19">Kaiser et al., 2017;</ref><ref type="bibr" target="#b23">Liu et al., 2017a;</ref><ref type="bibr" target="#b39">Sun et al., 20
target="#b29">Mongus et al., 2014;</ref><ref type="bibr" target="#b30">Niemeyer et al., 2014;</ref><ref type="bibr" target="#b37">Sampath and Shan, 2010;</ref><ref type="bibr" target="#b41">Wang et a
<ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b5">4]</ref> and the predictor in <ref type="bibr" target="#b13">[12]</ref> separate the stream of misses according to the PC of the m we show that it can be easily implemented on a simple extension to the Global History Buffer (GHB) <ref type="bibr" target="#b13">[12]</ref>, which is a convenient structure to implement various pref Bench suites and compare it against both a state-of-the-art PC based localization prefetcher -PC/DC <ref type="bibr" target="#b13">[12]</ref> -and its non-localized counterpart -G/DC <ref type="bibr" ages of localization and correlation, a taxonomy to classify prefetching algorithms was proposed in <ref type="bibr" target="#b13">[12]</ref>. In this taxonomy, each algorithm is denoted as a pair X/Y alization and constant stride correlation would be referred to as PC/CS, and the best prefetcher in <ref type="bibr" target="#b13">[12]</ref> uses PC based localization and address delta correlation a d amortize long memory access times, these prefetchers often resort to large degrees of prefetching <ref type="bibr" target="#b13">[12]</ref>. The final result is that this leads to two unfortunate be d><p>Many (hardware) data structures can be used to implement different prefetching algorithms, but <ref type="bibr" target="#b13">[12]</ref> has proposed a common data structure that is flexible enou alization schemes. Figure <ref type="figure">2a</ref> shows an example GHB for the PC/DC prefetcher <ref type="bibr" target="#b13">[12]</ref>. A detailed description of a hardware implementation of de y, such as tables: reduction of stale data, a more complete history, and lower storage requirements <ref type="bibr" target="#b13">[12]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head ely the Miss Graph prefetcher. To accommodate the new level of operation, we extend the taxonomy of <ref type="bibr" target="#b13">[12]</ref> with a third term, so that prefetching schemes are denoted ntries of the IT. As misses occur, the IT and the GHB are populated as described in Section 2.3 and <ref type="bibr" target="#b13">[12]</ref>. The new PreviousIT pointer is left pointing to the last s re-quires are the NextIT and Ctr for each IT entry and a single PreviousIT register. As observed in <ref type="bibr" target="#b13">[12]</ref> and in our own experience, both an IT and a GHB with 512 e <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Prefetching Algorithms</head><p>We use PC/DC <ref type="bibr" target="#b13">[12]</ref> as a representative of a modern highperformance prefetcher >[9]</ref> for TLB prefetching. The adaptation of delta correlation to data prefetching was done in <ref type="bibr" target="#b13">[12]</ref>, where the PC/DC prefetcher was proposed. That work also p red it against a state-of-the-art memory access instruction PC based localization prefetcher -PC/DC <ref type="bibr" target="#b13">[12]</ref>. Experimental results showed that the proposed prefetcher ="#b13">[12]</ref> -and its non-localized counterpart -G/DC <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b13">12]</ref>. Experimental results show that the proposed prefetcher con on that generated them <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b13">12]</ref>, or to the region of memory they reference <ref type="bibr" ntially lead to higher performance gains than those that treat all misses as a single global stream <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b16">15]</ref>, although this var for each stream depending on the "strength" of the links followed.</p><p>As with other prefetchers <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b14">13]</ref>, in order to avoid localization and was shown to consistently outperform other localized and non-localized prefetchers <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b16">15]</ref>. We also use a G/D
s of how to localize streams and correlate addresses, <ref type="bibr" target="#b11">[10]</ref> and <ref type="bibr" target="#b6">[5]</ref> proposed using a dead-block predictor to identify replaceabl
ve focused on exploiting temporal and spatial correlation in the miss address stream. Most notably, <ref type="bibr" target="#b24">[23]</ref> proposes a dynamic optimization framework that combines te

the same groups of shared data tend to be communicated in unison.</p><p>Concurrently with our work, <ref type="bibr" target="#b21">[20]</ref> also exploits temporal and spatial correlation (and also P
technique and is used in most commercial processors nowadays <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b12">11]</ref>. Such prefetchers, however, are usually limited to relative
rocessor is active. The access times for the different L2 cache sizes were computed using CACTI 4.2 <ref type="bibr" target="#b18">[17]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
than those that treat all misses as a single global stream <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b16">15]</ref>, although this varies according to individual applications. y outperform other localized and non-localized prefetchers <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b16">15]</ref>. We also use a G/DC prefetcher in order to assess the impac
k and introduced the taxonomy and notation for 2-level prefetchers that we extend here. The work in <ref type="bibr" target="#b7">[6]</ref> proposed correlating cache line tags instead of complete add
#b13">12]</ref>, or to the region of memory they reference <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b20">19]</ref>, or to some period get="#b7">[6]</ref> proposed correlating cache line tags instead of complete addresses. The work in <ref type="bibr" target="#b15">[14]</ref> proposed address localization using memory regions (called

that the model manifold and the true distribution's support have a non-negligible intersection (see <ref type="bibr" target="#b0">[1]</ref>), and this means that the KL distance is not defined (or sim ining GANs is well known for being delicate and unstable, for reasons theoretically investigated in <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this paper, we direct our attention on the various zero. This happens to be the case when two low dimensional manifolds intersect in general position <ref type="bibr" target="#b0">[1]</ref>.</p><p>Since the Wasserstein distance is much weaker than th ing gradients, as can be seen in Figure <ref type="figure">1</ref> of this paper and Theorem 2.4 of <ref type="bibr" target="#b0">[1]</ref>. In Figure <ref type="figure" target="#fig_1">2</ref> we sho e <ref type="bibr" target="#b3">[4]</ref>. This last phenomenon has been theoretically explained in <ref type="bibr" target="#b0">[1]</ref> and highlighted in <ref type="bibr" target="#b10">[11]</ref> nerated image, when the pixels were already normalized to be in the range <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>. This is a very high amount of noise, so much that when paper
="bibr" target="#b20">[21]</ref> which is known to perform well even on very nonstationary problems <ref type="bibr" target="#b12">[13]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
e this cosine was negative was in these situations of instability. We therefore switched to RMSProp <ref type="bibr" target="#b20">[21]</ref> which is known to perform well even on very nonstationary
y of the underlying space, and Wasserstein allows us to do exactly that.</p><p>Finally, the work of <ref type="bibr" target="#b2">[3]</ref> shows new algorithms for calculating Wasserstein distances b
re Φ(P)(f ) := E x∼P [f (x)] is a linear function over C b (X ). The Riesz Representation theorem ( <ref type="bibr" target="#b6">[7]</ref>, Theorem 10) tells us that Φ is an isometric immersion. This 2 f * (x) + m 2 . We then have 0 ≤ D(x) ≤ m and For completeness, we now show a proof for equation <ref type="bibr" target="#b6">(7)</ref> and the existence of said f * that attains the value of the
mality and thus limiting itself to very imperfect gradients.</p><p>• Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b4">[5]</ref> is a specific case of integral probability metrics when F = Reproducing Kernel Hilbert Space (RKHS) associated with a given kernel k : X × X → R. As proved on <ref type="bibr" target="#b4">[5]</ref> we know that MMD is a proper metric and not only a pseudomet eal life applications because of it. There are estimates with linear computational cost for the MMD <ref type="bibr" target="#b4">[5]</ref> which in a lot of cases makes MMD very useful, but they also ). While estimates that have linear computational cost as a function of the number of samples exist <ref type="bibr" target="#b4">[5]</ref>, they have worse sample complexity, and to the best of our k
ul, but they also have worse sample complexity.</p><p>• Generative Moment Matching Networks (GMMNs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2]</ref> are the generative co theoretical sample complexity of MMDs, which tends to be worse. However, in the original GMMN paper <ref type="bibr" target="#b9">[10]</ref> they indeed used a minibatch of size 1000, much larger than
mality and thus limiting itself to very imperfect gradients.</p><p>• Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b4">[5]</ref> is a specific case of integral probability metrics when F = Reproducing Kernel Hilbert Space (RKHS) associated with a given kernel k : X × X → R. As proved on <ref type="bibr" target="#b4">[5]</ref> we know that MMD is a proper metric and not only a pseudomet eal life applications because of it. There are estimates with linear computational cost for the MMD <ref type="bibr" target="#b4">[5]</ref> which in a lot of cases makes MMD very useful, but they also ). While estimates that have linear computational cost as a function of the number of samples exist <ref type="bibr" target="#b4">[5]</ref>, they have worse sample complexity, and to the best of our k
al, it is computationally difficult to generate samples given an arbitrary high dimensional density <ref type="bibr" target="#b15">[16]</ref>.</p><p>Variational Auto-Encoders (VAEs) <ref type="bibr" t
y of the underlying space, and Wasserstein allows us to do exactly that.</p><p>Finally, the work of <ref type="bibr" target="#b2">[3]</ref> shows new algorithms for calculating Wasserstein distances b
a generative context such as in GMMNs.</p><p>On another great line of research, the recent work of <ref type="bibr" target="#b13">[14]</ref> has explored the use of Wasserstein distances in the conte
target="#b17">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b20">Peters et al., 2018;</ref><ref type="bibr" target="#b2">Devlin et al., 2019;</ref><ref type="bibr" target="#b11">Joshi et al., e of the decoder.</p><p>Figure <ref type="figure">1</ref>: A schematic comparison of BART with BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> and GPT <ref type="bibr" target="#b21">(Ra are shown in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>Token Masking Following BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>, random tokens are sampled and replaced wi onal embeddings or attention across segments from XLNet.</p><p>Masked Language Model Following BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>, we replace 15% of tokens with [MASK] symb kipedia paragraphs. Answers are text spans extracted from a given document context. Similar to BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>, we use concatenated question and context g during pre-training.</p><p>Bidirectional encoders are crucial for SQuAD As noted in previous work <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>, just left-to-right decoder performs poorl > demonstrated that very large language models can act as unsupervised multitask models.</p><p>BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> introduced masked language modelling, whic
train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding as GPT-2 <ref type="bibr" target="#b22">(Radford et al., 2019)</ref>. Based on the results in Section ?4, we ft-only and right-only representations, but does not pre-train interactions between these features. <ref type="bibr" target="#b22">Radford et al. (2019)</ref> demonstrated that very large language mod
.1">Architecture</head><p>BART uses the standard sequence-to-sequence Transformer architecture from <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>, except, following GPT, that we modify R Table <ref type="table">6</ref>. We compare our results against a baseline Transformer architecture <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> with Transformerlarge settings (the base
okens unmasked and a left-to-right mask for the remainder.</p><p>Masked Seq-to-Seq Inspired by MASS <ref type="bibr" target="#b27">(Song et al., 2019)</ref>, we mask a span containing 50% of tokens, a ning and generation tasks, because the decoder is always trained on uncorrupted context.</p><p>MASS <ref type="bibr" target="#b27">(Song et al., 2019)</ref> is perhaps the most similar model to BART. anslation. The largest improvements have come from pre-training on both source and target languages <ref type="bibr" target="#b27">(Song et al., 2019;</ref><ref type="bibr" target="#b12">Lample &amp;
>We also evaluated performance on WMT16 Romanian-English, augmented with back-translation data from <ref type="bibr" target="#b25">Sennrich et al. (2016)</ref>. We use a 6-layer transformer source enc
n the well-studied SQuAD and GLUE tasks <ref type="bibr" target="#b30">(Warstadt et al., 2018;</ref><ref type="bibr" target="#b26">Socher et al., 2013;</ref><ref type="bibr" target="#b4">Dolan &amp; B
okens unmasked and a left-to-right mask for the remainder.</p><p>Masked Seq-to-Seq Inspired by MASS <ref type="bibr" target="#b27">(Song et al., 2019)</ref>, we mask a span containing 50% of tokens, a ning and generation tasks, because the decoder is always trained on uncorrupted context.</p><p>MASS <ref type="bibr" target="#b27">(Song et al., 2019)</ref> is perhaps the most similar model to BART. anslation. The largest improvements have come from pre-training on both source and target languages <ref type="bibr" target="#b27">(Song et al., 2019;</ref><ref type="bibr" target="#b12">Lample &amp;
>We also evaluated performance on WMT16 Romanian-English, augmented with back-translation data from <ref type="bibr" target="#b25">Sennrich et al. (2016)</ref>. We use a 6-layer transformer source enc
train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding as GPT-2 <ref type="bibr" target="#b22">(Radford et al., 2019)</ref>. Based on the results in Section ?4, we ft-only and right-only representations, but does not pre-train interactions between these features. <ref type="bibr" target="#b22">Radford et al. (2019)</ref> demonstrated that very large language mod
nan et al., 2019)</ref>, a dialogue response generation task, conditioned on context and a persona. <ref type="bibr" target="#b10">(Hermann et al., 2015)</ref>, a news summarization dataset. Summaries

echniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks <ref type="bibr" target="#b24">(Scarselli et al., 2009)</ref>, which we modify to use gated recurren ucing a sequence of outputs. Here, (1) is mostly achieved by previous work on Graph Neural Networks <ref type="bibr" target="#b24">(Scarselli et al., 2009)</ref>; we make several minor adaptations of on graphs, including Graph Neural Networks <ref type="bibr" target="#b10">(Gori et al., 2005;</ref><ref type="bibr" target="#b24">Scarselli et al., 2009)</ref>, spectral networks <ref type="bibr" tar ion, we review Graph Neural Networks (GNNs) <ref type="bibr" target="#b10">(Gori et al., 2005;</ref><ref type="bibr" target="#b24">Scarselli et al., 2009)</ref> and introduce notation and concepts tha irected edge v → v , but we note that the framework can easily be adapted to undirected graphs; see <ref type="bibr" target="#b24">Scarselli et al. (2009)</ref>. The node vector (or node representatio v = f * (l v , l CO(v) , l NBR(v) , h (t−1) NBR(v)</formula><p>). Several variants are discussed in <ref type="bibr" target="#b24">Scarselli et al. (2009)</ref> including positional graph forms, node- l graph forms, node-specific updates, and alternative representations of neighborhoods. Concretely, <ref type="bibr" target="#b24">Scarselli et al. (2009)</ref> suggest decomposing f * (•) to be a sum unction g(h v , l v ) that maps to an output. This is generally a linear or neural network mapping. <ref type="bibr" target="#b24">Scarselli et al. (2009)</ref> focus on outputs that are independent p r" target="#b10">(Gori et al., 2005;</ref><ref type="bibr" target="#b6">Di Massa et al., 2006;</ref><ref type="bibr" target="#b24">Scarselli et al., 2009;</ref><ref type="bibr">Uwents et al., 2011)</r
safety) and that the function indeed concatenates two lists using a Hoare-style verification scheme <ref type="bibr" target="#b13">(Hoare, 1969)</ref>.</p><p>The hardest part of this process is coming
as node-level regression or classification.</p><p>Learning is done via the Almeida-Pineda algorithm <ref type="bibr" target="#b0">(Almeida, 1990;</ref><ref type="bibr" target="#b21">Pineda, 1987)</ref
ww.tei-c.org/ns/1.0"><head>Test results appear in</head><p>The results for this task are given in   <ref type="bibr" target="#b23">Reynolds, 2002)</ref>, which uses inductive predicates to describe ab
re problem is to find mathematical descriptions of the data structures used in a program. Following <ref type="bibr" target="#b3">Brockschmidt et al. (2015)</ref>, we have phrased this as a machine le ate of memory, to a logical description of the data structures that have been instantiated. Whereas <ref type="bibr" target="#b3">Brockschmidt et al. (2015)</ref> relied on a large amount of hand-engi >A more complex scenario allowing for nested data structures (e.g., list of lists) was discussed in <ref type="bibr" target="#b3">Brockschmidt et al. (2015)</ref>. We have also successfully extended t ing for exact equality.</p><p>We compared our GGS-NN-based model with a method we developed earlier <ref type="bibr" target="#b3">(Brockschmidt et al., 2015)</ref>. The earlier approach treats each pr
rogram locations, and then predict a separation logic formula. Static program analysis tools (e.g., <ref type="bibr" target="#b22">(Piskac et al., 2014</ref>)) can check whether a candidate formula is
idze et al., 2011)</ref>, and methods that define graph features in terms of random walks on graphs <ref type="bibr" target="#b20">(Perozzi et al., 2014)</ref>. More closely related to our goal in thi raph-structured inputs, but we are not aware of work that learns the kernels and outputs sequences. <ref type="bibr" target="#b20">Perozzi et al. (2014)</ref> convert graphs into sequences by followin
to our goal in this work are methods that learn features on graphs, including Graph Neural Networks <ref type="bibr" target="#b10">(Gori et al., 2005;</ref><ref type="bibr" target="#b24">Scarselli et ><head n="2">GRAPH NEURAL NETWORKS</head><p>In this section, we review Graph Neural Networks (GNNs) <ref type="bibr" target="#b10">(Gori et al., 2005;</ref><ref type="bibr" target="#b24">Scarselli et model that differs from GNNs mainly in the output model. GNNs have been applied in several domains <ref type="bibr" target="#b10">(Gori et al., 2005;</ref><ref type="bibr" target="#b6">Di Massa et al
orks as a composition of learned components has a long history, dating back at least to the work of <ref type="bibr" target="#b12">Hinton (1988)</ref> on assembling neural networks according to a fami
eatures of an input graph, graph kernels <ref type="bibr" target="#b14">(Kashima et al., 2003;</ref><ref type="bibr" target="#b25">Shervashidze et al., 2011)</ref>, and methods that define graph featu &amp; Jain (2004)</ref> and <ref type="bibr" target="#b2">Bottou (2014)</ref>.</p><p>Graph kernels <ref type="bibr" target="#b25">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b14">Kashi
rogram locations, and then predict a separation logic formula. Static program analysis tools (e.g., <ref type="bibr" target="#b22">(Piskac et al., 2014</ref>)) can check whether a candidate formula is
ibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>, issuing prefetch requests <ref type="bibr" target="#b17">[18]</ref>, etc. However, the impact of these optimizations has not b ing into the dependence chain between them. For instance, we show that one such recent optimization <ref type="bibr" target="#b17">[18]</ref>, which prioritizes critical loads, does very well for SPEC 28]</ref>, caches <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, memory requests <ref type="bibr" target="#b10">[11]</ref> type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>, issuing prefetches <ref type="bibr" target="#b17">[18]</ref>, etc. Until now, these optimizations have been primarily p in prioritizing two important resources -one for memory which issues prefetches for critical loads <ref type="bibr" target="#b17">[18]</ref> and another for ALU resources in instruction scheduling <r ">[33]</ref>. These proposals identify high-fanout loads to mark them as critical to issue prefetch <ref type="bibr" target="#b17">[18]</ref> and prioritize the critical instructions for ALU resource has also been shown to outperform the latency based ways of identifying and exploiting criticality <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b33">[34]</ref>. We next evalua oritizing) and SPEC.float (34% from prefetching, 25% from prioritizing), re-affirming prior results <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Interestingly, 8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref> have targeted one critical instruction at a time, rather t e, 64KB d-cache, 2 cycle hit latency; 8-way 2MB L2 with System CLPT prefetcher (1024?7bits entries) <ref type="bibr" target="#b17">[18]</ref>   Data Processing call), and the 3-bit argument with it to 8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b78">[79]</ref> or even backend airly extensive hardware to identify these chains, and optimizing for them, e.g. techniques such as <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bib
f> and another for ALU resources in instruction scheduling <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. These proposal ibr" target="#b17">[18]</ref> and prioritize the critical instructions for ALU resource allocations <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. These techniqu rces proposed in <ref type="bibr" target="#b32">[33]</ref>, using the tracking hardware proposed in <ref type="bibr" target="#b31">[32]</ref>, which requires 1.5KB SRAM for maintaining the tokens. ? A
5">[96]</ref>, or optimize for only specific app domains <ref type="bibr" target="#b89">[90]</ref>- <ref type="bibr" target="#b91">[92]</ref>. This paper is amongst the first to show that mobile apps
#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. In this work, #b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref> and/or (b) using software profile-driven compilation. As e #b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> using metrics s 5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bib
rket level changes to the source/advertisement models, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b104">[105]</ref>, dynamic instrumentation mechanisms <ref type="bibr" tar

a hardware that has 4? the i-cache capacity (128KB vs. 32KB) to reduce instruction misses. ? EFetch <ref type="bibr" target="#b70">[71]</ref>: We implemented a recently proposed instruction prefetcher ref type="bibr" target="#b70">[71]</ref>: We implemented a recently proposed instruction prefetcher <ref type="bibr" target="#b70">[71]</ref> that is specifically useful for user-event driven applicat t is specifically useful for user-event driven applications, as in our mobile apps. This prefetcher <ref type="bibr" target="#b70">[71]</ref> tracks history of user-event call stack, and uses it to pr

cal instructions" <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bib d optimizations <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref> have targeted o many prior works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref> and/or (b) usin ical instructions <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bib d optimizations <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bib ings, single a instruction criticality optimization scheme as described previously, stalls 2 cycles <ref type="bibr" target="#b11">(12,</ref><ref type="bibr" target="#b12">13)</ref> in the execution.
prior works, specifically for high-end processors, in identifying and extracting dependence chains <ref type="bibr" target="#b79">[80]</ref>- <ref type="bibr" target="#b81">[82]</ref>. However, such
target="#b10">[11]</ref>, predicting instruction results <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>, issuing prefet


e improvements. While similar in spirit to some of the prior work on instruction stream compression <ref type="bibr" target="#b100">[101]</ref>- <ref type="bibr" target="#b102">[103]</ref>, we quantit

92">[93]</ref>), while others address overall efficiency <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b93">[94]</ref>- <ref type="bibr" target="#b95">[96]</ref>. Unlike our app
f these optimizations either provision more CPU hardware <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref>, or optimize fo
#b59">[60]</ref>. Further, we also simulate a detailed memory model for a 2GB LPDDR3 using DRAMSim2 <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. This setup ena

e improvements. While similar in spirit to some of the prior work on instruction stream compression <ref type="bibr" target="#b100">[101]</ref>- <ref type="bibr" target="#b102">[103]</ref>, we quantit
mber of software profiling frameworks have been proposed <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b103">[104]</ref>- <ref type="bibr" target="#b106">[107]</ref> -studying l ype="bibr" target="#b105">[106]</ref>, app-market level changes to the source/advertisement models, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b104">[105]</ref>, dynamic in
ing ARM decoders can decode any of these formats based on simple flags and pending queue structures <ref type="bibr" target="#b50">[51]</ref>.</p><p>We propose to represent each instruction of a CritI ke to optimize, in the 16-bit format (Fig. <ref type="figure">6(d)</ref>). Even though past studies <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bib b compression <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b50">[51]</ref> mechanism. Front-end Optimizations for Mobile Platforms: T ting ARM decoders can decode any of these formats based on simple flags and pending queue structures<ref type="bibr" target="#b50">[51]</ref>.We propose to represent each instruction of a CritIC seque sequence, that we would like to optimize, in the 16-bit format (Fig.6(d)). Even though past studies<ref type="bibr" target="#b50">[51]</ref>,<ref type="bibr" target="#b51">[52]</ref>,<ref type="bibr"
lution streams. This paper represents a very substantial extension of our previous conference paper <ref type="bibr" target="#b105">[105]</ref> with an additional material added from our unpublished t t object detection and instance segmentation frameworks. The main technical novelties compared with <ref type="bibr" target="#b105">[105]</ref> lie in threefold. <ref type="bibr" target="#b0">(1)</ref efold. <ref type="bibr" target="#b0">(1)</ref> We extend the network (named as HRNetV1) proposed in <ref type="bibr" target="#b105">[105]</ref>, to two versions: HRNetV2 and HRNetV2p, which explore al

ref>, to two versions: HRNetV2 and HRNetV2p, which explore all the four-resolution representations. <ref type="bibr" target="#b1">(2)</ref> We build the connection between multi-resolution fusion and

ple and heavy upsample processes <ref type="bibr" target="#b115">[115]</ref>, recombinator networks <ref type="bibr" target="#b40">[40]</ref>; improving skip connections with more or complicated convo
Net <ref type="bibr" target="#b27">[27]</ref>. We train the Faster R-CNN and Cascade 4. Same as FPN <ref type="bibr" target="#b73">[73]</ref>, we also use 5 levels.</p></div> <div xmlns="http://www.te
"bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b122">[122]</ref> with dense connections <ref type="bibr" target="#b110">[110]</ref>.</p><p>Maintaining high-resolution representations. Our
et <ref type="bibr" target="#b95">[95]</ref> and Hourglass <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr"
or both training and testing. We report the results over two scene parsing datasets, PASCAL-Context <ref type="bibr" target="#b81">[81]</ref> and Cityscapes <ref type="bibr" target="#b22">[22]</ref>, t the model learned on the train+val set. In both cases, HRNetV2-W48 achieves the best performance. <ref type="bibr" target="#b81">[81]</ref> includes 4, 998 scene images for training and 5, 105 image
lude: light upsample process <ref type="bibr">[5]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b124">[124]</ref>, possibly wit standard horizontal flipping. The input images are resized such that the shorter edge is 800 pixels <ref type="bibr" target="#b72">[72]</ref>. Inference is performed on a single image scale.</p><p>We
">[29]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b97">[97]</ref>, <ref type="bibr" target="#b98">[98]</ref>, <ref type="bib
blished work on MCTS, to provide the reader Fig. <ref type="figure">1</ref>. The basic MCTS process <ref type="bibr" target="#b16">[17]</ref>.</p><p>with the tools to solve new problems using MCTS and basic MCTS process is conceptually very simple, as shown in Figure <ref type="figure">1</ref> (from <ref type="bibr" target="#b16">[17]</ref>). A tree 1 is built in an incremental and asymmetric manne t two moves and uses LGR-1 if there is no LGR-2 entry for the last two moves.</p><p>Baier and Drake <ref type="bibr" target="#b16">[17]</ref> propose an extension to LGR-1 and LGR-2 called Last Good R ests using the Last Good Reply heuristic (6.1.8) to inform simulations, modified by Baier and Drake <ref type="bibr" target="#b16">[17]</ref> to include the forgetting of bad moves. Most programs use
struct degenerate cases in which flat Monte Carlo fails, as it does not allow for an opponent model <ref type="bibr" target="#b28">[29]</ref>.</p><p>Alth öfer describes the laziness of flat Monte Carl
ng and meta-search extension, and apply it to SameGame <ref type="bibr" target="#b192">[191]</ref>  <ref type="bibr" target="#b191">[190]</ref>. Their player achieved a higher score than any previous
edge, the algorithm outperformed baseline and human players across 12 scenarios. ORTS Naveed et al. <ref type="bibr" target="#b150">[150]</ref> apply UCT and RRTs (4.10.3) to the game engine ORTS. Bot
e extension of UCT to continuous stochastic problems through the use of double progressive widening <ref type="bibr" target="#b68">[69]</ref>, in which child nodes are either revisited, added or sampl nt and other toy problems, outperforming plain UCT with progressive widening and Q-learning methods <ref type="bibr" target="#b68">[69]</ref>, but did not work so well for complex real-world problems.
game trees.</p><p>To address the problem of strategy fusion in determinized UCT, Whitehouse et al. <ref type="bibr" target="#b231">[230]</ref> propose information set UCT (ISUCT), a variant of MCTS t the number of parent visits in which the child was compatible.</p><p>For the experimental domain in <ref type="bibr" target="#b231">[230]</ref>, ISUCT fails to outperform determinized UCT overall. How MC-AIXA agent. Dou Di Zhu is a popular Chinese card game with hidden information. Whitehouse et al. <ref type="bibr" target="#b231">[230]</ref> use information sets of states to store rollout statisti
ore for each state is used for selection rather than the (average) score expectation for that state <ref type="bibr" target="#b61">[62]</ref>. This modification is so named by Chevelu et al. due to it for natural language processing (NLP) tasks such as the paraphrasing of natural language statements <ref type="bibr" target="#b61">[62]</ref>.</p><p>Mahlmann et al. describe the use of UCT for content
design awards, and was the first "eurogame" to become widely popular outside Germany. Szita et al. <ref type="bibr" target="#b210">[209]</ref> implemented a multiplayer MCTS player (4.5) for Settlers
v> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Year</head><p>choice in the default policy for Go <ref type="bibr" target="#b36">[37]</ref>. These heuristic functions were in the form of symbolic ex
flat Monte Carlo is demonstrated by Ginsberg <ref type="bibr" target="#b96">[97]</ref> and Sheppard <ref type="bibr" target="#b200">[199]</ref>, who use such approaches to achieve world champion level g's GIB program competes with expert Bridge players. 1998 MAVEN defeats the world scrabble champion <ref type="bibr" target="#b200">[199]</ref>. 2002 Auer et al. <ref type="bibr" target="#b12">[13]</r
n that different moves can be chosen from different states in the same information set. Long et al. <ref type="bibr" target="#b130">[130]</ref> describe how this can be measured using synthetic game t t MCTS is successful for tricktaking card games, but less so for poker-like card games. Long et al. <ref type="bibr" target="#b130">[130]</ref> define three measurable parameters of game trees and sho
ias by using corpus level constraints, but is only practical for models with specialized structure. <ref type="bibr" target="#b13">Kusner et al. (2017)</ref> propose the method based on causal inferen
be closer to "programmer" than "woman" <ref type="bibr" target="#b1">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan et al., 2017)</ref>. Current state-of-art co-reference system mbeddings can encode sexist stereotypes <ref type="bibr" target="#b1">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan et al., 2017)</ref>. Similar observations have been made in v
2015a)</ref>, and neuralnetwork based <ref type="bibr" target="#b4">(Clark and Manning, 2016;</ref><ref type="bibr" target="#b15">Lee et al., 2017)</ref> have been proposed. While significant advance /ref> and the current best published system: the UW End-to-end Neural Coreference Resolution System <ref type="bibr" target="#b15">(Lee et al., 2017)</ref>. Despite qualitatively different approaches, et="#b6">(Durrett and Klein, 2013)</ref>, and end-to-end neural (the current state-ofthe-art), E2E, <ref type="bibr" target="#b15">(Lee et al., 2017)</ref>. The following sections show that performanc
lizes biased predictions. Various other approaches have been proposed to produce "fair" classifiers <ref type="bibr" target="#b2">(Calders et al., 2009;</ref><ref type="bibr" target="#b7">Feldman et a
WinoBias.This dataset follows the winograd format <ref type="bibr" target="#b9">(Hirst, 1981;</ref><ref type="bibr" target="#b20">Rahman and Ng, 2012;</ref><ref type="bibr" target="#b18">Peng et al.,
rich and rule-based approaches rely on corpus based gender statistics mined from external resources <ref type="bibr" target="#b0">(Bergsma and Lin, 2006)</ref>. Such lists were generated from large un
dataset. In combination with methods that remove bias from fixed resources such as word embeddings <ref type="bibr" target="#b1">(Bolukbasi et al., 2016)</ref>, our data augmentation approach complet ng their bias. To reduce bias from this resource, we replace GloVe embeddings with debiased vectors <ref type="bibr" target="#b1">(Bolukbasi et al., 2016)</ref>.</p><p>Gender Lists While current neura work has shown that they are severely biased: "man" tends to be closer to "programmer" than "woman" <ref type="bibr" target="#b1">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan e ematic instances have been demonstrated, for example, word embeddings can encode sexist stereotypes <ref type="bibr" target="#b1">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan e
se data augmentation based on gender swapping in order to reduce gender bias.</p><p>Concurrent work <ref type="bibr" target="#b22">(Rudinger et al., 2018</ref>) also studied gender bias in coreference
ce="foot" n="5" xml:id="foot_4">To exclude mentions such as "his mother", we use Collins head finder<ref type="bibr" target="#b5">(Collins, 2003)</ref> to identify the head word of each mention, and o
rich and rule-based approaches rely on corpus based gender statistics mined from external resources <ref type="bibr" target="#b0">(Bergsma and Lin, 2006)</ref>. Such lists were generated from large un
be closer to "programmer" than "woman" <ref type="bibr" target="#b1">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan et al., 2017)</ref>. Current state-of-art co-reference system mbeddings can encode sexist stereotypes <ref type="bibr" target="#b1">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan et al., 2017)</ref>. Similar observations have been made in v
ut <ref type="bibr" target="#b16">[17]</ref>.</p><p>When the graph structure of the input is known, <ref type="bibr" target="#b1">[2]</ref> introduced a model to generalize ConvNets using low learning r parameters. Our main contributions can be summarized as follows:</p><p>• We extend the ideas from <ref type="bibr" target="#b1">[2]</ref> to large-scale classification problems, specifically Imagene vised fashion. However, it does not attempt to exploit any weight-sharing strategy.</p><p>Recently, <ref type="bibr" target="#b1">[2]</ref> proposed a generalization of convolutions to graphs via the v xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spectral Networks</head><p>Our work builds upon <ref type="bibr" target="#b1">[2]</ref> which introduced spectral networks. We recall the definition mula_4">∂ k x(ξ) ∂ξ k ≤ C |u| k |x(u)|du ,</formula><p>where x(ξ) is the Fourier transform of x. In <ref type="bibr" target="#b1">[2]</ref> it was suggested to use the same principle in a general grap ction</head><p>Whereas some recognition tasks in non-Euclidean domains, such as those considered in <ref type="bibr" target="#b1">[2]</ref> or <ref type="bibr" target="#b11">[12]</ref>, might have a p
label></formula><p>In our experiments, we also consider the variant of self-tuning diffusion kernel <ref type="bibr" target="#b20">[21]</ref> ω(i, j) = exp</p><formula xml:id="formula_8">− d(i,j) σ i
lity which made the spectral networks tractable. As a baseline architecture, we used the network of <ref type="bibr" target="#b9">[10]</ref> which has 4 hidden layers and is regularized using dropout nuous variable, all networks were trained by minimizing the Root Mean-Squared Error loss. Following <ref type="bibr" target="#b9">[10]</ref>, we measured performance by computing the squared correlati
a wide variety of tasks, from computer vision and acoustic modeling to natural language processing <ref type="bibr" target="#b8">[9]</ref>. At the core of their success lies an important assumption o
local receptive fields <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref>, mostly with applications to image recognition. In particul
local receptive fields <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref>, mostly with applications to image recognition. In particul
label></formula><p>In our experiments, we also consider the variant of self-tuning diffusion kernel <ref type="bibr" target="#b20">[21]</ref> ω(i, j) = exp</p><formula xml:id="formula_8">− d(i,j) σ i
eech. These properties are exploited efficiently by ConvNets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, which are designed to extract local features that are shared
eech. These properties are exploited efficiently by ConvNets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, which are designed to extract local features that are shared
rough the identification of a certain graphical model using 1 -penalized logistic regression. Also, <ref type="bibr" target="#b2">[3]</ref> considers the problem of learning a deep architecture throug
raph or similarity estimation aspects have also been extensively studied in the past. For instance, <ref type="bibr" target="#b14">[15]</ref> studies the estimation of the graph from a statistical poi
uestion answering <ref type="bibr" target="#b22">(Yu et al., 2017)</ref>, knowledge base population <ref type="bibr" target="#b25">(Zhang et al., 2017)</ref>, and biomedical knowledge discovery <ref t our group compared (1) and ( <ref type="formula" target="#formula_1">2</ref>) with sequence models <ref type="bibr" target="#b25">(Zhang et al., 2017)</ref>, and we report these results; for (3) we r TM), and showed that it outperforms several CNN and dependency-based models by a substantial margin <ref type="bibr" target="#b25">(Zhang et al., 2017)</ref>. We compare with this strong baseline, and etup</head><p>We conduct experiments on two relation extraction datasets: (1) TACRED: Introduced in <ref type="bibr" target="#b25">(Zhang et al., 2017)</ref>, TACRED contains over 106k mention pairs d f the two entities.</p><p>More recently, <ref type="bibr" target="#b0">Adel et al. (2016)</ref> and <ref type="bibr" target="#b25">Zhang et al. (2017)</ref> have shown that relatively simple neural mo la_1">2</ref> For fair comparisons on the TACRED dataset, we follow the evaluation protocol used in <ref type="bibr" target="#b25">(Zhang et al., 2017</ref>) by selecting the model with the median dev
convolutional network (Kipf and Welling, 2017) is an adaptation of the convolutional neural network <ref type="bibr" target="#b5">(LeCun et al., 1998)</ref> for encoding graphs. Given a graph with n n
on relational learning between entities <ref type="bibr" target="#b16">(Santoro et al., 2017;</ref><ref type="bibr" target="#b6">Lee et al., 2017)</ref>, we obtain the final representation used for c
within the subtree rooted at the lowest common ancestor (LCA) of the two entities. Previous studies <ref type="bibr" target="#b21">(Xu et al., 2015b;</ref><ref type="bibr" target="#b11">Miwa and Bansa as also been shown to improve relation extraction performance by capturing long-distance relations. <ref type="bibr" target="#b21">Xu et al. (2015b)</ref> generalized the idea of dependency path kerne dependencybased features with other lexical features. (2) Shortest Dependency Path LSTM (SDP-LSTM) <ref type="bibr" target="#b21">(Xu et al., 2015b)</ref>, which applies a neural sequence model on th
<ref type="bibr" target="#b11">(Miwa and Bansal, 2016)</ref>. Another popular approach, inspired by <ref type="bibr" target="#b1">Bunescu and Mooney (2005)</ref>, is to reduce the parse tree to the sh rnels <ref type="bibr" target="#b23">(Zelenko et al., 2003)</ref> and dependency path-based kernels <ref type="bibr" target="#b1">(Bunescu and Mooney, 2005)</ref> are effective for this task.</p><p>Re
<ref type="bibr" target="#b11">(Miwa and Bansal, 2016)</ref>. Another popular approach, inspired by <ref type="bibr" target="#b1">Bunescu and Mooney (2005)</ref>, is to reduce the parse tree to the sh rnels <ref type="bibr" target="#b23">(Zelenko et al., 2003)</ref> and dependency path-based kernels <ref type="bibr" target="#b1">(Bunescu and Mooney, 2005)</ref> are effective for this task.</p><p>Re
convolutional network (Kipf and Welling, 2017) is an adaptation of the convolutional neural network <ref type="bibr" target="#b5">(LeCun et al., 1998)</ref> for encoding graphs. Given a graph with n n
entric pruning has been applied to reduce the space of possible arguments in semantic role labeling <ref type="bibr" target="#b2">(He et al., 2018)</ref>. The authors showed pruning words too far away
ed to shortest dependency pathbased approaches. Compared to tree-structured models (e.g., Tree-LSTM <ref type="bibr" target="#b17">(Tai et al., 2015)</ref>), it not only is able to capture more global CNN over the path. <ref type="bibr" target="#b11">Miwa and Bansal (2016)</ref> applied a Tree-LSTM <ref type="bibr" target="#b17">(Tai et al., 2015)</ref>, a generalized form of LSTM over dependency on the shortest path between the subject and object entities in the dependency tree. (3) Tree-LSTM <ref type="bibr" target="#b17">(Tai et al., 2015)</ref>, which is a recursive model that generalizes
scheme can further improve performance. <ref type="bibr" target="#b26">Zhou et al. (2016)</ref> and <ref type="bibr" target="#b19">Wang et al. (2016)</ref> proposed to use attention mechanisms over RN
lied a one-dimensional convolutional neural network (CNN) with manual features to encode relations. <ref type="bibr" target="#b18">Vu et al. (2016)</ref> showed that combining a CNN with a recurrent n
atasets and continuously set new stateof-the-art performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar − L)X is understood as features averaging and propagation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref>. In graph signal pr rast to the recent design principle of graph neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref>, our results sugges 9]</ref>. Started with the early success of ChebNet <ref type="bibr" target="#b5">[6]</ref> and GCN <ref type="bibr" target="#b15">[16]</ref> at vertex classification, many variants of GNN have been p e observe that the parameters of a graph convolutional layer in a Graph Convolutional Network (GCN) <ref type="bibr" target="#b15">[16]</ref> only contribute to overfitting. Similar observations have problem and provide insights to the mechanism underlying the most commonly used baseline model GCN <ref type="bibr" target="#b15">[16]</ref>, and its simplified variant SGC <ref type="bibr" target="# onding NNs using true features.</p><p>Theorem 7 implies that, under Assumption 1, both gfNN and GCN <ref type="bibr" target="#b15">[16]</ref> have similar high performance. Since gfNN does not require Network model by removing nonlinearity in the neural network and only averaging features.</p><p>GCN <ref type="bibr" target="#b15">[16]</ref> Graph Convolutional Neural Network ($) is the most commonl
hat backpropagation improves neither accuracy nor detectability of a GCN-based GNN model. Li et al. <ref type="bibr" target="#b17">[18]</ref> empirically analyzed GCN models with many layers under the
twork benchmarking <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> and artificially synthesized random graphs from the two cir
filter. In contrast to the recent design trend involving GCN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref> , our results imply that simply stacking GCN layers might o cently GCN-based GNNs have been applied in many important applications such as point-cloud analysis <ref type="bibr" target="#b23">[24]</ref> or weakly-supervised learning <ref type="bibr" target="#b8
sumption 1. Thus, we first verify this assumption in realworld datasets: Cora, Citeseer, and Pubmed <ref type="bibr" target="#b21">[22]</ref>. These are citation networks, in which vertices are scient orks, social networks, and biological networks) commonly used for graph neural network benchmarking <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" ta
twork benchmarking <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> and artificially synthesized random graphs from the two cir
"#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Started with the early success of ChebNet <ref type="bibr" target="#b5">[6]</ref> and GCN <ref type="bibr" target="#b15">[16]</ref> at vertex
filter. In contrast to the recent design trend involving GCN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref> , our results imply that simply stacking GCN layers might o cently GCN-based GNNs have been applied in many important applications such as point-cloud analysis <ref type="bibr" target="#b23">[24]</ref> or weakly-supervised learning <ref type="bibr" target="#b8
works) commonly used for graph neural network benchmarking <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> and artificially sy
section, we introduce the basic concepts of graph signal processing. We adopt a recent formulation <ref type="bibr" target="#b10">[11]</ref> of graph Fourier transform on irregular graphs.</p><p>Let
vertex classification, many variants of GNN have been proposed to solve problems in social networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>, biology <ref type ith the feature matrix X . The product (I − L)X is understood as features averaging and propagation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta
iology <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, chemistry <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, natural language pro tribution is the following Theorem:</p><p>Theorem 2 (Informal, see <ref type="bibr">Theorem 7,</ref><ref type="bibr" target="#b7">8)</ref>. Under Assumption 1, the outcomes of SGC, GCN, and gfNN are s
twork benchmarking <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> and artificially synthesized random graphs from the two cir
te to overfitting. Similar observations have been reported in both simple architectures such as SGC <ref type="bibr" target="#b27">[28]</ref> and more complex ones such as DGI <ref type="bibr" target= lex ones such as DGI <ref type="bibr" target="#b26">[27]</ref>. Based on this phenomenon, Wu et al. <ref type="bibr" target="#b27">[28]</ref> proposed to view graph neural networks simply as feature p y used baseline model GCN <ref type="bibr" target="#b15">[16]</ref>, and its simplified variant SGC <ref type="bibr" target="#b27">[28]</ref>.</p><p>Graph signal processing (GSP) regards data on the v In addition, gfNN is also more noise tolerant.</p><p>Finally, we compare our gfNN to the SGC model <ref type="bibr" target="#b27">[28]</ref>. While SGC is also computationally fast and accurate on be dden units for the hidden layer of GCN, MLP, and gfNN. Other hyperparameters are set similar to SGC <ref type="bibr" target="#b27">[28]</ref>.</p><p>gfNN We have three graph filters for our simple mod our simple model: Left Norm ( ), Augumented Normalized Adjacency ( ), and Bilateral ( ).</p><p>SGC <ref type="bibr" target="#b27">[28]</ref> Simple Graph Convolution ( ) simplifies the Graph Convolut ng and propagation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref>. In graph signal processing literature, such operation filt lace="foot" n="3" xml:id="foot_1">The shrinking of the maximum eigenvalue has already been proved in<ref type="bibr" target="#b27">[28,</ref> Theorem 1]. Our theorem is stronger than theirs since we s
te to overfitting. Similar observations have been reported in both simple architectures such as SGC <ref type="bibr" target="#b27">[28]</ref> and more complex ones such as DGI <ref type="bibr" target= lex ones such as DGI <ref type="bibr" target="#b26">[27]</ref>. Based on this phenomenon, Wu et al. <ref type="bibr" target="#b27">[28]</ref> proposed to view graph neural networks simply as feature p y used baseline model GCN <ref type="bibr" target="#b15">[16]</ref>, and its simplified variant SGC <ref type="bibr" target="#b27">[28]</ref>.</p><p>Graph signal processing (GSP) regards data on the v In addition, gfNN is also more noise tolerant.</p><p>Finally, we compare our gfNN to the SGC model <ref type="bibr" target="#b27">[28]</ref>. While SGC is also computationally fast and accurate on be dden units for the hidden layer of GCN, MLP, and gfNN. Other hyperparameters are set similar to SGC <ref type="bibr" target="#b27">[28]</ref>.</p><p>gfNN We have three graph filters for our simple mod our simple model: Left Norm ( ), Augumented Normalized Adjacency ( ), and Bilateral ( ).</p><p>SGC <ref type="bibr" target="#b27">[28]</ref> Simple Graph Convolution ( ) simplifies the Graph Convolut ng and propagation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref>. In graph signal processing literature, such operation filt lace="foot" n="3" xml:id="foot_1">The shrinking of the maximum eigenvalue has already been proved in<ref type="bibr" target="#b27">[28,</ref> Theorem 1]. Our theorem is stronger than theirs since we s
e="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>, computer vision <re
section, we introduce the basic concepts of graph signal processing. We adopt a recent formulation <ref type="bibr" target="#b10">[11]</ref> of graph Fourier transform on irregular graphs.</p><p>Let
twork benchmarking <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> and artificially synthesized random graphs from the two cir
e="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>, computer vision <re
twork benchmarking <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> and artificially synthesized random graphs from the two cir
xcellent results on several benchmark datasets and continuously set new stateof-the-art performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" targ ue signal optimization problem. In contrast to the recent design principle of graph neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" targ ld be thought of simply as a denoising filter. In contrast to the recent design trend involving GCN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref> , our results imply mlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b0">1</ref>. Compute the graph Fourier basis U from L; 2. Add Gaussian noi
2]</ref>, computer vision <ref type="bibr" target="#b20">[21]</ref>, and weakly-supervised learning <ref type="bibr" target="#b8">[9]</ref>.</p><p>In semi-supervised vertex classification, we observe uch as point-cloud analysis <ref type="bibr" target="#b23">[24]</ref> or weakly-supervised learning <ref type="bibr" target="#b8">[9]</ref>. As the input feature space becomes complex, we advocate rev
="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" t static pre-trained models <ref type="bibr" target="#b16">[17]</ref>. In recent work, Brendel et al. <ref type="bibr" target="#b13">[14]</ref> proposed Boundary Attack, which generates adversarial exam ttacks</head><p>Most related to our work is the Boundary Attack method introduced by Brendel et al. <ref type="bibr" target="#b13">[14]</ref>. Boundary Attack is an iterative algorithm based on reject s: We compare HopSkipJumpAttack with three state-of-the-art decision-based attacks: Boundary Attack <ref type="bibr" target="#b13">[14]</ref>, Limited Attack <ref type="bibr" target="#b8">[9]</ref> an ibr" target="#b5">[6]</ref>. A version normalized by image dimension was employed by Brendel et al. <ref type="bibr" target="#b13">[14]</ref> for evaluating Boundary Attack. The As an alternative metr <formula xml:id="formula_38">|E[φ x (x t + δ t u)]| &gt; 0,</formula><p>as we can see from Equation <ref type="bibr" target="#b13">(14)</ref>. To attempt to control the variance, we introduce a baseli
="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. We adopt the implementation in Cao and Gong <ref type="bi
ia training on a data set augmented by adversarial examples from multiple static pre-trained models <ref type="bibr" target="#b16">[17]</ref>. In recent work, Brendel et al. <ref type="bibr" target="# " target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref> is known to be one of the most effective defense mechanisms
data sets: MNIST, CIFAR-10 [27], CIFAR-100 <ref type="bibr" target="#b26">[27]</ref>, and ImageNet <ref type="bibr" target="#b27">[28]</ref> with the standard train/test split <ref type="bibr" target
e, and FGSM <ref type="bibr" target="#b1">[2]</ref>, and BIM <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref> for minimizing ∞ -distance.</p><p>For region-based classifi
ggregating to tree learners. We implement the random forest with default parameters in scikit-learn <ref type="bibr" target="#b41">[42]</ref>, using the Gini impurity as split criterion. For each spli
target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta re-based threat model <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>.</p><p>Subsequent work <ref type="bibr" target="#b21">[22]< al. <ref type="bibr" target="#b7">[8]</ref> and Ilyas et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> introduced score-based methods using zeroth-order gradient e
arget="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t ions alone. A widely studied type of the decision-based attack is transfer-based attack. Liu et al. <ref type="bibr" target="#b10">[11]</ref> showed that adversarial examples generated on an ensemble
="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. We adopt the implementation in Cao and Gong <ref type="bi
target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" targe ation of accessible components of the target model for each of the three threat models. Chen et al. <ref type="bibr" target="#b7">[8]</ref> and Ilyas et al. <ref type="bibr" target="#b8">[9,</ref><ref thms have been applied to the generation of adversarial examples under the score-based threat model <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" targe
s which add test-time randomness to the inputs or the model, causing the gradients to be randomized <ref type="bibr" target="#b33">[34]</ref>. Multiple variants have been proposed to randomize the gra /ref>. For regionbased classification, we include backward pass differentiable approximation (BPDA) <ref type="bibr" target="#b33">[34]</ref>, which calculates the gradient of the model at a randomize </ref> is known to be one of the most effective defense mechanisms against adversarial perturbation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>We evaluate
aswell.</p><p>? We evaluate the performance of a state-of-the-art indirect branch predictor, ITTAGE <ref type="bibr" target="#b30">[31]</ref>, proposed in the literature on the same interpreters, and cades, and presents the state-of-the-art branch predictors TAGE for conditional branches and ITTAGE <ref type="bibr" target="#b30">[31]</ref> for indirect branches. Section 4 presents experimental set er) with a tagged (PC+global branch history) indexed table.</p><p>More recently, Seznec and Michaud <ref type="bibr" target="#b30">[31]</ref> derived IT-TAGE from their TAGE predictor. Instead of simp >We also experimented with a state-of-the-art branch predictor from the literature: TAGE and ITTAGE <ref type="bibr" target="#b30">[31]</ref>. The performance is provided through simulation of traces ="#b14">[15]</ref> and geometric history length predictors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref>. All these propositions have influenced the design of the p loop predictor and maybe a local history predictor), TAGE <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref> is generally considered as the state-of-the-art in global h
racy of conditional branch prediction during the two past decades, e.g. two-level branch prediction <ref type="bibr" target="#b36">[37]</ref>, hybrid predictors <ref type="bibr" target="#b19">[20]</re
te loop. This explains the rather large number of different opcodes. As per Brunthaler's definition <ref type="bibr" target="#b2">[3]</ref>, the CLI interpreter is a low abstraction level interpreter.
t of the evaluation stack and the dispatch loop.</p><p>Very recent work by Savrun-Yenic ?eri et al. <ref type="bibr" target="#b26">[27]</ref> still references the original work of Ertl and Gregg <ref
"bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref>, multiple history length use <ref type="bibr" target="#b29">[30]</ref>, and more recently perceptroninspired predictors <ref type
gshare-like indexed table is used to store the indirect branch targets. However Driesen and Holzle <ref type="bibr" target="#b7">[8]</ref> point out that many indirect branches are correctly predicte
ll Figure <ref type="figure" target="#fig_0">1</ref> for illustration). Ertl proposed stack caching <ref type="bibr" target="#b9">[10]</ref> to force the top of the stack into registers. Together with
te loop. This explains the rather large number of different opcodes. As per Brunthaler's definition <ref type="bibr" target="#b2">[3]</ref>, the CLI interpreter is a low abstraction level interpreter.
t of the evaluation stack and the dispatch loop.</p><p>Very recent work by Savrun-Yenic ?eri et al. <ref type="bibr" target="#b26">[27]</ref> still references the original work of Ertl and Gregg <ref
ef type="bibr" target="#b35">[36]</ref>. Scientists at CERN also developed an interpreter for C/C++ <ref type="bibr" target="#b6">[7]</ref>.</p><p>Although they are designed for portability, interpret
te loop. This explains the rather large number of different opcodes. As per Brunthaler's definition <ref type="bibr" target="#b2">[3]</ref>, the CLI interpreter is a low abstraction level interpreter.
tical performance bottleneck in traditional server workloads <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" targe he observation that instruction cache miss or instruction execution sequences are highly repetitive <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>. These designs log the past sequences on subsequent triggers. The most recent proposal, Proactive Instruction Fetch (PIF) <ref type="bibr" target="#b8">[8]</ref>, can eliminate nearly all L1 instruction misses, but require ler indexing; we find that coverage saturates with only 64kB of storage (relative to over 200kB for <ref type="bibr" target="#b8">[8]</ref>). Using the gem5 <ref type="bibr" target="#b4">[4]</ref> ful control flow speculation to explore ahead of the instruction fetch unit. As shown by Ferdman et al. <ref type="bibr" target="#b8">[8]</ref>, these prefetchers suffer from interference caused by wrong ed leading to more accurate prefetching.</p><p>TIFS <ref type="bibr" target="#b9">[9]</ref> and PIF <ref type="bibr" target="#b8">[8]</ref> address the limitations of branchpredictor-directed prefetch entries, we first compress the sequence of misses using a scheme similar to that pro-posed for PIF <ref type="bibr" target="#b8">[8]</ref>. The key observation is that the misses associated with a pa che blocks in each Miss Table entry remains prohibitive. Fortunately, as observed by Ferdman et al. <ref type="bibr" target="#b8">[8]</ref>, misses tend to be closely clustered with only a few discont m best.</p><p>? PIF is our implementation of Proactive Instruction Fetch proposed by Ferdman et al. <ref type="bibr" target="#b8">[8]</ref>, which is the most effective instruction prefetching design
and the networking stack, resulting in a kernel instruction footprint that overwhelms the L1 cache <ref type="bibr" target="#b18">[18]</ref>. We simulate two different workloads running on top of Mem
ef type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33]</ref>. Run-ahead execution <ref type="bibr" target="#b22">[22]</ref>, wrong path instruction prefetching <ref type="bibr" targe
1 instruction fetch misses remain a critical performance bottleneck in traditional server workloads <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target ver, poor branch predictability and insufficient lookahead limit the effectiveness of these designs <ref type="bibr" target="#b9">[9]</ref>. To our knowledge, such instruction prefetchers have never b cations of a function call are uniquely identified leading to more accurate prefetching.</p><p>TIFS <ref type="bibr" target="#b9">[9]</ref> and PIF <ref type="bibr" target="#b8">[8]</ref> address the trigger address in the history buffer). The estimate for index table size is based on earlier work <ref type="bibr" target="#b9">[9]</ref>, which employs similar structures. RDIP reduces storage by a iss or instruction execution sequences are highly repetitive <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>. These designs log the miss/execution streams in a large circ
4kB of storage (relative to over 200kB for <ref type="bibr" target="#b8">[8]</ref>). Using the gem5 <ref type="bibr" target="#b4">[4]</ref> full-system simulation infrastructure running a suite of clo mlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulation Infrastructure</head><p>We use the gem5 <ref type="bibr" target="#b4">[4]</ref> simulation infrastructure for all of our analysis. All resul
be effective for branch prediction <ref type="bibr" target="#b23">[23]</ref>, dead-block prediction <ref type="bibr" target="#b17">[17]</ref> and last touch prediction <ref type="bibr" target="#b16">[
ef type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33]</ref>. Run-ahead execution <ref type="bibr" target="#b22">[22]</ref>, wrong path instruction prefetching <ref type="bibr" targe

and the networking stack, resulting in a kernel instruction footprint that overwhelms the L1 cache <ref type="bibr" target="#b18">[18]</ref>. We simulate two different workloads running on top of Mem
</ref><ref type="bibr" target="#b32">32]</ref>, cloud computing workloads <ref type="bibr">[7,</ref><ref type="bibr" target="#b19">19]</ref>, and even smartphone applications <ref type="bibr" target="
ruction prefetcher <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" tar volved into next-N-line and instruction stream prefetchers <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" tar
kolov et al., 2013)</ref>, ELMo <ref type="bibr" target="#b21">(Peters et al., 2018)</ref> and BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> are trained and tested mainly on datasets )</ref> uses machine translation to embed context information into word representations.</p><p>BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> is a contextualized word representation mo r" target="#b10">(Krallinger et al., 2017)</ref>. Due to the space limitations, we refer readers to <ref type="bibr" target="#b3">Devlin et al. (2019)</ref> for a more detailed description of BERT.</p pora were used for pre-training, we initialized BioBERT with the pre-trained BERT model provided by <ref type="bibr" target="#b3">Devlin et al. (2019)</ref>. We define BioBERT as a language representa han an hour as the size of the training data is much smaller than that of the training data used by <ref type="bibr" target="#b3">Devlin et al. (2019)</ref>. On the other hand, it takes more than 20 e

been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes <ref type="bibr" target="#b0">(Alsentzer et al., 2019)</ref>, human phenotype-gene RE <ref type="bib

target="#b6">(Giorgi and Bader, 2018;</ref><ref type="bibr" target="#b7">Habibi et al., 2017;</ref><ref type="bibr" target="#b32">Wang et al., 2018;</ref><ref type="bibr" target="#b36">Yoon et al., 2 target="#b6">(Giorgi and Bader, 2018;</ref><ref type="bibr" target="#b7">Habibi et al., 2017;</ref><ref type="bibr" target="#b32">Wang et al., 2018)</ref>, BERT has a simple architecture based on bid rget="#b12">(Lim and Kang, 2018;</ref><ref type="bibr" target="#b28">Tsatsaronis et al., 2015;</ref><ref type="bibr" target="#b32">Wang et al., 2018)</ref> for a fair evaluation; however, the splits o le" target="#tab_1">3</ref>. We used the pre-processed versions of all the NER datasets provided by <ref type="bibr" target="#b32">Wang et al. (2018)</ref> except the 2010 i2b2/VA, JNLPBA and Species-
bibr" target="#b7">Habibi et al., 2017;</ref><ref type="bibr" target="#b32">Wang et al., 2018;</ref><ref type="bibr" target="#b36">Yoon et al., 2019)</ref>. Other deep learning based models have made mbeddings trained on PubMed or PMC corpora <ref type="bibr" target="#b7">(Habibi et al., 2017;</ref><ref type="bibr" target="#b36">Yoon et al., 2019)</ref>, BioBERT directly learns WordPiece embedding h have a different architecture and training procedure. For instance, the state-of-the-art model by <ref type="bibr" target="#b36">Yoon et al. (2019)</ref> trained on the JNLPBA dataset is based on mu
n adapted versions of word representations <ref type="bibr" target="#b7">(Habibi et al., 2017;</ref><ref type="bibr" target="#b22">Pyysalo et al., 2013)</ref>.</p><p>In this study, we hypothesize that orpora which contain terms and expressions that are usually not included in a general domain corpus <ref type="bibr" target="#b22">(Pyysalo et al., 2013)</ref>. While ELMo and BERT have proven the eff
been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes <ref type="bibr" target="#b0">(Alsentzer et al., 2019)</ref>, human phenotype-gene RE <ref type="bib
35 460 LINNAEUS <ref type="bibr" target="#b5">(Gerner et al., 2010)</ref> Species 4077 Species-800 <ref type="bibr" target="#b19">(Pafilis et al., 2013)</ref> Species 3708</p><p>Note: The number of a

ase 12 694 BC5CDR <ref type="bibr" target="#b11">(Li et al., 2016)</ref> Drug/Chem. 15 411 BC4CHEMD <ref type="bibr" target="#b9">(Krallinger et al., 2015)</ref> Drug/Chem. 79 842 BC2GM <ref type="bib
e x i t s h a 1 _ b a s e 6 4 = " t A  We use primitives from an existing code generation framework <ref type="bibr" target="#b8">[9]</ref> to form S e . Our search space includes multi-level tiling o
>Black box optimization (auto-tuning) is used in high-performance computing libraries such as ATLAS <ref type="bibr" target="#b42">[43]</ref> and FFTW <ref type="bibr" target="#b11">[12]</ref>. Altern
gh-performance computing libraries such as ATLAS <ref type="bibr" target="#b42">[43]</ref> and FFTW <ref type="bibr" target="#b11">[12]</ref>. Alternatively, a hardware-dependent cost model can be bui
ref type="bibr" target="#b29">30]</ref>, each with with a different E, S e and g. Polyhedral models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" targ <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5]</ref>. Polyhedral methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref> use integer linear p ware-dependent cost model can be built to guide the search <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5]</ref>. Polyhedral methods <ref type="bibr" target="#b4">[5,</ref><r
submodular function, and we can apply the greedy algorithm <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22]</ref> to get an approximate solution.</p><p>Uncertainty Estimator
optimal schedule in S e .</p><p>There are many domain-specific languages (DSLs) for code generation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" ta del the loop domains as integer linear constraints. An alternative approach originating from Halide <ref type="bibr" target="#b31">[32]</ref> defines a schedule space using a set of transformation pri
et="#b33">[34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
many domain-specific languages (DSLs) for code generation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar
, each with with a different E, S e and g. Polyhedral models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41]</ref> are a popular choic f><ref type="bibr" target="#b4">5]</ref>. Polyhedral methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref> use integer linear programming to optimize cost.</p><p>Tens
optimal schedule in S e .</p><p>There are many domain-specific languages (DSLs) for code generation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" ta del the loop domains as integer linear constraints. An alternative approach originating from Halide <ref type="bibr" target="#b31">[32]</ref> defines a schedule space using a set of transformation pri
provided in the supplementary material.) We also included a weight pre-transformed Winograd kernel <ref type="bibr" target="#b23">[24]</ref> for 3 × 3 conv2d (AutoTVM PT). AutoTVM generated programs
f its neighbor pairs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. For example, SiGMa <ref type="bibr" target="#b8">[9]</ref> enerate candidate user pairs from all the pairs. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>, we only keep the user pairs if their names are similar to gate the matching scores from a confidential seed set of user pairs to their neighbor pairs. COSNET <ref type="bibr" target="#b27">[28]</ref> proposed a supervised method to infer the marginal probabi ls of the unlabeled pairs and update the model based on the inferred labels and the user attributes <ref type="bibr" target="#b27">[28]</ref>. However, error propagations may be introduced in above me fore resorting to the model, we can easily select the most useful neighbor pairs by heuristic rules <ref type="bibr" target="#b27">[28]</ref>. This paper simply selects the neighbor pairs if their nam by propagating the matching scores (predicted by SVM) through the two input networks.</p><p>COSNET <ref type="bibr" target="#b27">[28]</ref>: is a factor graph model that incorporates the attributes
ach node in a graph based on the states of neighboring nodes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>. Recently, Li et al. improved GNNs using gated recurrent un
by neural networks. Shallow models such as DeepWalk <ref type="bibr" target="#b17">[18]</ref>, LINE <ref type="bibr" target="#b20">[21]</ref> and node2vec <ref type="bibr" target="#b4">[5]</ref> learn
ch name is the most important one. Many attempts have been made to study how to leverage user names <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" ta
by neural networks. Shallow models such as DeepWalk <ref type="bibr" target="#b17">[18]</ref>, LINE <ref type="bibr" target="#b20">[21]</ref> and node2vec <ref type="bibr" target="#b4">[5]</ref> learn
l works studied how to generalize convolutions to graph data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target= naud et al. defined convolutions directly on graphs and learned node degree-specific weight matrix. <ref type="bibr" target="#b2">[3]</ref>. Thomas et al. considered a single weight matrix per layer a
rks <ref type="foot" target="#foot_0">4</ref> . Academia consists of networks collected from Aminer <ref type="bibr" target="#b21">[22]</ref>, an academic searching and mining service, LinkedIn, a bus
ttempts have been made to study how to leverage user names <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
ttempts have been made to study how to leverage user names <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
l works studied how to generalize convolutions to graph data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target= naud et al. defined convolutions directly on graphs and learned node degree-specific weight matrix. <ref type="bibr" target="#b2">[3]</ref>. Thomas et al. considered a single weight matrix per layer a
damic/Adar based on the shared neighbors as that proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" targe
cache set or in the SRAM in order to reduce the average write frequency of the STT-RAM cache lines <ref type="bibr" target="#b4">[4]</ref>.</p><p>However, there exist intrinsic limitations in both ap cache architecture with 4-way SRAM and 12-way STT-RAM, which is similar to the setting described in <ref type="bibr" target="#b4">[4]</ref> <ref type="bibr" target="#b7">[7]</ref>. The block-level ini quently. Note that life time of the STT-RAM mainly depends on the peak write count of all the cells <ref type="bibr" target="#b4">[4]</ref> <ref type="bibr" target="#b5">[5]</ref>. Even static optimiz 8]</ref> for both of the pure static <ref type="bibr" target="#b10">[10]</ref> and the pure dynamic <ref type="bibr" target="#b4">[4]</ref> scheme. The peak write count of static optimization is signi ctually write-intensive STT-RAM data blocks to SRAM. We use the dynamic migration scheme similar to <ref type="bibr" target="#b4">[4]</ref>, which is briefly described as follows. Each L2 cache block <head>Pure dynamic optimization (dynamic):</head><p>We use the dynamic migration scheme proposed in <ref type="bibr" target="#b4">[4]</ref>. Our dynamic migration scheme in Section 4.3 uses this schem he first STT-RAM line becomes defective, which is similar to the estimation methodology proposed in <ref type="bibr" target="#b4">[4]</ref> and <ref type="bibr" target="#b5">[5]</ref>.</p><p>Figure <r
tructures are similar to the missing tags <ref type="bibr" target="#b12">[12]</ref> and victim tags <ref type="bibr" target="#b14">[14]</ref>.</p><p>MTs and MT counters are integrated with the tag arr
access behavior</head><p>To capture this effect, we use the concept of memory reuse distance (MRD) <ref type="bibr" target="#b11">[11]</ref>, which equals the total size of unique data elements acces


s introduce data migration and re-computation to reduce the write frequency on PRAM main memory. In <ref type="bibr" target="#b10">[10]</ref>, the partitioning of the application working set into SRAM of the segmentation application <ref type="bibr" target="#b8">[8]</ref> for both of the pure static <ref type="bibr" target="#b10">[10]</ref> and the pure dynamic <ref type="bibr" target="#b4">[4]</re hich is widely used in modern processors because of easy coherence implementation.</p><p>Similar to <ref type="bibr" target="#b10">[10]</ref>, our compiler tends to place write-intensive references in ) L1 dirty evictions due to L1 cache replacement and (2) L2 cache replacement. However, the work in <ref type="bibr" target="#b10">[10]</ref> assumes there is no cache in the memory system, thus does -generated block placement hint. The compiler hints are generated based on the approach proposed in <ref type="bibr" target="#b10">[10]</ref>, and we further take the effect of L1 cache into considera
s introduce data migration and re-computation to reduce the write frequency on PRAM main memory. In <ref type="bibr" target="#b10">[10]</ref>, the partitioning of the application working set into SRAM of the segmentation application <ref type="bibr" target="#b8">[8]</ref> for both of the pure static <ref type="bibr" target="#b10">[10]</ref> and the pure dynamic <ref type="bibr" target="#b4">[4]</re hich is widely used in modern processors because of easy coherence implementation.</p><p>Similar to <ref type="bibr" target="#b10">[10]</ref>, our compiler tends to place write-intensive references in ) L1 dirty evictions due to L1 cache replacement and (2) L2 cache replacement. However, the work in <ref type="bibr" target="#b10">[10]</ref> assumes there is no cache in the memory system, thus does -generated block placement hint. The compiler hints are generated based on the approach proposed in <ref type="bibr" target="#b10">[10]</ref>, and we further take the effect of L1 cache into considera

s introduce data migration and re-computation to reduce the write frequency on PRAM main memory. In <ref type="bibr" target="#b10">[10]</ref>, the partitioning of the application working set into SRAM of the segmentation application <ref type="bibr" target="#b8">[8]</ref> for both of the pure static <ref type="bibr" target="#b10">[10]</ref> and the pure dynamic <ref type="bibr" target="#b4">[4]</re hich is widely used in modern processors because of easy coherence implementation.</p><p>Similar to <ref type="bibr" target="#b10">[10]</ref>, our compiler tends to place write-intensive references in ) L1 dirty evictions due to L1 cache replacement and (2) L2 cache replacement. However, the work in <ref type="bibr" target="#b10">[10]</ref> assumes there is no cache in the memory system, thus does -generated block placement hint. The compiler hints are generated based on the approach proposed in <ref type="bibr" target="#b10">[10]</ref>, and we further take the effect of L1 cache into considera

nted based on LLVM compiler infrastructure <ref type="bibr" target="#b15">[15]</ref>. Omega library <ref type="bibr" target="#b16">[16]</ref> is used in this flow to perform memory dependency analysis
eep learning entirely infeasible.</p><p>Applying RMD to hyperparameter optimization was proposed by <ref type="bibr" target="#b3">Bengio (2000)</ref> and <ref type="bibr" target="#b2">Baydin &amp; Pea , Eigenmann &amp; Nossek (1999)</ref>, <ref type="bibr" target="#b9">Chen &amp; Hagan (1999)</ref>, <ref type="bibr" target="#b3">Bengio (2000)</ref>, <ref type="bibr" target="#b0">Abdel-Gawad &amp; R
ef type="bibr" target="#b2">Baydin &amp; Pearlmutter (2014)</ref>, and applied to small problems by <ref type="bibr" target="#b12">Domke (2012)</ref>. However, the naïve approach fails for real-sized s="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>The most closely-related work is <ref type="bibr" target="#b12">Domke (2012)</ref>, who derived algorithms to compute reverse-mode de
., 2012;</ref><ref type="bibr" target="#b6">Bergstra et al., 2011;</ref><ref type="bibr">2013;</ref><ref type="bibr" target="#b20">Hutter et al., 2011)</ref>. Hyperparameters are chosen to optimize th
but these can be computed exactly by applying RMD to the dot product of the gradient with a vector <ref type="bibr" target="#b31">(Pearlmutter, 1994)</ref>. Thus the time complexity of reverse SGD is
<p>The current gold standard for hyperparameter selection is gradient-free model-based optimization <ref type="bibr" target="#b34">(Snoek et al., 2012;</ref><ref type="bibr" target="#b6">Bergstra et a t separate regularization hyperparameter for each parameter type would improve performance. Indeed, <ref type="bibr" target="#b34">Snoek et al. (2012)</ref> optimized separate regularization parameter
ors w 1 , w 2 , . . . , w T , making it impractical for large models with many training iterations. <ref type="bibr" target="#b23">Larsen et al. (1998</ref><ref type="bibr" target="#b13">), Eigenmann

two hyperparameters set by crossvalidation <ref type="bibr" target="#b11">(Dahl et al., 2014;</ref><ref type="bibr" target="#b36">Sutskever et al., 2013)</ref>. These schedule choices are supported b
ents</head><p>Reverse-mode differentiation (RMD) has been an asset to the field of machine learning <ref type="bibr" target="#b24">(LeCun et al., 1989</ref>) (see the appendix for a refresher). The RM


, of sizes 784, 50, 50, and 50.</p><p>Because learning schedules can implicitly regularize networks <ref type="bibr" target="#b14">(Erhan et al., 2010)</ref>, for example by enforcing early stopping,
ckage for Python, available at github.com/HIPS/autograd. This package differentiates standard Numpy <ref type="bibr" target="#b28">(Oliphant, 2007)</ref> code, and can differentiate code containing wh

ckage for Python, available at github.com/HIPS/autograd. This package differentiates standard Numpy <ref type="bibr" target="#b28">(Oliphant, 2007)</ref> code, and can differentiate code containing wh

ors w 1 , w 2 , . . . , w T , making it impractical for large models with many training iterations. <ref type="bibr" target="#b23">Larsen et al. (1998</ref><ref type="bibr" target="#b13">), Eigenmann

rs. For example, Gaussian-process-based optimization methods could incorporate gradient information <ref type="bibr" target="#b35">(Solak et al., 2003)</ref>. Such methods could make use of parallel e
, of sizes 784, 50, 50, and 50.</p><p>Because learning schedules can implicitly regularize networks <ref type="bibr" target="#b14">(Erhan et al., 2010)</ref>, for example by enforcing early stopping,

s with this technique that became apparent in our experiments.</p><p>When are gradients meaningful? <ref type="bibr" target="#b4">Bengio et al. (1994)</ref> noted that "learning long-term dependencies
g of cellular function under normal and disease conditions <ref type="bibr" target="#b24">[27]</ref><ref type="bibr" target="#b25">[28]</ref><ref type="bibr" target="#b26">[29]</ref><ref type="bibr" t
e-related, highly studied hubs, like P53. It also applies to networks obtained by mass-spectrometry <ref type="bibr" target="#b32">[35,</ref><ref type="bibr" target="#b33">36]</ref>, where the observe
tral role in the mechanistic understanding of cellular function under normal and disease conditions <ref type="bibr" target="#b24">[27]</ref><ref type="bibr" target="#b25">[28]</ref><ref type="bibr" t
rediction tools rely on the Triadic Closure Principle (TCP) <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, stating that two
erns of the already known interactome to predict undetected, yet biologically relevant interactions <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" targe he heart of current network-based link prediction algorithms is the triadic closure principle (TCP) <ref type="bibr" target="#b6">[7]</ref> (SI Table <ref type="table">I</ref>.), connecting two nodes hey must participate in the same cellular function, hence they likely also interact with each other <ref type="bibr" target="#b6">[7]</ref>. Despite its central role in network-based link prediction a formation on sequence, evolutionary history or 3D structure, used by some PPI prediction algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target arithmically based on the Jaccard similarity values. (d) PPIs often require complementary interfaces<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, hence two proteins, X
derstanding cellular mechanisms requires us to catalogue all physical interactions between proteins <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" targe
protein-protein interactions (PPIs) continues to exceed the experimentally documented interactions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Computational tools t imates the performance of L3 as only a fraction of the existing PPIs are detectable in a HT setting <ref type="bibr" target="#b4">[5]</ref>. We therefore also assessed the performance of L3 in yeast-t
hare even more partners (SI. C) <ref type="bibr" target="#b14">[15]</ref><ref type="bibr">[16]</ref><ref type="bibr" target="#b15">[17]</ref>. To test the L3 principle, we measured the number of = 3 p
rediction tools rely on the Triadic Closure Principle (TCP) <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, stating that two
relevant interactions <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. Such network-based link prediction tools rely on the Triadi
target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Despite spectacular advances in high-throughput mapping, th
to a nurse. The primary embedding studied in this paper is the popular publicly-available word2vec <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> embedding trained g we refer to in this paper is the aforementioned w2vNEWS embedding, a d = 300-dimensional word2vec <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> embedding, which h
within online contexts and specifically related to the contexts we study such as online news (e.g., <ref type="bibr" target="#b30">[31]</ref>), Web search (e.g., <ref type="bibr" target="#b20">[21]</r
re. Stereotypes have commonalities across cultures, though there is some variation between cultures <ref type="bibr" target="#b4">[5]</ref>. Complimentary stereotypes are common between females and ma
re. Stereotypes have commonalities across cultures, though there is some variation between cultures <ref type="bibr" target="#b4">[5]</ref>. Complimentary stereotypes are common between females and ma
he curation of machine learning data-sets have explored in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Independent from our work, Schmidt <ref type="bibr" ta
exhibit various biases, such as racial discrimination and gender bias in the ads presented to users <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b5">6]</ref>. A recent study foun
rent angles (e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b8">9]</ref>), and the difficulty
ined on "standard" data-sets. Biases in the curation of machine learning data-sets have explored in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Independent f
se research agenda focused on improving the quality of word embeddings from different angles (e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" ta /ref>, section 2.2).</p><p>Related issues are discussed in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23]</ref>, the latter of which proposes the 3CosMul objective to find
pposing analogies: a:x :: b:y as well as a:y :: b:x, which violates a desideratum of analogies (see <ref type="bibr" target="#b37">[38]</ref>, section 2.2).</p><p>Related issues are discussed in <ref (see <ref type="bibr" target="#b37">[38]</ref>, section 2.2).</p><p>Related issues are discussed in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23]</ref>, the latter of whi
30">[31]</ref>), Web search (e.g., <ref type="bibr" target="#b20">[21]</ref>), and Wikipedia (e.g., <ref type="bibr" target="#b38">[39]</ref>). In Wikipedia, Wager et al. <ref type="bibr" target="#b38 /ref>), and Wikipedia (e.g., <ref type="bibr" target="#b38">[39]</ref>). In Wikipedia, Wager et al. <ref type="bibr" target="#b38">[39]</ref> found that, as suggested by prior work on gender bias in l
n years later, Zheng et al. evaluated the performance of exclusive caches with respect to inclusive <ref type="bibr" target="#b39">[40]</ref>. They found that exclusive caching is beneficial for most
pace with the rapid improvement of CMOS-based processors, giving rise to the so-called Memory Wall <ref type="bibr" target="#b37">[38]</ref>. Modern multi-core processors rely on large on-chip caches
h an inclusive directory to keep the positive features of both inclusive and non-inclusive policies <ref type="bibr" target="#b38">[39]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>
ing the older ones <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar
target="#b25">[26]</ref> X Hawkeye <ref type="bibr" target="#b17">[18]</ref> X Bypass and Insertion <ref type="bibr" target="#b9">[10]</ref> X CHAR <ref type="bibr" target="#b4">[5]</ref> X X ExDRRIP proposed to use the number of trips to LLC (LLC hits) and their use count in the L2 (L2 hits) cache <ref type="bibr" target="#b9">[10]</ref>. Blocks with either a high use or trip count will usually b
dict.</p><p>B?l?dy proposed an optimal cache replacement algorithm assuming knowledge on the future <ref type="bibr" target="#b3">[4]</ref>. As a processor does not have such knowledge, there has been
RPs and PFs, and their effect in each of the cache levels <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>. Those studies show
pace with the rapid improvement of CMOS-based processors, giving rise to the so-called Memory Wall <ref type="bibr" target="#b37">[38]</ref>. Modern multi-core processors rely on large on-chip caches
policy. There has been a lower effort in exclusive caches.</p><p>Inclusive Non-Incl. Exclusive RRIP <ref type="bibr" target="#b20">[21]</ref> X SDBP <ref type="bibr" target="#b24">[25]</ref> X SHiP <r Many RPs are based on the block's reuse, by aging the blocks over time and evicting the older ones <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta
ing the older ones <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar
<ref type="bibr" target="#b24">[25]</ref> X SHiP <ref type="bibr" target="#b35">[36]</ref> X GIPPR <ref type="bibr" target="#b21">[22]</ref> X MDPP <ref type="bibr" target="#b32">[33]</ref> X EAF <re by aging the blocks over time and evicting the older ones <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar
dhuber, 1997)</ref> system can readily outperform the previous state-of-the-art system, CogCompTime <ref type="bibr" target="#b32">(Ning et al., 2018d)</ref>, by a large margin. The fact that a standa ead><label>2</label><figDesc>Performances on the MATRES test set (i.e., the PT section). CogCompTime<ref type="bibr" target="#b32">(Ning et al., 2018d)</ref> is the previous state-of-the-art feature-b hat in Table <ref type="table" target="#tab_2">2</ref>, CogCompTime performed slightly different to <ref type="bibr" target="#b32">Ning et al. (2018d)</ref>: Cog-CompTime reportedly had F 1 =65.9 (Tab f type="table" target="#tab_2">2</ref> Line 3 therein) and here we obtained F 1 =66.6. In addition, <ref type="bibr" target="#b32">Ning et al. (2018d)</ref> only reported F 1 scores, while we also use
>Lin et al., 2017;</ref><ref type="bibr" target="#b38">Tourille et al., 2017)</ref> and in newswire <ref type="bibr" target="#b11">(Cheng and Miyao, 2017;</ref><ref type="bibr" target="#b24">Meng and
wledge base that provides typical TempRels between events<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b30">(Ning et al., 2018b)</ref>. Altogether, these components improve over without context, we know that die is typically after explode and schedule typically before attend. <ref type="bibr" target="#b30">Ning et al. (2018b)</ref> was an initial attempt to acquire such know figure">1c</ref>). Note that the TEMPROB we use is reconstructed using the same method described in <ref type="bibr" target="#b30">Ning et al. (2018b)</ref> with the base method changed to CogCompTime
ef type="bibr">Uz-Zaman et al., 2013;</ref><ref type="bibr" target="#b26">Minard et al., 2015;</ref><ref type="bibr" target="#b22">Llorens et al., 2015;</ref><ref type="bibr" target="#b29">Ning et al.
s/1.0"><head n="2">Related Work</head><p>Early computational attempts to TempRel extraction include <ref type="bibr" target="#b23">Mani et al. (2006)</ref>; <ref type="bibr" target="#b10">Chambers et
t="#b11">(Cheng and Miyao, 2017;</ref><ref type="bibr" target="#b24">Meng and Rumshisky, 2018;</ref><ref type="bibr" target="#b20">Leeuwenberg and Moens, 2018)</ref>. However, their improvements over
<ref type="bibr" target="#b15">(Do et al., 2012;</ref><ref type="bibr">Uz-Zaman et al., 2013;</ref><ref type="bibr" target="#b26">Minard et al., 2015;</ref><ref type="bibr" target="#b22">Llorens et a
<ref type="bibr" target="#b15">(Do et al., 2012;</ref><ref type="bibr">Uz-Zaman et al., 2013;</ref><ref type="bibr" target="#b26">Minard et al., 2015;</ref><ref type="bibr" target="#b22">Llorens et a
. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops <ref type="bibr" target="#b40">(Verhagen et al., 2007</ref><ref type="bibr" target="#b42">(Verhagen
n et al., 2014)</ref>, FastText <ref type="bibr" target="#b4">(Bojanowski et al., 2016)</ref>, ELMo <ref type="bibr" target="#b36">(Peters et al., 2018)</ref>, and BERT <ref type="bibr" target="#b12">
we introduce common sense encoder (CSE): We fit an updated version of TEMPROB via a Siamese network <ref type="bibr" target="#b5">(Bromley et al., 1994)</ref> that generalizes to unseen tuples through
operties that could be used to craft adversarial samples <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b34">[36]</ref>. Simply put, th ages that are unrecognizable to humans, but are nonetheless labeled as recognizable objects by DNNs <ref type="bibr" target="#b28">[30]</ref>. For instance, they demonstrated how a DNN will classify a e backpropagation procedure used during network training <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b34">[36]</ref>. This approach
type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b35">[37]</ref>, speech recognition <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b30">[32]</ref>, <ref type="bib
lns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>The security of machine learning <ref type="bibr" target="#b1">[2]</ref> is an active research topic within the security and machine
ification systems <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In the physical domain, consider a driverless car system ning communities. A broad taxonomy of attacks and required adversarial capabilties are discussed in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b2">[3]</ref> along with con
ned in two categories, depending on whether DNNs are trained in a supervised or unsupervised manner <ref type="bibr" target="#b27">[29]</ref>. Supervised training leads to models that map unseen sampl <div xmlns="http://www.tei-c.org/ns/1.0"><head>Increasing complexity</head><p>Decreasing knowledge <ref type="bibr" target="#b27">[29]</ref> ADVERSARIAL GOALS ADVERSARIAL CAPABILITIES Fig. <ref type=
arial samples <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b34">[36]</ref>. Simply put, these techniques exploit gradients computed b tes adversarial samples by perturbing inputs in a way that creates source/target misclassifications <ref type="bibr" target="#b34">[36]</ref>. The perturbations made by their work, which focused on a work training <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b34">[36]</ref>. This approach creates adversarial samples by defining an ef>, while Szegedy et al. showed that adversarial samples can indeed be misclassified across models <ref type="bibr" target="#b34">[36]</ref>. They report that once an adversarial sample is generated
f>. In the physical domain, consider a driverless car system that uses DL to identify traffic signs <ref type="bibr" target="#b11">[12]</ref>. If slightly altering "STOP" signs causes DNNs to misclass
at a two player game between two DNNs can lead to the generation of new samples from a training set <ref type="bibr" target="#b16">[17]</ref>. This can help augment training datasets. Furthermore, add
f>. In the physical domain, consider a driverless car system that uses DL to identify traffic signs <ref type="bibr" target="#b11">[12]</ref>. If slightly altering "STOP" signs causes DNNs to misclass
executables, which are not as easy to perturb as images <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>. This distortion reduction comes with a performance cost.
Hornik et al. states one hidden layer is sufficient to represent arbitrarily accurately a function <ref type="bibr" target="#b20">[21]</ref>. Thus, one can intuitively conceive that improving the tra
manifold learning algorithms <ref type="bibr" target="#b37">[38]</ref>, and geometric deep learning <ref type="bibr" target="#b6">[7]</ref>-all of which involve representation learning with graph-stru 3]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and <ref type="bibr" target="#b6">[7]</ref> for comprehensive overviews of these areas.</p></div> <div x lly similar to Algorithm 1, with some minor variations, and we refer the reader to Bronstein et al. <ref type="bibr" target="#b6">[7]</ref> for a thorough discussion of these techniques.</p></div> <di
levant work, which we do not review in detail here-including latent space models of social networks <ref type="bibr" target="#b32">[33]</ref>, embedding methods for statistical relational learning <re ef>-all of which involve representation learning with graph-structured data. We refer the reader to <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bib etric relations in this latent space correspond to interactions (e.g., edges) in the original graph <ref type="bibr" target="#b32">[33]</ref>. Figure <ref type="figure">2</ref> visualizes an example e
apture structural roles (Figure <ref type="figure">5</ref>). However, more recently, Ribeiro et al. <ref type="bibr" target="#b50">[51]</ref> and Donnat et al. <ref type="bibr" target="#b19">[20]</ref een the ordered degree sequences R k (v i ) and R k (v j ) (e.g., computed via dynamic time warping <ref type="bibr" target="#b50">[51]</ref>). After computing these weighted auxillary graphs, struc2v
for dimensionality reduction <ref type="bibr" target="#b3">[4]</ref> and multi-dimensional scaling <ref type="bibr" target="#b36">[37]</ref>. Indeed, many of these approaches were originally motivate inspired by classic techniques for dimensionality reduction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>. Laplacian eigenmaps. One of the earliest, and most well-kn
http://www.tei-c.org/ns/1.0"><head n="3.1.2">Graph-coarsening approaches</head><p>Defferrard et al. <ref type="bibr" target="#b17">[18]</ref> and Bruna et al. <ref type="bibr" target="#b7">[8]</ref> a and the process repeats. Unlike the convolutional approaches discussed in 2.3.2, Defferrard et al. <ref type="bibr" target="#b17">[18]</ref> and Bruna et al. <ref type="bibr" target="#b7">[8]</ref> a foot_5"><p>These methods also have theoretical connections to approximate spectral kernels on graphs<ref type="bibr" target="#b17">[18]</ref>; see<ref type="bibr" target="#b34">[35]</ref> for a furthe
target="#b57">[58]</ref>, or carefully engineered features to measure local neighborhood structures <ref type="bibr" target="#b39">[40]</ref>. However, these approaches are limited because these hande
rchers can easily leverage existing, generic techniques for visualization high-dimensional datasets <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b54">55]</ref>. For example, node 4">55]</ref>. For example, node embeddings can be combined with well-known techniques such as t-SNE <ref type="bibr" target="#b56">[57]</ref> or principal components analysis (PCA) in order to generat
achine approaches often rely on summary graph statistics (e.g., degrees or clustering coefficients) <ref type="bibr" target="#b5">[6]</ref>, kernel functions <ref type="bibr" target="#b57">[58]</ref>,
for dimensionality reduction <ref type="bibr" target="#b3">[4]</ref> and multi-dimensional scaling <ref type="bibr" target="#b36">[37]</ref>. Indeed, many of these approaches were originally motivate inspired by classic techniques for dimensionality reduction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>. Laplacian eigenmaps. One of the earliest, and most well-kn
ptimization, though some algorithms do permit closed-form solutions via matrix decomposition (e.g., <ref type="bibr" target="#b8">[9]</ref>). However, note that we will not focus on optimization algor algorithm<ref type="foot" target="#foot_4">4</ref>  <ref type="bibr" target="#b0">[1]</ref>, GraRep <ref type="bibr" target="#b8">[9]</ref>, and HOPE <ref type="bibr" target="#b44">[45]</ref> all fall adjacency matrix (e.g., s G (v i , v j ) A 2 i,j ) in order to capture higher-order node similarity <ref type="bibr" target="#b8">[9]</ref>; and the HOPE algorithm supports general similarity measures e s G corresponds to more general notions of neighborhood overlap (e.g., s G (v i , v j ) = A 2 i,j <ref type="bibr" target="#b8">[9]</ref>). We refer to these methods in this section as matrix-factor kip" or "hop" over multiple nodes at each step, resulting in a similarity measure similar to GraRep <ref type="bibr" target="#b8">[9]</ref>, while Chamberlan et al. <ref type="bibr" target="#b10">[11]
above, there is a related-and chronologically prior-line of work on "graph neural networks" (GNNs) <ref type="bibr" target="#b51">[52]</ref>. Conceptually, the GNN idea is closely related to Algorith ng scaffolding for a "message passing" algorithm between nodes.</p><p>In the original GNN framework <ref type="bibr" target="#b51">[52]</ref> every node v i is initialized with a random embedding h 0 K i ), where g is an arbitrary differentiable function of the form g : R d ? R d . Scarselli et al. <ref type="bibr" target="#b51">[52]</ref> discuss various parameterizations of h and g based on mult any of the aggregation procedures described in Section 3.1 could be employed, but Scarselli et al. <ref type="bibr" target="#b51">[52]</ref> also suggest that the aggregation can be done by introduci
ems, allowing relational knowledge about interacting entities to be efficiently stored and accessed <ref type="bibr" target="#b1">[2]</ref>.</p><p>However, graphs are not only useful as structured kno
ational biology (e.g., discovering related drugs) to marketing (e.g., discovering related products) <ref type="bibr" target="#b22">[23]</ref>. Again, because each node is associated with real-valued v
for dimensionality reduction <ref type="bibr" target="#b3">[4]</ref> and multi-dimensional scaling <ref type="bibr" target="#b36">[37]</ref>. Indeed, many of these approaches were originally motivate inspired by classic techniques for dimensionality reduction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>. Laplacian eigenmaps. One of the earliest, and most well-kn
oaches are largely inspired by classic matrix factorization techniques for dimensionality reduction <ref type="bibr" target="#b3">[4]</ref> and multi-dimensional scaling <ref type="bibr" target="#b36" maps. One of the earliest, and most well-known instances, is the Laplacian eigenmaps (LE) technique <ref type="bibr" target="#b3">[4]</ref>, which we can view within the encoder-decoder framework as a rization approaches, which are directly inspired by classic techniques for dimensionality reduction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>. Laplacian eigenmaps
ks <ref type="bibr" target="#b32">[33]</ref>, embedding methods for statistical relational learning <ref type="bibr" target="#b42">[43]</ref>, manifold learning algorithms <ref type="bibr" target="#b3 rning with graph-structured data. We refer the reader to <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and <ref type= graphs-contains many related techniques for decoding a large number of edge types (i.e., relations) <ref type="bibr" target="#b42">[43]</ref>. 7  Recently, Dong et al. <ref type="bibr" target="#b18">[ 4]</ref>, where a common task is to predict missing relations between entities in a knowledge graph <ref type="bibr" target="#b42">[43]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head d="foot_7"><p>We do not review this literature in detail here, and refer the reader to Nickel et al.<ref type="bibr" target="#b42">[43]</ref> for a recent review.</p></note> 			<note xmlns="http://www ype="bibr" target="#b11">[12]</ref> and (ii) extend pairwise decoders with type-specific parameters <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b52">53]</ref>. For example, in g e., z i z j ? A i,j ) can be replaced with a bilinear form <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b52">53]</ref>:</p><formula xml:id
strategy for dealing with this issue is to (i) use different encoders for nodes of different types <ref type="bibr" target="#b11">[12]</ref> and (ii) extend pairwise decoders with type-specific param e standard inner-product edge decoder (i.e., z i z j ? A i,j ) can be replaced with a bilinear form <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" ta
ly any generic clustering algorithm to the set of learned node embeddings (e.g., k-means or DB-scan <ref type="bibr" target="#b21">[22]</ref>).</p><p>This offers an open-ended and powerful alternative
tions (DNGR) <ref type="bibr" target="#b9">[10]</ref> and Structural Deep Network Embeddings (SDNE) <ref type="bibr" target="#b58">[59]</ref> address the first problem outlined above: unlike the shall ) with the Laplacian eigenmaps objective (Equation <ref type="formula" target="#formula_9">6</ref>) <ref type="bibr" target="#b58">[59]</ref>.</p><p>Note that the encoder in Equation ( <ref type="form
e.g., degrees or clustering coefficients) <ref type="bibr" target="#b5">[6]</ref>, kernel functions <ref type="bibr" target="#b57">[58]</ref>, or carefully engineered features to measure local neighbo s closely related to the design of graph kernels, which define a distance measure between subgraphs <ref type="bibr" target="#b57">[58]</ref>. That said, we omit a detailed discussion of graph kernels ssion of graph kernels, which is a large and rich research area of its own, and refer the reader to <ref type="bibr" target="#b57">[58]</ref> for a detailed discussion. The methods we review differ fr
or one could use distances between the embeddings to recommend friendship links in a social network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref> (Section 2.7 discuss diction, where the goal is to predict missing edges, or edges that are likely to form in the future <ref type="bibr" target="#b2">[3]</ref>. Link prediction is at the core of recommender systems and c
al idea are proposed by Neipert et al. <ref type="bibr" target="#b43">[44]</ref> and Kearnes et al. <ref type="bibr" target="#b33">[34]</ref>. Both advocate alternative methods for aggregating sets of <ref type="bibr" target="#b15">[16]</ref>, or predicting the therapeutic effect of candidate drugs <ref type="bibr" target="#b33">[34]</ref>. More generally, subgraph embeddings have been used to cla et="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b33">34]</ref>. Subgraph embeddings can be used to classify or predict var
he time of its last reference. From the last reference until the block is evicted the block is dead <ref type="bibr" target="#b12">[13]</ref>. Cache blocks are dead on average 86.2% of the time over a everal dead block predictors and applied them to problems such as prefetching and block replacement <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bib "#b0">[1]</ref>.</p><p>1) Trace Based Predictor: Dead block prediction was introduced by Lai et al. <ref type="bibr" target="#b12">[13]</ref>. The Lai et al. predictor is used to prefetch data into de . In keeping with the methodology of recent cache papers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bib
ampling predictor uses the idea of a skewed organization <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b15">[16]</ref> to reduce the impact of conflicts in the table. The predic
bypass policy with a sampling dead block predictor.</p><p>Dead blocks lead to poor cache efficiency <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b3">[4]</ref>. In the least-rec em to problems such as prefetching and block replacement <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bib erences rather than cycles for reducing cache leakage.</p><p>3) Cache Burst Predictor: Cache bursts <ref type="bibr" target="#b14">[15]</ref> can be used with trace, counting, or time based dead block evel cache to remain consistent with other previous work <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bib ">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" t the reftrace predictor performs quite poorly compared with its observed behavior in previous work <ref type="bibr" target="#b14">[15]</ref>. In that work, reftrace was used for L1 or L2 caches with plan to investigate sampling techniques for counting predictors as well as cache-bursts predictors <ref type="bibr" target="#b14">[15]</ref> at all levels of the memory hierarchy. We plan to evaluate
k replacement <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" ns/1.0"><head>B. Counting Predictor</head><p>The counting-based Live-time Predictor (LvP) predictor <ref type="bibr" target="#b10">[11]</ref> is a 256 ? 256 table of entries, each of which includes th ead block predictors in the context of a combined dead block replacement and bypassing optimization <ref type="bibr" target="#b10">[11]</ref>. When it is time to choose a victim block, a predicted dea 18">[19]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr"
L2 cache to be used as a "virtual victim cache" into which LRU victims from hot sets can be stored <ref type="bibr" target="#b9">[10]</ref>.</p><p>2) Time-Based Predictor: Hu et al. propose a time-ba
ition while cache-friendly workloads continue to insert in the MRU position.</p><p>Keramidas et al. <ref type="bibr" target="#b8">[9]</ref> proposed a cache replacement policy that uses sampling-based ">[20]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr"
="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref> and perform dynamic self-invalidation <ref type="bibr" target="#b13">[14]</ref>. A skewed tracebased predictor is used to identify a pool
="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref> and perform dynamic self-invalidation <ref type="bibr" target="#b13">[14]</ref>. A skewed tracebased predictor is used to identify a pool
ctor is also used to optimize a cache coherence protocol <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref> and perform dynamic self-invalidation <ref type="bibr" tar
categorizes blocks as near re-reference, distant re-reference and long re-reference interval blocks <ref type="bibr" target="#b7">[8]</ref>. On a miss the block that is predicted to be referenced most #b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we choose a memory-intensive subset of the benchmarks. We u
ctor is also used to optimize a cache coherence protocol <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref> and perform dynamic self-invalidation <ref type="bibr" tar
ual to physical translations directly in memory. Similar in design to the recently proposed PoM-TLB <ref type="bibr" target="#b48">(Ryoo et al. 2017)</ref>, we propose a TLB in GPU DRAM (DRAM-TLB) tha to as a Stacked-TLB while a DRAM-TLB architected using system memory is referred to as a SysMem-TLB <ref type="bibr" target="#b48">(Ryoo et al. 2017)</ref>. DRAM-TLBs are physically placed in a large erage recent work that uses a simple predictor to learn the page size for any given virtual address <ref type="bibr" target="#b48">(Ryoo et al. 2017)</ref>. In this work, the authors propose a 512-ent -TLBs also resemble recent CPU-centric work that extends TLB reach of CPUs (referred to as PoM-TLB) <ref type="bibr" target="#b48">(Ryoo et al. 2017)</ref>. Our work explores the DRAM-TLB design space

pe="bibr" target="#b2">(Barr et al. 2011;</ref><ref type="bibr" target="#b3">Basu et al. 2013;</ref><ref type="bibr" target="#b6">Bhattacharjee et al. 2011;</ref><ref type="bibr" target="#b41">Pham et el cache and TLB are private to each SM while the memory-side LLC and LLT are shared by all the SMs <ref type="bibr" target="#b6">(Bhattacharjee et al. 2011)</ref>. All caches use 128B cache line size 012</ref><ref type="bibr" target="#b40">(Pham et al. , 2014) )</ref> and enhanced TLB organizations <ref type="bibr" target="#b6">(Bhattacharjee et al. 2011;</ref><ref type="bibr" target="#b9">Chen et
ibr" target="#b27">Jevdjic et al. 2014;</ref><ref type="bibr" target="#b35">Loh and Hill 2011;</ref><ref type="bibr" target="#b50">Sim et al. 2012</ref><ref type="bibr" target="#b51">Sim et al. , 2013
ory imbalance <ref type="bibr" target="#b18">(Gaud et al. 2014)</ref>, memory fragmentation, paging <ref type="bibr" target="#b11">(Chou et al. 2014)</ref>, page creation, and page splitting <ref type bibr" target="#b54">Talluri et al. 1992;</ref><ref type="bibr" target="#b18">Gaud et al. 2014;</ref><ref type="bibr" target="#b11">Chou et al. 2014;</ref><ref type="bibr" target="#b42">Pham et al. 201 bibr" target="#b54">Talluri et al. 1992;</ref><ref type="bibr" target="#b18">Gaud et al. 2014;</ref><ref type="bibr" target="#b11">Chou et al. 2014;</ref><ref type="bibr" target="#b42">Pham et al. 201
" target="#b15">(Dashti et al. 2013;</ref><ref type="bibr" target="#b55">Verghese et al. 1996;</ref><ref type="bibr" target="#b8">Bolosky et al. 1989</ref>). Our work is orthogonal these proposals.</p
ts 128-entry read and write queues per channel, open-page policy, minimalist address mapping policy <ref type="bibr" target="#b32">(Kaseridis et al. 2011</ref>) and FR-FCFS scheduling policy (prioriti

isses instead. The IOMMU (and GMMU) is configured with a highly-threaded hardware page table walker <ref type="bibr" target="#b44">(Power et al. 2014;</ref><ref type="bibr" target="#b43">Pichai et al. nt and high performing address translation on GPUs is an important area of research. Recent studies <ref type="bibr" target="#b44">(Power et al. 2014;</ref><ref type="bibr" target="#b43">Pichai et al. Kandiraju and Sivasubramaniam 2002;</ref><ref type="bibr" target="#b49">Saulsbury et al. 2000;</ref><ref type="bibr" target="#b44">Power et al. 2014)</ref>, or speculating on the address translation o rs to GPUs do not perform well. Consequently, novel mechanisms such as highly threaded page walkers <ref type="bibr" target="#b44">(Power et al. 2014</ref>) and intelligent page walk scheduling <ref t
ue to stress the on-chip Last-Level TLB (LLT) <ref type="bibr" target="#b2">(Barr et al. 2011;</ref><ref type="bibr" target="#b3">Basu et al. 2013;</ref><ref type="bibr" target="#b6">Bhattacharjee et t Segments. Our studies show that the use of large pages (e.g., 2MB, 64MB, 1GB) and direct segments <ref type="bibr" target="#b3">(Basu et al. 2013</ref>) can significantly improve TLB coverage. Howev
target="#foot_3">3</ref> (Lee and Kim 2012; <ref type="bibr" target="#b23">Jaleel et al. 2010;</ref><ref type="bibr" target="#b45">Qureshi et al. 2007;</ref><ref type="bibr" target="#b58">Wu et al. 20 ed after cache insertion (Lee and Kim 2012; <ref type="bibr" target="#b23">Jaleel et al. 2010;</ref><ref type="bibr" target="#b45">Qureshi et al. 2007;</ref><ref type="bibr" target="#b58">Wu et al. 20
ding on the page table implementation, address translation requires one or more page table accesses <ref type="bibr" target="#b4">(Bhargava et al. 2008)</ref>. To avoid the long memory access latency,
pe="bibr" target="#b2">(Barr et al. 2011;</ref><ref type="bibr" target="#b3">Basu et al. 2013;</ref><ref type="bibr" target="#b6">Bhattacharjee et al. 2011;</ref><ref type="bibr" target="#b41">Pham et el cache and TLB are private to each SM while the memory-side LLC and LLT are shared by all the SMs <ref type="bibr" target="#b6">(Bhattacharjee et al. 2011)</ref>. All caches use 128B cache line size 012</ref><ref type="bibr" target="#b40">(Pham et al. , 2014) )</ref> and enhanced TLB organizations <ref type="bibr" target="#b6">(Bhattacharjee et al. 2011;</ref><ref type="bibr" target="#b9">Chen et
d Hill 1994;</ref><ref type="bibr" target="#b54">Talluri et al. 1992</ref>) due to memory imbalance <ref type="bibr" target="#b18">(Gaud et al. 2014)</ref>, memory fragmentation, paging <ref type="bib target="#b53">(Talluri and Hill 1994;</ref><ref type="bibr" target="#b54">Talluri et al. 1992;</ref><ref type="bibr" target="#b18">Gaud et al. 2014;</ref><ref type="bibr" target="#b11">Chou et al. 201 target="#b53">(Talluri and Hill 1994;</ref><ref type="bibr" target="#b54">Talluri et al. 1992;</ref><ref type="bibr" target="#b18">Gaud et al. 2014;</ref><ref type="bibr" target="#b11">Chou et al. 201
highly-threaded hardware page table walker <ref type="bibr" target="#b44">(Power et al. 2014;</ref><ref type="bibr" target="#b43">Pichai et al. 2014</ref>) and on-chip Page Walk Caches (PWCs) <ref ty important area of research. Recent studies <ref type="bibr" target="#b44">(Power et al. 2014;</ref><ref type="bibr" target="#b43">Pichai et al. 2014)</ref> show that simply extending CPU-style TLBs a lkers <ref type="bibr" target="#b44">(Power et al. 2014</ref>) and intelligent page walk scheduling <ref type="bibr" target="#b43">(Pichai et al. 2014</ref>) have been proposed. Our work focuses on im

ding on the page table implementation, address translation requires one or more page table accesses <ref type="bibr" target="#b4">(Bhargava et al. 2008)</ref>. To avoid the long memory access latency,
<ref type="bibr">(Chou et al. 2015b;</ref><ref type="bibr" target="#b46">Qureshi and Loh 2012;</ref><ref type="bibr" target="#b27">Jevdjic et al. 2014;</ref><ref type="bibr" target="#b35">Loh and Hill
/ref> and enhanced TLB organizations <ref type="bibr" target="#b6">(Bhattacharjee et al. 2011;</ref><ref type="bibr" target="#b9">Chen et al. 1992</ref>) that modify the existing TLB structures. Other
ding on the page table implementation, address translation requires one or more page table accesses <ref type="bibr" target="#b4">(Bhargava et al. 2008)</ref>. To avoid the long memory access latency,
ee and Martonosi 2010;</ref><ref type="bibr" target="#b31">Kandiraju and Sivasubramaniam 2002;</ref><ref type="bibr" target="#b49">Saulsbury et al. 2000;</ref><ref type="bibr" target="#b44">Power et a
ng Page Walk Caches (PWC) for each level of the page table <ref type="bibr">(Barr et al. 2010;</ref><ref type="bibr" target="#b5">Bhattacharjee 2013)</ref>. PWCs exploit temporal and spatial locality hai et al. 2014</ref>) and on-chip Page Walk Caches (PWCs) <ref type="bibr">(Barr et al. 2010;</ref><ref type="bibr" target="#b5">Bhattacharjee 2013)</ref> for each level of the page table. The PWCs a ccelerate page walks by caching portions of the page table <ref type="bibr">(Barr et al. 2010;</ref><ref type="bibr" target="#b5">Bhattacharjee 2013)</ref>, prefetching TLB entries <ref type="bibr" ta re small, fully associative hardware structures that are indexed by portions of the virtual address <ref type="bibr" target="#b5">(Bhattacharjee 2013)</ref>. We model a 16-entry PML4-cache, 16-entry P
timization <ref type="bibr" target="#b3">(Bengio, 2000)</ref>, or, more recently, few-shot learning <ref type="bibr" target="#b11">(Finn et al., 2017)</ref>. In essence, we turn the gradient-based opt </ref> or initial weights that enable rapid adaptation to new tasks or domains in few-shot learning <ref type="bibr" target="#b11">(Finn et al., 2017)</ref>.</p><p>Meta-gradients (e.g., gradients w.r. s is expensive both from a computational and a memory point-of-view.</p><p>To alleviate this issue, <ref type="bibr" target="#b11">Finn et al. (2017)</ref> propose a first-order approximation, leading thus more knowledge than all competing methods. First-order refers to the approximation proposed by <ref type="bibr" target="#b11">Finn et al. (2017)</ref>, i.e. ignoring all second-order derivatives.
ally, graph convolutional approaches <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b22">Pham et al., 2017)</ref> have improved the state of the art in node c ose, we use Graph Convolutional Networks (GCN) (Kipf &amp; Welling, 2017) and Column Networks (CLN) <ref type="bibr" target="#b22">(Pham et al., 2017)</ref>. Both are models utilizing the message pass
ally, graph convolutional approaches <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b22">Pham et al., 2017)</ref> have improved the state of the art in node c ose, we use Graph Convolutional Networks (GCN) (Kipf &amp; Welling, 2017) and Column Networks (CLN) <ref type="bibr" target="#b22">(Pham et al., 2017)</ref>. Both are models utilizing the message pass
ally, graph convolutional approaches <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b22">Pham et al., 2017)</ref> have improved the state of the art in node c ose, we use Graph Convolutional Networks (GCN) (Kipf &amp; Welling, 2017) and Column Networks (CLN) <ref type="bibr" target="#b22">(Pham et al., 2017)</ref>. Both are models utilizing the message pass
n the principle of meta learning, which has traditionally been used for hyperparameter optimization <ref type="bibr" target="#b3">(Bengio, 2000)</ref>, or, more recently, few-shot learning <ref type=" ning models more time and/or data efficient, e.g. by finding suitable hyperparameter configurations <ref type="bibr" target="#b3">(Bengio, 2000)</ref> or initial weights that enable rapid adaptation t n, is the task of optimizing the learning algorithm itself; e.g., by optimizing the hyperparameters <ref type="bibr" target="#b3">Bengio (2000)</ref>, learning to update the parameters of a neural net
well as training time (poisoning attacks) <ref type="bibr" target="#b28">(Zügner et al., 2018;</ref><ref type="bibr" target="#b10">Dai et al., 2018)</ref>. A core strength of models using graph convol <p>Only recently researchers have started to study adversarial attacks on deep learning for graphs. <ref type="bibr" target="#b10">Dai et al. (2018)</ref> consider test-time (i.e., evasion) attacks on ating their impact by training a classifier on the data modified by their algorithm. In contrast to <ref type="bibr" target="#b10">Dai et al. (2018)</ref>, their attacks can both insert and remove edg fied by our algorithm. In contrast to <ref type="bibr" target="#b28">Zügner et al. (2018)</ref> and <ref type="bibr" target="#b10">Dai et al. (2018)</ref>, our algorithm is designed for global attacks
>Bojchevski &amp; Günnemann, 2018a;</ref><ref type="bibr" target="#b2">Battaglia et al., 2018;</ref><ref type="bibr" target="#b21">Perozzi et al., 2014;</ref><ref type="bibr">Bojchevski et al., 2018;< chieved by training a standard logistic regression model on the node embeddings learned by DeepWalk <ref type="bibr" target="#b21">(Perozzi et al., 2014)</ref>. DeepWalk itself is trained in an unsupe
n the principle of meta learning, which has traditionally been used for hyperparameter optimization <ref type="bibr" target="#b3">(Bengio, 2000)</ref>, or, more recently, few-shot learning <ref type=" ning models more time and/or data efficient, e.g. by finding suitable hyperparameter configurations <ref type="bibr" target="#b3">(Bengio, 2000)</ref> or initial weights that enable rapid adaptation t n, is the task of optimizing the learning algorithm itself; e.g., by optimizing the hyperparameters <ref type="bibr" target="#b3">Bengio (2000)</ref>, learning to update the parameters of a neural net

17">(Monti et al., 2017;</ref><ref type="bibr" target="#b4">Bojchevski &amp; Günnemann, 2018a;</ref><ref type="bibr" target="#b2">Battaglia et al., 2018;</ref><ref type="bibr" target="#b21">Perozzi et
pproach on the well-known CITESEER <ref type="bibr" target="#b24">(Sen et al., 2008)</ref>, CORA-ML <ref type="bibr" target="#b16">(McCallum et al., 2000)</ref>, and POLBLOGS (Adamic &amp; Glance, 200
nt at University of Chinese Academy of Sciences (UCAS), Beijing, China. e-mail: baiye2016@ia.ac.cn. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr">[15]</ref>, <ref type="bibr" target="#b1
The source model is trained jointly on several languages <ref type="bibr" target="#b17">[19]</ref>, <ref type="bibr" target="#b18">[20]</ref>. In addition, language identification based multilingual t
<ref type="bibr" target="#b16">[18]</ref>. The source model is trained jointly on several languages <ref type="bibr" target="#b17">[19]</ref>, <ref type="bibr" target="#b18">[20]</ref>. In addition, l
ef> use adversarial strategy to obtain bilingual lexicon without cross-lingual knowledge. Shinohara <ref type="bibr" target="#b36">[38]</ref> utilizes adversarial training to perform environment adapt
[47]</ref> in NIST OpenKWS 2013 reported WER of 48.1% or higher on Turkish FLP condition. Past work <ref type="bibr" target="#b43">[45]</ref>, <ref type="bibr" target="#b44">[46]</ref> on Vietnamese F
e-mail: baiye2016@ia.ac.cn. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr">[15]</ref>, <ref type="bibr" target="#b14">[16]</ref>, <ref type="bibr" target="#b15">[17]</ref>. This approach
ed into two categories: transferring bottleneck features <ref type="bibr" target="#b15">[17]</ref>, <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b22">[24]</ref>, <ref type="bib Models are proposed to use hidden layers and softmax layers to learn language dependent information <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b30">[32]</ref>. These models h lingual models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b30">[32]</ref>. The SHL-Model er kind of SHL-Models is proposed to use hidden layers to learn more language dependent information <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b30">[32]</ref>. These models h
improvement for automatic speech recognition (ASR) systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t
Pashto FLP condition using the Babel data reported WER as low as 45.7%. But many competitive teams <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b45">[47]</ref> in NIST OpenKWS ref> in NIST OpenKWS 2013 reported WER of 47.1% or higher. In addition, lots of competitive systems <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b45">[47]</ref> in NIST OpenKWS R of 48.1% or higher on Turkish FLP condition. Past work <ref type="bibr" target="#b43">[45]</ref>, <ref type="bibr" target="#b44">[46]</ref> on Vietnamese FLP condition using the Babel data reported
del parameters <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b25">[27]</ref>. This paper focuses on the latter.</p><p>The basic idea of s of SHL-Models are proposed to train multilingual models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b21">[23]</ref>, <ref type="bib . One kind of the methods is widely used in previous work <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b28">[30]</ref>. This method is
Pashto FLP condition using the Babel data reported WER as low as 45.7%. But many competitive teams <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b45">[47]</ref> in NIST OpenKWS ref> in NIST OpenKWS 2013 reported WER of 47.1% or higher. In addition, lots of competitive systems <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b45">[47]</ref> in NIST OpenKWS R of 48.1% or higher on Turkish FLP condition. Past work <ref type="bibr" target="#b43">[45]</ref>, <ref type="bibr" target="#b44">[46]</ref> on Vietnamese FLP condition using the Babel data reported
]</ref>, <ref type="bibr" target="#b15">[17]</ref>. This approach benefits from multi-task learning <ref type="bibr" target="#b16">[18]</ref>. The source model is trained jointly on several languages <head>B. Multilingual Training</head><p>Multilingual training is an instance of multi-task learning <ref type="bibr" target="#b16">[18]</ref>. The source model is trained simultaneously on the trainin
rce languages have been studied by Thomas <ref type="bibr" target="#b26">[28]</ref>. Scanzio et al. <ref type="bibr" target="#b27">[29]</ref> present a front-end consisting of an artificial neural net .0"><head>A. Shared Hidden Layer Model</head><p>The SHL-Model is widely used for multilingual tasks <ref type="bibr" target="#b27">[29]</ref>, <ref type="bibr" target="#b28">[30]</ref>, <ref type="bib
ge invariant low-level components across various languages <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. An acoustic mod rget="#b23">[25]</ref>, <ref type="bibr" target="#b24">[26]</ref> and transferring model parameters <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr target="#b29">[31]</ref>. A lot of variants of SHL-Models are proposed to train multilingual models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr transfer shared layers to the target model. One kind of the methods is widely used in previous work <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr
g to rapidly build an ASR system for a novel language with significantly less labeled training data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" t
coustic models have obtained significant improvement for automatic speech recognition (ASR) systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" t
coustic models have obtained significant improvement for automatic speech recognition (ASR) systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" t
improvement for automatic speech recognition (ASR) systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t
Pashto FLP condition using the Babel data reported WER as low as 45.7%. But many competitive teams <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b45">[47]</ref> in NIST OpenKWS ref> in NIST OpenKWS 2013 reported WER of 47.1% or higher. In addition, lots of competitive systems <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b45">[47]</ref> in NIST OpenKWS R of 48.1% or higher on Turkish FLP condition. Past work <ref type="bibr" target="#b43">[45]</ref>, <ref type="bibr" target="#b44">[46]</ref> on Vietnamese FLP condition using the Babel data reported
bottleneck features for low resource speech recognition <ref type="bibr" target="#b19">[21]</ref>, <ref type="bibr" target="#b20">[22]</ref>. The knowledge from the source model can be transferred to
">[17]</ref>, <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b22">[24]</ref>, <ref type="bibr" target="#b23">[25]</ref>, <ref type="bibr" target="#b24">[26]</ref> and transferrin
dversarial strategy for domain adaptation in image classification tasks. More recently, Chen et al. <ref type="bibr" target="#b34">[36]</ref> utilize adversarial learning for Chinese word segmentation
del via transfer learning. The goal of transfer learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> is to improve the performance of the target model via usin
type="bibr" target="#b30">[32]</ref> and the open source deep learning framework called TensorFlow <ref type="bibr" target="#b40">[42]</ref>. The Gaussian mixture model hidden Markov models (GMM-HMM)
arial learning on domain adaptation <ref type="bibr" target="#b31">[33]</ref>, adversarial learning <ref type="bibr" target="#b32">[34]</ref> is used to ensure that the shared layers of the source mod
del via transfer learning. The goal of transfer learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> is to improve the performance of the target model via usin
e model can be transferred to the target model via transfer learning. The goal of transfer learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> is to improve t
type="bibr" target="#b30">[32]</ref> and the open source deep learning framework called TensorFlow <ref type="bibr" target="#b40">[42]</ref>. The Gaussian mixture model hidden Markov models (GMM-HMM)
">[23]</ref>, <ref type="bibr" target="#b22">[24]</ref>, <ref type="bibr" target="#b23">[25]</ref>, <ref type="bibr" target="#b24">[26]</ref> and transferring model parameters <ref type="bibr" target=
del parameters <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b25">[27]</ref>. This paper focuses on the latter.</p><p>The basic idea of s of SHL-Models are proposed to train multilingual models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b21">[23]</ref>, <ref type="bib . One kind of the methods is widely used in previous work <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b28">[30]</ref>. This method is
e-mail: baiye2016@ia.ac.cn. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr">[15]</ref>, <ref type="bibr" target="#b14">[16]</ref>, <ref type="bibr" target="#b15">[17]</ref>. This approach
="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However it is still challenging to rapidly build an ASR sys
e-mail: baiye2016@ia.ac.cn. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr">[15]</ref>, <ref type="bibr" target="#b14">[16]</ref>, <ref type="bibr" target="#b15">[17]</ref>. This approach
tput mappings, potentially stochastic, with learnable parameters using directed acyclic graphs (see <ref type="bibr" target="#b38">Schulman et al. (2015)</ref> for a review). The state of each non-inp

logistic function. Most recently, to avoid the difficulty associated with likelihood-ratio methods <ref type="bibr" target="#b23">(Kočiský et al., 2016)</ref> relaxed the discrete sampling operation
can be relaxed and approximated reasonably well with sigmoidal functions or the softmax (see e.g., <ref type="bibr" target="#b12">Grefenstette et al., 2015;</ref><ref type="bibr" target="#b10">Graves
target="#b30">(Paisley et al., 2012;</ref><ref type="bibr" target="#b13">Gregor et al., 2013;</ref><ref type="bibr" target="#b33">Ranganath et al., 2014;</ref><ref type="bibr">Mnih &amp; Gregor, 2014

ork has been done to generalize it <ref type="bibr" target="#b5">(Connor &amp; Mosimann, 1969;</ref><ref type="bibr" target="#b1">Aitchison, 1985;</ref><ref type="bibr" target="#b34">Rayens &amp; Srin

functions or the softmax (see e.g., <ref type="bibr" target="#b12">Grefenstette et al., 2015;</ref><ref type="bibr" target="#b10">Graves et al., 2016)</ref>, but, if a distribution over discrete stat

riance, various variance reduction techniques can be used to make the estimator much more effective <ref type="bibr" target="#b11">(Greensmith et al., 2004)</ref>.</p><p>Baselines are the most importa
<ref type="bibr" target="#b21">[22]</ref>) and region-based convolutional neural networks (R-CNNs) <ref type="bibr" target="#b5">[6]</ref>. Although region-based CNNs were computationally expensive a b5">[6]</ref>. Although region-based CNNs were computationally expensive as originally developed in <ref type="bibr" target="#b5">[6]</ref>, their cost has been drastically reduced thanks to sharing c rk whose last fc layer simultaneously predicts multiple (e.g., 800) boxes, which are used for R-CNN <ref type="bibr" target="#b5">[6]</ref> object detection. Their proposal network is applied on a sin rget="#foot_3">3</ref>For regression, we adopt the parameterizations of the 4 coordinates following <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_2">t x = (x − x a )/w a , t y = odel for ImageNet classification <ref type="bibr" target="#b16">[17]</ref>, as is standard practice <ref type="bibr" target="#b5">[6]</ref>. We tune all layers of the ZF net, and conv3 1 and up for th
ve proposed ways of using deep networks for locating class-specific or classagnostic bounding boxes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta
ion (k = 9 is a typical value).</p><p>Our RPNs are thus a kind of fully-convolutional network (FCN) <ref type="bibr" target="#b13">[14]</ref> and they can be trained end-toend specifically for the tas . <ref type="foot" target="#foot_0">1</ref> We model this process with a fullyconvolutional network <ref type="bibr" target="#b13">[14]</ref>, which we describe in this section. Because our ultimate g head>Optimization</head><p>The RPN, which is naturally implemented as a fully-convolutional network <ref type="bibr" target="#b13">[14]</ref>, can be trained end-to-end by back-propagation and stochas
tion of convolutions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref> has been attracting inc ype="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref> and semantic segmentation <ref type="bibr" target="#b1">[2]</ref>. Fast R-CNN <ref type="bibr" target="#b4">[5]</ref> enables
elective Search is an order of magnitude slower, at 2s per image in a CPU implementation. EdgeBoxes <ref type="bibr" target="#b23">[24]</ref> currently provides the best tradeoff between proposal qual target="#b21">[22]</ref>, we generate about 2k SS proposals by the "fast" mode. For EdgeBoxes (EB) <ref type="bibr" target="#b23">[24]</ref>, we generate the proposals by the default EB setting tuned
tic bounding boxes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20]</ref>. In the OverFeat meth then turned into a conv layer for detecting multiple class-specific objects. The Multi-Box methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref> generate region prop
13">[14]</ref>, can be trained end-to-end by back-propagation and stochastic gradient descent (SGD) <ref type="bibr" target="#b11">[12]</ref>. We follow the "imagecentric" sampling strategy from <ref
20k mini-batches on the PASCAL dataset. We also use a momentum of 0.9 and a weight decay of 0.0005 <ref type="bibr" target="#b10">[11]</ref>. Our implementation uses Caffe <ref type="bibr" target="#b
n n × n conv layer followed by two sibling 1 × 1 conv layers (for reg and cls, respectively). ReLUs <ref type="bibr" target="#b14">[15]</ref> are applied to the output of the n × n conv layer.</p></di
s share a common set of conv layers. In our experiments, we investigate the Zeiler and Fergus model <ref type="bibr" target="#b22">[23]</ref> (ZF), which has 5 shareable conv layers and the Simonyan a nchmark for a few models. For the ImageNet pre-trained network, we use the "fast" version of ZF net <ref type="bibr" target="#b22">[23]</ref> that has 5 conv layers and 3 fc layers, and the public VGG
tic bounding boxes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20]</ref>. In the OverFeat meth then turned into a conv layer for detecting multiple class-specific objects. The Multi-Box methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref> generate region prop
ing instructions out of order. We believe that a solution similar to those proposed by Stark et al. <ref type="bibr" target="#b18">[19]</ref> and Cher et al. <ref type="bibr" target="#b2">[3]</ref> ca .</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Stark et al. <ref type="bibr" target="#b18">[19]</ref> proposed a limited form of out-of-order instruction fetch
edictors can achieve equivalent or higher prediction accuracies than conventional branch predictors <ref type="bibr" target="#b6">[7]</ref>.</p><p>Once future control flow can be predicted at trace gr ">Trace Prediction.</head><p>We u s e t h e t r a c e p r e d i c t o r proposed by Jacobson et al. <ref type="bibr" target="#b6">[7]</ref>. Each trace is assigned a trace identifier obtained by combi
<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>, speculative threads <ref type="bibr" target="#b21">[22]</ref>, etc., that require fetching multiple threads. In a multit
<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>, speculative threads <ref type="bibr" target="#b21">[22]</ref>, etc., that require fetching multiple threads. In a multit
than 80% for four benchmarks (bzip2, gzip, mcf, and ammp). The trace buffers act as a filter cache <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref> making the number of
small working set. The reader is referred to other papers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> for a more detailed exploration of trace selection techniqu
ic execution order in the cache (i.e. using a trace cache) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>. The first solution
all and constructing all the traces just before control flow reaches them.</p><p>Fetch Target Queue <ref type="bibr" target="#b13">[14]</ref> was proposed by Reinman et al. to decouple instruction fet
small working set. The reader is referred to other papers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> for a more detailed exploration of trace selection techniqu
much easier to implement techniques like dual-path execution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>, speculative threads <ref type="bibr" target="#b21">[22]</re
ic execution order in the cache (i.e. using a trace cache) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>. The first solution
i-gate Mixture-of-Experts (MMoE) structure, which is inspired by the Mixture-of-Experts (MoE) model <ref type="bibr" target="#b20">[21]</ref> and the recent MoE layer <ref type="bibr" target="#b15">[1 "4">MODELING APPROACHES 4.1 Mixture-of-Experts</head><p>The Original Mixture-of-Experts (MoE) Model <ref type="bibr" target="#b20">[21]</ref> can be formulated as:</p><formula xml:id="formula_5">= n i
models can improve model predictions on all tasks by utilizing regularization and transfer learning <ref type="bibr" target="#b7">[8]</ref>. However, in practice, multi-task learning models do not alw <p>The backbone of MMoE is built upon the most commonly used Shared-Bottom multi-task DNN structure <ref type="bibr" target="#b7">[8]</ref>. The Shared-Bottom model structure is shown in Figure <ref t igure <ref type="figure" target="#fig_0">1</ref> (a), which is a framework proposed by Rich Caruana <ref type="bibr" target="#b7">[8]</ref> and widely adopted in many multi-task learning applications sks.</p><p>Prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref> investigated task di erences in multi-task learning by assumi target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Instead of sharing hidden layers and same model parame lt in both improved e ciency and model quality for each task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>. One of the widely us " target="#b29">30]</ref>. One of the widely used multi-task learning models is proposed by Caruana <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, which has a shared-bo
ussian Process model similar to Spearmint as introduced in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>. To make the comparison fair, we constrain the maximum mode
en successfully applied in many real world large-scale applications, such as recommendation systems <ref type="bibr" target="#b10">[11]</ref>. Such recommendation systems often need to optimize multip ecommendation task is improved by sharing feature representations and lower level hidden layers. In <ref type="bibr" target="#b10">[11]</ref>, a shared-bottom model is used to learn a ranking algorith tion system adopts similar framework as proposed in some existing content recommendation frameworks <ref type="bibr" target="#b10">[11]</ref>, which has a candidate generator followed by a deep rankin
rs are extensively shared among all tasks.</p><p>Prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref> investigated task di er s of multitask models <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Instead of shar
tasks, particularly when model parameters are extensively shared among all tasks.</p><p>Prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target cross di erent tasks. Doing so can result in both improved e ciency and model quality for each task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target nipulated di erent types of task relatedness so as to evaluate the e ectiveness of multitask models <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe
cale recommendation systems have adopted multi-task learning using Deep Neural Network (DNN) models <ref type="bibr" target="#b2">[3]</ref>.</p><p>Researchers have reported multi-task learning models commendations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>. In <ref type="bibr" target="#b2">[3]</ref>, a text recommendation task is improved by sharing feature r
mendation systems, multi-task learning is found helpful for providing context-aware recommendations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>. In <ref type="bib
<ref type="bibr" target="#b7">[8]</ref> and widely adopted in many multi-task learning applications <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref>. Therefore, we tre -task model is widely used. The shared part of the model saves a lot of computation at serving time <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref>. All of the three
sk Learning Applications</head><p>Thanks to the development of distributed machine learning systems <ref type="bibr" target="#b12">[13]</ref>, many large-scale real-world applications have adopted DNN
sk Learning Applications</head><p>Thanks to the development of distributed machine learning systems <ref type="bibr" target="#b12">[13]</ref>, many large-scale real-world applications have adopted DNN
quality for each task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>. One of the widely used multi-task learning models is propo
found helpful for providing context-aware recommendations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>. In <ref type="bibr" target="#b2">[3]</ref>, a text recomme
quality for each task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>. One of the widely used multi-task learning models is propo
cale recommendation systems have adopted multi-task learning using Deep Neural Network (DNN) models <ref type="bibr" target="#b2">[3]</ref>.</p><p>Researchers have reported multi-task learning models commendations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>. In <ref type="bibr" target="#b2">[3]</ref>, a text recommendation task is improved by sharing feature r
found helpful for providing context-aware recommendations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>. In <ref type="bibr" target="#b2">[3]</ref>, a text recomme
quality for each task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>. One of the widely used multi-task learning models is propo
periment where we can measure and control task relatedness by their Pearson correlation. Similar to <ref type="bibr" target="#b23">[24]</ref>, we use two synthetic regression tasks and use sinusoidal ic data where we can easily measure and control the task relatedness.</p><p>Inspired by Kang et al. <ref type="bibr" target="#b23">[24]</ref>, we generate two regression tasks and use the Pearson corr icator of task relationships. Since we focus on DNN models, instead of the linear functions used in <ref type="bibr" target="#b23">[24]</ref>, we set the regression model as a combination of sinusoida
="#b23">[24]</ref>, we set the regression model as a combination of sinusoidal functions as used in <ref type="bibr" target="#b32">[33]</ref>. Speci cally, we generate the synthetic data as follows.</
idely used multi-task learning models is proposed by Caruana <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, which has a shared-bottom model structure, where the bottom nsemble models and ensemble of subnetworks have been proven to be able to improve model performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Eigen et al <
<ref type="bibr" target="#b7">[8]</ref> and widely adopted in many multi-task learning applications <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref>. Therefore, we tre -task model is widely used. The shared part of the model saves a lot of computation at serving time <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref>. All of the three
quality for each task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>. One of the widely used multi-task learning models is propo
exed by nodes of an arbitrary directed or undirected graph <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b50">[51]</ref>. This choice is satisfying in the sense that, when the sig • 1 is norm 1, and A norm = 1 λmax A. Other norms could be used to define the total variation, see <ref type="bibr" target="#b50">[51]</ref> <ref type="bibr" target="#b2">[3]</ref>. Using this, graph ustified theoretically that the frequency bases obtained from the shift operator tend to be ordered <ref type="bibr" target="#b50">[51]</ref>.</p><p>Up to this point, we have focused primarily on freq odel makes it possible to detect outliers or abnormal values by highpass filtering and thresholding <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b154">[155]</ref>, or to build f type="bibr" target="#b4">[5]</ref>, optimizing the prediction of unknown labels in classification <ref type="bibr" target="#b50">[51]</ref> or semisupervised learning problems <ref type="bibr" targe
roposed solutions for different aspects of the problem <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b120">[121]</ref>, <ref type="bibr" target="#b121">[122]</ref>. In particu arget="#b121">[122]</ref>. In particular, sampling results have been generalized to directed graphs <ref type="bibr" target="#b120">[121]</ref>, <ref type="bibr" target="#b121">[122]</ref> and to othe ng set selection from an experiment design perspective <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b120">[121]</ref>, <ref type="bibr" target="#b121">[122]</ref> setting as ge-scale graphs. Some techniques require computing and storing the first K basis vectors of the GFT <ref type="bibr" target="#b120">[121]</ref>. For larger graph sizes, where this may not be practical always lead to performance comparable to those of more complex greedy optimization methods such as <ref type="bibr" target="#b120">[121]</ref>, <ref type="bibr" target="#b121">[122]</ref>.</p><p>Give
The field that gathers all these questions under a common umbrella is graph signal processing (GSP) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>While the pr of the prior work that is more directly connected and in the spirit of signal processing on graphs, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. We organize the di /ref>. We organize the discussion along two main lines; some parts of the exposition follow closely <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b44">[45]</ref>.</p><p>1) From al te the graph signal model for signals indexed by nodes of an arbitrary directed or undirected graph <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b50">[51]</ref>. This choice is s /ref> studies time signals. Graph signal processing (GSP)<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t erpretation of DSP can be extended to develop a linear time shift invariant Graph Signal Processing <ref type="bibr" target="#b1">[2]</ref>. Consider now a graph signal s ∈ C N , where the entries of lized Laplacian L = D −1/2 LD −1/2 .</formula><p>The adjacency matrix A can be adopted as the shift <ref type="bibr" target="#b1">[2]</ref> for this general graph. Other choices have been proposed, in nt if it commutes with the shift,</p><formula xml:id="formula_18">AH = HA.</formula><p>As proven in <ref type="bibr" target="#b1">[2]</ref>, if the characteristic polynomial p A (z) and the minimum po aph filtering to two graph Fourier transforms and a pointwise multiplication in the spectral domain <ref type="bibr" target="#b1">[2]</ref>. With a notion of frequency we can now consider the GSP equi Section II-C). If these conditions do not hold, the Jordan canonical form is used to obtain the GFT <ref type="bibr" target="#b1">[2]</ref>, but this is well known to be a numerically unstable procedu
spectral interpretation and vertex domain localization <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b130">[131]</ref>. Notions of stationarity can help develop probabilistic essing methods leading to graph-based Wiener filtering <ref type="bibr" target="#b131">[132]</ref>, <ref type="bibr" target="#b130">[131]</ref>.</p><p>A study of vertex/spectral localization and uncer
entations <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b105">[106]</ref>, <ref type="bibr" target="#b107">[108]</ref>, <ref type="bibr" target="#b108">[109]</ref>, ii) develo
been generalized with help of GSP elements for the extraction of feature descriptors for 3D shapes <ref type="bibr" target="#b204">[205]</ref>, <ref type="bibr" target="#b205">[206]</ref>. A localize
ies prediction <ref type="bibr" target="#b210">[211]</ref>, classification tasks on social networks <ref type="bibr" target="#b211">[212]</ref>, autism spectrum disorder classification <ref type="bibr
). Subsequently, authors have proposed other shifts obtained from the adjacency matrix of the graph <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> that attempt to o preserve isometry of the shift, but in some cases lose the locality of the adjacency matrix shift <ref type="bibr" target="#b51">[52]</ref> 2) From graph Laplacian spectral clustering to Laplacianba , including the Laplacians <ref type="bibr" target="#b2">[3]</ref>, or variations of these matrices <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Different choi type="bibr" target="#b128">[129]</ref>, is to introduce alternative graph shift operators (see also <ref type="bibr" target="#b51">[52]</ref>) or localization operators that have both a spectral inter
of graph-based techniques in specific domains are considered in other papers in this special issue <ref type="bibr" target="#b151">[152]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><hea
ome columns of V having non-zero entries only in some of the clusters. Other contributions, such as <ref type="bibr" target="#b134">[135]</ref>, <ref type="bibr" target="#b135">[136]</ref>, have also
data is smooth on the data space (manifold). References <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> choose discrete
="bibr" target="#b65">[66]</ref>, filter banks on graphs <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, de-noising <ref type="bibr" target="#b68">[69]</ref>, and vertex localized implementation. These types of filterbanks have been designed for bipartite graphs <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b101">[102]</ref>, thus requiri "#b101">[102]</ref>, thus requiring the graph to be decomposed into a series of bipartite subgraphs <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b102">[103]</ref>. An alternati
Here, the GSP framework leads to the development of fast localized convolutional filters on graphs <ref type="bibr" target="#b208">[209]</ref> along with adapted pooling operators <ref type="bibr" ta
nsor networks <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b152">[153]</ref>, <ref type="bibr" target="#b153">[154]</ref>.</p><p>A fi
entations <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b105">[106]</ref>, <ref type="bibr" target="#b107">[108]</ref>, <ref type="bibr" target="#b108">[109]</ref>, ii) develo
MRI) data <ref type="bibr" target="#b173">[174]</ref>, <ref type="bibr" target="#b174">[175]</ref>, <ref type="bibr" target="#b175">[176]</ref>. Interestingly, it is also possible to combine different
s include <ref type="bibr" target="#b145">[146]</ref>, <ref type="bibr" target="#b146">[147]</ref>, <ref type="bibr" target="#b147">[148]</ref>, <ref type="bibr" target="#b148">[149]</ref>. While not cture of protein interaction networks has also been addressed with help of spectral graph templates <ref type="bibr" target="#b147">[148]</ref>. In particular, the observed matrix of mutual informatio
s such as <ref type="bibr" target="#b160">[161]</ref>, <ref type="bibr" target="#b161">[162]</ref>, <ref type="bibr" target="#b162">[163]</ref> have used graph signal processing tools for analyzing tr
">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bib
making decisions by collecting information about how other users rate particular services or items <ref type="bibr" target="#b195">[196]</ref>. Leveraging the notions of graph frequency and graph fil
data is smooth on the data space (manifold). References <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> choose discrete
/ref>.</p><p>1) From algebraic signal processing to graph signal processing: The sequence of papers <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bib P can be developed, e.g., from the signal model introduced in the Algebraic Signal Processing (ASP) <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bib
="bibr" target="#b61">[62]</ref>, wavelet decompositions <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bib data. Indeed, some of the initial explorations of graph-based processing focused on sensor networks <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bib
making decisions by collecting information about how other users rate particular services or items <ref type="bibr" target="#b195">[196]</ref>. Leveraging the notions of graph frequency and graph fil
etwork state space {0, 1} N grows exponentially fast (2 N , for N agents). To study these processes <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> one usually con
of the first and most important research goals in graph signal processing. Pioneering contributions <ref type="bibr" target="#b99">[100]</ref> and <ref type="bibr" target="#b60">[61]</ref>, provided e d on vertex domain and spectral domain characteristics, respectively. Vertex domain designs such as <ref type="bibr" target="#b99">[100]</ref> or <ref type="bibr" target="#b100">[101]</ref> have the a
at can take into consideration the specific localization properties encountered in irregular graphs <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b137">[138]</ref>, <ref type=
Here, the GSP framework leads to the development of fast localized convolutional filters on graphs <ref type="bibr" target="#b208">[209]</ref> along with adapted pooling operators <ref type="bibr" ta
"#b67">[68]</ref>, <ref type="bibr" target="#b102">[103]</ref>. An alternative approach proposed in <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b84">[85]</ref> can be applie
f>.</p><p>A study of vertex/spectral localization and uncertainty principles was first developed by <ref type="bibr" target="#b132">[133]</ref>, where it was shown that in general it is not possible t
ns, perceived alliances, quantifying connectedness, or determining the relevance of specific agents <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" t
genvector of the Laplacian).</p><p>to introduce definitions appropriate for directed graphs as well <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref>. In summary, a
eristics, respectively. Vertex domain designs such as <ref type="bibr" target="#b99">[100]</ref> or <ref type="bibr" target="#b100">[101]</ref> have the advantage of defining exactly localized basis f ="#b65">[66]</ref> were not critically sampled, unlike <ref type="bibr" target="#b60">[61]</ref> or <ref type="bibr" target="#b100">[101]</ref>. Thus, much recent work has focused on developing critic
hbased diffusion. Examples of these approaches include <ref type="bibr" target="#b145">[146]</ref>, <ref type="bibr" target="#b146">[147]</ref>, <ref type="bibr" target="#b147">[148]</ref>, <ref type=
72]</ref> and popular image-dependent filtering methods can be interpreted from a graph perspective <ref type="bibr" target="#b72">[73]</ref>. Models used in computer graphics applications can often b connected to popular image processing techniques, such as the bilateral filter and related methods <ref type="bibr" target="#b72">[73]</ref>, which also apply signal dependent filtering and are wider
s under a common umbrella is graph signal processing (GSP) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>While the precise definition of a graph signal will b onnected and in the spirit of signal processing on graphs, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. We organize the discussion along two main lines; some parts by undirected graphs with real, non-negative edge weights. This approach is more fully developed in <ref type="bibr" target="#b2">[3]</ref>, which adopts the graph Laplacian as basic building block to [48]</ref>, <ref type="bibr" target="#b48">[49]</ref> or from the spectral perspective developed in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b65">[66]</ref> based on spectral processing (GSP)<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b44">[45]</ref> extends DSP to si ="#b1">[2]</ref> for this general graph. Other choices have been proposed, including the Laplacians <ref type="bibr" target="#b2">[3]</ref>, or variations of these matrices <ref type="bibr" target="#b er norms could be used to define the total variation, see <ref type="bibr" target="#b50">[51]</ref> <ref type="bibr" target="#b2">[3]</ref>. Using this, graph frequency λ m is larger than graph freque
eristics, respectively. Vertex domain designs such as <ref type="bibr" target="#b99">[100]</ref> or <ref type="bibr" target="#b100">[101]</ref> have the advantage of defining exactly localized basis f ="#b65">[66]</ref> were not critically sampled, unlike <ref type="bibr" target="#b60">[61]</ref> or <ref type="bibr" target="#b100">[101]</ref>. Thus, much recent work has focused on developing critic
pollution <ref type="bibr" target="#b158">[159]</ref>, or to monitor and analyze power consumption <ref type="bibr" target="#b159">[160]</ref>, for example. Some works such as <ref type="bibr" target
br" target="#b185">[186]</ref>), other works such as <ref type="bibr" target="#b186">[187]</ref> or <ref type="bibr" target="#b187">[188]</ref> use more specific GSP operators for denoising or filteri 187]</ref> use graph spectral denoising methods to enhance the quality of images, while the work in <ref type="bibr" target="#b187">[188]</ref> uses graph-based filters that influence the strength and
e different sources of informations in the analysis of the brain networks. For example, the work in <ref type="bibr" target="#b176">[177]</ref> integrates infra-slow neural oscillations and anatomical
e different sources of informations in the analysis of the brain networks. For example, the work in <ref type="bibr" target="#b176">[177]</ref> integrates infra-slow neural oscillations and anatomical
n infect any other node (full mixing or complete network). To account for the impact of the network <ref type="bibr" target="#b16">[17]</ref>, resorting to numerical studies is precluded except for ve requency response</p><formula xml:id="formula_33">[h (λ 0 ) • • • h (λ N −1 )]</formula><p>given by <ref type="bibr" target="#b16">(17)</ref>. Finally, an inverse graph Fourier transform computes the
function estimation in a reinforcement learning scenario <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><p>3) Graphical models: The focus in this area is on i
">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> introduced algebraic signal processing (ASP), an axiomatic
are highly related, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type="bibr" target="#b1">(Caruana 1997</ref>) by sharing some meta-knowledge.</p><p>We propose from existing work, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type="bibr" target="#b1">(Caruana 1997</ref>) by sharing some metaknowledge. Intuitively, answe
determine whether the question is unanswerable <ref type="bibr" target="#b3">(Hu et al. 2018;</ref><ref type="bibr" target="#b11">Tan et al. 2018</ref>). However, this kind of approaches usually has n information within the fused information. The self-attention layer is constructed the same way as <ref type="bibr" target="#b11">(Vaswani et al. 2017</ref>):</p><formula xml:id="formula_11">A = A × nsion <ref type="bibr">(Seo et al. 2016;</ref><ref type="bibr" target="#b6">Kumar et al. 2015;</ref><ref type="bibr" target="#b11">Sukhbaatar et al. 2015;</ref><ref type="bibr" target="#b2">Cui et al. ple, cannot effectively deal with the answerability of a question. <ref type="bibr">Hu et al.;</ref><ref type="bibr" target="#b11">Tan et al. (2018;</ref><ref type="bibr">2018</ref>) introduced an ans
achieved great successes for machine reading comprehension <ref type="bibr">(Seo et al. 2016;</ref><ref type="bibr" target="#b6">Kumar et al. 2015;</ref><ref type="bibr" target="#b11">Sukhbaatar et a
6;</ref><ref type="bibr" target="#b10">Shen et al. 2016;</ref><ref type="bibr">Hu et al. 2017;</ref><ref type="bibr" target="#b13">Wang, Yan, and Wu 2018)</ref>. Most of these models consist of three
ning techniques and large-scale benchmarks <ref type="bibr" target="#b2">(Hermann et al. 2015;</ref><ref type="bibr" target="#b3">Hill et al. 2015;</ref><ref type="bibr" target="#b8">Rajpurkar et al. n. Another approach introduces an answer verifier to determine whether the question is unanswerable <ref type="bibr" target="#b3">(Hu et al. 2018;</ref><ref type="bibr" target="#b11">Tan et al. 2018</ t can be boosted with an additional post-processing step to verify answers using approaches such as <ref type="bibr" target="#b3">(Hu et al. 2018)</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/
the question representation. Then we feed c q into the answer pointer to find boundaries of answers <ref type="bibr" target="#b12">(Wang and Jiang 2016)</ref>, and the classification layer to distingu swerable (i.e., the answer is a span in the passage). This layer is a classic pointer net structure <ref type="bibr" target="#b12">(Vinyals, Fortunato, and Jaitly 2015)</ref>. We use two trainable mat anism is very effective to fuse information of the question and passage. Finally, a pointer network <ref type="bibr" target="#b12">(Vinyals, Fortunato, and Jaitly 2015)</ref> is used to predict the sp
the question representation. Then we feed c q into the answer pointer to find boundaries of answers <ref type="bibr" target="#b12">(Wang and Jiang 2016)</ref>, and the classification layer to distingu swerable (i.e., the answer is a span in the passage). This layer is a classic pointer net structure <ref type="bibr" target="#b12">(Vinyals, Fortunato, and Jaitly 2015)</ref>. We use two trainable mat anism is very effective to fuse information of the question and passage. Finally, a pointer network <ref type="bibr" target="#b12">(Vinyals, Fortunato, and Jaitly 2015)</ref> is used to predict the sp
6;</ref><ref type="bibr" target="#b10">Shen et al. 2016;</ref><ref type="bibr">Hu et al. 2017;</ref><ref type="bibr" target="#b13">Wang, Yan, and Wu 2018)</ref>. Most of these models consist of three
the question representation. Then we feed c q into the answer pointer to find boundaries of answers <ref type="bibr" target="#b12">(Wang and Jiang 2016)</ref>, and the classification layer to distingu swerable (i.e., the answer is a span in the passage). This layer is a classic pointer net structure <ref type="bibr" target="#b12">(Vinyals, Fortunato, and Jaitly 2015)</ref>. We use two trainable mat anism is very effective to fuse information of the question and passage. Finally, a pointer network <ref type="bibr" target="#b12">(Vinyals, Fortunato, and Jaitly 2015)</ref> is used to predict the sp
achieved great successes for machine reading comprehension <ref type="bibr">(Seo et al. 2016;</ref><ref type="bibr" target="#b6">Kumar et al. 2015;</ref><ref type="bibr" target="#b11">Sukhbaatar et a
the question representation. Then we feed c q into the answer pointer to find boundaries of answers <ref type="bibr" target="#b12">(Wang and Jiang 2016)</ref>, and the classification layer to distingu swerable (i.e., the answer is a span in the passage). This layer is a classic pointer net structure <ref type="bibr" target="#b12">(Vinyals, Fortunato, and Jaitly 2015)</ref>. We use two trainable mat anism is very effective to fuse information of the question and passage. Finally, a pointer network <ref type="bibr" target="#b12">(Vinyals, Fortunato, and Jaitly 2015)</ref> is used to predict the sp
stic depth for the Transformer inspired by the Stochastic Residual Network for image classification <ref type="bibr" target="#b9">[10]</ref>.</p><p>We discovered that its ability to regularize is the r" target="#b15">[16]</ref>, and thus there are redundant layers. Motivated by the previous work of <ref type="bibr" target="#b9">[10]</ref>, we propose to apply stochastic residual layers into our Tr sub-layers inside). This way we have one hyper-parameter p for each layer.</p><p>• As suggested by <ref type="bibr" target="#b9">[10]</ref>, the lower layers of the networks handle raw-level acoustic
fective, but combined with an LSTM brought marginal improvement and greater interpretability, while <ref type="bibr" target="#b8">[9]</ref> did not find any notable improvement using the Transformer i e Transformer has been applied to ASR with additional TDNN layers to downsample the acoustic signal <ref type="bibr" target="#b8">[9]</ref>. Though self-attention has provided various benefits such as
s. The Transformer model using self-attention achieved the state-of-the-art in mainstream NLP tasks <ref type="bibr" target="#b6">[7]</ref>. The attractiveness of self-attention networks originates fr la xml:id="formula_0">Attention(Q, K, V ) = softmax(QK T )V<label>(1)</label></formula><p>Recently, <ref type="bibr" target="#b6">[7]</ref> improves dot-product attention by scaling the queries before rames into one step. Subsequently we combine the input features with sinusoidal positional encoding <ref type="bibr" target="#b6">[7]</ref>. While directly adding acoustic features to the positional e connection.</p><p>The decoder is the standard Transformer decoder in the recent translation systems <ref type="bibr" target="#b6">[7]</ref>. The notable difference between the decoder and the encoder es around the Base configuration of the machine translation model in the original Transformer paper <ref type="bibr" target="#b6">[7]</ref>. For all of our experiments in this work, the embedding dime
we propose to apply stochastic residual layers into our Transformers. The method resembles Dropout <ref type="bibr" target="#b16">[17]</ref>, in which the key idea is the layers are randomly dropped a><p>Mask M only takes 0 or 1 as values, generated from a Bernoulli distribution similar to dropout <ref type="bibr" target="#b16">[17]</ref>. When M = 1, the inner function F is activated, while it i 6)</ref> in which the init lr is set to 2, and we warm up the learning rate for 8000 steps. Dropout <ref type="bibr" target="#b16">[17]</ref> (applied before residual connection and on the attention w
e the filter-bank features, we did not employ any auxiliary features. We followed the approach from <ref type="bibr" target="#b17">[18]</ref> to generate a speech perturbation training set.</p><p>Extr
we propose to apply stochastic residual layers into our Transformers. The method resembles Dropout <ref type="bibr" target="#b16">[17]</ref>, in which the key idea is the layers are randomly dropped a><p>Mask M only takes 0 or 1 as values, generated from a Bernoulli distribution similar to dropout <ref type="bibr" target="#b16">[17]</ref>. When M = 1, the inner function F is activated, while it i 6)</ref> in which the init lr is set to 2, and we warm up the learning rate for 8000 steps. Dropout <ref type="bibr" target="#b16">[17]</ref> (applied before residual connection and on the attention w
apply character dropout <ref type="bibr" target="#b20">[21]</ref> with p = 0.1 and label smoothing <ref type="bibr" target="#b21">[22]</ref> with = 0.1. The experiment results on SWB testsets are sho

cally, the positional encoding offers a clear advantage compared to learnable positional embeddings <ref type="bibr" target="#b11">[12]</ref>, because the speech signals can be arbitrarily long with a
n fit our model in the GPU, and we accumulate the gradients and update every 25000 characters. Adam <ref type="bibr" target="#b19">[20]</ref> with adaptive learning rate over the training progress: lr

we propose to apply stochastic residual layers into our Transformers. The method resembles Dropout <ref type="bibr" target="#b16">[17]</ref>, in which the key idea is the layers are randomly dropped a><p>Mask M only takes 0 or 1 as values, generated from a Bernoulli distribution similar to dropout <ref type="bibr" target="#b16">[17]</ref>. When M = 1, the inner function F is activated, while it i 6)</ref> in which the init lr is set to 2, and we warm up the learning rate for 8000 steps. Dropout <ref type="bibr" target="#b16">[17]</ref> (applied before residual connection and on the attention w

s. The Transformer model using self-attention achieved the state-of-the-art in mainstream NLP tasks <ref type="bibr" target="#b6">[7]</ref>. The attractiveness of self-attention networks originates fr la xml:id="formula_0">Attention(Q, K, V ) = softmax(QK T )V<label>(1)</label></formula><p>Recently, <ref type="bibr" target="#b6">[7]</ref> improves dot-product attention by scaling the queries before rames into one step. Subsequently we combine the input features with sinusoidal positional encoding <ref type="bibr" target="#b6">[7]</ref>. While directly adding acoustic features to the positional e connection.</p><p>The decoder is the standard Transformer decoder in the recent translation systems <ref type="bibr" target="#b6">[7]</ref>. The notable difference between the decoder and the encoder es around the Base configuration of the machine translation model in the original Transformer paper <ref type="bibr" target="#b6">[7]</ref>. For all of our experiments in this work, the embedding dime
s. The Transformer model using self-attention achieved the state-of-the-art in mainstream NLP tasks <ref type="bibr" target="#b6">[7]</ref>. The attractiveness of self-attention networks originates fr la xml:id="formula_0">Attention(Q, K, V ) = softmax(QK T )V<label>(1)</label></formula><p>Recently, <ref type="bibr" target="#b6">[7]</ref> improves dot-product attention by scaling the queries before rames into one step. Subsequently we combine the input features with sinusoidal positional encoding <ref type="bibr" target="#b6">[7]</ref>. While directly adding acoustic features to the positional e connection.</p><p>The decoder is the standard Transformer decoder in the recent translation systems <ref type="bibr" target="#b6">[7]</ref>. The notable difference between the decoder and the encoder es around the Base configuration of the machine translation model in the original Transformer paper <ref type="bibr" target="#b6">[7]</ref>. For all of our experiments in this work, the embedding dime
re residual connection and on the attention weights) is set at 0.2. We also apply character dropout <ref type="bibr" target="#b20">[21]</ref> with p = 0.1 and label smoothing <ref type="bibr" target="

g the network consists of multiple sub-networks taking different paths through shortcut connections <ref type="bibr" target="#b15">[16]</ref>, and thus there are redundant layers. Motivated by the pre full network is presented, causing the effect of ensembling different sub-networks, as analyzed in <ref type="bibr" target="#b15">[16]</ref>. It is non-trivial regarding how to the parameter p for dr
te a speech perturbation training set.</p><p>Extra experiments are also conducted on the TED-LIUM 3 <ref type="bibr" target="#b18">[19]</ref> dataset which is more challenging due to longer sequences.

eneral suffer from either overfitting due to more complex architectures and optimization difficulty <ref type="bibr" target="#b14">[15]</ref>. Studies about residual networks have shown that during tr
apply character dropout <ref type="bibr" target="#b20">[21]</ref> with p = 0.1 and label smoothing <ref type="bibr" target="#b21">[22]</ref> with = 0.1. The experiment results on SWB testsets are sho
have explored cross-domain generalization ability of recognition models. For example, recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50]</ref> borrow the ideas f ethods such as transfer learning <ref type="bibr" target="#b49">[50]</ref> and adversarial learning <ref type="bibr" target="#b19">[20]</ref> latently generate features of data samples in the target d ns are added. As an example, we evaluate the performance of an adversarial learning based model, EI <ref type="bibr" target="#b19">[20]</ref> over different domain factors (e.g., environment, location ral alternative state-of-the-arts methodologies, CARM <ref type="bibr" target="#b43">[44]</ref>, EI <ref type="bibr" target="#b19">[20]</ref> and CrossSense <ref type="bibr" target="#b49">[50]</ref>, ually adopted to shift the task of separating gesture-related features from domain-related ones. EI <ref type="bibr" target="#b19">[20]</ref> incorporates an adversarial network to obtain domain-indep ="#b52">53]</ref> and developing domain-independent features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref>. In the former type
either statistical <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar
activates all three antennas which are placed in a line. We implement Widar3.0 in MATLAB and Keras <ref type="bibr" target="#b9">[10]</ref>.</p><p>Evaluation setup. To fully explore the performance o
s <ref type="bibr" target="#b44">[45]</ref>, CARM <ref type="bibr" target="#b43">[44]</ref>, WiGest <ref type="bibr" target="#b0">[1]</ref> and WIMU <ref type="bibr" target="#b37">[38]</ref> have been
ce and virtual reality. Traditional approaches use cameras <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>, wearable devices a
get="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" tar strength distribution of commercial Wi-Fi signals and KNN to recognize human activities. Niu et al. <ref type="bibr" target="#b29">[30]</ref> uses signal waveforms for fine-grained gesture recognition
="bibr" target="#b41">42]</ref>, wearable devices and phones <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref> or sonar <ref type=
tensively studied, Widar3.0 can exploit existing sophisticated passive tracking systems, e.g., LiFS <ref type="bibr" target="#b40">[41]</ref>, IndoTrack <ref type="bibr" target="#b25">[26]</ref> and W y tracking systems <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref>, as the location and orientation of the user in Widar3.0. N ref><ref type="bibr" target="#b24">25]</ref> and attenuation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref>. Based on types of devices used, parameters with different rks only tracks coarse human motion status, such as location <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>, velocity <ref type="bibr" target="#b25">[26,</ref><ref typ
"bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>, wearable devices and phones <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" targ
ef><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> and attenuation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref>. Based on types of d
ing approaches extract signal features, either statistical <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar


lied to sequences for decades <ref type="bibr" target="#b61">(Sejnowski &amp; Rosenberg, 1987;</ref><ref type="bibr" target="#b30">Hinton, 1989)</ref>. They were used prominently for speech recognitio



ks have aimed to combine aspects of RNN and CNN architectures. This includes the Convolutional LSTM <ref type="bibr" target="#b62">(Shi et al., 2015)</ref>, which replaces the fully-connected layers i

the canonical textbook on deep learning is titled "Sequence Modeling: Recurrent and Recursive Nets" <ref type="bibr" target="#b22">(Goodfellow et al., 2016)</ref>, capturing the common association of (El Hihi &amp; Bengio, 1995;</ref><ref type="bibr" target="#b60">Schuster &amp; Paliwal, 1997;</ref><ref type="bibr" target="#b22">Gers et al., 2002;</ref><ref type="bibr" target="#b40">Koutnik et al.
rget="#b57">Pascanu et al., 2014;</ref><ref type="bibr" target="#b35">Jozefowicz et al., 2015;</ref><ref type="bibr" target="#b74">Zhang et al., 2016)</ref>. This indicates that the recent successes o r" target="#b43">Le et al., 2015;</ref><ref type="bibr" target="#b35">Jozefowicz et al., 2015;</ref><ref type="bibr" target="#b74">Zhang et al., 2016)</ref>.</p></div> <div xmlns="http://www.tei-c.org r" target="#b43">Le et al., 2015;</ref><ref type="bibr" target="#b35">Jozefowicz et al., 2015;</ref><ref type="bibr" target="#b74">Zhang et al., 2016)</ref>. The intention is to conduct the evaluation bibr" target="#b43">Le et al., 2015;</ref><ref type="bibr" target="#b1">Arjovsky et al., 2016;</ref><ref type="bibr" target="#b74">Zhang et al., 2016)</ref>.</p><p>Sequential MNIST and P-MNIST. Sequen so found that "none of the variants can improve upon the standard LSTM architecture significantly". <ref type="bibr" target="#b74">Zhang et al. (2016)</ref> systematically analyzed the connecting arch y to retain information from the distant past <ref type="bibr" target="#b43">(Le et al., 2015;</ref><ref type="bibr" target="#b74">Zhang et al., 2016;</ref><ref type="bibr" target="#b69">Wisdom et al. s are dominated by recurrent architectures, with many specialized designs developed for these tasks <ref type="bibr" target="#b74">(Zhang et al., 2016;</ref><ref type="bibr" target="#b27">Ha et al., 2
ectures are notoriously difficult to train <ref type="bibr" target="#b4">(Bengio et al., 1994;</ref><ref type="bibr" target="#b56">Pascanu et al., 2013)</ref> and more elaborate architectures are comm peatedly as a stress test for sequence models <ref type="bibr">(Martens &amp; Sutskever, 2011;</ref><ref type="bibr" target="#b56">Pascanu et al., 2013;</ref><ref type="bibr" target="#b43">Le et al.,
based merely on implicit feedbacks, i.e., user clicks, in the current session.</p><p>Hidasi et al. <ref type="bibr" target="#b11">[12]</ref> apply recurrent neural networks (RNN) with Gated Recurrent al Networks (RNN) have been devised to model variable-length sequence data. Recently, Hidasi et al. <ref type="bibr" target="#b11">[12]</ref> apply RNN to sessionbased recommendation and achieve signi ARM. We use a RNN with Gated Recurrent Units (GRU) rather than a standard RNN because Hidasi et al. <ref type="bibr" target="#b11">[12]</ref> demonstrate that GRU can outperform the Long Short-Term Me re S i , To learn the parameters of the model, we do not utilize the proposed training procedure in <ref type="bibr" target="#b11">[12]</ref>, where the model is trained in a session-parallel, sequenc sessions of subsequent week for testing. Because we did not train NARM in a session-parallel manner <ref type="bibr" target="#b11">[12]</ref>, a </p><formula xml:id="formula_15">],V (x 2 ), ([x 1 , x nt representations when computing recommendation scores. • GRU-Rec: We denote the model proposed in <ref type="bibr" target="#b11">[12]</ref> as GRU-Rec, which utilizes session-parallel mini-batch tra
erarchical softmax layer <ref type="bibr" target="#b23">[24]</ref>, and negative sampling at random <ref type="bibr" target="#b21">[22]</ref>, they are not the best choices for our model.</p><p>We pro
bibr" target="#b12">13]</ref> and neural language processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar
"bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref> and neural language processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targ
bibr" target="#b12">13]</ref> and neural language processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar
r" target="#b41">42]</ref> cannot perform well. To tackle this problem, sessionbased recommendation <ref type="bibr" target="#b32">[33]</ref> is proposed to predict the next item that the user is prob
t item embeddings. We adopt the dynamic setting in our model, more details will be described in §3. <ref type="bibr" target="#b3">4</ref>.</p><p>The basic idea of our work is to learn a recommendation egularization is also included to avoid coincidental high similarities between rarely visited items <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. • BPR-MF: BPR-MF <r
no explicit preferences (e.g., ratings) but only positive observations (e.g., clicks) are available <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar
hough there are some approaches to reduce the parameters such as using a hierarchical softmax layer <ref type="bibr" target="#b23">[24]</ref>, and negative sampling at random <ref type="bibr" target="
rget="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> cannot perform well. To tackle this problem, sessionbased r
n="3.2">Overview</head><p>In this paper, we propose an improved neural encoder-decoder architecture <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref> to address the ses
e queue was extensively studied around 2000. A comprehensive survey was carried out by Abella et al <ref type="bibr" target="#b23">[28]</ref>.</p><p>Butler et al. investigated the effect of several se
o-back.</p><p>There are two types of wakeup logic circuits: content addressable memory (CAM) or RAM <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr" target="#b5">[7]</ref>. In the CAM type, t request is output.</p><p>In contrast, the RAM type has two matrices for each of two source operands <ref type="bibr" target="#b3">[5]</ref>. Each row and column of the matrix is associated with an ins
hat predicts the criticality of an instruction, including the consideration of branch misprediction <ref type="bibr" target="#b25">[30]</ref>. However, the scheme is difficult to implement because of
branches, and forking the slice from the original thread as a helper thread in a different context <ref type="bibr" target="#b26">[31]</ref>, <ref type="bibr" target="#b27">[32]</ref>. The precomputa
is significant, the wire delay is difficult to reduce in the modern fine LSI technology in general <ref type="bibr" target="#b21">[24]</ref>. To find the increase of the delay, we designed the IQ (th ies, the wire delay increases significantly as the generation of LSI technology advances in general <ref type="bibr" target="#b21">[24]</ref>, <ref type="bibr" target="#b22">[27]</ref>, which is a lon
up logic circuits: content addressable memory (CAM) or RAM <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr" target="#b5">[7]</ref>. In the CAM type, the wakeup logic is a one-dimensional arra ue.</p><p>To mitigate IPC degradation in the random queue, there is a circuit called the age matrix <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b9">[11]</ref>. The age matrix is rms of IPC.</p><p>Each row and column of the age matrix is associated with an instruction in the IQ <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b9">[11]</ref>. Each cell of the r" target="#b9">[11]</ref>. A different but essentially the same age matrix circuit is presented in <ref type="bibr" target="#b5">[7]</ref>. D-BP (see "GM diff"), although it is slightly more than tha es transposed issue request lines for a group of instructions to reduce the width of the age matrix <ref type="bibr" target="#b5">[7]</ref>. The downside of this scheme is that it still requires an ar was proposed in <ref type="bibr" target="#b9">[11]</ref>, while a similar circuit was presented in <ref type="bibr" target="#b5">[7]</ref>. Although the age matrix increases IPC, the delay of the IQ
riginal thread as a helper thread in a different context <ref type="bibr" target="#b26">[31]</ref>, <ref type="bibr" target="#b27">[32]</ref>. The precomputation thread forwards the branch outcome to
hat predicts the criticality of an instruction, including the consideration of branch misprediction <ref type="bibr" target="#b25">[30]</ref>. However, the scheme is difficult to implement because of
riginal thread as a helper thread in a different context <ref type="bibr" target="#b26">[31]</ref>, <ref type="bibr" target="#b27">[32]</ref>. The precomputation thread forwards the branch outcome to
o-back.</p><p>There are two types of wakeup logic circuits: content addressable memory (CAM) or RAM <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr" target="#b5">[7]</ref>. In the CAM type, t request is output.</p><p>In contrast, the RAM type has two matrices for each of two source operands <ref type="bibr" target="#b3">[5]</ref>. Each row and column of the matrix is associated with an ins
ce per unit length of the wire predicted by the International Technology Roadmap for Semiconductors <ref type="bibr" target="#b22">[27]</ref>. Drivers and repeaters were optimally inserted on long wir as the generation of LSI technology advances in general <ref type="bibr" target="#b21">[24]</ref>, <ref type="bibr" target="#b22">[27]</ref>, which is a long and firm trend in LSI technology.</p></di
ibr" target="#b6">(Levy and Goldberg, 2014</ref>)), and the Noise Contrastive Estimation methods of <ref type="bibr" target="#b9">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b4">Jozefowicz et pled per training example):</p><p>• For any K 1, a binary classification variant of NCE, as used by <ref type="bibr" target="#b9">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b8">Mikolov et al. n square error as the MLE) as K ! 1.</p><p>• We discuss application of our results to approaches of <ref type="bibr" target="#b9">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b8">Mikolov et al. history x. This is the most straightforward extension of NCE to the conditional case; it is used by <ref type="bibr" target="#b9">(Mnih and Teh, 2012)</ref>. It has the clear drawback however of intro o motivate the importance of the two algorithms, we now discuss their application in previous work. <ref type="bibr" target="#b9">Mnih and Teh (2012)</ref> consider language modeling, where x = w 1 w n of the parameters c</p><p>x corresponding to normalization terms for each history. Interestingly, <ref type="bibr" target="#b9">Mnih and Teh (2012)</ref> acknowledge the difficulties in maintaining
f type="bibr" target="#b1">(Berger et al., 1996)</ref>, and later work on conditional random fields <ref type="bibr" target="#b5">(Lafferty et al., 2001)</ref>, the scoring function s(x, y; ✓) = ✓ • f
6)</ref> partially motivate the ranking-based variant throught the importance sampling viewpoint of <ref type="bibr" target="#b0">Bengio and Senécal (2008)</ref>. However there are two critical differ 0">Bengio and Senécal (2008)</ref>. However there are two critical differences: 1) the algorithm of <ref type="bibr" target="#b0">Bengio and Senécal (2008)</ref> does not lead to the same objective L
on binary classification or ranking. Prominent examples are the binary objective used in word2vec ( <ref type="bibr" target="#b8">(Mikolov et al., 2013)</ref>, see also <ref type="bibr" target="#b6">( s not vary with x.</p><p>Levy and Goldberg (2014) make a connection between the NCE-based method of <ref type="bibr" target="#b8">(Mikolov et al., 2013)</ref>, and factorization of a matrix of pointwi y classification variant of NCE, as used by <ref type="bibr" target="#b9">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b8">Mikolov et al., 2013)</ref>, gives consistent parameter estimates unde application of our results to approaches of <ref type="bibr" target="#b9">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b8">Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Levy and Gold d set c x = 0 for all x, noting that empirically this works well, but without giving justification. <ref type="bibr" target="#b8">Mikolov et al. (2013)</ref> consider an NCE-based method using the bin
two-layer LSTMs and the parameters are estimated by MLE (note that the current state-of-the-art is <ref type="bibr" target="#b10">(Yang et al., 2018)</ref>). <ref type="bibr" target="#b11">Zaremba et
y objective used in word2vec ( <ref type="bibr" target="#b8">(Mikolov et al., 2013)</ref>, see also <ref type="bibr" target="#b6">(Levy and Goldberg, 2014</ref>)), and the Noise Contrastive Estimation br" target="#b9">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b8">Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b4">Jozefowicz ) or equivalently PMI(x, y) = log p(y|x) p(y) = v 0 y • v x log H(✓)</formula><p>That is, following <ref type="bibr" target="#b6">(Levy and Goldberg, 2014)</ref>, the inner product v 0 y • v</p><p>x i
this form in NLP. In log-linear models, including both the original work on maximum-entropy models <ref type="bibr" target="#b1">(Berger et al., 1996)</ref>, and later work on conditional random fiel
y objective used in word2vec ( <ref type="bibr" target="#b8">(Mikolov et al., 2013)</ref>, see also <ref type="bibr" target="#b6">(Levy and Goldberg, 2014</ref>)), and the Noise Contrastive Estimation br" target="#b9">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b8">Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b4">Jozefowicz ) or equivalently PMI(x, y) = log p(y|x) p(y) = v 0 y • v x log H(✓)</formula><p>That is, following <ref type="bibr" target="#b6">(Levy and Goldberg, 2014)</ref>, the inner product v 0 y • v</p><p>x i
nce of the two NCE algorithms on a language modeling problem, using the Penn Treebank (PTB) dataset <ref type="bibr" target="#b7">(Marcus et al., 1993)</ref>. We choose <ref type="bibr" target="#b11">
f type="bibr" target="#b1">(Berger et al., 1996)</ref>, and later work on conditional random fields <ref type="bibr" target="#b5">(Lafferty et al., 2001)</ref>, the scoring function s(x, y; ✓) = ✓ • f
nce of the two NCE algorithms on a language modeling problem, using the Penn Treebank (PTB) dataset <ref type="bibr" target="#b7">(Marcus et al., 1993)</ref>. We choose <ref type="bibr" target="#b11">
through which they occur, are important and challenging problems that have attracted much attention <ref type="bibr" target="#b9">[10]</ref>. This paper focuses on predicting protein interfaces. Despi ppears to be saturated. This calls for new methodologies or sources of information to be exploited" <ref type="bibr" target="#b9">[10]</ref>. Most machine learning methods for interface prediction use
elded the best performance in our validation experiments. We implemented our networks in TensorFlow <ref type="bibr" target="#b0">[1]</ref> v1.0.1 to make use of rapid training on GPUs. Training times
; the feature representation of an example is the concatenation of the features of the two residues <ref type="bibr" target="#b2">[3]</ref>. In preliminary experiments both approaches yielded similar
d inspire our implementation.</p><p>In their Molecular Fingerprint Networks (MFNs), Duvenaud et al. <ref type="bibr" target="#b8">[9]</ref> proposed a spatial graph convolution approach similar to Equ (DCNN) (5 hops) <ref type="bibr" target="#b4">[5]</ref>) 0.828 (0.018) ---Single Weight Matrix (MFN <ref type="bibr" target="#b8">[9]</ref>) 0.865 (0.007) 0.871 (0.013) 0.873 (0.017) 0.869 (0.017) Nod
hese techniques applicable to the wide range of prediction problems that can be modeled in this way <ref type="bibr" target="#b7">[8]</ref>.</p><p>In this work we propose a graph convolution approach convolution over a regular grid. Spectral graph theory forms the basis for several of these methods <ref type="bibr" target="#b7">[8]</ref>, in which convolutional filters are viewed as linear operato
hese techniques applicable to the wide range of prediction problems that can be modeled in this way <ref type="bibr" target="#b7">[8]</ref>.</p><p>In this work we propose a graph convolution approach convolution over a regular grid. Spectral graph theory forms the basis for several of these methods <ref type="bibr" target="#b7">[8]</ref>, in which convolutional filters are viewed as linear operato
viewed as linear operators on the eigenvectors of the graph Laplacian (or an approximation thereof <ref type="bibr" target="#b12">[13]</ref>). Our protein dataset consists of multiple graphs with no
r problem, and without it, Equation (1) closely resembles their convolution operator. Schütt et al. <ref type="bibr" target="#b20">[21]</ref> define Deep Tensor Neural Networks (DTNNs) for predicting "formula" target="#formula_1">2</ref>) 0.876 (0.005) 0.898 (0.005) 0.895 (0.006) 0.889 (0.007) DTNN <ref type="bibr" target="#b20">[21]</ref> 0.867 (0.007) 0.880 (0.007) 0.882 (0.008) 0.873 (0.012) Or
of challenging machine learning tasks from computer vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> and speech recognition <ref type="bibr" target="#b11">[12]< scribe protein structure. In classifying image data, CNNs are usually applied to the raw pixel data <ref type="bibr" target="#b14">[15]</ref>. The analogous level of description for protein structure
sefulness in classification tasks different than the ones they were originally trained on (see e.g. <ref type="bibr" target="#b21">[22]</ref>). Similarly, we expect the convolution operators we propos
set, which is the standard benchmark dataset for assessing docking and interface prediction methods <ref type="bibr" target="#b24">[25]</ref>. These complexes are a carefully selected subset of struct
target="#b13">[14]</ref>, which was shown to be useful in large-scale conditional generation tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">34]</ref>.</p><p>The model-agn "#b17">[19]</ref>, but replace downsampling and upsampling layers with residual blocks similarly to <ref type="bibr" target="#b5">[6]</ref> (with batch normalization [15] replaced by instance normaliz tional and fully connected layers in all the networks. We also use self-attention blocks, following <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b40">[42]</ref>. They are inse
valuation: VoxCeleb1 <ref type="bibr" target="#b24">[26]</ref> (256p videos at 1 fps) and VoxCeleb2 <ref type="bibr" target="#b7">[8]</ref>  than the former. VoxCeleb1 is used for comparison with base
or multiple static frames. Both classical warping algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">28]</ref> and warping fields synthesized using machine learning (incl
or multiple static frames. Both classical warping algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">28]</ref> and warping fields synthesized using machine learning (incl
ted head sequences by warping a single or multiple static frames. Both classical warping algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">28]</ref> and warping fields s
the perceptual similarity measure <ref type="bibr" target="#b17">[19]</ref>, corresponding to VGG19 <ref type="bibr" target="#b28">[30]</ref> network trained for ILSVRC classification and VGGFace <ref
lism and identity preservation of generated images. Namely, we use Frechet-inception distance (FID) <ref type="bibr" target="#b12">[13]</ref>, mostly measuring perceptual realism, structured similarit
even minor mistakes in the appearance modeling of human heads (the so-called uncanny valley effect <ref type="bibr" target="#b22">[24]</ref>). Such low tolerance to modeling mistakes explains the cur
b5">[6,</ref><ref type="bibr" target="#b32">34]</ref>.</p><p>The model-agnostic meta-learner (MAML) <ref type="bibr" target="#b9">[10]</ref> uses meta-learning to obtain the initial state of an image
alism score computed by the discriminator, which needs to be maximized, and a feature matching term <ref type="bibr" target="#b36">[38]</ref>, which essentially is a perceptual similarity measure, com our model against two other systems: X2Face <ref type="bibr" target="#b38">[40]</ref> and Pix2pixHD <ref type="bibr" target="#b36">[38]</ref>. For X2Face, we have used the model, as well as pretrained
alism score computed by the discriminator, which needs to be maximized, and a feature matching term <ref type="bibr" target="#b36">[38]</ref>, which essentially is a perceptual similarity measure, com our model against two other systems: X2Face <ref type="bibr" target="#b38">[40]</ref> and Pix2pixHD <ref type="bibr" target="#b36">[38]</ref>. For X2Face, we have used the model, as well as pretrained
st Meltdown, a different speculative execution vulnerability <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref>. PTI introduced a new trampoline used during system calls t
ndirect calls into conditional direct calls. Indirect call promotion is often employed by compilers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> to take advantage of p ocessor will mis-speculate an indirect call using a code fragment similar to that shown in Figure 2 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Promoting indir
get="#b10">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Collection of targ
x.</p><p>While conditional direct calls are vulnerable to Spectre variant 1 ("Bounds Check Bypass") <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta e variant 2, known as "Branch Target Injection" specifically targeted indirect branches and the BTB <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>. In variant 2, eit 2, as stated by Intel <ref type="bibr" target="#b13">[14]</ref> and the original Spectre disclosure <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>It is also note that while retpolines can serve as a defense against Spectre variant 1 ("Bounds Check Bypass") <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta
r solution was introduced by Abeni to reduce the overhead of indirect branches in the network stack <ref type="bibr" target="#b1">[2]</ref>. This solution is also static, requiring the developer to de
]</ref>), preventing a direct jump. Unfortunately, the resulting indirect jump must use a retpoline <ref type="bibr" target="#b51">[52]</ref> on Spectre vulnerable hardware, resulting in lower than ex
el architectures before Skylake, speculation for RET instructions were always computed from the RSB <ref type="bibr" target="#b13">[14]</ref>. On Skylake and after however, the contents of the BTB may BRS), Single Thread Indirect Branch Predictors (STIBP) and Indirect Branch Predictor Barrier (IBPB) <ref type="bibr" target="#b13">[14]</ref>. IBRS works by defining four privilege levels: host and gu anced IBRS) <ref type="bibr" target="#b15">[16]</ref> and control flow enforcement technology (CET) <ref type="bibr" target="#b13">[14]</ref>. Enhanced IBRS eliminates the overhead of IBRS by removing ume that only indirect branches and returns are vulnerable to Spectre variant 2, as stated by Intel <ref type="bibr" target="#b13">[14]</ref> and the original Spectre disclosure <ref type="bibr" targe
.tei-c.org/ns/1.0"><head n="2.3">Alternative Solutions</head><p>After we released parts of our code <ref type="bibr" target="#b3">[4]</ref>, several solutions that employ indirect call promotion have
analysis and selective masking/serialization are typically used to defend against Spectre variant 1 <ref type="bibr" target="#b11">[12]</ref>. Notably, future "Spectre safe" hardware which do not need In Linux, static analysis and selective masking/serialization is used to defend against Spectre v1 <ref type="bibr" target="#b11">[12]</ref>, since Spectre variant 1 defenses are still necessary even
r solution was introduced by Abeni to reduce the overhead of indirect branches in the network stack <ref type="bibr" target="#b1">[2]</ref>. This solution is also static, requiring the developer to de
><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> and JIT compilers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" ta


el architecture that achieves 2.89% test error, which is on par with the 2.65% test error of NASNet <ref type="bibr" target="#b48">(Zoph et al., 2018)</ref>.</p></div> 			</abstract> 		</profileDesc> volutional network, one can design smaller modules and then connect them together to form a network <ref type="bibr" target="#b48">(Zoph et al., 2018)</ref>. Figure <ref type="figure" target="#fig_2"> " target="#fig_2">4</ref>), ENAS achieves 3.54% test error, on par with the 3.41% error of NASNet-A <ref type="bibr" target="#b48">(Zoph et al., 2018)</ref>. With CutOut (DeVries &amp; Taylor, 2017), image classification and language models <ref type="bibr" target="#b46">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b48">Zoph et al., 2018;</ref><ref type="bibr" target="#b5">Cai et al., 201 ed validation set, as done by other works <ref type="bibr" target="#b46">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b48">Zoph et al., 2018;</ref><ref type="bibr" target="#b26">Liu et al., 20 ite its impressive empirical performance, NAS is computationally expensive and time consuming, e.g. <ref type="bibr" target="#b48">Zoph et al. (2018)</ref> use 450 GPUs for 3-4 days <ref type="bibr">( of 2. A reduction cell thus reduces the spatial dimensions of its input by a factor of 2. Following <ref type="bibr" target="#b48">Zoph et al. (2018)</ref>, we sample the reduction cell conditioned on s similar to dropout with a rate of 0.5 on the skip connections, and to drop-path on the operations <ref type="bibr" target="#b48">(Zoph et al., 2018;</ref><ref type="bibr" target="#b24">Larsson et al
l on a particular task can be used for other models on other tasks, with little to no modifications <ref type="bibr" target="#b35">(Razavian et al., 2014;</ref><ref type="bibr" target="#b47">Zoph et a
representing computations using a DAG is inspired by the concept of stochastic computational graph <ref type="bibr" target="#b39">(Schulman et al., 2015)</ref>, which introduces nodes with stochastic
th filter sizes 3 × 3 and 5 × 5, depthwise-separable convolutions with filter sizes 3 × 3 and 5 × 5 <ref type="bibr" target="#b6">(Chollet, 2017)</ref>, and max pooling and average pooling of kernel s
head><p>The decision of what previous nodes to connect to allows the model to form skip connections <ref type="bibr" target="#b15">(He et al., 2016a;</ref><ref type="bibr" target="#b46">Zoph &amp; Le,
th filter sizes 3 × 3 and 5 × 5, depthwise-separable convolutions with filter sizes 3 × 3 and 5 × 5 <ref type="bibr" target="#b6">(Chollet, 2017)</ref>, and max pooling and average pooling of kernel s
ed by human experts.</p><p>When trained with a strong regularization technique, such as Shake-Shake <ref type="bibr" target="#b11">(Gastaldi, 2016)</ref>, and a data augmentation technique, such as Cu

ections, and to drop-path on the operations <ref type="bibr" target="#b48">(Zoph et al., 2018;</ref><ref type="bibr" target="#b24">Larsson et al., 2017)</ref>. At convergence, the model has the error
2017)</ref>. Each architecture search is run for 310 epochs. We initialize ω with He initialization <ref type="bibr" target="#b14">(He et al., 2015)</ref>. We also apply an 2 weight decay of 10 −4 . W

ed by human experts.</p><p>When trained with a strong regularization technique, such as Shake-Shake <ref type="bibr" target="#b11">(Gastaldi, 2016)</ref>, and a data augmentation technique, such as Cu
sing ideas of using performance prediction <ref type="bibr" target="#b1">(Baker et al., 2017b;</ref><ref type="bibr" target="#b8">Deng et al., 2017)</ref>, using iterative search method for architectu
/1.0"><head n="3.1.">Language Model with Penn Treebank</head><p>Dataset and Settings. Penn Treebank <ref type="bibr" target="#b30">(Marcus et al., 1994</ref>) is a well-studied benchmark for language
59.5 LSTM <ref type="bibr" target="#b42">(Yang et al., 2018)</ref> VD, WT, 2, AWD, MoC 22 57.6 LSTM <ref type="bibr" target="#b32">(Merity et al., 2017)</ref> VD, WT, 2, AWD 24 57.3 LSTM <ref type="bi
59.5 LSTM <ref type="bibr" target="#b42">(Yang et al., 2018)</ref> VD, WT, 2, AWD, MoC 22 57.6 LSTM <ref type="bibr" target="#b32">(Merity et al., 2017)</ref> VD, WT, 2, AWD 24 57.3 LSTM <ref type="bi
br" target="#b46">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b48">Zoph et al., 2018;</ref><ref type="bibr" target="#b5">Cai et al., 2018;</ref><ref type="bibr" target="#b26">Liu et al., 2017
head><p>The decision of what previous nodes to connect to allows the model to form skip connections <ref type="bibr" target="#b15">(He et al., 2016a;</ref><ref type="bibr" target="#b46">Zoph &amp; Le,

l on a particular task can be used for other models on other tasks, with little to no modifications <ref type="bibr" target="#b35">(Razavian et al., 2014;</ref><ref type="bibr" target="#b47">Zoph et a
e number of MAC operations by directly pruning the CNN model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref>. Unimportant weights ar GA without further information disclosed on resource utilization and working frequency. The work of <ref type="bibr" target="#b1">[2]</ref> presented an algorithm-hardware codesign scheme to improve t
><p>The third type of designs reduce the number of MAC operations by directly pruning the CNN model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target igns.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Study of <ref type="bibr" target="#b0">[1]</ref> presented an energy-efficient accelerator that deployed spar
pruning the CNN model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref>. Unimportant weights are forced to zero during the training ( el evaluated was of low complexity (0.44 GOP) and the performance achieved was only 31.79 GOP/s. In <ref type="bibr" target="#b7">[8]</ref>, the authors reported an design framework which mapped spars
existing designs can be divided into three major categories. The first category of designs, such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ r achieved by FPGA accelerator rather than the whole system in the following discussion. Designs of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ e achieve considerably 3.8? improvement in throughput when normalized by frequency. To compare with <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref> and <ref type="bi
pruning the CNN model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref>. Unimportant weights are forced to zero during the training ( el evaluated was of low complexity (0.44 GOP) and the performance achieved was only 31.79 GOP/s. In <ref type="bibr" target="#b7">[8]</ref>, the authors reported an design framework which mapped spars
pruning the CNN model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref>. Unimportant weights are forced to zero during the training ( el evaluated was of low complexity (0.44 GOP) and the performance achieved was only 31.79 GOP/s. In <ref type="bibr" target="#b7">[8]</ref>, the authors reported an design framework which mapped spars
existing designs can be divided into three major categories. The first category of designs, such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ r achieved by FPGA accelerator rather than the whole system in the following discussion. Designs of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ e achieve considerably 3.8? improvement in throughput when normalized by frequency. To compare with <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref> and <ref type="bi
><p>The third type of designs reduce the number of MAC operations by directly pruning the CNN model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target igns.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Study of <ref type="bibr" target="#b0">[1]</ref> presented an energy-efficient accelerator that deployed spar
pruning the CNN model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref>. Unimportant weights are forced to zero during the training ( el evaluated was of low complexity (0.44 GOP) and the performance achieved was only 31.79 GOP/s. In <ref type="bibr" target="#b7">[8]</ref>, the authors reported an design framework which mapped spars
200 MHz. (each DSP can perform two 16/8-bit fixedpoint MACs).</p><p>The second category of designs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> perform convolution i </ref><ref type="bibr" target="#b12">13]</ref> are based on spatial convolution, while the works of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> use frequency domain is the reduction rate in MAC operation. For instance, up to 69.2% of the MAC operation is saved in <ref type="bibr" target="#b2">[3]</ref>, resulting in a theoretical speedup of 3.3? in peak performa ]</ref>, resulting in a theoretical speedup of 3.3? in peak performance. The throughput achieved by <ref type="bibr" target="#b2">[3]</ref> on a Intel HARP platform is 669.1 GOP/s, which is very close -based ones. The performance of the reported SpConv-based FPGA accelerators have not exceed that of <ref type="bibr" target="#b2">[3]</ref>.</p><p>From an architecture view, existing FPGA accelerators l operations (accumulate and multiply) is saved compared to SDConv, while the reduction over FDConv <ref type="bibr" target="#b2">[3]</ref> and SpConv <ref type="bibr" target="#b6">[7]</ref> are 47.1% ><ref type="bibr" target="#b9">10]</ref> use frequency domain convolution.</p><p>The latest work of <ref type="bibr" target="#b2">[3]</ref> uses a frequency domain convolution scheme which gains 3.3? duction rate of 3.06?. The implemented accelerator achieves 1.55? speedup in throughput compared to <ref type="bibr" target="#b2">[3]</ref> as a result of being able to utilize 1.6? accumulators to ac r scheme quantizes the CNN model in 8-bit, the precision of the datapath is of the same (16-bit) as <ref type="bibr" target="#b2">[3]</ref>. For AlexNet, the pruning scheme adopted by us only reduces pruning scheme adopted by us only reduces the total MAC operations by 2.3? (30%  lower than that of <ref type="bibr" target="#b2">[3]</ref>), but our scheme still improves the inference throughput by
200 MHz. (each DSP can perform two 16/8-bit fixedpoint MACs).</p><p>The second category of designs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> perform convolution i </ref><ref type="bibr" target="#b12">13]</ref> are based on spatial convolution, while the works of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> use frequency domain is the reduction rate in MAC operation. For instance, up to 69.2% of the MAC operation is saved in <ref type="bibr" target="#b2">[3]</ref>, resulting in a theoretical speedup of 3.3? in peak performa ]</ref>, resulting in a theoretical speedup of 3.3? in peak performance. The throughput achieved by <ref type="bibr" target="#b2">[3]</ref> on a Intel HARP platform is 669.1 GOP/s, which is very close -based ones. The performance of the reported SpConv-based FPGA accelerators have not exceed that of <ref type="bibr" target="#b2">[3]</ref>.</p><p>From an architecture view, existing FPGA accelerators l operations (accumulate and multiply) is saved compared to SDConv, while the reduction over FDConv <ref type="bibr" target="#b2">[3]</ref> and SpConv <ref type="bibr" target="#b6">[7]</ref> are 47.1% ><ref type="bibr" target="#b9">10]</ref> use frequency domain convolution.</p><p>The latest work of <ref type="bibr" target="#b2">[3]</ref> uses a frequency domain convolution scheme which gains 3.3? duction rate of 3.06?. The implemented accelerator achieves 1.55? speedup in throughput compared to <ref type="bibr" target="#b2">[3]</ref> as a result of being able to utilize 1.6? accumulators to ac r scheme quantizes the CNN model in 8-bit, the precision of the datapath is of the same (16-bit) as <ref type="bibr" target="#b2">[3]</ref>. For AlexNet, the pruning scheme adopted by us only reduces pruning scheme adopted by us only reduces the total MAC operations by 2.3? (30%  lower than that of <ref type="bibr" target="#b2">[3]</ref>), but our scheme still improves the inference throughput by
r" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" tar evel power analysis <ref type="bibr" target="#b20">[21]</ref>, or a per-control-step ML power model <ref type="bibr" target="#b13">[14]</ref> for power estimation.</p><p>Compared to behavioral-level a
get="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. These eorts typica le power of the whole design can be obtained by summing up the outputs from multiple models. PrEsto <ref type="bibr" target="#b22">[23]</ref> uses linear models to characterize larger modules, where h
seline. We also experiment with gradient tree boosting, a promising non-linear regression technique <ref type="bibr" target="#b16">[17]</ref>. For linear models and gradient tree boosting models, we a
level. There exists a rich body of research on power analysis at RTL or a higher abstraction level <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target er eorts use the HLS tool to perform scheduling and back-annotation, and rely on RTL power analysis <ref type="bibr" target="#b2">[3]</ref>, gate-level power analysis <ref type="bibr" target="#b20">[2
" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targe r estimation provides optimization guidance early in the design ow. In an earlier work, Chen et al. <ref type="bibr" target="#b6">[7]</ref> combine proling and simple analytical models to estimate FPG
arget="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar , and rely on RTL power analysis <ref type="bibr" target="#b2">[3]</ref>, gate-level power analysis <ref type="bibr" target="#b20">[21]</ref>, or a per-control-step ML power model <ref type="bibr" tar
gher abstraction level <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target= n and regression trees, to characterize small circuit blocks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>. The regression model
d using Keras <ref type="bibr" target="#b1">[2]</ref>. Other ML models are realized in scikit-learn <ref type="bibr" target="#b18">[19]</ref> and XGBoost <ref type="bibr" target="#b7">[8]</ref>. We co
ed register mapping is shown in Figure <ref type="figure" target="#fig_5">5b</ref>. We use node2vec <ref type="bibr" target="#b9">[10]</ref> for node embedding, then apply PCA <ref type="bibr" target= " target="#b10">[11]</ref>, metis <ref type="bibr" target="#b12">[13]</ref>, and a node2vec package <ref type="bibr" target="#b9">[10]</ref>. MLP and CNN models are implemented using Keras <ref type="
Other ML models are realized in scikit-learn <ref type="bibr" target="#b18">[19]</ref> and XGBoost <ref type="bibr" target="#b7">[8]</ref>. We conduct our experiments on a server with an Intel Xeon E
" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targe r estimation provides optimization guidance early in the design ow. In an earlier work, Chen et al. <ref type="bibr" target="#b6">[7]</ref> combine proling and simple analytical models to estimate FPG
-to-video generation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref> to text-to-video generation <ref type="bibr" target="#b22">[2
"#b22">[23,</ref><ref type="bibr" target="#b18">19]</ref> and to skeleton-to-image/video generation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref>. This paper consider
aches has been mainly limited to synthesizing a talking face from speech audio of a specific person <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" targ
"><head n="3.3.">Regression-Based Discriminator</head><p>Recently, people find that perceptual loss <ref type="bibr" target="#b15">[16]</ref> is helpful for generating sharp images in GAN/VAE <ref typ
type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref> to text-to-video generation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19]</ref> and to skeleton-to
zing a talking face from speech audio of a specific person <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29]</ref>. For example, Suwajan
"><head n="3.3.">Regression-Based Discriminator</head><p>Recently, people find that perceptual loss <ref type="bibr" target="#b15">[16]</ref> is helpful for generating sharp images in GAN/VAE <ref typ
of a specific person <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29]</ref>. For example, Suwajanakorn et al. <ref type="bibr" target=" br" target="#b8">9,</ref><ref type="bibr" target="#b28">29]</ref>. For example, Suwajanakorn et al. <ref type="bibr" target="#b28">[29]</ref> synthesized a taking face of President Obama with accurate
. We evaluate our model along with state-of-the-art methods on several popular datasets (e.g., GRID <ref type="bibr" target="#b5">[6]</ref>, LRW <ref type="bibr" target="#b4">[5]</ref>, VoxCeleb <ref tively evaluate our ATVGnet on LRW dataset <ref type="bibr" target="#b3">[4]</ref> and GRID dataset <ref type="bibr" target="#b5">[6]</ref>. The LRW dataset consists of 500 different words spoken by h eleb <ref type="bibr" target="#b23">[24]</ref>, TCD <ref type="bibr" target="#b12">[13]</ref>, GRID <ref type="bibr" target="#b5">[6]</ref> and realworld samples from YouTube (in total 38 videos). Thr
in natural language tasks <ref type="bibr" target="#b19">[20]</ref> and image/video generation task <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta f><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. Pumarola et al. <ref type="bibr" target="#b25">[26]</ref> generated facial expression conditioned on action units an or to generate consistent pixels along temporal axis.</p><p>As mentioned in Sec. 2, Pumarola et al. <ref type="bibr" target="#b25">[26]</ref> exploited a generator that regresses an attention mask and
aches has been mainly limited to synthesizing a talking face from speech audio of a specific person <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" targ
e target problem <ref type="bibr" target="#b14">(Qi et al., 2018)</ref>. The approaches proposed by <ref type="bibr" target="#b18">Snell et al. (2017)</ref> and <ref type="bibr" target="#b20">Sung et ication</head><p>Few-shot classification <ref type="bibr" target="#b21">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b18">Snell et al., 2017)</ref> is a task in which a classifier must be ada ng Networks <ref type="bibr" target="#b21">(Vinyals et al., 2016)</ref> 65.73 Prototypical Networks <ref type="bibr" target="#b18">(Snell et al., 2017)</ref> 68.17 Graph Network <ref type="bibr" targe </p><p>• Prototypical Networks: a deep metric-based method using sample average as class prototypes <ref type="bibr" target="#b18">(Snell et al., 2017)</ref>.</p><p>• Graph Network: a graph-based few- the performance by few-shot classification accuracy following previous studies in few-shot learning <ref type="bibr" target="#b18">(Snell et al., 2017;</ref><ref type="bibr" target="#b20">Sung et al.,
ule</head><p>This module is a bi-direction recurrent neural network with self-attention as shown in <ref type="bibr" target="#b10">Lin et al. (2017)</ref>. Given an input text x = (w 1 , w 2 , ..., w
2.1">Few-Shot Learning</head><p>The seminal work on few-shot learning dates back to the early 2000s <ref type="bibr" target="#b2">(Fe-Fei et al., 2003;</ref><ref type="bibr" target="#b3">Fei-Fei et al
used by data sparseness, only to a limited extent. Instead, researchers have explored meta-learning <ref type="bibr" target="#b4">(Finn et al., 2017)</ref> to leverage the distribution over similar ta th complex iterative inference strategies. More recently, many approaches have used a meta-learning <ref type="bibr" target="#b4">(Finn et al., 2017;</ref><ref type="bibr" target="#b11">Mishra et al.,
one or very few examples challenges the standard fine-tuning method in deep learning. Early studies <ref type="bibr" target="#b17">(Salamon and Bello, 2017)</ref> applied data augmentation and regular
018)</ref>. The approaches proposed by <ref type="bibr" target="#b18">Snell et al. (2017)</ref> and <ref type="bibr" target="#b20">Sung et al. (2018)</ref>, which combine non-parametric methods and me thout suffering from overfitting. In general, these approaches can be divided into two categories.  <ref type="bibr" target="#b20">(Sung et al., 2018)</ref>.</p></div> <div xmlns="http://www.tei-c.org 7 Graph Network <ref type="bibr" target="#b5">(Garcia and Bruna, 2017)</ref> 82.61 Relation Network <ref type="bibr" target="#b20">(Sung et al., 2018)</ref> 83.07 SNAIL <ref type="bibr" target="#b11"> eural network as the distance metric and sums up sample vectors in the support set as class vectors <ref type="bibr" target="#b20">(Sung et al., 2018)</ref>.</p><p>• SNAIL: a class of simple and gener wing previous studies in few-shot learning <ref type="bibr" target="#b18">(Snell et al., 2017;</ref><ref type="bibr" target="#b20">Sung et al., 2018)</ref>. To evaluate the proposed model with the bas
nvariant semantic relationships between lower level sample features and higher level class features <ref type="bibr" target="#b7">(Hinton et al., 2011)</ref>. To ensure the class vector encapsulates t
<ref type="bibr" target="#b3">(Fei-Fei et al., 2006)</ref>, complex gradient transfer between tasks <ref type="bibr" target="#b12">(Munkhdalai and Yu, 2017)</ref>, and fine-tuning the target problem <
ennington et al., 2014)</ref> for ARSC dataset and 300-dimension Chinese word embeddings trained by <ref type="bibr" target="#b9">Li et al. (2018)</ref> for ODIC. We set the hidden state size of LSTM
br" target="#b25">Xu et al. (2018)</ref> studied lifelong domain word embeddings via meta-learning. <ref type="bibr" target="#b27">Yu et al. (2018)</ref> argued that the optimal meta-model may vary ac org/ns/1.0"><head n="5.1">Datasets</head><p>Amazon Review Sentiment Classification (ARSC) Following <ref type="bibr" target="#b27">Yu et al. (2018)</ref>, we use the multiple tasks with the multi-doma ifferent binary classification tasks. These buckets then form 23 × 3 = 69 tasks in total. Following <ref type="bibr" target="#b27">Yu et al. (2018)</ref>, we select 12(4 × 3) tasks from 4 domains (Boo pe="bibr" target="#b27">(Yu et al., 2018)</ref>.</p><p>The baseline results on ARSC are reported in <ref type="bibr" target="#b27">Yu et al. (2018)</ref> and we implemented the baseline models on ODIC wed by a fully connected layer activated by sigmoid. We build 2-way 5-shot models on ARSC following <ref type="bibr" target="#b27">Yu et al. (2018)</ref>, and build episode-based meta training with C n in both 5-shot and 10-shot scenarios. Note that for ARSC, the support set for testing is fixed by <ref type="bibr" target="#b27">Yu et al. (2018)</ref>. Consequently, we just need to run the test ep target tasks. The mean accuracy of the 12 target task is compared to the baseline models following <ref type="bibr" target="#b27">Yu et al. (2018)</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns 18)</ref> 83.07 SNAIL <ref type="bibr" target="#b11">(Mishra et al., 2018)</ref> 82.57 ROBUSTTC-FSL <ref type="bibr" target="#b27">(Yu et al., 2018)</ref> 83.12 Induction Networks (ours) 85.63  <ref t .</p><p>• ROBUSTTC-FSL: This approach combines several metric-based methods by clustering the tasks <ref type="bibr" target="#b27">(Yu et al., 2018)</ref>.</p><p>The baseline results on ARSC are repor
tween c i and e q , which is a scalar between 0 and 1. Specifically, we use the neural tensor layer <ref type="bibr" target="#b19">(Socher et al., 2013)</ref> in this module, which has shown great adv
th the same text encoder module.</p><p>Implementation Details We use 300-dimension Glove embeddings <ref type="bibr" target="#b13">(Pennington et al., 2014)</ref> for ARSC dataset and 300-dimension Ch
<ref type="bibr" target="#b3">(Fei-Fei et al., 2006)</ref>, complex gradient transfer between tasks <ref type="bibr" target="#b12">(Munkhdalai and Yu, 2017)</ref>, and fine-tuning the target problem <
success in many fields such as computer vision, speech recognition and natural language processing <ref type="bibr" target="#b8">(Kuang et al., 2018)</ref>. However, supervised deep learning is notor
nvariant semantic relationships between lower level sample features and higher level class features <ref type="bibr" target="#b7">(Hinton et al., 2011)</ref>. To ensure the class vector encapsulates t
tween c i and e q , which is a scalar between 0 and 1. Specifically, we use the neural tensor layer <ref type="bibr" target="#b19">(Socher et al., 2013)</ref> in this module, which has shown great adv
used by data sparseness, only to a limited extent. Instead, researchers have explored meta-learning <ref type="bibr" target="#b4">(Finn et al., 2017)</ref> to leverage the distribution over similar ta th complex iterative inference strategies. More recently, many approaches have used a meta-learning <ref type="bibr" target="#b4">(Finn et al., 2017;</ref><ref type="bibr" target="#b11">Mishra et al.,
th the same text encoder module.</p><p>Implementation Details We use 300-dimension Glove embeddings <ref type="bibr" target="#b13">(Pennington et al., 2014)</ref> for ARSC dataset and 300-dimension Ch
one or very few examples challenges the standard fine-tuning method in deep learning. Early studies <ref type="bibr" target="#b17">(Salamon and Bello, 2017)</ref> applied data augmentation and regular
odeling the relationship between two vectors <ref type="bibr" target="#b22">(Wan et al., 2016;</ref><ref type="bibr" target="#b6">Geng et al., 2017)</ref>. We choose it as an interaction function in t
n model for multi-label text classification where there was a known structure over the label space. <ref type="bibr" target="#b24">Xu et al. (2019)</ref> proposed a open-world learning model to deal w
one or very few examples challenges the standard fine-tuning method in deep learning. Early studies <ref type="bibr" target="#b17">(Salamon and Bello, 2017)</ref> applied data augmentation and regular
and hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. However, these approaches rely on manual feature engineeri (3) Hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> combine the above two categories and learn user/item embedd ecommender systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar ized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type="bibr" target="#b27">[28]</ref>, we use a trainable and personalized relation scoring func where GNNs can be used directly, while here we investigate GNNs for heterogeneous KGs. Wang et al. <ref type="bibr" target="#b27">[28]</ref> use GCNs in KGs for recommendation, but simply applying GC a user-personalized weighted graph, which characterizes user's preferences. To this end, similar to <ref type="bibr" target="#b27">[28]</ref>, we use a user-specific relation scoring function s u (r ) ear that the performance of KGNN-LS with a non-zero λ is better than λ = 0 (the case of Wang et al. <ref type="bibr" target="#b27">[28]</ref>), which justifies our claim that LS regularization can ass
ing process, which leads to better generalization. We develop an approach based on label smoothness <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref>, which assumes tha therefore learnable <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>. Inspired by these methods, we design a module of label smo get="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>. Therefore, more re r the unlabeled nodes l * u (E\V). To solve the issue, we propose minimizing the leave-one-out loss <ref type="bibr" target="#b34">[35]</ref>. Suppose we hold out a single item v and treat it unlabele
rmance degradation as we will show later. Schlichtkrull et al. also propose using GNNs to model KGs <ref type="bibr" target="#b16">[17]</ref>, but not for the purpose of recommendations.</p></div> <di
rmance degradation as we will show later. Schlichtkrull et al. also propose using GNNs to model KGs <ref type="bibr" target="#b16">[17]</ref>, but not for the purpose of recommendations.</p></div> <di
rget="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>, and hybrid methods <ref type="bibr" target="#b17">[18,</re rget="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> pre-process a KG with knowledge graph embedding (KGE) <ref nt" for Dianping-Food. The settings of dimension and learning rate are the same as SVD.</p><p>• CKE <ref type="bibr" target="#b33">[34]</ref> is a representative of embedding-based methods, which comb
rget="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>, and hybrid methods <ref type="bibr" target="#b17">[18,</re rget="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> pre-process a KG with knowledge graph embedding (KGE) <ref nt" for Dianping-Food. The settings of dimension and learning rate are the same as SVD.</p><p>• CKE <ref type="bibr" target="#b33">[34]</ref> is a representative of embedding-based methods, which comb
issue can be addressed by introducing additional sources of information such as user/item profiles <ref type="bibr" target="#b22">[23]</ref> or social networks <ref type="bibr" target="#b21">[22]</re
se methods are classified as: (1) Edge weights are assumed to be given as input and therefore fixed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" targ
(2) Edge weights are parameterized and therefore learnable <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>. Inspired by these ount of prior works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar
Food R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 SVD 0. • LibFM <ref type="bibr" target="#b15">[16]</ref> is a widely used feature-based factorization model for CTR
pproximate the convolutional filters by Chebyshev expansion of the graph Laplacian, and Kipf et al. <ref type="bibr" target="#b10">[11]</ref> propose a convolutional architecture via a first-order app
ommender systems that are based on collaborative filtering <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> usually suffer from the coldstart problem and have trouble information such as user/item profiles <ref type="bibr" target="#b22">[23]</ref> or social networks <ref type="bibr" target="#b21">[22]</ref>.</p><p>Knowledge graphs (KGs) capture structured informati
et="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, but these approach rix completion and design GNNs for representation learning on user-item bipartite graphs. Wu et al. <ref type="bibr" target="#b30">[31]</ref> use GNNs on user/item structure graphs to learn user/item
all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.</p><p>• SVD <ref type="bibr" target="#b11">[12]</ref> is a classic CF-based model using inner product to model u
ef type="bibr" target="#b33">34]</ref>, and hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. However, these app hs, which are hard to tune in practice. (3) Hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> combine the above t ht for KG part is 0.1 for all datasets. The learning rate are the same as in SVD.</p><p>• RippleNet <ref type="bibr" target="#b23">[24]</ref> is a representative of hybrid methods, which is a memory-n
pproximate the convolutional filters by Chebyshev expansion of the graph Laplacian, and Kipf et al. <ref type="bibr" target="#b10">[11]</ref> propose a convolutional architecture via a first-order app
ommender systems that are based on collaborative filtering <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> usually suffer from the coldstart problem and have trouble information such as user/item profiles <ref type="bibr" target="#b22">[23]</ref> or social networks <ref type="bibr" target="#b21">[22]</ref>.</p><p>Knowledge graphs (KGs) capture structured informati
type="bibr" target="#b35">36]</ref>, embedding-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar lassified into three categories: (1) Embedding-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar
ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>, and hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" ta on manually designed metapaths/meta-graphs, which are hard to tune in practice. (3) Hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" ta
information overload <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. Traditional recommender systems that are based on collabor get="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, but these approaches are mostly designed for homogeneous b et="#b14">15]</ref>.</p><p>Recently, researchers also deployed GNNs in recommender systems: PinSage <ref type="bibr" target="#b31">[32]</ref> applies GNNs to the pin-board bipartite graph in Pinterest
ommender systems that are based on collaborative filtering <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> usually suffer from the coldstart problem and have trouble information such as user/item profiles <ref type="bibr" target="#b22">[23]</ref> or social networks <ref type="bibr" target="#b21">[22]</ref>.</p><p>Knowledge graphs (KGs) capture structured informati
/ref><ref type="bibr" target="#b33">34]</ref> pre-process a KG with knowledge graph embedding (KGE) <ref type="bibr" target="#b29">[30]</ref> algorithms, then incorporate learned entity embeddings int
">[2]</ref>, phase-based sampling <ref type="bibr" target="#b2">[3]</ref>, and statistical sampling <ref type="bibr" target="#b3">[4]</ref>. Of these techniques, the sampling based approaches typicall 8">[9]</ref> extended SimPoint to provide statistical confidence measures.</p><p>Wunderlich, et al. <ref type="bibr" target="#b3">[4]</ref> developed the SMARTS framework, which applies statistical sa wn techniques for inferring statistics about a population given a sample of that population. SMARTS <ref type="bibr" target="#b3">[4]</ref> demonstrated that systematic sampling can be used to approxi mpared LiveSim with no sampling simulation and with a sampling mode that was very similar to SMARTS <ref type="bibr" target="#b3">[4]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n= f type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> and statistical sampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
approach is to accelerate the timing simulation using FPGAs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, but this requires cust
ficult to ensure that these results are comparable to those obtained with standard benchmark inputs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref>. Another approach is t
including: benchmarks size reduction <ref type="bibr" target="#b0">[1]</ref>, specialized hardware <ref type="bibr" target="#b1">[2]</ref>, phase-based sampling <ref type="bibr" target="#b2">[3]</ref e="bibr" target="#b0">1]</ref>. Another approach is to accelerate the timing simulation using FPGAs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target
icroarchitecture simulation, earlier work from Conte et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> also explored using statistical sampling with microarchitec statistical sampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> for microarchitecture simulation in Section 2. To the best
il depending on the region of code that is being simulated <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. While these techni
rchers have looked for ways to speed up thermal simulation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, multithreaded simulation <ref type="bibr" target="#b16">[1
rchers have looked for ways to speed up thermal simulation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, multithreaded simulation <ref type="bibr" target="#b16">[1
"#b0">[1]</ref>, specialized hardware <ref type="bibr" target="#b1">[2]</ref>, phase-based sampling <ref type="bibr" target="#b2">[3]</ref>, and statistical sampling <ref type="bibr" target="#b3">[4]< benchmark that are representative of the behavior of the full benchmark execution. Sherwood et al. <ref type="bibr" target="#b2">[3]</ref> developed SimPoint, which works by profiling a benchmark and p simulation for decades and we surveyed some of the seminal work related to profile based sampling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> and statistical sampli
e="bibr" target="#b27">28]</ref>, multithreaded simulation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, and simulation of
wever, LiveSim is able to take advantage of the correlation between code signatures and performance <ref type="bibr" target="#b15">[16]</ref> and use this information to group the checkpoints into clu
elow.</p><p>Visual Feature Extraction For extracting visual features from the videos, we use 3D-CNN <ref type="bibr" target="#b15">[16]</ref>. 3D-CNN has achieved state-of-the-art results in object cl /ref>. 3D-CNN has achieved state-of-the-art results in object classification on tridimensional data <ref type="bibr" target="#b15">[16]</ref>. 3D-CNN not only extracts features from each image frame,
t f , of dimension 300 for an input text (transcript), t.</p><p>Audio Feature Extraction openSMILE <ref type="bibr" target="#b21">[22]</ref> is an open-source toolkit used to extract high dimensional
by the subject under consideration. Use of more complex features such as psycholinguistic features <ref type="bibr" target="#b6">[7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexico
> identified particular hand gesture to be important to identify the act of deception. Cohen et al. <ref type="bibr" target="#b12">[13]</ref> found that fewer iconic hand gestures were a sign of a dec
>19]</ref> to extract features from the transcript of a video, v. First, we use pretrained Word2Vec <ref type="bibr" target="#b19">[20]</ref> model to extract the vector representations for every word
as short involuntary expressions, which could potentially indicate deceptive behavior. Caso et al. <ref type="bibr" target="#b11">[12]</ref> identified particular hand gesture to be important to iden
by the subject under consideration. Use of more complex features such as psycholinguistic features <ref type="bibr" target="#b6">[7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexico
by the subject under consideration. Use of more complex features such as psycholinguistic features <ref type="bibr" target="#b6">[7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexico
va and Rudzicz studied the relation between the syntactic complexity of text and deceptive behavior <ref type="bibr" target="#b7">[8]</ref>. 1 In non-verbal deception detection, physiological measures
ocial media. Cyberbullying is increasingly becoming a common problem amongst the teenagers nowadays <ref type="bibr" target="#b2">[3]</ref>. These include spreading rumors about a person, threats, and
as short involuntary expressions, which could potentially indicate deceptive behavior. Caso et al. <ref type="bibr" target="#b11">[12]</ref> identified particular hand gesture to be important to iden
> identified particular hand gesture to be important to identify the act of deception. Cohen et al. <ref type="bibr" target="#b12">[13]</ref> found that fewer iconic hand gestures were a sign of a dec
ty. Reports suggest that the ability of humans to detect deception without special aids is only 54% <ref type="bibr" target="#b0">[1]</ref>. A study by DePaulo et al. <ref type="bibr" target="#b1">[2]
> identified particular hand gesture to be important to identify the act of deception. Cohen et al. <ref type="bibr" target="#b12">[13]</ref> found that fewer iconic hand gestures were a sign of a dec
ps. Subsequently, a full-connected layer with 300 neurons is used with rectified linear unit (ReLU) <ref type="bibr" target="#b20">[21]</ref> as the activation function. The activations of this full-c 1024 followed by a linear output layer. We use the rectified linear unit (ReLU) activation function <ref type="bibr" target="#b20">[21]</ref> for non-linearity at the hidden layer. A dropout <ref type
ocial media. Cyberbullying is increasingly becoming a common problem amongst the teenagers nowadays <ref type="bibr" target="#b2">[3]</ref>. These include spreading rumors about a person, threats, and
without special aids is only 54% <ref type="bibr" target="#b0">[1]</ref>. A study by DePaulo et al. <ref type="bibr" target="#b1">[2]</ref> found that deception without any particular motivation or in
by the subject under consideration. Use of more complex features such as psycholinguistic features <ref type="bibr" target="#b6">[7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexico
by the subject under consideration. Use of more complex features such as psycholinguistic features <ref type="bibr" target="#b6">[7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexico
function <ref type="bibr" target="#b20">[21]</ref> for non-linearity at the hidden layer. A dropout <ref type="bibr" target="#b23">[24]</ref> of keep probability, p = 0.5, is applied to the hidden lay
by the subject under consideration. Use of more complex features such as psycholinguistic features <ref type="bibr" target="#b6">[7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexico
function <ref type="bibr" target="#b20">[21]</ref> for non-linearity at the hidden layer. A dropout <ref type="bibr" target="#b23">[24]</ref> of keep probability, p = 0.5, is applied to the hidden lay
get="#b17">15,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b9">7]</ref>. Unlike traditional supervised learning, dense vectorized rep arget="#b28">26,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b8">6,</ref><ref type="bibr" target="#b9">7]</ref>. Many of these methods are technically inspired by word embed
ng problems, good representations of data are very important, as demonstrated by many previous work <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" ta br" target="#b9">7]</ref>. Unlike traditional supervised learning, dense vectorized representations <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15]</ref> are not directly a screte distribution based on author degree (with a similar idea of unigram distribution in word2vec <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15]</ref>).</p></div> <div x e (e.g. millions of papers), so the evaluation of Eq. 5 can be prohibitively expensive. Inspired by <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15]</ref>, we apply negative o some pre-defined P r n (j ), such as "smoothed" node degree distribution under specific edge type <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15]</ref>. Finally the param type="bibr" target="#b9">7]</ref>. Many of these methods are technically inspired by word embedding <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15]</ref>. Different from tr tic Gradient Descent (ASGD), where samples are randomly drawn and training is performed in parallel <ref type="bibr" target="#b18">[16]</ref>. The challenge here is that we have two different tasks th
NTRODUCTION</head><p>Heterogeneous networks are ubiquitous. Examples include bibliographic networks <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b24">22]</ref>, movie recommendat t="#b21">19]</ref>. Many existing work on mining heterogeneous networks rely on feature engineering <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b24">22]</ref>, while we adopt em ilar problems in social and information networks are also studied, such as collaboration prediction <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b24">22]</ref>. The major differe
get="#b34">[32]</ref> and many online social networks containing information of heterogeneous types <ref type="bibr" target="#b21">[19]</ref>. Different from their homogeneous counterparts, heterogene etwork has gained a lot of attention in the past few years <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b21">19]</ref>.</p><p>In this work, we are interested in the problem of mi et="#b23">[21,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b21">19]</ref>. To study such networks with multiple types of nodes and/or et="#b23">[21,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b21">19]</ref>. Many existing work on mining heterogeneous networks rely o
,</ref><ref type="bibr" target="#b17">15]</ref>. Different from traditional graph embedding methods <ref type="bibr" target="#b33">[31]</ref>, such as multi-dimensional scaling <ref type="bibr">[8]</r
get="#b34">[32]</ref> and many online social networks containing information of heterogeneous types <ref type="bibr" target="#b21">[19]</ref>. Different from their homogeneous counterparts, heterogene etwork has gained a lot of attention in the past few years <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b21">19]</ref>.</p><p>In this work, we are interested in the problem of mi et="#b23">[21,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b21">19]</ref>. To study such networks with multiple types of nodes and/or et="#b23">[21,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b21">19]</ref>. Many existing work on mining heterogeneous networks rely o
et="#b18">[16,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b9">7]</ref>. Unlike traditional s target="#b5">[3]</ref>. Several network embedding methods <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b27">25]</ref> have been proposed tification task. Compared to traditional network embedding <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b27">25]</ref>, our method uses th data representation, embedding methods are widely adopted <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b27">25]</ref>, where nodes in the works.</p><p>Most of existing network embedding techniques <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b27">25]</ref> are based on the id embedding also attracts lots of attentions in recent years <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" tar 18">[16,</ref><ref type="bibr" target="#b17">15]</ref> are not directly available in networked data <ref type="bibr" target="#b28">[26]</ref>. Hence, many traditional methods under network settings he ks, and they are usually designed for homogeneous networks <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26]</ref>. When it comes to author identification problem under the h embeddings are more scalable and shown better performance <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26]</ref>. Some existing network embedding methods are based on homog network embedding methods are based on homogeneous network <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26]</ref>, while others are based on heterogeneous networks <ref type twork embedding methods, the observed neighborhood of a node is usually defined by original network <ref type="bibr" target="#b28">[26,</ref><ref type="bibr" target="#b27">25]</ref> or by random walk
any traditional methods under network settings heavily rely on problem specific feature engineering <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" ta ed feature-based baselines. As widely used in similar author identification/disambiguation problems <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" ta 11]</ref>. And we also notice KDD Cup 2013 has similar author identification/disambiguation problem <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" ta
ect the best type of information due to the heterogeneity of the network. As shown in previous work <ref type="bibr" target="#b25">[23,</ref><ref type="bibr" target="#b23">21]</ref>, proximity in hete is shown in Figure <ref type="figure" target="#fig_1">2</ref>. Definition 2 (Meta path) A meta path <ref type="bibr" target="#b25">[23]</ref> is a path defined on the network schema TG = (O, L) and is twork, one can easily enrich the semantic of neighbors by considering different types of meta paths <ref type="bibr" target="#b25">[23]</ref>. As shown in <ref type="bibr" target="#b25">[23]</ref>, di by considering different types of meta paths <ref type="bibr" target="#b25">[23]</ref>. As shown in <ref type="bibr" target="#b25">[23]</ref>, different meta paths encode different semantic of links. andidate paths for general heterogeneous network embedding. Although other path similarity measures <ref type="bibr" target="#b25">[23]</ref> can be explored, for simplicity, we set weights of a path ted to mining heterogeneous networks in the past few years <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" tar of nodes and/or links, meta paths are proposed and studied <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" tar
c embedding. Pre-training has been found useful to improve neural network based supervised learning <ref type="bibr" target="#b12">[10]</ref>. So instead of training task-specific author identificatio
ks are ubiquitous. Examples include bibliographic networks <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b24">22]</ref>, movie recommendation networks <ref type="bibr" target="#b3 ument with certain target, such as reviewer recommendation <ref type="bibr" target="#b31">[29,</ref><ref type="bibr" target="#b24">22]</ref>.</p><p>To tackle the author identification problem, as well mining heterogeneous networks rely on feature engineering <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b24">22]</ref>, while we adopt embedding methods for automatic feature lea etworks are also studied, such as collaboration prediction <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b24">22]</ref>. The major difference between those work and ours is the me
noisy labeling, previous studies adopt multi-instance learning to consider the noises of instances <ref type="bibr" target="#b13">(Riedel, Yao, and McCallum 2010;</ref><ref type="bibr" target="#b5">H te this issue, many studies formulated relation classification as a multi-instance learning problem <ref type="bibr" target="#b13">(Riedel, Yao, and McCallum 2010;</ref><ref type="bibr" target="#b5">H the framework of reinforcement learning <ref type="bibr" target="#b16">(Sutton and Barto 1998;</ref><ref type="bibr" target="#b13">Narasimhan, Yala, and Barzilay 2016)</ref> and then predicts relation 4</ref> generated by the sentences in NYT<ref type="foot" target="#foot_2">5</ref> and developed by <ref type="bibr" target="#b13">(Riedel, Yao, and McCallum 2010)</ref>. There are 522,611 sentences,
train the word embeddings on the NYT corpus. For entity embedding, we implemented the TransE model <ref type="bibr" target="#b1">(Bordes et al. 2013</ref>) and trained it on a set of Freebase fact tr
assification adopt supervised learning approaches, either based on traditional handcrafted features <ref type="bibr" target="#b12">(Mooney and Bunescu 2005;</ref><ref type="bibr" target="#b24">Zhou et ural language processing. Many approaches have been developed, particularly with supervised methods <ref type="bibr" target="#b12">(Mooney and Bunescu 2005;</ref><ref type="bibr" target="#b24">Zhou et bibr" target="#b22">(Zeng et al. 2014;</ref><ref type="bibr">dos Santos, Xiang, and Zhou 2015;</ref><ref type="bibr" target="#b12">Mooney and Bunescu 2005;</ref><ref type="bibr" target="#b20">Yang et 015;</ref><ref type="bibr" target="#b10">Liu et al. 2015)</ref>, and long short-term memory network <ref type="bibr" target="#b12">(Miwa and Bansal 2016;</ref><ref type="bibr" target="#b19">Xu et al. bibr" target="#b12">(Miwa and Bansal 2016;</ref><ref type="bibr" target="#b19">Xu et al. 2015;</ref><ref type="bibr" target="#b12">Miwa and Bansal 2016)</ref>. In <ref type="bibr" target="#b18">(Wang
ng active learning <ref type="bibr" target="#b14">(Sterckx et al. 2014</ref>) and negative patterns <ref type="bibr" target="#b17">(Takamatsu, Sato, and Nakagawa 2012)</ref>.</p><p>Previous methods ar
the sentence-level prediction. All the baselines were implemented with the source codes released by <ref type="bibr" target="#b7">(Li et al. 2016)</ref>. </p></div> <div xmlns="http://www.tei-c.org/ns
" target="#b19">Xu et al. 2015;</ref><ref type="bibr" target="#b12">Miwa and Bansal 2016)</ref>. In <ref type="bibr" target="#b18">(Wang et al. 2016)</ref>, two levels of attention is proposed in orde
nsider the noises of instances <ref type="bibr" target="#b13">(Riedel, Yao, and McCallum 2010;</ref><ref type="bibr" target="#b5">Hoffmann et al. 2011;</ref><ref type="bibr" target="#b15">Surdeanu et ulti-instance learning problem <ref type="bibr" target="#b13">(Riedel, Yao, and McCallum 2010;</ref><ref type="bibr" target="#b5">Hoffmann et al. 2011;</ref><ref type="bibr" target="#b15">Surdeanu et
ref>, and long short-term memory network <ref type="bibr" target="#b12">(Miwa and Bansal 2016;</ref><ref type="bibr" target="#b19">Xu et al. 2015;</ref><ref type="bibr" target="#b12">Miwa and Bansal 2
4">Zhou et al. 2005)</ref> or based on the features automatically generated by deep neural networks <ref type="bibr" target="#b22">(Zeng et al. 2014;</ref><ref type="bibr">dos Santos, Xiang, and Zhou ity labeled data.</p><p>Recently, neural models have been widely applied to relation classification <ref type="bibr" target="#b22">(Zeng et al. 2014;</ref><ref type="bibr">dos Santos, Xiang, and Zhou sentences <ref type="bibr" target="#b9">(Lin et al. 2016)</ref> or retaining one sentence in a bag <ref type="bibr" target="#b22">(Zeng et al. 2014</ref>), our method is better at dealing with noisy on embedding. Word embeddings are obtained from word2vec 3 , and the dimension is d w . Similar to  <ref type="bibr" target="#b22">(Zeng et al. 2014)</ref>, we use d p -dimensional position embeddings
d on traditional handcrafted features <ref type="bibr" target="#b12">(Mooney and Bunescu 2005;</ref><ref type="bibr" target="#b24">Zhou et al. 2005)</ref> or based on the features automatically genera particularly with supervised methods <ref type="bibr" target="#b12">(Mooney and Bunescu 2005;</ref><ref type="bibr" target="#b24">Zhou et al. 2005;</ref><ref type="bibr" target="#b21">Zelenko, Aone,
igh-quality annotated data.</p><p>In order to obtain large-scale training data, distant supervision <ref type="bibr" target="#b11">(Mintz et al. 2009</ref>) was proposed by assuming that if two entiti ion classification task.</p><p>The results are compared under the held-out evaluation configuration <ref type="bibr" target="#b11">(Mintz et al. 2009</ref>) which provides an approximate measure of re
a novel adaptive multi-compositionality layer in recursive neural network, which is named as AdaRNN <ref type="bibr" target="#b1">(Dong et al., 2014)</ref>. It consists of more than one composition fu
The neural models use distributed representation <ref type="bibr" target="#b6">(Hinton, 1986;</ref><ref type="bibr" target="#b12">Rumelhart et al., 1986;</ref><ref type="bibr" target="#b0">Bengio et ularization penalty is used.</p><p>Based on the converted tree, we employ backpropagation algorithm <ref type="bibr" target="#b12">(Rumelhart et al., 1986)</ref> to propagate the errors from root node
rom root node to the leaf nodes. We calculate the derivatives to update the parameters. The AdaGrad <ref type="bibr" target="#b2">(Duchi et al., 2011)</ref> is employed to solve this optimization prob
cts as targets, and sentiments for them are heuristically determined by the dominant opinion words. <ref type="bibr" target="#b8">Jiang et al. (2011)</ref> combine the target-independent features (con s. These features are all target-independent.</p><p>SVM-dep: We re-implement the method proposed by <ref type="bibr" target="#b8">Jiang et al. (2011)</ref>. It combines both 1 http://goo.gl/5Enpu7 the wever, the accuracy and F1-score do not gain significantly. This is caused by mismatch of the rules <ref type="bibr" target="#b8">(Jiang et al., 2011)</ref> used to extract the target-dependent featur
type="bibr" target="#b3">(Fan et al., 2008)</ref> is used for baselines. A tweet-specific tokenizer <ref type="bibr" target="#b5">(Gimpel et al., 2011)</ref> is employed, and the dependency parsing re
analysis results for movie review dataset <ref type="bibr" target="#b14">(Socher et al., 2012;</ref><ref type="bibr" target="#b15">Socher et al., 2013)</ref>. The recursive neural models employ the se
type="bibr" target="#b3">(Fan et al., 2008)</ref> is used for baselines. A tweet-specific tokenizer <ref type="bibr" target="#b5">(Gimpel et al., 2011)</ref> is employed, and the dependency parsing re
) to leverage the ability of deep learning models. The neural models use distributed representation <ref type="bibr" target="#b6">(Hinton, 1986;</ref><ref type="bibr" target="#b12">Rumelhart et al., 1
"bibr" target="#b6">(Hinton, 1986;</ref><ref type="bibr" target="#b12">Rumelhart et al., 1986;</ref><ref type="bibr" target="#b0">Bengio et al., 2003)</ref> to automatically learn features for target-
</ref> are independent to the targets, hence the results are computed despite what the targets are. <ref type="bibr" target="#b7">Hu and Liu (2004)</ref> regard the features of products as targets, an
analysis results for movie review dataset <ref type="bibr" target="#b14">(Socher et al., 2012;</ref><ref type="bibr" target="#b15">Socher et al., 2013)</ref>. The recursive neural models employ the se
pting the machine learning approach.</p><p>One is based on the COMT (Co-Training Model Tree) method <ref type="bibr" target="#b16">[17]</ref>, which assumes a linear relation between parameters and th
arameters are provided to users, as systems in the cloud aim to support a wide variety of use cases <ref type="bibr" target="#b40">[42]</ref>. For example, Hadoop <ref type="bibr" target="#b17">[18]</
chmarking community has proved theoretically and practically <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar
of other configuration tuning algorithms. ? We propose the divide-and-diverge sampling method ( ?4. <ref type="bibr" target="#b0">1)</ref> and the recursive-bound-and-search method ( ?4.2) to enable c en more important for repetitive workloads, and recurring workloads are in fact a common phenomenon <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>. Third, automatic co hich the DDS method can be directly applied. We can map a sampled value within ranges of [0, 1) and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2)</ref> respectively to the va
tisfactory performances under atypical application workloads <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar mooth performance surface. Previously, an unexpected performance surface is reported for PostgreSQL <ref type="bibr" target="#b10">[11]</ref>, which even costs system developers months of efforts to r 6,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b41">43]</ref>, databases <ref type="bibr" target="#b10">[11]</ref> and Hadoop <ref type="bibr" target="#b3">[4,</ref><ref typ query is a smooth surface with regard to the configuration parameters of cache size and buffer size <ref type="bibr" target="#b10">[11]</ref>, the cache size interacts with the buffer size in a way th p><p>The other machine learning model we have tried is the GPR (Gaussian Process Regression) method <ref type="bibr" target="#b10">[11]</ref>, which assumes a differentiable performance function on pa imal problem.</p><p>Works on automatic configuration tuning for database systems also exist. iTuned <ref type="bibr" target="#b10">[11]</ref> assumes a smooth performance surface for the SUT so as to
="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">44]</ref>, e.g., simulated annealing <ref type="bibr" target="#b38">[40]</ref> and genetic algorithms <ref type="bibr" target="#b14">[15]
arameters are provided to users, as systems in the cloud aim to support a wide variety of use cases <ref type="bibr" target="#b40">[42]</ref>. For example, Hadoop <ref type="bibr" target="#b17">[18]</
stems under tune, we embed widely adopted benchmark tools in the workload generator. We use HiBench <ref type="bibr" target="#b21">[22]</ref> for Hive+Hadoop and Spark, YCSB <ref type="bibr" target="#
of other configuration tuning algorithms. ? We propose the divide-and-diverge sampling method ( ?4. <ref type="bibr" target="#b0">1)</ref> and the recursive-bound-and-search method ( ?4.2) to enable c en more important for repetitive workloads, and recurring workloads are in fact a common phenomenon <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>. Third, automatic co hich the DDS method can be directly applied. We can map a sampled value within ranges of [0, 1) and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2)</ref> respectively to the va
h a high-dimensional parameter space, thus a large sample set is commonly needed to find a solution <ref type="bibr" target="#b15">[16]</ref>. However, collecting a large set of performance-configurat target="#b20">[21]</ref>, but previous research typically studies the problem based on simulations <ref type="bibr" target="#b15">[16]</ref>; the overhead aspect is rarely considered to the extent as For example, some solve the optimization problem with around 10 parameters using about 2000 samples <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">43]</ref>. Except through si f><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">39]</ref> and search-based <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" ta ll climbing <ref type="bibr" target="#b39">[41]</ref>, or require a huge number of initial testings <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">44]</ref>, e.g., simulated a
pplication workloads <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">41]</ref>. In fact, configura es. Two categories of PO algorithms exist, i.e., model-based <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">39]</ref> and search-based <r tabases <ref type="bibr" target="#b10">[11]</ref> and Hadoop <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> respectively, these solutions are not generally applicable t build or have users build performance models for the tuning purpose as previously done for Hadoop <ref type="bibr" target="#b18">[19]</ref>. Some deployed systems are distributed, e.g., Spark and Ha ning process.</p><p>Automatic configuration tuning is also proposed for the Hadoop system. Starfish <ref type="bibr" target="#b18">[19]</ref> is built based upon a strong understanding of the Hadoop s
as been achieved by using LSTM-CRF models <ref type="bibr" target="#b17">(Lample et al., 2016;</ref><ref type="bibr" target="#b27">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b5">Chiu and Nichol ad><p>We follow the best English NER model <ref type="bibr" target="#b15">(Huang et al., 2015;</ref><ref type="bibr" target="#b27">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b17">Lample et al., concatenated as its representation:</p><p>Integrating character representations Both character CNN <ref type="bibr" target="#b27">(Ma and Hovy, 2016)</ref> and LSTM <ref type="bibr" target="#b17">(La
y predicted. The current stateof-the-art for English NER has been achieved by using LSTM-CRF models <ref type="bibr" target="#b17">(Lample et al., 2016;</ref><ref type="bibr" target="#b27">Ma and Hovy ibr" target="#b15">(Huang et al., 2015;</ref><ref type="bibr" target="#b27">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b17">Lample et al., 2016)</ref>, using LSTM-CRF as the main network struct epresentations Both character CNN <ref type="bibr" target="#b27">(Ma and Hovy, 2016)</ref> and LSTM <ref type="bibr" target="#b17">(Lample et al., 2016)</ref> have been used for representing the chara
egmentation, our word-based methods give competitive results to the state-of-the-art on the dataset <ref type="bibr" target="#b0">(Che et al., 2013;</ref><ref type="bibr" target="#b43">Wang et al., 20
, and indices are from 1, then t(2, 1) = 4 (长) and t(1, 3) = 3 (市). We use the BIOES tagging scheme <ref type="bibr" target="#b36">(Ratinov and Roth, 2009)</ref> for both wordbased and character-based lexicon. In this respect, our automatic lexicon also played to some extent the role of a gazetteer <ref type="bibr" target="#b36">(Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b5">Chiu and
bibr" target="#b23">Liu et al., 2014;</ref><ref type="bibr" target="#b35">Qiu and Zhang, 2015;</ref><ref type="bibr" target="#b4">Chen et al., 2017;</ref><ref type="bibr" target="#b14">Huang et al., 2
segmentation remains an unsolved problem <ref type="bibr" target="#b22">(Liu and Zhang, 2012;</ref><ref type="bibr" target="#b16">Jiang et al., 2013;</ref><ref type="bibr" target="#b23">Liu et al., 2
017a)</ref> off the shelf<ref type="foot" target="#foot_3">5</ref> , which is trained using CTB 6.0 <ref type="bibr" target="#b47">(Xue et al., 2005)</ref>. Word Embeddings. We pretrain word embedding

been the dominant approach for Chinese NER <ref type="bibr" target="#b2">(Chen et al., 2006b;</ref><ref type="bibr" target="#b25">Lu et al., 2016;</ref><ref type="bibr" target="#b7">Dong et al., 2016 2006;</ref><ref type="bibr" target="#b54">Zhou et al., 2013)</ref> and character embedding features <ref type="bibr" target="#b25">(Lu et al., 2016)</ref>. <ref type="bibr" target="#b7">Dong et al. (2
br" target="#b29">Passos et al., 2014;</ref><ref type="bibr" target="#b15">Huang et al., 2015;</ref><ref type="bibr" target="#b26">Luo et al., 2015)</ref>. <ref type="bibr" target="#b37">Rei (2017)</r

cydiscourse DAGs <ref type="bibr">(Peng et al., 2017)</ref>, as well as speech tokenization lattice <ref type="bibr" target="#b38">(Sperber et al., 2017)</ref> and multi-granularity segmentation outpu

been the dominant approach for Chinese NER <ref type="bibr" target="#b2">(Chen et al., 2006b;</ref><ref type="bibr" target="#b25">Lu et al., 2016;</ref><ref type="bibr" target="#b7">Dong et al., 2016 2006;</ref><ref type="bibr" target="#b54">Zhou et al., 2013)</ref> and character embedding features <ref type="bibr" target="#b25">(Lu et al., 2016)</ref>. <ref type="bibr" target="#b7">Dong et al. (2
lidated training set. The best statistical models on the dataset leverage rich handcrafted features <ref type="bibr" target="#b1">(Chen et al., 2006a;</ref><ref type="bibr" target="#b52">Zhang et al.,
"bibr" target="#b42">(Tai et al., 2015)</ref> to DAGs. They have been used to model motion dynamics <ref type="bibr" target="#b41">(Sun et al., 2017)</ref>, dependencydiscourse DAGs <ref type="bibr">(
r" target="#b22">(Liu and Zhang, 2012;</ref><ref type="bibr" target="#b16">Jiang et al., 2013;</ref><ref type="bibr" target="#b23">Liu et al., 2014;</ref><ref type="bibr" target="#b35">Qiu and Zhang,
ibr" target="#b15">Huang et al., 2015;</ref><ref type="bibr" target="#b26">Luo et al., 2015)</ref>. <ref type="bibr" target="#b37">Rei (2017)</ref> uses a word-level language modeling objective to aug
="bibr" target="#b2">(Chen et al., 2006b;</ref><ref type="bibr" target="#b25">Lu et al., 2016;</ref><ref type="bibr" target="#b7">Dong et al., 2016)</ref>. There have been explicit discussions compari 013)</ref> and character embedding features <ref type="bibr" target="#b25">(Lu et al., 2016)</ref>. <ref type="bibr" target="#b7">Dong et al. (2016)</ref>  </p><formula xml:id="formula_24">Models P R
34">Peters et al. (2017)</ref> pretrain a character language model to enhance word representations. <ref type="bibr" target="#b51">Yang et al. (2017b)</ref> exploit cross-domain and cross-lingual know

bibr" target="#b35">Qiu and Zhang, 2015;</ref><ref type="bibr" target="#b4">Chen et al., 2017;</ref><ref type="bibr" target="#b14">Huang et al., 2017)</ref>. It has been shown that character-based met
ent label prediction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b23">24]</ref>. Adversarial examples have been shown to be ubiquitous beyo " target="#b15">16,</ref><ref type="bibr" target="#b70">71]</ref>. Among them, adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref> is one of the most side in a large, contiguous region and a significant portion of the adversarial subspaces is shared <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta e sense of robustness against adversarial attacks due to gradient masking, and adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" ta se method against adversarial attacks. It improves model robustness by solving a minimax problem as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>:</p><formula xml:i he proposed formulation deviates from the conventional minimax formulation for adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. More specifically ≡ i L θ (x i , y i ), the proposed approach reduces to the conventional adversarial training setup <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. The overall proce gn method (FGSM) for adversarial attack generation is developed and used in adversarial training in <ref type="bibr" target="#b23">[24]</ref>. Many variants of attacks have been developed later <ref t inner maximization can be solved approximately, using for example a one-step approach such as FGSM <ref type="bibr" target="#b23">[24]</ref>, or a multi-step projected gradient descent (PGD) method < y measuring the accuracy of the model under different adversarial attacks, including white-box FGSM <ref type="bibr" target="#b23">[24]</ref>, PGD <ref type="bibr" target="#b35">[36]</ref>, CW <ref ty
type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b17">18]</ref> to speech recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9]</ref>. Many encouraging pr
as samples from it <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref>. Modern classifiers
a non-empty intersection contained in a set of measure zero <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>The optimal transport (OT) distance is an alternativ s on a low dimensional manifold of the input embedding space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref>, which is the case for natural images. It has been widely a ng the difficulties encountered in the original GAN training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref>. This technique has been further extended to generating dis generative modeling <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" tar s coupled over all input samples. Wasserstein GAN and OT-GAN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b9">10]</ref>. Generative Adversar act minimization of Eqn.(4) over T is intractable in general <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar
ns and different local minima have been shown to be equally effective under the clean image setting <ref type="bibr" target="#b13">[14]</ref>. However, different solution points might leverage differe
ess against adversarial examples under different scenarios <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar target="#b70">71]</ref>. Among them, adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref> is one of the most popular technique <ref type="bibr" targe et="#b23">[24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b35">36]</ref> is one of the effective defense method against adversarial improves model robustness by solving a minimax problem as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>:</p><formula xml:id="formula_0">min θ max x ∈Sx L(x , y; θ) conventional minimax formulation for adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. More specifically, it can be regarded as an instance of th ach reduces to the conventional adversarial training setup <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. The overall procedure for the proposed approach is in Algo >. Currently available remedies either increase the number of iterations for generating the attacks <ref type="bibr" target="#b35">[36]</ref> or use classes other than the ground-truth for attack gene he outer minimization corresponds to minimizing the "adversarial loss" induced by the inner attacks <ref type="bibr" target="#b35">[36]</ref>. The inner maximization can be solved approximately, using <ref type="bibr" target="#b23">[24]</ref>, or a multi-step projected gradient descent (PGD) method <ref type="bibr" target="#b35">[36]</ref> x t+1 = P Sx x t + α • sign ∇ x L(x t , y; θ) ,</p><p>wher ide ResNet (WRN-28-10) <ref type="bibr" target="#b67">[68]</ref> as the network structure following <ref type="bibr" target="#b35">[36]</ref>. We compare the performance of the proposed method with a target="#b30">[31]</ref>, ii) PGD-based approach from Madry et al. <ref type="bibr">(Madry)</ref>  <ref type="bibr" target="#b35">[36]</ref>, which is one of the most effective defense method <ref ty [31]</ref> during training. The perturbation budget of = 8 is used in training following literature <ref type="bibr" target="#b35">[36]</ref>. Label smoothing of 0.5, attack iteration T=1 and Sinkhorn ferent adversarial attacks, including white-box FGSM <ref type="bibr" target="#b23">[24]</ref>, PGD <ref type="bibr" target="#b35">[36]</ref>, CW <ref type="bibr" target="#b7">[8]</ref> (CW-loss <ref d attack step of 20, with the results shown in Figure <ref type="figure" target="#fig_3">3</ref>    <ref type="bibr" target="#b35">[36]</ref> and Bilateral <ref type="bibr" target="#b60">[61]</ref> me on CIFAR10 under different threat models.</p><p>further boosts the performance over the Madry model <ref type="bibr" target="#b35">[36]</ref> by a large margin under different attack budgets. We also b-c</ref>) and also Table <ref type="table" target="#tab_1">1</ref>. It is observed that both Madry <ref type="bibr" target="#b35">[36]</ref> and Proposed can maintain a fairly stable performance when f attack iterations is increased. Notably, the proposed approach consistently outperforms the Madry <ref type="bibr" target="#b35">[36]</ref> model across a wide range of attack iterations. From Table posed approach outperforms all baseline methods significantly, which is about 20% better than Madry <ref type="bibr" target="#b35">[36]</ref> and Bilateral <ref type="bibr" target="#b60">[61]</ref> un "#b30">[31]</ref>, which is a popular dataset that is widely use in adversarial training literature <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b60">61]</ref> with 10 classes, 5 y on the original test images (Clean) and under PGD and CW attack with T iterations (PGDT and CWT ) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b7">8]</ref>. The evaluation resu
/ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>, auto-encoding <ref type="bibr" target="#b56">[57]</ref> and dictionary learning <ref type="bibr" target="#b46">[47
type="bibr" target="#b17">18]</ref> to speech recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9]</ref>. Many encouraging progresses been made towards improving mode
ng in <ref type="bibr" target="#b23">[24]</ref>. Many variants of attacks have been developed later <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" targ
ess against adversarial examples under different scenarios <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar target="#b70">71]</ref>. Among them, adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref> is one of the most popular technique <ref type="bibr" targe et="#b23">[24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b35">36]</ref> is one of the effective defense method against adversarial improves model robustness by solving a minimax problem as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>:</p><formula xml:id="formula_0">min θ max x ∈Sx L(x , y; θ) conventional minimax formulation for adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. More specifically, it can be regarded as an instance of th ach reduces to the conventional adversarial training setup <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. The overall procedure for the proposed approach is in Algo >. Currently available remedies either increase the number of iterations for generating the attacks <ref type="bibr" target="#b35">[36]</ref> or use classes other than the ground-truth for attack gene he outer minimization corresponds to minimizing the "adversarial loss" induced by the inner attacks <ref type="bibr" target="#b35">[36]</ref>. The inner maximization can be solved approximately, using <ref type="bibr" target="#b23">[24]</ref>, or a multi-step projected gradient descent (PGD) method <ref type="bibr" target="#b35">[36]</ref> x t+1 = P Sx x t + α • sign ∇ x L(x t , y; θ) ,</p><p>wher ide ResNet (WRN-28-10) <ref type="bibr" target="#b67">[68]</ref> as the network structure following <ref type="bibr" target="#b35">[36]</ref>. We compare the performance of the proposed method with a target="#b30">[31]</ref>, ii) PGD-based approach from Madry et al. <ref type="bibr">(Madry)</ref>  <ref type="bibr" target="#b35">[36]</ref>, which is one of the most effective defense method <ref ty [31]</ref> during training. The perturbation budget of = 8 is used in training following literature <ref type="bibr" target="#b35">[36]</ref>. Label smoothing of 0.5, attack iteration T=1 and Sinkhorn ferent adversarial attacks, including white-box FGSM <ref type="bibr" target="#b23">[24]</ref>, PGD <ref type="bibr" target="#b35">[36]</ref>, CW <ref type="bibr" target="#b7">[8]</ref> (CW-loss <ref d attack step of 20, with the results shown in Figure <ref type="figure" target="#fig_3">3</ref>    <ref type="bibr" target="#b35">[36]</ref> and Bilateral <ref type="bibr" target="#b60">[61]</ref> me on CIFAR10 under different threat models.</p><p>further boosts the performance over the Madry model <ref type="bibr" target="#b35">[36]</ref> by a large margin under different attack budgets. We also b-c</ref>) and also Table <ref type="table" target="#tab_1">1</ref>. It is observed that both Madry <ref type="bibr" target="#b35">[36]</ref> and Proposed can maintain a fairly stable performance when f attack iterations is increased. Notably, the proposed approach consistently outperforms the Madry <ref type="bibr" target="#b35">[36]</ref> model across a wide range of attack iterations. From Table posed approach outperforms all baseline methods significantly, which is about 20% better than Madry <ref type="bibr" target="#b35">[36]</ref> and Bilateral <ref type="bibr" target="#b60">[61]</ref> un "#b30">[31]</ref>, which is a popular dataset that is widely use in adversarial training literature <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b60">61]</ref> with 10 classes, 5 y on the original test images (Clean) and under PGD and CW attack with T iterations (PGDT and CWT ) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b7">8]</ref>. The evaluation resu
arget="#b0">1,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>, auto-encoding <ref type="bibr" target="#b56">[57]</ref> and stein GAN and OT-GAN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b9">10]</ref>. Generative Adversarial Networks (GAN) is a family of techni #b48">49]</ref>. This technique has been further extended to generating discrete data such as texts <ref type="bibr" target="#b9">[10]</ref>. Different from GANs, which maximizes a discrimination crit
="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> try to improve the static injection framework. CriticalFau structions using symbolic execution, which enumerates all potential hardware errors.</p><p>The work <ref type="bibr" target="#b15">[16]</ref> presents a selective protection technique that allows user tion technique that allows users to selectively protect these SDC-prone data. The main idea of work <ref type="bibr" target="#b15">[16]</ref> is predicting the SDC proneness of a program's data firstl put program are selected by GA.</p><p>Step 3 strengthens the identified vulnerable blocks. The work <ref type="bibr" target="#b15">[16]</ref> introduces a prediction model named SDCAuto to predict the causing errors. Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the diagram of the work <ref type="bibr" target="#b15">[16]</ref>. The work <ref type="bibr" target="#b15">[16]</ref> first _1">2</ref> illustrates the diagram of the work <ref type="bibr" target="#b15">[16]</ref>. The work <ref type="bibr" target="#b15">[16]</ref> first compiles the source code into LLVM IR, and extracts sidered, since it always causes illegal opcode exception rather than SDC.</p><p>Finally, as in work <ref type="bibr" target="#b15">[16]</ref>, we assume that at most one fault occurs during a program' e our results with the work <ref type="bibr" target="#b14">[15]</ref> and SDCAuto presented in work <ref type="bibr" target="#b15">[16]</ref>. We use our approach to maximize SDC coverage under the us /1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Block diagram of the work<ref type="bibr" target="#b15">[16]</ref> </figDesc><graphic url="image-2.png" coords="4,99.47,451.0
ly, a lot of works <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" t reduced, the statistical fault injection (SFI) experiments are still time-consuming. SmartInjector <ref type="bibr" target="#b11">[12]</ref> proposes an intelligent fault injection framework to ident nly one representative fault is selected to implement fault injection for each group. SmartInjector <ref type="bibr" target="#b11">[12]</ref> proposes an intelligent fault injection framework to ident
e API exposed by Pin to inject faults. We select 15 benchmarks which are drawn from SPEC benchmarks <ref type="bibr" target="#b20">[21]</ref>, NAS parallel benchmarks <ref type="bibr" target="#b21">[2 hoose five programs, namely Gzip, Ferret, Queens, CG and LBM, which are chosen form SPEC benchmarks <ref type="bibr" target="#b20">[21]</ref>, NAS parallel benchmarks <ref type="bibr" target="#b21">[2
t="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" t time for a single fault simulation by predicting the fault outcome prediction technique. Shoestring <ref type="bibr" target="#b12">[13]</ref> leverages compiler to analyze and identify the statistical lnerability analysis is another important solution to identify SDC-causing instructions. Shoestring <ref type="bibr" target="#b12">[13]</ref> uses a static compiler analysis technology to identify SDC
for most of SDCs, and protect these instructions selectively can achieve high coverage against SDCs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. These instructions ar
tructions selectively <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>.</p><p>It was recently reported that a small part of program s. Different end points of data dependency chains have different effects on the outputs of program. <ref type="bibr" target="#b6">(7)</ref> Code structure related features. The SDC proneness of instru
="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> try to improve th more time than SFI due to the state explosion problem caused by symbolic execution.</p><p>The work <ref type="bibr" target="#b14">[15]</ref> employs genetic algorithm (GA) to identify the most vulner g these instructions will incur high and unnecessary performance overhead. The work process of work <ref type="bibr" target="#b14">[15]</ref> is shown in Fig. <ref type="figure" target="#fig_0">1</ref culate the SDC coverage according the results calculated. We also compare our results with the work <ref type="bibr" target="#b14">[15]</ref> and SDCAuto presented in work <ref type="bibr" target="#b1 coverage of each program under the same performance overhead bound, we compare our method with work <ref type="bibr" target="#b14">[15]</ref> and SDCAuto. We select three performance overhead bounds: target="#fig_4">4</ref> shows the statistical results obtained by our approach (SCDPredictor), work <ref type="bibr" target="#b14">[15]</ref> and SDCAuto for each benchmark. As it can be seen in Fig. n Fig. <ref type="figure" target="#fig_4">4</ref>, the averages SDC coverage for SCDPredictor, Work <ref type="bibr" target="#b14">[15]</ref> and SDCAuto are 34.68%, 25.84% and 32.3% respectively for edictor acquires highest SDC coverage at the same performance overhead bound. That came out of work <ref type="bibr" target="#b14">[15]</ref> uses genetic algorithm (GA) to identify the most vulnerabl s are duplicated and protected. Thus, SCDPredictor and SDCAuto obtain higher SDC coverage than work <ref type="bibr" target="#b14">[15]</ref>. Since SDCAuto is built using CART, which is easy to cause type="figure" target="#fig_5">5</ref> show that the average consuming times for SCDPredictor, Work <ref type="bibr" target="#b14">[15]</ref> and SDCAuto are 0.67h, 1.60h and 0.6h respectively for the the SCDPredictor model performs worse than the SDCAuto model, but still manages to outperform Work <ref type="bibr" target="#b14">[15]</ref> in terms of time efficiency. For SCDPredictor and SDCAuto, tween SCDPredictor and SDCAuto is mainly due to the process of SDC proneness prediction.</p><p>Work <ref type="bibr" target="#b14">[15]</ref> exploits the genetic algorithm (GA) to identify the most v f> exploits the genetic algorithm (GA) to identify the most vulnerable blocks of a program. In Work <ref type="bibr" target="#b14">[15]</ref>, the BB subsequence in code execution path which is select the fitness of a chromosome, BBs of the chromosome are several times. Thus, the complexity of Work <ref type="bibr" target="#b14">[15]</ref> can be approximated as ( ln )</p><p>O n n , where n is the can be approximated as ( ln )</p><p>O n n , where n is the total number of BBs. Therefore, the Work <ref type="bibr" target="#b14">[15]</ref> is the most time-consuming.</p></div> <div xmlns="http://w
br" target="#b18">[19]</ref> to implement fault injection experiment. PINFI is built with Intel Pin <ref type="bibr" target="#b19">[20]</ref> and uses the API exposed by Pin to inject faults. We selec
t="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" t time for a single fault simulation by predicting the fault outcome prediction technique. Shoestring <ref type="bibr" target="#b12">[13]</ref> leverages compiler to analyze and identify the statistical lnerability analysis is another important solution to identify SDC-causing instructions. Shoestring <ref type="bibr" target="#b12">[13]</ref> uses a static compiler analysis technology to identify SDC
ining samples (i.e., SDC proneness). We build the trees compliance with the random forest framework <ref type="bibr" target="#b26">[27]</ref>. For each tree in the random forest, we use the bootstrap
="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" t and lack of dynamic analysis of program instructions, resulting in lower error coverage. SymPLIFIED <ref type="bibr" target="#b13">[14]</ref> identifies SDC-prone instructions using symbolic execution rrors are left unprotected. Although Shoestring is time-saving, the SDC coverage is low. SymPLIFIED <ref type="bibr" target="#b13">[14]</ref> identifies SDC-prone instructions using symbolic execution
tions becomes a key problem.</p><p>Recently, a lot of works <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t rget="#b9">[10]</ref> applies vulnerability analysis to avoid the derated fault injections. Relyzer <ref type="bibr" target="#b10">[11]</ref> employs pruning techniques to decrease the quantity of fau br" target="#b9">[10]</ref> make use of vulnerability analysis to avoid derated injections. Relyzer <ref type="bibr" target="#b10">[11]</ref> employs pruning techniques to decrease the quantity of fau ct faults into these selected instructions with the aid of statistical fault injection like Relyzer <ref type="bibr" target="#b10">[11]</ref>. To simulate the data corruption faults, we flip one certa t faults into these protected instructions with the aid of statistical fault injection like Relyzer <ref type="bibr" target="#b10">[11]</ref>, and then collect the number of errors detected by our err
tructions selectively can achieve high coverage against SDCs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. These instructions are called SDC-prone instructions. Theref
ata set with the help of faults injection experiments. We use the famous fault injection tool PINFI <ref type="bibr" target="#b18">[19]</ref> to implement fault injection experiment. PINFI is built wi
="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" t and lack of dynamic analysis of program instructions, resulting in lower error coverage. SymPLIFIED <ref type="bibr" target="#b13">[14]</ref> identifies SDC-prone instructions using symbolic execution rrors are left unprotected. Although Shoestring is time-saving, the SDC coverage is low. SymPLIFIED <ref type="bibr" target="#b13">[14]</ref> identifies SDC-prone instructions using symbolic execution
b22">[23]</ref>, Parboil benchmarks <ref type="bibr" target="#b23">[24]</ref> and PARSEC benchmarks <ref type="bibr" target="#b24">[25]</ref>. These benchmarks are divided into two groups randomly: tr b22">[23]</ref>, Parboil benchmarks <ref type="bibr" target="#b23">[24]</ref> and PARSEC benchmarks <ref type="bibr" target="#b24">[25]</ref> mentioned in section 4.2.</p><p>We compile these benchmark
b22">[23]</ref>, Parboil benchmarks <ref type="bibr" target="#b23">[24]</ref> and PARSEC benchmarks <ref type="bibr" target="#b24">[25]</ref>. These benchmarks are divided into two groups randomly: tr b22">[23]</ref>, Parboil benchmarks <ref type="bibr" target="#b23">[24]</ref> and PARSEC benchmarks <ref type="bibr" target="#b24">[25]</ref> mentioned in section 4.2.</p><p>We compile these benchmark
e, how to identify the SDC-prone instructions becomes a key problem.</p><p>Recently, a lot of works <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta f type="bibr" target="#b15">[16]</ref> try to improve the static injection framework. CriticalFault <ref type="bibr" target="#b9">[10]</ref> applies vulnerability analysis to avoid the derated fault i is modeling the SDC rate of SDC-causing instructions by performing fault injections. CriticalFault <ref type="bibr" target="#b9">[10]</ref> make use of vulnerability analysis to avoid derated injecti
ata set with the help of faults injection experiments. We use the famous fault injection tool PINFI <ref type="bibr" target="#b18">[19]</ref> to implement fault injection experiment. PINFI is built wi
rks tend to protect these SDC-prone instructions selectively <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>.</p><p>It was recentl
b22">[23]</ref>, Parboil benchmarks <ref type="bibr" target="#b23">[24]</ref> and PARSEC benchmarks <ref type="bibr" target="#b24">[25]</ref>. These benchmarks are divided into two groups randomly: tr b22">[23]</ref>, Parboil benchmarks <ref type="bibr" target="#b23">[24]</ref> and PARSEC benchmarks <ref type="bibr" target="#b24">[25]</ref> mentioned in section 4.2.</p><p>We compile these benchmark
b22">[23]</ref>, Parboil benchmarks <ref type="bibr" target="#b23">[24]</ref> and PARSEC benchmarks <ref type="bibr" target="#b24">[25]</ref>. These benchmarks are divided into two groups randomly: tr b22">[23]</ref>, Parboil benchmarks <ref type="bibr" target="#b23">[24]</ref> and PARSEC benchmarks <ref type="bibr" target="#b24">[25]</ref> mentioned in section 4.2.</p><p>We compile these benchmark
get="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" tar e a novel face-focused cross-stream network (FFCSN). Different from the popular two-stream networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" targ d for action recognition in videos and has been popular for many human-centric video analysis tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b5">6]</ref>. Various improvement
igure" target="#fig_0">1</ref>). For the training data scarcity problem, we introduce meta learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" ta used as a data augmentation strategy to deal with the lack of training data. However, meta-learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" ta number of model parameters. To deal with the data scarcity problem, we propose to use meta learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" ta
rwise losses under the same setting: loss of Siamese network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref>, and loss of triplet network <ref type="bibr" target="#b1">
et="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref> to train our FFCSN (see Figure <ref type="figure">2</ref>).
s such as temporal segment network (TSN) <ref type="bibr" target="#b46">[47]</ref> and its variants <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b61">62]</ref> have been designed
uction</head><p>With the recent rapid development of human-centric AI, human-centric video analysis <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" ta
ric video analysis <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar br" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, emotion recognition in videos <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b17">18]</ref>, personality compu haviors <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, emotions <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b17">18]</ref>, or personality tr other human-centric video analysis problems such as emotion recognition from user-generated videos <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b17">18]</ref>. Extensive experim et="#b33">[34,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b53">54]</ref>. In this paper, we show that our FFCSN model can be easily get="#b33">34,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b53">54]</ref>. The comparative results are presented in Table <ref type="
et="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref> to train our FFCSN (see Figure <ref type="figure">2</ref>).
is thus in doubt. The change towards deception detection with real-life data was first advocated in <ref type="bibr" target="#b6">[7]</ref>, where the identification of deception in statements issued
s such as temporal segment network (TSN) <ref type="bibr" target="#b46">[47]</ref> and its variants <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b61">62]</ref> have been designed
anks to this benchmark dataset, more advancing ADD methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50]</ref> have been developed t in ADD. We show in experiments that our model outperforms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50]</ref> by big margin, thanks tudy results are presented in Table <ref type="table" target="#tab_0">1</ref>. It can be seen that: <ref type="bibr" target="#b0">(1)</ref> The performance continuously increases when more modules are
is problem, multi-modal fusion and knowledge transfer approaches have been proposed in recent works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" ta mpare our FFCSN model to the state-of-the-art alternatives <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" tar
d biological methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" tar
anks to this benchmark dataset, more advancing ADD methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50]</ref> have been developed t in ADD. We show in experiments that our model outperforms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50]</ref> by big margin, thanks tudy results are presented in Table <ref type="table" target="#tab_0">1</ref>. It can be seen that: <ref type="bibr" target="#b0">(1)</ref> The performance continuously increases when more modules are
ost videos: humans. This topic covers a wide range of research problems such as deception detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, emotion recogniti r" target="#b58">59]</ref>. For example, it is often important to recognize the deceptive behaviors <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, emotions <ref typ arget="#b55">56]</ref> of the subject of a video in real-world scenarios.</p><p>Deception detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> is a late addition large-scale deployment of deception detection thus depends upon automated deception detection (ADD) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37</ref>]. An ADD system can icularly, a new multimodal benchmark dataset of real-life videos from court trials is introduced in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. However, with onl effectively even with the very sparse data in the existing real-life deception detection benchmarks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>.  <ref type="formu d using a corpus collected from hearings in Italian courts (i.e., no visual data was available). In <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, a new multimodal r" target="#b49">[50]</ref> that, given the small size of the real-life ADD benchmark introduced in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, hand-crafted feat s is not surprising: deep learning models are known to be data hungry. The real-life ADD dataset in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> only provides arou e and negative classes. The former has been widely used in previous research on deception detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta Dataset. We evaluate our full FFCSN model for deception detection on a real-life multimodal dataset <ref type="bibr" target="#b35">[36]</ref>. This dataset consists of 121 court room trial video clips used like visual feature learning.</p><p>The comparative results on the real-life benchmark dataset <ref type="bibr" target="#b35">[36]</ref> are given in Table <ref type="table" target="#tab_1">2</re
get="#b48">49,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" tar
get="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b26">27]</ref> has also begun to draw much attention from the computer vis
rbal, and acoustic <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar eceptive behaviors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar
get="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b58">59]</ref>. For example, it is two-stream networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b34">35]</ref>, our FFCSN model ha <ref type="bibr" target="#b5">6]</ref>. Various improvements such as temporal segment network (TSN) <ref type="bibr" target="#b46">[47]</ref> and its variants <ref type="bibr" target="#b59">[60,</ref> le frame appearance. Therefore, we choose to modify the original twostream temporal segment network <ref type="bibr" target="#b46">[47]</ref> designed for videobased action recognition by replacing it we can define a segmental consensus classification loss similar to that of temporal segment network <ref type="bibr" target="#b46">[47]</ref>. Specifically, we divide each video into K segments {S 1 , truth label with respect to class i. We define the consensus function E with average pooling, as in <ref type="bibr" target="#b46">[47]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head 28">[29]</ref> datasets, and then pretrain the motion branch of our cross-stream base network as in <ref type="bibr" target="#b46">[47]</ref> on the UCF101 <ref type="bibr" target="#b40">[41]</ref> da ison to Temporal Segment Network. Different from the state-of-the-art temporal segment network (TSN)<ref type="bibr" target="#b46">[47]</ref>, our FFCSN model has two novel components: face detection
ype="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25]</ref> and adversarial learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta ly, based on a meta learning and adversarial learning based training strategy. Adversarial learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta
like the conventional physiological and biological methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" targe
ide range of research problems such as deception detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, emotion recognition in videos <ref type="bibr" target="#b5 it is often important to recognize the deceptive behaviors <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, emotions <ref type="bibr" target="#b53">[54,</ref><ref typ a video in real-world scenarios.</p><p>Deception detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> is a late addition to humancentric video analysis and still set of real-life videos from court trials is introduced in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. However, with only 121 video clips and half of them contai a in the existing real-life deception detection benchmarks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>.  <ref type="formula" target="#formula_3">3</ref>) We demon in Italian courts (i.e., no visual data was available). In <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, a new multimodal deception dataset of real-life videos fro he small size of the real-life ADD benchmark introduced in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, hand-crafted features are much better than deep features. are known to be data hungry. The real-life ADD dataset in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> only provides around 100 video clips, which is a number of tion thus depends upon automated deception detection (ADD) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37</ref>]. An ADD system can find applications in many real-world sce en widely used in previous research on deception detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
neration.</p><p>Unlike other trace-driven runtime optimizers for native binary code, such as Dynamo <ref type="bibr" target="#b3">[4]</ref>, we have both the rich V-ISA and a cooperating code generato amic optimization of programs. Transmeta's CMS <ref type="bibr" target="#b10">[11]</ref> and Dynamo <ref type="bibr" target="#b3">[4]</ref> identify and optimize hot traces at runtime, similar to our
n strategies in search of cost-effective long-term solutions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" tar explicit on-chip parallelism becomes more prevalent (e.g., <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>), raising potential
n we discussed in the Introduction).</p><p>Previous authors have developed Typed Assembly Languages <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b6">7]</ref> with goals that gene
languages combine fast initial compilation with adaptive reoptimization of "hot" methods (e.g., see <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target
igners. Prior work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b10">11]</ref> has discussed many potential hardware design options enable 1">[32]</ref>, and Transmeta's Crusoe family of processors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11]</ref>. All of these distinguish the virtual and physical ISAs as mechanisms can be used to assist these tasks in many ways <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar ulties faced by previous translators for DAISY <ref type="bibr" target="#b13">[14]</ref> and Crusoe <ref type="bibr" target="#b10">[11]</ref>.</p><p>? It describes a translation strategy that allows o ut (at least) a processor design with hardware mechanisms that support translation and optimization <ref type="bibr" target="#b10">[11]</ref>), and (preferably) basic cooperative hardware/software mec ely, precise exceptions, memory dependences, and self-modifying code (as well as memory-mapped I/O) <ref type="bibr" target="#b10">[11]</ref>. They use a trace-driven reoptimization scheme to optimize software and hardware techniques for transparent dynamic optimization of programs. Transmeta's CMS <ref type="bibr" target="#b10">[11]</ref> and Dynamo <ref type="bibr" target="#b3">[4]</ref> identif ge places constraints on the translator, as can be seen in DAISY's and Crusoe's translation schemes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. Not only is the e
ure">1(c</ref>) in <ref type="bibr" target="#b31">[32]</ref> and Fig. <ref type="figure">5</ref> in <ref type="bibr" target="#b23">[24]</ref>.</p><p>damental benefits to the hardware processor design s work uses both the techniques above, plus an interprocedural array bounds check removal algorithm <ref type="bibr" target="#b23">[24]</ref> and some custom interprocedural dataflow and control flow ugh to perform complete, static analysis of memory safety for a large class of type-safe C programs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13]</ref>. This work uses bo
of memory safety for a large class of type-safe C programs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13]</ref>. This work uses both the techniques above, plus an interpro ="bibr" target="#b23">[24]</ref> and some custom interprocedural dataflow and control flow analyses <ref type="bibr" target="#b12">[13]</ref>.</p><p>The interprocedural techniques listed above are tra
chitectures we term Virtual Instruction Set Computers (VISC) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar mental benefits could be exploited in potentially unlimited ways by processor designers. Prior work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" ta s been in the past. Furthermore, hardware mechanisms can be used to assist these tasks in many ways <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" ta eptions and selfmodifying code to minimize the difficulties faced by previous translators for DAISY <ref type="bibr" target="#b13">[14]</ref> and Crusoe <ref type="bibr" target="#b10">[11]</ref>.</p>< tem/38 and AS/400 family <ref type="bibr" target="#b8">[9]</ref>, the DAISY project at IBM Research <ref type="bibr" target="#b13">[14]</ref>, Smith et al.'s proposal for Codesigned Virtual Machines i ed to support modern static and dynamic optimization techniques for general-purpose software. DAISY <ref type="bibr" target="#b13">[14]</ref> developed a dynamic translation scheme for emulating multi as can be seen in DAISY's and Crusoe's translation schemes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. Not only is the entire translator program located in ROM,
et="#b13">[14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar namic optimization <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. When combined with a rich V-ISA that supports more effecti
of memory safety for a large class of type-safe C programs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13]</ref>. This work uses both the techniques above, plus an interpro ="bibr" target="#b23">[24]</ref> and some custom interprocedural dataflow and control flow analyses <ref type="bibr" target="#b12">[13]</ref>.</p><p>The interprocedural techniques listed above are tra
ure">1(c</ref>) in <ref type="bibr" target="#b31">[32]</ref> and Fig. <ref type="figure">5</ref> in <ref type="bibr" target="#b23">[24]</ref>.</p><p>damental benefits to the hardware processor design s work uses both the techniques above, plus an interprocedural array bounds check removal algorithm <ref type="bibr" target="#b23">[24]</ref> and some custom interprocedural dataflow and control flow ugh to perform complete, static analysis of memory safety for a large class of type-safe C programs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13]</ref>. This work uses bo
running program. Although previous work has quantified the theoretical benefits of high adaptivity <ref type="bibr" target="#b0">[1]</ref>, predicting and delivering this adaptation is still an open out-of-order superscalar processor and is similar to spaces that other researchers have considered <ref type="bibr" target="#b0">[1]</ref>. We vary fourteen different microarchitectural parameters ac org/ns/1.0"><head>IX. PRIOR WORK ON MICROARCHITECTURAL ADAPTIVITY</head><p>Recently, Lee and Brooks <ref type="bibr" target="#b0">[1]</ref> showed that it is possible to significantly increase process
substrate and its flexibility when implementing very different architectures named "Smart Memories" <ref type="bibr" target="#b14">[15]</ref>. Later Sankaralingam et. al. proposed the TRIPS architectu
k et. al. "Core Fusion" <ref type="bibr" target="#b8">[9]</ref> and Tarjan et al. "Core Federation" <ref type="bibr" target="#b9">[10]</ref>. These last two approaches merge simple cores together in o
argest structures and the highest level of branch speculation (named the profiling configuration).  <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> Total 627bn</p> ncy architectures where additional stages can be added to the pipeline to combat process variations <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>However,
branch speculation (named the profiling configuration).  <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> Total 627bn</p><p>For each program phase we gather hardwar an be added to the pipeline to combat process variations <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>However, these studies considered only a limited ad
overheads that are amortized over the execution of the whole phase. Model: Work by Jim?nez and Lin <ref type="bibr" target="#b28">[29]</ref> has shown how to build a perceptron-based neural branch pr
example. Wavelet analysis has also gained some attention <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>
ion to SimpleScalar <ref type="bibr" target="#b22">[23]</ref>. We altered Wattch's underlying Cacti <ref type="bibr" target="#b23">[24]</ref> models to updated circuit parameters. We also removed the
</ref> looked at multimedia applications characterised by repeated frame processing. Hsu and Kremer <ref type="bibr" target="#b33">[34]</ref> implemented a compiler algorithm that adapts the voltage a
substrate and its flexibility when implementing very different architectures named "Smart Memories" <ref type="bibr" target="#b14">[15]</ref>. Later Sankaralingam et. al. proposed the TRIPS architectu
argest structures and the highest level of branch speculation (named the profiling configuration).  <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> Total 627bn</p> ncy architectures where additional stages can be added to the pipeline to combat process variations <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>However,
be accessed multiple times within a short time interval. However, as identi ed by prior work (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta to mitigate the negative e ects of pollution and thrashing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar llution or thrashing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar ompiler-based techniques.</p><p>Much prior research (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar e. When multiple threads share the cache, prior approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> learn the thrashing behavior of individual threads using a her prior approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> to address cache thrashing or pollution individually (see T target="#b15">[16]</ref> (TA-DIP), 2) Thread-aware dynamic re-reference interval prediction policy <ref type="bibr" target="#b16">[17]</ref> (TA-DRRIP), 3) Signature-based Hit Prediction using Instru ave di erent reuse behavior.</p><p>Thread-Aware Dynamic Re-Reference Interval Prediction (TA-DRRIP) <ref type="bibr" target="#b16">[17]</ref> improves upon TA-DIP by using a better replacement policy line LRU and DIP, the last-level cache uses the re-reference interval prediction replacement policy <ref type="bibr" target="#b16">[17]</ref>. For our EAF proposals, we assume that the operations on t mproves weighted speedup by 17% compared to the LRU policy. These workloads are known to have scans <ref type="bibr" target="#b16">[17]</ref> (accesses to a large number of blocks with no reuse) which EAF mechanism to 1) a cache following the LRU replacement policy, and 2) a cache following the RRIP <ref type="bibr" target="#b16">[17]</ref> policy for all 4-core workloads. As the gure shows, EAF-ca
time interval. However, as identi ed by prior work (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" tar tion and thrashing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" tar rget="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" tar ,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>To prevent cache pollution, prior approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" ta >[34,</ref><ref type="bibr" target="#b54">55]</ref> or the memory region to which the blocks belong <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b54">55]</ref>. The mechanisms su <ref type="bibr" target="#b52">53]</ref>. Similar to some approaches we have discussed in our paper <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" ta Instruction pointers (SHIP) <ref type="bibr" target="#b54">[55]</ref>, 4) Run-time cache bypassing <ref type="bibr" target="#b18">[19]</ref> (RTB), and 5) Miss classi cation table <ref type="bibr" ta ng cache thrashing problem. <ref type="foot" target="#foot_1">2</ref>Run-time Cache Bypassing (RTB) <ref type="bibr" target="#b18">[19]</ref> addresses cache pollution by predicting reuse behavior of
of individual threads using a technique called set-dueling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, and use BIP for those threads that are determined to su er ictions.</p><p>To address this problem, we use set-dueling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> to determine if the system as a whole bene ts from an alway g at a thread granularity. It does so by using set-dueling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> to determine if a thread incurs fewer misses with the conve
d techniques (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54]</ref>) to mitigate the negative impact of pollution and thrashing
>[11]</ref>. We also evaluate three other metrics, namely, instruction throughput, harmonic speedup <ref type="bibr" target="#b27">[28]</ref>, and maximum slowdown <ref type="bibr" target="#b24">[25,< "#b10">[11,</ref><ref type="bibr" target="#b49">50]</ref>, instruction throughput, harmonic speedup <ref type="bibr" target="#b27">[28]</ref> and maximum slowdown <ref type="bibr" target="#b24">[25,</
et="#b36">[37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56]</ref> or provide QoS in multi-core systems with shared caches <re
number of page replacement policies have been proposed to improve virtual memory performance (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targ
number of page replacement policies have been proposed to improve virtual memory performance (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targ
et="#b36">[37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56]</ref> or provide QoS in multi-core systems with shared caches <re
m lter and a counter. Bloom lters have low hardware overhead and complexity. Several previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" targ
sed to modify the cache insertion policy to mitigate the negative e ects of pollution and thrashing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" ta working set is larger than the cache size. When multiple threads share the cache, prior approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> learn the thrashin ent approaches that aim to prevent pollution or thrashing: 1) Thread-aware dynamic insertion policy <ref type="bibr" target="#b15">[16]</ref> (TA-DIP), 2) Thread-aware dynamic re-reference interval pr . We now describe each mechanism individually.</p><p>Thread-Aware Dynamic Insertion Policy (TA-DIP) <ref type="bibr" target="#b15">[16]</ref> addresses the thrashing problem by determining thrashing a storage overhead compared to certain other prior approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> to address cache th st closely related work on addressing pollution or thrashing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar
