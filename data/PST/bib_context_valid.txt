preliminary work of a neural influence Diff usion Network (i.e., DiffNet) for social recommendation <ref type="bibr" target="#b42">[43]</ref>. DiffNet models the recursive social diffusion process for ">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p><p>In fact, as users play a central role in social pla ">[51]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b42">[43]</ref>. On one hand, given the useritem interest graph, NGCF is p that the higher-order social structure is directly modeled in the recursive user embedding process <ref type="bibr" target="#b42">[43]</ref>. These graph based models showed superior performance comp ary, our main contributions are listed as follows:</p><p>? Compared to our previous work of DiffNet <ref type="bibr" target="#b42">[43]</ref>, we revisit the social recommendation problem as predictin d social recommendation model, DiffNet, for modeling the social diffusion process in recommendation <ref type="bibr" target="#b42">[43]</ref>. DiffNet advances classical embedding based models with ca at the up to K-th order social network structure is injected into the social recommendation process <ref type="bibr" target="#b42">[43]</ref>. In this part, we propose DiffNet++, an enhanced model of with user and item attributes, and are adopted as datasets of our previously proposed DiffNet model <ref type="bibr" target="#b42">[43]</ref>. The remaining two datasets of Epinions and Dianping do no t, in order to transform this model for the recommendation task. For our proposed models of DiffNet <ref type="bibr" target="#b42">[43]</ref> and DiffNet++, since both models are flexible and could be 9]</ref> and Normalized Discounted Cummulative Gain (NDCG) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Specifically, HR measures the percentage of hit items in formance with large itemset, similar as many other works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b42">[43]</ref>, to evaluate the performance, for each user, we randomly s
for learning graph structures with theoretical elegance, practical flexibility and high performance <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" t ns to non-Euclidean graph and have empirically shown great success in graph representation learning <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" t
<ref type="bibr" target="#b38">[39]</ref>, and efficient training models without negative sampling <ref type="bibr" target="#b5">[6]</ref>. All these previous works focused on how to explore the soci
for learning graph structures with theoretical elegance, practical flexibility and high performance <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" t ns to non-Euclidean graph and have empirically shown great success in graph representation learning <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" t
he user embedding learning process with social neighbors <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bib r" target="#b14">[15]</ref>. For example, SocialMF <ref type="bibr" target="#b18">[19]</ref> and SR <ref type="bibr" target="#b29">[30]</ref> added social regularization terms based on social neighbor ong users could be leveraged to alleviate the sparsity in CF and enhance recommendation performance <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bib o categories: the social regularization based approaches <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bib
nds of graphs <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b42">[43]</ref>. On one hand, g treating the user-item interaction as a graph structure, GCNs have been applied for recommendation <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Earlier works ="#b50">[51]</ref>. Therefore, many recent works focus on the spatial based GCNs for recommendation <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" based content recommendation model by propagating item features in the item-item correlation graph <ref type="bibr" target="#b47">[48]</ref>. GC-MC applied graph neural network for CF, with the first the graph based recommendation models of GraphRec <ref type="bibr" target="#b9">[10]</ref>, PinSage <ref type="bibr" target="#b47">[48]</ref>, NGCF <ref type="bibr" target="#b40">[41]</ref>. Please no lated studies have also empirically found similar trends <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b47">[48]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>
ender systems <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Therefore, the
user-item graph, and further leveraged the sequence embedding techniques for social recommendation <ref type="bibr" target="#b10">[11]</ref>. This model could better capture the higher-order social n p of all attention modeling in the following, as all of them share the similar form as shown in Eq. <ref type="bibr" target="#b10">(11)</ref>. Similarly, we calculate the interest influence score ? k+
ng social relations among users to alleviate data sparsity and enhancing recommendation performance <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bib ocial neighbors' records, or regularizing the user embedding learning process with social neighbors <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bib e="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b14">[15]</ref>. For example, SocialMF <ref type="bibr" target="#b18">[19]</ref> and SR <ref type="bibr" target="#b29">[30]</ref> added soc s could be summarized into the following two categories: the social regularization based approaches <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bib ]</ref>, FM <ref type="bibr" target="#b35">[36]</ref>), social based recommendation model (SocialMF <ref type="bibr" target="#b18">[19]</ref>, TrustSVD <ref type="bibr" target="#b13">[14]</ref>, Conte
ial neighbors <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b14">[15]</ref>. For example, S ed approaches <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bib work under a collective matrix factorization framework with carefully designed regularization terms <ref type="bibr" target="#b20">[21]</ref>. Social recommendation has also been extended with social type="bibr" target="#b18">[19]</ref>, TrustSVD <ref type="bibr" target="#b13">[14]</ref>, ContextMF <ref type="bibr" target="#b20">[21]</ref>, CNSR <ref type="bibr" target="#b43">[44]</ref>), as well
">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b43">[44]</ref> and the user be argest Chinese location based social network, and we use this dataset that is crawled by authors in <ref type="bibr" target="#b25">[26]</ref>. This dataset is also publicly available 5 .</p><p>Among t
erent KGs. Recent works on multilingual KG embeddings provide support for automated entity matching <ref type="bibr" target="#b5">(Chen et al., 2017</ref><ref type="bibr" target="#b4">(Chen et al., , extended embedding models to bridge multiple KGs, typically for KGs of multiple languages. MTransE <ref type="bibr" target="#b5">(Chen et al., 2017)</ref> jointly learns a transformation across two s The embedding learning process jointly trains the knowledge model and the alignment model following <ref type="bibr" target="#b5">Chen et al. (2017)</ref>, while self-learning is added to improve the G i and G j . Î» is a positive hyperparameter that weights the two model components.</p><p>Following <ref type="bibr" target="#b5">Chen et al. (2017)</ref>, instead of directly optimizing J in Eq. ( <r
ods include AdaBoost <ref type="bibr" target="#b10">(Freund and Schapire, 1997)</ref> and RankBoost <ref type="bibr" target="#b9">(Freund et al., 2004)</ref>, which target at classification and rankin ng model that makes more accurate predictions should receive a higher weight. Inspired by RankBoost <ref type="bibr" target="#b9">(Freund et al., 2004)</ref>, we reduce the ranking combination problem = {q1 = (The Tale of Genji, country, ?t) q2 = (The Tale of Genji, genre, ?t)} Similar to RankBoost <ref type="bibr" target="#b9">(Freund et al., 2004)</ref> Ranking loss. The overall objective of KEn <p>Let the set of all the critical entity pairs from all the validation queries of an entity as P . <ref type="bibr" target="#b9">Freund et al. (2004)</ref> have proved that, when using RankBoost, thi
lations, constituting actionable knowledge that is crucial to various knowledge-driven applications <ref type="bibr" target="#b16">(Koncel-Kedziorski et al., 2019;</ref><ref type="bibr" target="#b3">C
provide support for automated entity matching <ref type="bibr" target="#b5">(Chen et al., 2017</ref><ref type="bibr" target="#b4">(Chen et al., , 2018b;;</ref><ref type="bibr" target="#b26">Sun et al. also leverage side information to enhance the alignment performance, including entity descriptions <ref type="bibr" target="#b4">(Chen et al., 2018b;</ref><ref type="bibr" target="#b40">Zhang et al.,
a pair of unaligned entities in two KGs are mutual nearest neighbors according to the CSLS measure <ref type="bibr" target="#b7">(Conneau et al., 2018)</ref>, KEnS adds this highly confident alignmen
edge bases, including DBpedia <ref type="bibr" target="#b17">(Lehmann et al., 2015)</ref>, Wikidata <ref type="bibr" target="#b33">(VrandeÄiÄ and KrÃ¶tzsch, 2014)</ref> and YAGO <ref type="bibr" target
t al., 2017;</ref><ref type="bibr" target="#b39">Yang et al., 2019)</ref>, neighborhood information <ref type="bibr" target="#b36">(Wang et al., 2018;</ref><ref type="bibr" target="#b38">Yang et al.,
br">), attributes (Trsedya et al., 2019;</ref><ref type="bibr" target="#b25">Sun et al., 2017;</ref><ref type="bibr" target="#b39">Yang et al., 2019)</ref>, neighborhood information <ref type="bibr" t
embedding. There are also various methods falling into the groups of Bilinear models such as RESCAL <ref type="bibr" target="#b20">(Nickel et al., 2011)</ref> and DistMult <ref type="bibr" target="#b3
embedding. There are also various methods falling into the groups of Bilinear models such as RESCAL <ref type="bibr" target="#b20">(Nickel et al., 2011)</ref> and DistMult <ref type="bibr" target="#b3
er is to extend the ensemble transfer mechanism to population sparse domain knowledge in biological <ref type="bibr" target="#b12">(Hao et al., 2020)</ref> and medical knowledge bases <ref type="bibr"
odels achieve satisfactory performance on KG completion and are robust against the sparsity of data <ref type="bibr" target="#b11">(Hao et al., 2019)</ref>. RotatE <ref type="bibr" target="#b30">(Sun
e="bibr" target="#b38">Yang et al., 2015;</ref><ref type="bibr" target="#b18">Li et al., 2019;</ref><ref type="bibr" target="#b28">Sun et al., 2019a</ref><ref type="bibr" target="#b27">Sun et al., , 2
Pei et al., 2019)</ref>. A systematic summary of relevant approaches is given in a recent survey by <ref type="bibr" target="#b29">Sun et al. (2020b)</ref>. Although these approaches focus on the KG a
ell as neural models like HolE <ref type="bibr" target="#b19">(Nickel et al., 2016)</ref> and ConvE <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref>. Due to the large body of work in this l
bibr" target="#b36">(Wang et al., 2018;</ref><ref type="bibr" target="#b38">Yang et al., 2015;</ref><ref type="bibr" target="#b18">Li et al., 2019;</ref><ref type="bibr" target="#b28">Sun et al., 2019
ang et al., 2014)</ref> and bilinear models <ref type="bibr" target="#b38">(Yang et al., 2015;</ref><ref type="bibr" target="#b31">Trouillon et al., 2016)</ref> have achieved satisfactory performance ibr" target="#b0">(Bordes et al., 2013;</ref><ref type="bibr" target="#b38">Yang et al., 2015;</ref><ref type="bibr" target="#b31">Trouillon et al., 2016)</ref>, which may also serve as a weight metri
">(Breiman, 1996)</ref>, stacking <ref type="bibr" target="#b37">(Wolpert, 1992)</ref> and boosting <ref type="bibr" target="#b10">(Freund and Schapire, 1997)</ref>. Boosting methods seek to combine m learning model weights from the sample distribution.</p><p>Representative methods include AdaBoost <ref type="bibr" target="#b10">(Freund and Schapire, 1997)</ref> and RankBoost <ref type="bibr" targ
for estimating the prediction confidence of each language-specific embedding in ensemble inference <ref type="bibr" target="#b23">(Shen et al., 2017)</ref>. Let the MRR of f i be u i on the validatio
a pair of unaligned entities in two KGs are mutual nearest neighbors according to the CSLS measure <ref type="bibr" target="#b7">(Conneau et al., 2018)</ref>, KEnS adds this highly confident alignmen
we only provide a highly selective summary here. Interested readers are referred to recent surveys <ref type="bibr" target="#b34">(Wang et al., 2017;</ref><ref type="bibr" target="#b14">Ji et al., 20
knowledge-driven applications <ref type="bibr" target="#b16">(Koncel-Kedziorski et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Bordes et al.,
o gather to the requesting core, only the requested elements from the data array.</p><p>Song et al. <ref type="bibr" target="#b10">[11]</ref> propose a graph processor based on sparse matrix algebra,
a graph. Determining the relationships between entities in a graph is the basis of graph analytics <ref type="bibr" target="#b1">[2]</ref>. Graph analytics poses important challenges on existing proc
#fig_2">3</ref>. PIUMA multi-threaded cores (MTC) are round-robin multi-threaded in-order pipelines <ref type="bibr" target="#b5">[6]</ref>. At any moment, each thread can only have one in-flight inst
the network needs to have a high radix and a low diameter. This is achieved with a HyperX topology <ref type="bibr" target="#b6">[7]</ref>, with all-toall connections on each level. Links on the high
the network needs to have a high radix and a low diameter. This is achieved with a HyperX topology <ref type="bibr" target="#b6">[7]</ref>, with all-toall connections on each level. Links on the high
progress. Nevertheless, GPUs usually perform better on graph algorithms than CPUs for small graphs <ref type="bibr" target="#b4">[5]</ref>, because they have more threads, which hides memory latency,
d "dense" compute, such as matrix computations. The continuing exponential growth in generated data <ref type="bibr" target="#b0">[1]</ref> has shifted compute to offload to GPUs and other focused acc
are important contributors to PUMA's performance and energy efficiency.</p><p>The Emu architecture <ref type="bibr" target="#b8">[9]</ref> is a recently proposed architecture for big data analysis, i
even worse performance for small node counts, due to the overhead of the fine-grained communication <ref type="bibr" target="#b12">[13]</ref>. For PIUMA, the application code does not need to change f
d "dense" compute, such as matrix computations. The continuing exponential growth in generated data <ref type="bibr" target="#b0">[1]</ref> has shifted compute to offload to GPUs and other focused acc
even worse performance for small node counts, due to the overhead of the fine-grained communication <ref type="bibr" target="#b12">[13]</ref>. For PIUMA, the application code does not need to change f
ous success in the field of natural language processing (NLP) since the development of Transformers <ref type="bibr" target="#b27">(Vaswani et al. 2017)</ref> which are currently the best performing n nal information. Since Laplacian PEs are generalization of the PE used in the original transformers <ref type="bibr" target="#b27">(Vaswani et al. 2017)</ref> to graphs and these better help encode di r</head><p>The Graph Transformer is closely the same transformer architecture initially proposed in <ref type="bibr" target="#b27">(Vaswani et al. 2017)</ref>, see Figure <ref type="figure">1</ref> (L target="#tab_2">2</ref>), which employs multi-headed attention inspired by the original transformer <ref type="bibr" target="#b27">(Vaswani et al. 2017)</ref> and have been often used in the literatur e each word attending to each other word in a sentence, as followed by the Transformer architecture <ref type="bibr" target="#b27">(Vaswani et al. 2017</ref>). -b) Next, the so-called graph considered
on specialized cases such as on heterogeneous graphs, temporal networks, generative modeling, etc. <ref type="bibr" target="#b35">(Yun et al. 2019;</ref><ref type="bibr" target="#b33">Xu, Joshi, and
and have achieved significant success on a wide range of applications, such as in knowledge graphs <ref type="bibr" target="#b25">(Schlichtkrull et al. 2018;</ref><ref type="bibr" target="#b5">Chami
f type="bibr" target="#b33">Xu, Joshi, and Bresson 2019;</ref><ref type="bibr">Hu et al. 2020;</ref><ref type="bibr" target="#b37">Zhou et al. 2020</ref>). The model proposed in <ref type="bibr" targe based on the timestamp differences of the central node and the message-passing nodes. Furthermore, <ref type="bibr" target="#b37">Zhou et al. (2020)</ref> proposed a transformer based generative mode
ef>. Transformers based models have led to state-of-the-art performance on several NLP applications <ref type="bibr" target="#b8">(Devlin et al. 2018;</ref><ref type="bibr" target="#b23">Radford et al or node Laplacian position eigenvectors <ref type="bibr" target="#b2">(Belkin and Niyogi 2003;</ref><ref type="bibr" target="#b8">Dwivedi et al. 2020)</ref>, or relative learnable positional informati You, Ying, and Leskovec 2019;</ref><ref type="bibr" target="#b26">Srinivasan and Ribeiro 2020;</ref><ref type="bibr" target="#b8">Dwivedi et al. 2020;</ref><ref type="bibr" target="#b14">Li et al. 202 arget="#b11">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type="bibr" target="#b8">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref ty 'Graph Transformer' for this task. The size of this dataset is 12K graphs. We refer the readers to <ref type="bibr" target="#b8">(Dwivedi et al. 2020)</ref> for additional information, inlcuding prep e the 'Graph Transformer with edge features' for this task. We use the 12K subset of the data as in <ref type="bibr" target="#b8">Dwivedi et al. (2020)</ref>.</p><p>PATTERN, Node Classification PATTER
trained on graph datasets learn structural node information that are invariant to the node position <ref type="bibr" target="#b26">(Srinivasan and Ribeiro 2020)</ref>. This is a critical reason why si ="#b19">(Murphy et al. 2019;</ref><ref type="bibr" target="#b34">You, Ying, and Leskovec 2019;</ref><ref type="bibr" target="#b26">Srinivasan and Ribeiro 2020;</ref><ref type="bibr" target="#b8">Dwive
ositions is challenging as there are symmetries which prevent canonical node positional information <ref type="bibr" target="#b19">(Murphy et al. 2019)</ref>. In fact, most of the GNNs which are train ormance on graph datasets. The issue of positional embeddings has been explored in recent GNN works <ref type="bibr" target="#b19">(Murphy et al. 2019;</ref><ref type="bibr" target="#b34">You, Ying, a
original graph computed using WL algorithm <ref type="bibr" target="#b36">(Zhang et al. 2020;</ref><ref type="bibr" target="#b21">Niepert, Ahmed, and Kutzkov 2016)</ref>, are not variant to the subgr
and have achieved significant success on a wide range of applications, such as in knowledge graphs <ref type="bibr" target="#b25">(Schlichtkrull et al. 2018;</ref><ref type="bibr" target="#b5">Chami

and have achieved significant success on a wide range of applications, such as in knowledge graphs <ref type="bibr" target="#b25">(Schlichtkrull et al. 2018;</ref><ref type="bibr" target="#b5">Chami
practical to have a Graph Transformer where a node attends to local node neighbors, same as in GNNs <ref type="bibr" target="#b7">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" ta
ef>. Transformers based models have led to state-of-the-art performance on several NLP applications <ref type="bibr" target="#b8">(Devlin et al. 2018;</ref><ref type="bibr" target="#b23">Radford et al or node Laplacian position eigenvectors <ref type="bibr" target="#b2">(Belkin and Niyogi 2003;</ref><ref type="bibr" target="#b8">Dwivedi et al. 2020)</ref>, or relative learnable positional informati You, Ying, and Leskovec 2019;</ref><ref type="bibr" target="#b26">Srinivasan and Ribeiro 2020;</ref><ref type="bibr" target="#b8">Dwivedi et al. 2020;</ref><ref type="bibr" target="#b14">Li et al. 202 arget="#b11">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type="bibr" target="#b8">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref ty 'Graph Transformer' for this task. The size of this dataset is 12K graphs. We refer the readers to <ref type="bibr" target="#b8">(Dwivedi et al. 2020)</ref> for additional information, inlcuding prep e the 'Graph Transformer with edge features' for this task. We use the 12K subset of the data as in <ref type="bibr" target="#b8">Dwivedi et al. (2020)</ref>.</p><p>PATTERN, Node Classification PATTER
r" target="#b13">Kipf and Welling 2017;</ref><ref type="bibr" target="#b17">Monti et al. 2017;</ref><ref type="bibr" target="#b9">Gilmer et al. 2017;</ref><ref type="bibr" target="#b28">VeliÄkoviÄ et
s <ref type="bibr" target="#b36">(Zhang et al. 2020)</ref>, or node Laplacian position eigenvectors <ref type="bibr" target="#b2">(Belkin and Niyogi 2003;</ref><ref type="bibr" target="#b8">Dwivedi et ormula">2020</ref>) make the use of available graph structure to pre-compute Laplacian eigenvectors <ref type="bibr" target="#b2">(Belkin and Niyogi 2003)</ref> and use them as node positional informa
ef>. Transformers based models have led to state-of-the-art performance on several NLP applications <ref type="bibr" target="#b8">(Devlin et al. 2018;</ref><ref type="bibr" target="#b23">Radford et al or node Laplacian position eigenvectors <ref type="bibr" target="#b2">(Belkin and Niyogi 2003;</ref><ref type="bibr" target="#b8">Dwivedi et al. 2020)</ref>, or relative learnable positional informati You, Ying, and Leskovec 2019;</ref><ref type="bibr" target="#b26">Srinivasan and Ribeiro 2020;</ref><ref type="bibr" target="#b8">Dwivedi et al. 2020;</ref><ref type="bibr" target="#b14">Li et al. 202 arget="#b11">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type="bibr" target="#b8">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref ty 'Graph Transformer' for this task. The size of this dataset is 12K graphs. We refer the readers to <ref type="bibr" target="#b8">(Dwivedi et al. 2020)</ref> for additional information, inlcuding prep e the 'Graph Transformer with edge features' for this task. We use the 12K subset of the data as in <ref type="bibr" target="#b8">Dwivedi et al. (2020)</ref>.</p><p>PATTERN, Node Classification PATTER
s, same as in GNNs <ref type="bibr" target="#b7">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b13">Kipf and Welling 2017;</ref><ref type="bibr" target="#b17">Monti et a ERT uses  <ref type="table" target="#tab_1">1</ref>) on each dataset against the GNN baselines (GCN <ref type="bibr" target="#b13">(Kipf and Welling 2017)</ref>, GAT <ref type="bibr" target="#b28">(Ve
Ä¥ +1 i</formula><p>, Ä¥ +1 i denote intermediate representations, and Norm can either be Layer-Norm <ref type="bibr" target="#b1">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type="bibr" targe
s <ref type="bibr" target="#b36">(Zhang et al. 2020)</ref>, or node Laplacian position eigenvectors <ref type="bibr" target="#b2">(Belkin and Niyogi 2003;</ref><ref type="bibr" target="#b8">Dwivedi et ormula">2020</ref>) make the use of available graph structure to pre-compute Laplacian eigenvectors <ref type="bibr" target="#b2">(Belkin and Niyogi 2003)</ref> and use them as node positional informa
original graph computed using WL algorithm <ref type="bibr" target="#b36">(Zhang et al. 2020;</ref><ref type="bibr" target="#b21">Niepert, Ahmed, and Kutzkov 2016)</ref>, are not variant to the subgr
r" target="#b13">Kipf and Welling 2017;</ref><ref type="bibr" target="#b17">Monti et al. 2017;</ref><ref type="bibr" target="#b9">Gilmer et al. 2017;</ref><ref type="bibr" target="#b28">VeliÄkoviÄ et
s, same as in GNNs <ref type="bibr" target="#b7">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b13">Kipf and Welling 2017;</ref><ref type="bibr" target="#b17">Monti et a ERT uses  <ref type="table" target="#tab_1">1</ref>) on each dataset against the GNN baselines (GCN <ref type="bibr" target="#b13">(Kipf and Welling 2017)</ref>, GAT <ref type="bibr" target="#b28">(Ve
s h a 1 _ b a s e 6 4 = " e A Z 8 7  This paper presents progress in diffusion probabilistic models <ref type="bibr" target="#b49">[50]</ref>. A diffusion probabilistic model (which we will call a "di onals in p Î¸ (x tâ1 |x t ), because both processes have the same functional form when Î² t are small <ref type="bibr" target="#b49">[50]</ref>. A notable property of the forward process is that it admi ing to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance <ref type="bibr" target="#b49">[50]</ref>.</p><p>Second, to represent the mean Âµ Î¸ (x t , t), we pro orithms serve as a compression interpretation of the variational bound (5) of Sohl-Dickstein et al. <ref type="bibr" target="#b49">[50]</ref>, not yet as a practical compression system. </p></div> <di educed variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. <ref type="bibr" target="#b49">[50]</ref>; we include it here only for completeness.</p><formula xml ments so that the number of neural network evaluations needed during sampling matches previous work <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52]</ref>. We set the forwar
ef>, variational walkback <ref type="bibr" target="#b12">[13]</ref>, generative stochastic networks <ref type="bibr" target="#b0">[1]</ref>, and others <ref type="bibr" target="#b46">[47,</ref><ref ty
ed Work</head><p>While diffusion models might resemble flows <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targ
we use a U-Net backbone similar to an unmasked PixelCNN++ <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b44">45]</ref> with group normalization throughout <ref type="bibr" target ture follows the backbone of PixelCNN++ <ref type="bibr" target="#b48">[49]</ref>, which is a U-Net <ref type="bibr" target="#b44">[45]</ref> based on a Wide ResNet <ref type="bibr" target="#b63">[64]
tortion curves can be computed over distortion penalties in one run of annealed importance sampling <ref type="bibr" target="#b21">[22]</ref>. Our progressive decoding argument can be seen in convolut
rget="#b22">23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" tar
stributions used in VAE decoders and autoregressive models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b48">49]</ref>, our choice here ensures that the variational bound is a lo .</p><p>To represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b44">45]</ref> with group normali >B Experimental details</head><p>Our neural network architecture follows the backbone of PixelCNN++ <ref type="bibr" target="#b48">[49]</ref>, which is a U-Net <ref type="bibr" target="#b44">[45]</ref we obtained poorer samples reminiscent of the overfitting artifacts in an unregularized PixelCNN++ <ref type="bibr" target="#b48">[49]</ref>. We set dropout rate on the other datasets to zero without
ef>, variational walkback <ref type="bibr" target="#b12">[13]</ref>, generative stochastic networks <ref type="bibr" target="#b0">[1]</ref>, and others <ref type="bibr" target="#b46">[47,</ref><ref ty
arget="#b48">[49,</ref><ref type="bibr" target="#b44">45]</ref> with group normalization throughout <ref type="bibr" target="#b61">[62]</ref>. Parameters are shared across time, which is specified to We replaced weight normalization <ref type="bibr" target="#b45">[46]</ref> with group normalization <ref type="bibr" target="#b61">[62]</ref> to make the implementation simpler. Apart from an initial
s different aspects of reconstructions that Î¸ must perform <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>. We will see in our experiments that this reweighting leads
arget="#b29">30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref> and VAEs <ref type="bibr" target="#b30">[31,</ref><ref type
0"><head n="1">INTRODUCTION</head><p>Self-attention-based architectures, in particular Transformers <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref>, have become the model of choice in natu ww.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In model design we follow the original Transformer <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> as closely as possible. An advantage of sulting sequence of embedding vectors serves as input to the encoder.</p><p>The Transformer encoder <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> consists of alternating layers of multih xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Transformers were proposed by <ref type="bibr" target="#b43">Vaswani et al. (2017)</ref> for machine translation, and have since b i-c.org/ns/1.0"><head>APPENDIX A MULTIHEAD SELF-ATTENTION</head><p>Standard qkv self-attention (SA, <ref type="bibr" target="#b43">Vaswani et al. (2017)</ref>) is a popular building block for neural a
replacing the convolutions entirely <ref type="bibr" target="#b36">(Ramachandran et al., 2019;</ref><ref type="bibr" target="#b44">Wang et al., 2020a)</ref>. The latter models, while theoretically eff n the extreme case only along individual axes <ref type="bibr" target="#b17">(Ho et al., 2019;</ref><ref type="bibr" target="#b44">Wang et al., 2020a)</ref>. Many of these specialized attention archit
2020)</ref>, CIFAR-10/100 <ref type="bibr" target="#b23">(Krizhevsky, 2009)</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b31">(Parkhi et al., 2012)</ref>, and Oxford Flowers-102 <ref type="bibr"
></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 AXIAL ATTENTION</head><p>Axial Attention <ref type="bibr" target="#b19">(Huang et al., 2020;</ref><ref type="bibr" target="#b17">Ho et al., 2


architectures are still state of the art <ref type="bibr" target="#b29">(Mahajan et al., 2018;</ref><ref type="bibr" target="#b51">Xie et al., 2020;</ref><ref type="bibr" target="#b22">Kolesnikov et a target="#b29">(Mahajan et al., 2018;</ref><ref type="bibr" target="#b40">Touvron et al., 2019;</ref><ref type="bibr" target="#b51">Xie et al., 2020)</ref>. Moreover, <ref type="bibr" target="#b38">Sun </ref>, which performs supervised transfer learning with large ResNets. The second is Noisy Student <ref type="bibr" target="#b51">(Xie et al., 2020)</ref>, which is a large EfficientNet trained using
></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 AXIAL ATTENTION</head><p>Axial Attention <ref type="bibr" target="#b19">(Huang et al., 2020;</ref><ref type="bibr" target="#b17">Ho et al., 2

arget="#b25">(LeCun et al., 1989;</ref><ref type="bibr" target="#b24">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b15">He et al., 2016)</ref>. Inspired by NLP successes, multiple works try h smaller patch size are computationally more expensive.</p><p>For the baseline CNNs, we use ResNet <ref type="bibr" target="#b15">(He et al., 2016)</ref>, but replace the Batch Normalization layers <

get="#b21">22]</ref>, there are several attempts to adopt GNNs to learn with heterogeneous networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta ntly, studies have attempted to extend GNNs for modeling heterogeneous graphs. Schlichtkrull et al. <ref type="bibr" target="#b13">[14]</ref> propose the relational graph convolutional networks (RGCN) heterogeneous GNNs as baselines, including:</p><p>â¢ Relational Graph Convolutional Networks (RGCN) <ref type="bibr" target="#b13">[14]</ref>, which keeps a different weight for each relationship, i.e refers to HGT +RT E +H e t e r .</p><p>GNN Models GCN <ref type="bibr" target="#b8">[9]</ref> RGCN <ref type="bibr" target="#b13">[14]</ref> GAT <ref type="bibr" target="#b21">[22]</ref> HetGNN <ref
ssical paradigms is to define and use meta paths to model heterogeneous structures, such as PathSim <ref type="bibr" target="#b17">[18]</ref> and metapath2vec <ref type="bibr" target="#b2">[3]</ref>. ents the inverse of Ï(e). The classical meta path paradigm <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> is defined as a s esentation learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>, while the dynami
ttempts to adopt GNNs to learn with heterogeneous networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar that adopts different RNNs for different node types to integrate multi-modal features. Wang et al. <ref type="bibr" target="#b22">[23]</ref> extend graph attention networks by maintaining different w s model in PyG following the authors' official code. â¢ Heterogeneous Graph Attention Networks (HAN) <ref type="bibr" target="#b22">[23]</ref> design hierarchical attentions to aggregate neighbor infor GAT <ref type="bibr" target="#b21">[22]</ref> HetGNN <ref type="bibr" target="#b26">[27]</ref> HAN <ref type="bibr" target="#b22">[23]</ref> HGT the same. All baselines are optimized via the AdamW op regarded to map heterogeneous data into the same distribution, which is also adopted in literature <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Implementat
formance of the proposed model and its realworld applications, we use the Open Academic Graph (OAG) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" ta
formance of the proposed model and its realworld applications, we use the Open Academic Graph (OAG) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" ta
formance of the proposed model and its realworld applications, we use the Open Academic Graph (OAG) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" ta
ks. All baselines as well as our own model, are implemented via the PyTorch Geometric (PyG) package <ref type="bibr" target="#b4">[5]</ref>.</p><p>The first class of GNN baselines is designed for homo
ssical paradigms is to define and use meta paths to model heterogeneous structures, such as PathSim <ref type="bibr" target="#b17">[18]</ref> and metapath2vec <ref type="bibr" target="#b2">[3]</ref>. ents the inverse of Ï(e). The classical meta path paradigm <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> is defined as a s esentation learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>, while the dynami
ond, OAG has been consistently evolving, e.g., 1) the volume of publications doubles every 12 years <ref type="bibr" target="#b3">[4]</ref>, and 2) the KDD conference was more related to database in t
alworld applications, we use the Open Academic Graph (OAG) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref> as our experimental
linear projection A-Linear Ï (t ) to the updated vector H (l ) [t], followed by residual connection <ref type="bibr" target="#b7">[8]</ref> as:</p><formula xml:id="formula_11">H (l ) [t] = A-Linear Ï
rks <ref type="bibr">(Xu et al., 2018b;</ref><ref type="bibr" target="#b23">Liao et al., 2019;</ref><ref type="bibr" target="#b20">Klicpera et al., 2018;</ref><ref type="bibr" target="#b21">Li et al., empirically demonstrated in many recent works <ref type="bibr" target="#b34">(Wu et al., 2019;</ref><ref type="bibr" target="#b20">Klicpera et al., 2018;</ref><ref type="bibr">Xu et al., 2018a)</ref>, We also established a strong connection between AdaGCN and previous state-of-the-art PPNP and APPNP <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref> method that leverages personalized page al networks suffer from overfitting to a single splitting of training, validation and test datasets <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref>. To address this problem, inspired by < <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref>. To address this problem, inspired by <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref>, we test all approaches on multiple ran igure" target="#fig_5">3</ref>. In Table <ref type="table">2</ref>, we employ the same baselines as <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref>: V.GCN (vanilla GCN) (Kipf &amp; Wellin f type="bibr" target="#b13">(Hamilton et al., 2017)</ref>. We refer to the result of baselines from <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref> / and the implementation of AdaGCN is a
NP, showing more efficiency on graphs with few labeled nodes. Inspired by the Layer Effect on graphs<ref type="bibr" target="#b31">(Sun et al., 2019)</ref>, we argue that the increase of layers in Ada
, which is definitely costly in computation. Besides, Multi-Scale Deep Graph Convolutional Networks <ref type="bibr" target="#b25">(Luan et al., 2019)</ref> also theoretically demonstrated that the ou
epresentation for vision tasks. This insight has been empirically demonstrated in many recent works <ref type="bibr" target="#b34">(Wu et al., 2019;</ref><ref type="bibr" target="#b20">Klicpera et al. ve joint optimization of multiple parameter matrices. Similarly, Simplified Graph Convolution (SGC) <ref type="bibr" target="#b34">(Wu et al., 2019</ref>) also adopted this practice, arguing that nonl ( ?l X) Z = AdaBoost(Z (l) ) (7)</formula><p>Note that f ? is non-linear, rather than linear in SGC <ref type="bibr" target="#b34">(Wu et al., 2019)</ref>, to guarantee the representation power. As sh d 2, there is an apparent difference among the architectures of GCN (Kipf &amp; Welling, 2017), SGC <ref type="bibr" target="#b34">(Wu et al., 2019)</ref>, JK <ref type="bibr">(Xu et al., 2018b)</ref> daGCN with <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref> and Simple Graph Convolution (SGC) <ref type="bibr" target="#b34">(Wu et al., 2019)</ref> in Figure <ref type="figure" target="#fig_5">
sy to implement and keeps competitive in terms of both practical performance and computational cost <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>. Moreover, boosting theory has been used e apply SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss function) algorithm <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>, a natural and clean multi-class extensio -err (l) ) &gt; 1/K, i.e., the accuracy of each weak classifier should be better than random guess <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>. This can be met easily to guarantee the v xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM</head><p>In practice, we employ SAMME.R <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>, the soft version of SAMME, in AdaGCN. SA ">(Hastie et al., 2009)</ref>, the soft version of SAMME, in AdaGCN. SAMME.R (R for Real) algorithm <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref> leverages real-valued confidence-rated pr .org/ns/1.0"><head n="5">DISCUSSIONS AND CONCLUSION</head><p>One potential concern is that AdaBoost <ref type="bibr" target="#b14">(Hastie et al., 2009;</ref><ref type="bibr" target="#b10">Freund et a
epresentation for vision tasks. This insight has been empirically demonstrated in many recent works <ref type="bibr" target="#b34">(Wu et al., 2019;</ref><ref type="bibr" target="#b20">Klicpera et al. ve joint optimization of multiple parameter matrices. Similarly, Simplified Graph Convolution (SGC) <ref type="bibr" target="#b34">(Wu et al., 2019</ref>) also adopted this practice, arguing that nonl ( ?l X) Z = AdaBoost(Z (l) ) (7)</formula><p>Note that f ? is non-linear, rather than linear in SGC <ref type="bibr" target="#b34">(Wu et al., 2019)</ref>, to guarantee the representation power. As sh d 2, there is an apparent difference among the architectures of GCN (Kipf &amp; Welling, 2017), SGC <ref type="bibr" target="#b34">(Wu et al., 2019)</ref>, JK <ref type="bibr">(Xu et al., 2018b)</ref> daGCN with <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref> and Simple Graph Convolution (SGC) <ref type="bibr" target="#b34">(Wu et al., 2019)</ref> in Figure <ref type="figure" target="#fig_5">
al., 2017)</ref> has already successfully incorporated boosting algorithm into the training of GAN <ref type="bibr" target="#b11">(Goodfellow et al., 2014)</ref>.</p><p>In this work, we focus on inco
ive way for better predictions.</p><p>There are some works <ref type="bibr">(Xu et al., 2018b;</ref><ref type="bibr" target="#b23">Liao et al., 2019;</ref><ref type="bibr" target="#b20">Klicpera et al
sy to implement and keeps competitive in terms of both practical performance and computational cost <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>. Moreover, boosting theory has been used e apply SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss function) algorithm <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>, a natural and clean multi-class extensio -err (l) ) &gt; 1/K, i.e., the accuracy of each weak classifier should be better than random guess <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>. This can be met easily to guarantee the v xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM</head><p>In practice, we employ SAMME.R <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>, the soft version of SAMME, in AdaGCN. SA ">(Hastie et al., 2009)</ref>, the soft version of SAMME, in AdaGCN. SAMME.R (R for Real) algorithm <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref> leverages real-valued confidence-rated pr .org/ns/1.0"><head n="5">DISCUSSIONS AND CONCLUSION</head><p>One potential concern is that AdaBoost <ref type="bibr" target="#b14">(Hastie et al., 2009;</ref><ref type="bibr" target="#b10">Freund et a
/ref>. Moreover, boosting theory has been used to analyze the success of ResNets in computer vision <ref type="bibr" target="#b16">(Huang et al., 2018)</ref> and <ref type="bibr">AdaGAN (Tolstikhin et
rget="#b27">Mannor et al., 2003)</ref> can still be preserved when the samples are weakly dependent <ref type="bibr" target="#b24">(Lozano et al., 2013)</ref>. More discussion can refer to Appendix A.

ve commonly used graphs: CiteSeer, Cora-ML <ref type="bibr">(Bojchevski &amp; G?nnemann, 2018;</ref><ref type="bibr" target="#b28">McCallum et al., 2000)</ref>, PubMed <ref type="bibr" target="#b29">(
" target="#b12">(Gori et al., 2005;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017;</ref><ref type="bibr" target="#b33">Veli?kovi? et al., 2018)</ref>, particularly graph convolutional netw network of GCN) <ref type="bibr">(Abu-El-Haija et al., 2018a)</ref>, GAT (Graph Attention Networks) <ref type="bibr" target="#b33">(Veli?kovi? et al., 2018)</ref>, BT.FP (bootstrapped feature propagat


ski &amp; G?nnemann, 2018;</ref><ref type="bibr" target="#b28">McCallum et al., 2000)</ref>, PubMed <ref type="bibr" target="#b29">(Sen et al., 2008)</ref>, MS-Academic <ref type="bibr" target="#b30">
/ref>. Moreover, boosting theory has been used to analyze the success of ResNets in computer vision <ref type="bibr" target="#b16">(Huang et al., 2018)</ref> and <ref type="bibr">AdaGAN (Tolstikhin et
h hop of neighbors. The efficacy of this kind of layer-wise training has been similarly verified in <ref type="bibr" target="#b2">(Belilovsky et al., 2018)</ref> recently. Further, we combine the pred
sy to implement and keeps competitive in terms of both practical performance and computational cost <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>. Moreover, boosting theory has been used e apply SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss function) algorithm <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>, a natural and clean multi-class extensio -err (l) ) &gt; 1/K, i.e., the accuracy of each weak classifier should be better than random guess <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>. This can be met easily to guarantee the v xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM</head><p>In practice, we employ SAMME.R <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>, the soft version of SAMME, in AdaGCN. SA ">(Hastie et al., 2009)</ref>, the soft version of SAMME, in AdaGCN. SAMME.R (R for Real) algorithm <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref> leverages real-valued confidence-rated pr .org/ns/1.0"><head n="5">DISCUSSIONS AND CONCLUSION</head><p>One potential concern is that AdaBoost <ref type="bibr" target="#b14">(Hastie et al., 2009;</ref><ref type="bibr" target="#b10">Freund et a
ithm into the design of deep graph models. As an important realization of boosting theory, AdaBoost <ref type="bibr" target="#b10">(Freund et al., 1999)</ref> is extremely easy to implement and keeps <p>One potential concern is that AdaBoost <ref type="bibr" target="#b14">(Hastie et al., 2009;</ref><ref type="bibr" target="#b10">Freund et al., 1999</ref>) is established on i.i.d. hypothesis while

ections, background, and reflection images, respectively. Here, Î± and Î² are the mixing coefficients <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" targ generation results and clearer separation results. Moreover, we introduce the gradient constraints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref> to make the model le ><head n="4.1.">Framework of the Proposed Scheme</head><p>In contrast to the conventional pipelines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" targ ird columns in Figure <ref type="figure" target="#fig_3">4</ref> 1 ) than previous linear functions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" targ n, and the background edge map (E) concurrently. Instead of one-toone framework in previous methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>, our separator learn . Recently, deep learning based reflection removal methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5]</ref> with better generalization ability have been proposed to addr h previous methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref>, that heavily rely on the simplified model in Equation 1 and learn the edge features of the reflections with the light field camera. The framework introduced in <ref type="bibr" target="#b4">[5]</ref> exploited the edge information when training the whole netwo and reflection, and three discriminator networks to produce the adversarial losses. Existing method <ref type="bibr" target="#b4">[5]</ref> can be treated as a special instance of our method when the type="bibr" target="#b28">[29]</ref>, CycleGAN <ref type="bibr" target="#b29">[30]</ref>, and FY17 <ref type="bibr" target="#b4">[5]</ref>. The yellow boxes highlight some noticeable differences.</p> 5">[26]</ref> 0.833 0.877 22.39 NR17 <ref type="bibr" target="#b0">[1]</ref> 0.832 0.882 23.70 FY17 <ref type="bibr" target="#b4">[5]</ref> 0.820 0.871 22.51 CycleGAN <ref type="bibr" target="#b29">[3 <ref type="bibr" target="#b28">[29]</ref>, CycleGAN <ref type="bibr" target="#b29">[30]</ref>, FY17 <ref type="bibr" target="#b4">[5]</ref>, NR17 <ref type="bibr" target="#b0">[1]</ref>, WS16 <ref typ able" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Efficiency comparisons with FY17<ref type="bibr" target="#b4">[5]</ref>, Zhang18<ref type="bibr" target="#b28">[29]</ref> and Wan18<
mixing coefficients <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>.</p><formula xml:id="formula_0">M = Î±B + Î²R,<label>(1)</lab ion results. Moreover, we introduce the gradient constraints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref> to make the model learning more effective, in which the edg In this scenario, image priors such as different blur levels between the background and reflection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref>, ghosting effects " target="#fig_3">4</ref> 1 ) than previous linear functions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar <p>SSIM r SSIM PSNR(dB)</p><p>LB14 <ref type="bibr" target="#b16">[17]</ref> 0.801 0.829 21.77 WS16 <ref type="bibr" target="#b25">[26]</ref> 0.833 0.877 22.39 NR17 <ref type="bibr" target="#b0">[1]</ >, FY17 <ref type="bibr" target="#b4">[5]</ref>, NR17 <ref type="bibr" target="#b0">[1]</ref>, WS16 <ref type="bibr" target="#b25">[26]</ref>, and LB14 <ref type="bibr" target="#b16">[17]</ref>. For a
rea remove reflections by virtue of multiple images. By exploiting different image correlation cues <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, the modelling based m
nslation frameworks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> have been proposed. UNIT <ref type="bibr" target="#b17">[18 type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> have been proposed. UNIT <ref type="bibr" target="#b17">[18]</ref> combines variational autoencoders (VAEs) <ref type="bibr"
rategy</head><p>The model is implemented with PyTorch 2 . To inject scale-invariance to the network <ref type="bibr" target="#b20">[21]</ref>, we adopt a multi-size training strategy by feeding images egional properties of the reflection <ref type="bibr" target="#b22">[23]</ref>, we also adopt SSIMr <ref type="bibr" target="#b20">[21]</ref> to assess the quality by focusing on local reflections.  < ions and evaluate the SSIM values in these regions analogously to the evaluation method proposed in <ref type="bibr" target="#b20">[21]</ref>. As a result, higher SSIM r results have been obtained as
the reflection removal problems and shown very promising results. For example, Chandramouli et al. <ref type="bibr" target="#b3">[4]</ref> proposed a two-stage deep learning approach to learn the edg
beyond the straightforward linear combination. For example, either non-uniform lighting conditions <ref type="bibr" target="#b11">[12]</ref> or the non-flat surface of glass <ref type="bibr" target="
ground and reflection layers, including the sparsity prior <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>, the blur level differences between the background and refl
ty. For the discriminator networks, we use 70 Ã 70 PatchGANs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> which can be applied to arbitrarily-sized images in a fully
ifferent blur levels between the background and reflection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref>, ghosting effects <ref type="bibr" target="#b21">[22]</ref> br" target="#b13">14]</ref>, the blur level differences between the background and reflection layer <ref type="bibr" target="#b16">[17]</ref>, the ghosting effects <ref type="bibr" target="#b21">[22]< state-ofthe-arts methods using three different error metrics.</p><p>SSIM r SSIM PSNR(dB)</p><p>LB14 <ref type="bibr" target="#b16">[17]</ref> 0.801 0.829 21.77 WS16 <ref type="bibr" target="#b25">[26] 7 <ref type="bibr" target="#b0">[1]</ref>, WS16 <ref type="bibr" target="#b25">[26]</ref>, and LB14 <ref type="bibr" target="#b16">[17]</ref>. For a fair comparison, we use the codes provided by their ous linear functions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1]</ref> with fixed coefficien
ges, respectively. Here, Î± and Î² are the mixing coefficients <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>.</p><formula xml:id form lighting conditions <ref type="bibr" target="#b11">[12]</ref> or the non-flat surface of glass <ref type="bibr" target="#b26">[27]</ref> may make Equation 1 invalid. As such, a general image form
rea remove reflections by virtue of multiple images. By exploiting different image correlation cues <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, the modelling based m
sting effects <ref type="bibr" target="#b21">[22]</ref>, and the non-local similarity in the images <ref type="bibr" target="#b24">[25]</ref>, have been explored. However, these low-level image priors
rategy</head><p>The model is implemented with PyTorch 2 . To inject scale-invariance to the network <ref type="bibr" target="#b20">[21]</ref>, we adopt a multi-size training strategy by feeding images egional properties of the reflection <ref type="bibr" target="#b22">[23]</ref>, we also adopt SSIMr <ref type="bibr" target="#b20">[21]</ref> to assess the quality by focusing on local reflections.  < ions and evaluate the SSIM values in these regions analogously to the evaluation method proposed in <ref type="bibr" target="#b20">[21]</ref>. As a result, higher SSIM r results have been obtained as
/ref> combines variational autoencoders (VAEs) <ref type="bibr" target="#b10">[11]</ref> with CoGAN <ref type="bibr" target="#b18">[19]</ref>, a GAN framework where two generators share weights to lea
/ref> combines variational autoencoders (VAEs) <ref type="bibr" target="#b10">[11]</ref> with CoGAN <ref type="bibr" target="#b18">[19]</ref>, a GAN framework where two generators share weights to lea
ed Scheme</head><p>In contrast to the conventional pipelines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24]</ref> that treat the imag urrently. Instead of one-toone framework in previous methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>, our separator learns the mapping function as S : M â (B, R f> dataset. The comparison methods include Wan18 <ref type="bibr" target="#b23">[24]</ref>, Zhang18 <ref type="bibr" target="#b28">[29]</ref>, CycleGAN <ref type="bibr" target="#b29">[30]</ref>, and F ref> 0.820 0.871 22.51 CycleGAN <ref type="bibr" target="#b29">[30]</ref> 0.794 0.813 20.10 Zhang18 <ref type="bibr" target="#b28">[29]</ref> 0.842 0.885 24.01 Wan18 <ref type="bibr" target="#b23">[24 e">7</ref>. Perceptual study results on the whole SIR 2 dataset for the three best methods (Zhang18 <ref type="bibr" target="#b28">[29]</ref>, Wan18 <ref type="bibr" target="#b23">[24]</ref>, and ours mage reflection removal methods, including Wan18 <ref type="bibr" target="#b23">[24]</ref>, Zhang18 <ref type="bibr" target="#b28">[29]</ref>, CycleGAN <ref type="bibr" target="#b29">[30]</ref>, FY17 ls (e.g., the dataset of SIR 2 <ref type="bibr" target="#b22">[23]</ref> and the dataset of Zhang18 <ref type="bibr" target="#b28">[29]</ref>), we have observed that the dataset gap problem is worth f l>3</label><figDesc>Efficiency comparisons with FY17<ref type="bibr" target="#b4">[5]</ref>, Zhang18<ref type="bibr" target="#b28">[29]</ref> and Wan18<ref type="bibr" target="#b23">[24]</ref> of an i
ed images and reflection-free images directly. Reflection-Removal Perceptual Study. Recent research <ref type="bibr" target="#b2">[3]</ref> pointed out that PSNR and SSIM may not exactly tell the perc
rategy</head><p>The model is implemented with PyTorch 2 . To inject scale-invariance to the network <ref type="bibr" target="#b20">[21]</ref>, we adopt a multi-size training strategy by feeding images egional properties of the reflection <ref type="bibr" target="#b22">[23]</ref>, we also adopt SSIMr <ref type="bibr" target="#b20">[21]</ref> to assess the quality by focusing on local reflections.  < ions and evaluate the SSIM values in these regions analogously to the evaluation method proposed in <ref type="bibr" target="#b20">[21]</ref>. As a result, higher SSIM r results have been obtained as
ional layers to increase the receptive field size, a feature extraction unit with 9 residual blocks <ref type="bibr" target="#b7">[8]</ref> to extract robust features and an upsampling unit at the las
nslation frameworks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> have been proposed. UNIT <ref type="bibr" target="#b17">[18 type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> have been proposed. UNIT <ref type="bibr" target="#b17">[18]</ref> combines variational autoencoders (VAEs) <ref type="bibr"
/ref> combines variational autoencoders (VAEs) <ref type="bibr" target="#b10">[11]</ref> with CoGAN <ref type="bibr" target="#b18">[19]</ref>, a GAN framework where two generators share weights to lea
al Predictions (APPNP) framework is most relevant to our work, as they also smooth base predictions <ref type="bibr" target="#b22">(Klicpera et al., 2018)</ref>. However, they focus on integrating thi ep, decoupled from the other steps. This type of prediction smoothing is similar in spirit to APPNP <ref type="bibr" target="#b22">(Klicpera et al., 2018)</ref>, which we compare against later. Howeve

numerous ways to get useful features from just the graph topology to augment the raw node features <ref type="bibr" target="#b15">(Henderson et al., 2011;</ref><ref type="bibr">2012;</ref><ref type="
ype="bibr" target="#b41">Wang &amp; Leskovec (2020)</ref> (in contrast to lower label rate settings <ref type="bibr" target="#b45">(Yang et al., 2016)</ref>) to ameliorate sensitivity to hyperparamete
e dorm residences and features are attributes such as gender, major, and class year, amongst others <ref type="bibr" target="#b38">(Traud et al., 2012)</ref>, as well as a geographic dataset of US cou
three classic citation network benchmarks <ref type="bibr" target="#b10">(Getoor et al., 2001;</ref><ref type="bibr" target="#b9">Getoor, 2005;</ref><ref type="bibr" target="#b29">Namata et al., 2012) al to 0.01.</p><p>â¢ Cora, Citseer, Pubmed <ref type="bibr" target="#b10">(Getoor et al., 2001;</ref><ref type="bibr" target="#b9">Getoor, 2005;</ref><ref type="bibr" target="#b29">Namata et al., 2012)
ef>. In the semi-supervised learning literature, the analog is the smoothness or cluster assumption <ref type="bibr" target="#b2">(Chapelle et al., 2003;</ref><ref type="bibr">Zhu, 2005)</ref>. The go
nds to homophily or assortative mixing <ref type="bibr" target="#b27">(McPherson et al., 2001;</ref><ref type="bibr" target="#b30">Newman, 2003;</ref><ref type="bibr" target="#b6">Easley &amp; Kleinbe
formance gains. We can also incorporate our techniques into big GNN models, providing modest gains. <ref type="bibr" target="#b11">Mahoney, 2015)</ref>. The salient point for this paper is that we ass
nt membership and there are no features <ref type="bibr" target="#b24">(Leskovec et al., 2007;</ref><ref type="bibr" target="#b46">Yin et al., 2017)</ref>.</p><p>Data splits. The training/validation/t 9">Namata et al., 2012)</ref> and Email <ref type="bibr" target="#b24">(Leskovec et al., 2007;</ref><ref type="bibr" target="#b46">Yin et al., 2017)</ref>: 3 layers and 64 hidden channels with learnin
on et al., 2017b)</ref>. In our pipeline, we augment features with a regularized spectral embedding <ref type="bibr" target="#b3">(Chaudhuri et al., 2012;</ref><ref type="bibr" target="#b48">Zhang &am
somorphism Networks <ref type="bibr" target="#b44">(Xu et al., 2018)</ref>, and various deep models <ref type="bibr" target="#b25">(Li et al., 2019;</ref><ref type="bibr" target="#b34">Rong et al., 20
="bibr" target="#b25">(Li et al., 2019;</ref><ref type="bibr" target="#b34">Rong et al., 2019;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref>. Many ideas for new GNN architectures are ada rd, as of October 1, 2020). For Cora, Citeseer and Pubmed, we reuse the top performance scores from <ref type="bibr" target="#b4">Chen et al. (2020)</ref>. For Email and US County, we use GCNII <ref t from <ref type="bibr" target="#b4">Chen et al. (2020)</ref>. For Email and US County, we use GCNII <ref type="bibr" target="#b4">(Chen et al., 2020)</ref>. For Rice31, we use GCN with spectral and no
type="bibr" target="#b13">(Hamilton et al., 2017a)</ref>; examples include Graph Attention Networks <ref type="bibr" target="#b40">(VeliÄkoviÄ et al., 2018)</ref>, Graph Isomorphism Networks <ref type
numerous ways to get useful features from just the graph topology to augment the raw node features <ref type="bibr" target="#b15">(Henderson et al., 2011;</ref><ref type="bibr">2012;</ref><ref type="
nodes is at the center of much network analysis and corresponds to homophily or assortative mixing <ref type="bibr" target="#b27">(McPherson et al., 2001;</ref><ref type="bibr" target="#b30">Newman,

on et al., 2017b)</ref>. In our pipeline, we augment features with a regularized spectral embedding <ref type="bibr" target="#b3">(Chaudhuri et al., 2012;</ref><ref type="bibr" target="#b48">Zhang &am
re easily scales to large datasets. Our framework also complements the Simplified Graph Convolution <ref type="bibr" target="#b42">(Wu et al., 2019)</ref>, as well as algorithms designed to increase s that GCNs gain performance by having smoothed outputs over the graph, a similar observation made by <ref type="bibr" target="#b42">Wu et al. (2019)</ref>. However, there are still gaps in performance
;</ref><ref type="bibr" target="#b19">Jia &amp; Benson, 2020)</ref> as well as Markov Random fields <ref type="bibr" target="#b32">(Qu et al., 2019;</ref><ref type="bibr" target="#b8">Gao et al., 2019
nodes is at the center of much network analysis and corresponds to homophily or assortative mixing <ref type="bibr" target="#b27">(McPherson et al., 2001;</ref><ref type="bibr" target="#b30">Newman,

tention to, attention networks achieve a better performance with fewer layers. As an example, SENet <ref type="bibr" target="#b5">[5]</ref> introduces Squeeze-and-Excitation (SE) blocks to study the c rs in the spatial pyramid structure are not learnable, which is nearly cost-free. Compared to SENet <ref type="bibr" target="#b5">[5]</ref>, our structure only modifies the first fully-connected layer ter concatenation as input to the next block. More recently, attention based networks such as SENet <ref type="bibr" target="#b5">[5]</ref> and CBAM <ref type="bibr">[6]</ref> provide an independent a s suppress insignificant features. Thus, visual features could be better captured and exploited. In <ref type="bibr" target="#b5">[5]</ref>, a Squeeze-and-Extraction block was proposed to learn the ch e last feature map which is small in size (7 ? 7 for example). However, attention based CNNs (e.g., <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr">[6]</ref>, <ref type="bibr" target="#b7">[ effectiveness of the attention mechanism. To address this problem, we leverage the excitation block <ref type="bibr" target="#b5">[5]</ref> to encode v and generate a 1D attention map ?. The excitatio re ? is a rectified linear unit (ReLU) function and sig denotes the sigmoid function. Like in SENet <ref type="bibr" target="#b5">[5]</ref>, we set r to 16.</p></div> <div xmlns="http://www.tei-c.org/ ls and whistles, SPANet outperforms related stateof-art work <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" targe tention map from a feature map and then apply the learned attention map to the original feature map <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b18">18]</ref> . However, being con rg/ns/1.0"><head n="3.3.">Spatial Pyramid Attention</head><p>Many existing attention based networks <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">6,</ref><ref type="bibr">6,</ref><ref type="

feature maps into a 1D vector using global average pooling. They achieve structural regularization <ref type="bibr" target="#b20">[20]</ref>, but miss the structural information. In contrast, the spa br" target="#b7">[7]</ref>, etc.) apply global average pooling on each feature map. As presented in <ref type="bibr" target="#b20">[20]</ref>, GAP behaves similarly to a structural regularizer and is
sidered similar to SPPNet <ref type="bibr" target="#b8">[8]</ref> and Region of Interesting Pooling <ref type="bibr" target="#b9">[9]</ref>. In contrast, our spatial pyramid structure encodes a featur

<ref type="bibr" target="#b1">[1]</ref> to 1000-layer ResNet <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>, they aim to improve the accuracy of image recognition. Inevi Subsequently, He et al. proposed Residual Networks (ResNet) <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>, which learn the residual functions by adding skip-connection dentity mapping shortcut is crucial to ease the optimization <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. Hence, ResNet discards the gating units used in Highway Netw ). For training, we adopt a data augmentation scheme used in <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. We pad an original image by 4 pixels with value zero on each
ure. For example, from 8-layer AlexNet <ref type="bibr" target="#b1">[1]</ref> to 1000-layer ResNet <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>, they aim to improve t n-sampled ImageNet dataset. Without bells and whistles, SPANet outperforms related stateof-art work <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target loyed to regulate the information flow. Subsequently, He et al. proposed Residual Networks (ResNet) <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>, which learn the resid connections. The ResNet shows that an identity mapping shortcut is crucial to ease the optimization <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. Hence, ResNet discard ersion of the original ImageNet dataset). For training, we adopt a data augmentation scheme used in <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. We pad an original im Competitive-SENet achieves promising performance, it is tailored particularly for Residual Networks <ref type="bibr" target="#b2">[2]</ref>, which limits its generalization to other models. Without be nal fully-connected layers in CNNs. Since then, it has prevailed in computer vision for recognition <ref type="bibr" target="#b2">[2]</ref>, detection <ref type="bibr" target="#b22">[22]</ref>, segmen
t connections, there are works studying the internal multi-path connections in convolutional blocks <ref type="bibr" target="#b15">[15]</ref>. The InceptionV4 Network <ref type="bibr" target="#b15">[1 nections in convolutional blocks <ref type="bibr" target="#b15">[15]</ref>. The InceptionV4 Network <ref type="bibr" target="#b15">[15]</ref> is one of this kind. Besides a shortcut connection, each i
t connections, there are works studying the internal multi-path connections in convolutional blocks <ref type="bibr" target="#b15">[15]</ref>. The InceptionV4 Network <ref type="bibr" target="#b15">[1 nections in convolutional blocks <ref type="bibr" target="#b15">[15]</ref>. The InceptionV4 Network <ref type="bibr" target="#b15">[15]</ref> is one of this kind. Besides a shortcut connection, each i
has prevailed in computer vision for recognition <ref type="bibr" target="#b2">[2]</ref>, detection <ref type="bibr" target="#b22">[22]</ref>, segmentation <ref type="bibr" target="#b23">[23]</ref>, a
sidered similar to SPPNet <ref type="bibr" target="#b8">[8]</ref> and Region of Interesting Pooling <ref type="bibr" target="#b9">[9]</ref>. In contrast, our spatial pyramid structure encodes a featur
for modeling bipartite graphs. As a result, they are suboptimal to learn bipartite graph embedding <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8]</ref>. To remedy such a prob e roughly divided into two branches: random walk-based and reconstruction-based methods. The former <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target ures with the assumption that nodes within the sliding window or neighborhoods are closely relevant <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" targ <ref type="bibr" target="#b44">[44]</ref>, PinSage <ref type="bibr" target="#b40">[40]</ref>, BiNE <ref type="bibr" target="#b7">[7]</ref> and FOBE <ref type="bibr" target="#b32">[32]</ref> are speci iv xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Data Preprocessing.</head><p>As used in BiNE <ref type="bibr" target="#b7">[7]</ref>, we select 60% edges for training and remaining edges for te [36]</ref>. â¢ Bipartite graph embedding: PinSage <ref type="bibr" target="#b40">[40]</ref> and BiNE <ref type="bibr" target="#b7">[7]</ref>.</p><p>PinSage integrates random walk into GNN architectures
plit graphs. So DMGI still puts more emphasis on learning the correlation of homogeneous nodes. GMI <ref type="bibr" target="#b26">[26]</ref> proposes a new approach  The yellow dotted lines (Eq.( <re
e="bibr" target="#b19">[19]</ref>. Some representative heterogeneous graph methods are Metapath2vec <ref type="bibr" target="#b6">[6]</ref> and DMGI <ref type="bibr" target="#b41">[41]</ref>. But they
t provides a new approach for the task of unsupervised node classification. Based on DIM, InfoGraph <ref type="bibr" target="#b31">[31]</ref> tries to learn unsupervised graph representations via maxi
types. It has been widely adopted in many realworld applications, arranging from recommender system <ref type="bibr" target="#b34">[34]</ref>, drug discovery <ref type="bibr" target="#b39">[39]</ref> partite graphs closely. They propose various DNNs to solve recommendation tasks. For example, GC-MC <ref type="bibr" target="#b34">[34]</ref> uses one relation-aware graph convolution layer to learn n ointly optimizes explicit and implicit relations in a unified framework. â¢ Matrix completion: GC-MC <ref type="bibr" target="#b34">[34]</ref> and IGMC <ref type="bibr" target="#b42">[42]</ref>. GC-MC uction-based works <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" tar tempt to reconstruct the adjacency matrix by learning different encoders. In particular, some works <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" ta ey mainly focus on how to model local graph structures in the latent space.</p><p>Matrix completion <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b42">42]</ref> and collaborative
s coming from the joint distribution of two random variables or the product of their marginals. DIM <ref type="bibr" target="#b16">[16]</ref> introduces the structural information into input patches a
rget="#b9">[9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b38">38]</ref> to learn node representations via aggregating features of n
types. It has been widely adopted in many realworld applications, arranging from recommender system <ref type="bibr" target="#b34">[34]</ref>, drug discovery <ref type="bibr" target="#b39">[39]</ref> partite graphs closely. They propose various DNNs to solve recommendation tasks. For example, GC-MC <ref type="bibr" target="#b34">[34]</ref> uses one relation-aware graph convolution layer to learn n ointly optimizes explicit and implicit relations in a unified framework. â¢ Matrix completion: GC-MC <ref type="bibr" target="#b34">[34]</ref> and IGMC <ref type="bibr" target="#b42">[42]</ref>. GC-MC uction-based works <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" tar tempt to reconstruct the adjacency matrix by learning different encoders. In particular, some works <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" ta ey mainly focus on how to model local graph structures in the latent space.</p><p>Matrix completion <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b42">42]</ref> and collaborative
target="#b10">[10]</ref>, and the "GAN" distance and Jensen-Shannon divergence are closely related <ref type="bibr" target="#b24">[24]</ref>.</p><p>From Eq.( <ref type="formula" target="#formula_4">5
>[35]</ref>. However, estimating MI is generally intractable in highdimensional continuous settings <ref type="bibr" target="#b25">[25]</ref>. MINE <ref type="bibr" target="#b1">[1]</ref> derives a lo
f type="bibr" target="#b33">[33]</ref>, Node2vec <ref type="bibr" target="#b11">[11]</ref> and VGAE <ref type="bibr" target="#b19">[19]</ref>. Some representative heterogeneous graph methods are Metap f type="bibr" target="#b33">[33]</ref>, Node2vec <ref type="bibr" target="#b11">[11]</ref> and VGAE <ref type="bibr" target="#b19">[19]</ref>. DeepWalk and Node2vec are typically random-walk based. LI
intractable in highdimensional continuous settings <ref type="bibr" target="#b25">[25]</ref>. MINE <ref type="bibr" target="#b1">[1]</ref> derives a lower bound of MI and works by training a discrimi
he between-cluster dispersion. It is also commonly used to evaluate the task of community detection <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b23">23]</ref>. As shown in Figure
s within a sliding window <ref type="bibr" target="#b45">[45]</ref>. The reconstruction-based works <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" ta "bibr" target="#b34">[34,</ref><ref type="bibr" target="#b42">42]</ref> and collaborative filtering <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b37">37]</ref> are also connected ed on local subgraphs for the task of inductive matrix completion. â¢ Collaborative filtering: NeuMF <ref type="bibr" target="#b15">[15]</ref> and NGCF <ref type="bibr" target="#b37">[37]</ref>. NeuMF
et="#b15">[15,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" tar by learning different encoders. In particular, some works <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" tar hin the sliding window or neighborhoods are closely relevant <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41]</ref>. We argue that they "bibr" target="#b42">42]</ref> and collaborative filtering <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b37">37]</ref> are also connected with modeling bipartite graphs closely. learn node embeddings, thus only the direct links in user-item bipartite graphs are exploited. NGCF <ref type="bibr" target="#b37">[37]</ref> incorporates collaborative signals into the embedding proc rix completion. â¢ Collaborative filtering: NeuMF <ref type="bibr" target="#b15">[15]</ref> and NGCF <ref type="bibr" target="#b37">[37]</ref>. NeuMF uses MLP to learn the nonlinear interactions betwee he proposed encoder is well compatible with our infomax objective. Compared with other GNN encoders <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b40">40]</ref> for bipartite grap
ed methods. The former <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b44">44]</ref> relies on designing the heuristics of random walks to gener graphs, and the structural characteristics of bipartite graph are hard to be preserved by them. IGE <ref type="bibr" target="#b44">[44]</ref>, PinSage <ref type="bibr" target="#b40">[40]</ref>, BiNE <
intractable in highdimensional continuous settings <ref type="bibr" target="#b25">[25]</ref>. MINE <ref type="bibr" target="#b1">[1]</ref> derives a lower bound of MI and works by training a discrimi
ually used for modeling bipartite graphs. The pioneering homogeneous graph methods include DeepWalk <ref type="bibr" target="#b27">[27]</ref>, LINE <ref type="bibr" target="#b33">[33]</ref>, Node2vec following strong baselines which can be divided into:</p><p>â¢ Homogeneous graph embedding: DeepWalk <ref type="bibr" target="#b27">[27]</ref>, LINE <ref type="bibr" target="#b33">[33]</ref>, Node2vec
ually used for modeling bipartite graphs. The pioneering homogeneous graph methods include DeepWalk <ref type="bibr" target="#b27">[27]</ref>, LINE <ref type="bibr" target="#b33">[33]</ref>, Node2vec following strong baselines which can be divided into:</p><p>â¢ Homogeneous graph embedding: DeepWalk <ref type="bibr" target="#b27">[27]</ref>, LINE <ref type="bibr" target="#b33">[33]</ref>, Node2vec

oneering homogeneous graph methods include DeepWalk <ref type="bibr" target="#b27">[27]</ref>, LINE <ref type="bibr" target="#b33">[33]</ref>, Node2vec <ref type="bibr" target="#b11">[11]</ref> and VG into:</p><p>â¢ Homogeneous graph embedding: DeepWalk <ref type="bibr" target="#b27">[27]</ref>, LINE <ref type="bibr" target="#b33">[33]</ref>, Node2vec <ref type="bibr" target="#b11">[11]</ref> and VG
rget="#b9">[9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b38">38]</ref> to learn node representations via aggregating features of n
veloped Transformers, a new neural architecture for even more effective natural language processing <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref>. Transformers overcome a major drawback al., 2019)</ref>.</p><p>A.3 Why not Positional Encoding? Some Transformers uses positional encoding <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref> or positional embedding <ref type="bibr" presented in Sec 2.3.1. For other details (especially on the multi-head attention), please refer to <ref type="bibr" target="#b42">Vaswani et al. (2017)</ref> and in particular, <ref type="bibr">GPT-2
. 2017b) by 37.0%, the Deep3 system (Raychev et al., 2016a) by 29.7%, and an adaptation of Code2Seq <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref> for code prediction by 30.0%. These are sig "bibr" target="#b45">, Yang and Xiang, 2019)</ref>. We include an adaptation of path-based Code2Seq <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref> in our evaluations and show that our models Devanbu, 2017b)</ref>, Deep3 <ref type="bibr" target="#b35">(Raychev et al., 2016a)</ref>, Code2Seq <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref>).</p><p>Fig 3 puts these models in perspect #b35">(Raychev et al., 2016a</ref>)) vs. TravTrans+ ; â¢ from 43.6% to 73.6% when comparing Code2Seq <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref>  Thus, we argue that our proposal of using ">, Svyatkovskiy et al., 2019)</ref>; this model works on token sequences. We also include Code2Seq <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref> to compare our efforts against a popular co ven a method body, how well can Code2Seq generate the correct method name? The training proposed in <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref> is not well suited for next token predictio tsis et al., 2020</ref><ref type="bibr" target="#b28">, Li et al., 2018)</ref>), to paths in an AST <ref type="bibr" target="#b7">(Alon et al., 2019a</ref><ref type="bibr">(Alon et al., ,b, 2020))</re iv> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Code2Seq</head><p>Code2Seq is a model by <ref type="bibr" target="#b7">Alon et al. 2019a</ref> that embeds code snippets by embedding AST pat
elopers can come up with arbitrary identifier names. Possible mitigation includes copying mechanism <ref type="bibr" target="#b4">(Allamanis et al., 2016</ref><ref type="bibr">, Brockschmidt et al., 2
ing progress in the application of machine learning (ML) techniques to developer productivity tools <ref type="bibr" target="#b5">(Allamanis et al., 2018a)</ref>, and in particular, to code prediction suggestions provided to a user by learning the statistical property of code, exploiting naturalness <ref type="bibr" target="#b5">(Allamanis et al., 2018a)</ref>.</p><p>Traditional ML-based techniques
language model over the training corpus, and it is generally believed to do so better than n-grams <ref type="bibr" target="#b27">(Karampatsis et al., 2020)</ref>. Indeed, several papers in the liter s smaller (e.g. characters <ref type="bibr" target="#b12">(Bielik et al., 2016b)</ref> or subtokens <ref type="bibr" target="#b27">(Karampatsis et al., 2020)</ref>) or larger (e.g. sub-ASTs <ref type= s="http://www.tei-c.org/ns/1.0"><head>BPE.</head><p>We have not integrated byte-pair encoding (BPE) <ref type="bibr" target="#b27">(Karampatsis et al., 2020)</ref> into our RNN model. We expect BPE to equence (as for code prediction <ref type="bibr" target="#b21">(Hellendoorn and Devanbu, 2017a</ref><ref type="bibr" target="#b27">, Karampatsis et al., 2020</ref><ref type="bibr" target="#b28">, Li e ds used in recent papers for code prediction (Aye and Kaiser, 2020, Hellendoorn and Devanbu, 2017b, <ref type="bibr" target="#b27">Karampatsis et al., 2020</ref><ref type="bibr" target="#b34">, Rayche 2019)</ref> and open-vocabulary models <ref type="bibr" target="#b14">(Cvitkovic et al., 2019</ref><ref type="bibr" target="#b27">, Karampatsis et al., 2020)</ref>.</p><p>Exposing Tree Structure even
target="#b10">(Aye and Kaiser, 2020</ref><ref type="bibr" target="#b34">, Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkovskiy et al., 2019)</ref> has used RNNs. Attempts has been m get="#b27">Karampatsis et al., 2020</ref><ref type="bibr" target="#b34">, Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkovskiy et al., 2019)</ref>; this model works on token sequenc ser, 2020, Hellendoorn and Devanbu, 2017b, <ref type="bibr" target="#b34">Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkovskiy et al., 2019)</ref>. In this way, we create a language
Bayesian optimization to rank the list <ref type="bibr" target="#b18">(DÃ¢ÄÅ¹Souza et al., 2016</ref><ref type="bibr" target="#b33">, Proksch et al., 2015)</ref>. However, a type-based autocomplete is br" target="#b16">(Devlin et al., 2018</ref><ref type="bibr" target="#b17">, Dong et al., 2019</ref><ref type="bibr" target="#b33">, Radford et al., 2019)</ref> for a variety of NLP tasks such as lang lock, number of context = 1000, and embedding dimension = 300. We borrow other hyperparameters from <ref type="bibr" target="#b33">Radford et al. (2019)</ref>. We limit the token vocabulary size to 10 tional encoding <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref> or positional embedding <ref type="bibr" target="#b33">(Radford et al., 2019)</ref> to provide model extra positional inform

ncluding work done in industrial contexts <ref type="bibr" target="#b10">(Aye and Kaiser, 2020</ref><ref type="bibr" target="#b34">, Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkov 2020, Hellendoorn and Devanbu, 2017b, <ref type="bibr" target="#b27">Karampatsis et al., 2020</ref><ref type="bibr" target="#b34">, Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkov type="bibr" target="#b9">, Alon et al., 2020</ref><ref type="bibr">, Brockschmidt et al., 2019</ref><ref type="bibr" target="#b34">, Raychev et al., 2014)</ref> or where the granularity of prediction source sequence tokens into an RNN (or LSTM) (Aye and Kaiser, 2020, Hellendoorn and Devanbu, 2017b, <ref type="bibr" target="#b34">Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkovsk
nd in particular, to code prediction <ref type="bibr" target="#b13">(Brockschmidt et al., 2019</ref><ref type="bibr" target="#b24">, Hindle et al., 2016</ref><ref type="bibr" target="#b28">, Li et al. r Code Completion. Some of the early ML models for code prediction relied on n-gram language models <ref type="bibr" target="#b24">(Hindle et al., 2016</ref><ref type="bibr" target="#b31">, Nguyen et
target="#b10">(Aye and Kaiser, 2020</ref><ref type="bibr" target="#b34">, Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkovskiy et al., 2019)</ref> has used RNNs. Attempts has been m get="#b27">Karampatsis et al., 2020</ref><ref type="bibr" target="#b34">, Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkovskiy et al., 2019)</ref>; this model works on token sequenc ser, 2020, Hellendoorn and Devanbu, 2017b, <ref type="bibr" target="#b34">Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkovskiy et al., 2019)</ref>. In this way, we create a language
et="#b13">(Brockschmidt et al., 2019</ref><ref type="bibr" target="#b24">, Hindle et al., 2016</ref><ref type="bibr" target="#b28">, Li et al., 2018</ref><ref type="bibr">, Raychev et al., 2016a)</ref #b29">(Liu et al., 2016)</ref>; accuracy is further improved by using more AST guided architectures <ref type="bibr" target="#b28">(Li et al., 2018</ref><ref type="bibr" target="#b30">, Liu et al., 20 (Hellendoorn and Devanbu, 2017a</ref><ref type="bibr" target="#b27">, Karampatsis et al., 2020</ref><ref type="bibr" target="#b28">, Li et al., 2018)</ref>), to paths in an AST <ref type="bibr" target
handle beyond sequential structures for NLP <ref type="bibr" target="#b1">(Ahmed et al., 2019</ref><ref type="bibr" target="#b32">, Nguyen et al., 2020</ref><ref type="bibr">, Wang et al., 2019)</ref
other flavors of code completion, such as where program after the predicting location is available <ref type="bibr" target="#b6">(Allamanis et al., 2018b</ref><ref type="bibr" target="#b9">, Alon et , 2020))</ref>, and sometimes even ways to convey static analysis information to the neural network <ref type="bibr" target="#b6">(Allamanis et al., 2018b</ref><ref type="bibr">, Brockschmidt et al., niques for code summarization <ref type="bibr" target="#b8">(Alon et al., 2019b)</ref>, bug finding <ref type="bibr" target="#b6">(Allamanis et al., 2018b)</ref>, repair <ref type="bibr" target="#b41" de ways to communicate program analysis results such as def-use information, as in Allamanis et al. <ref type="bibr" target="#b6">(Allamanis et al., 2018b)</ref>.</p><p>We also limit our study to the
mproved by using more AST guided architectures <ref type="bibr" target="#b28">(Li et al., 2018</ref><ref type="bibr" target="#b30">, Liu et al., 2020)</ref>. We include a variation of an RNN (LSTM)-ba

arning techniques for code, beyond code completion. These include techniques for code summarization <ref type="bibr" target="#b8">(Alon et al., 2019b)</ref>, bug finding <ref type="bibr" target="#b6"> n problem-a non-neural but tree aware engine could outperform RNNs. In the same spirit, Alon et al. <ref type="bibr" target="#b8">(Alon et al., 2019b)</ref> had found-for code summarization problem (t
maintain the hidden states at each segment to pass along more context information, as explained in <ref type="bibr" target="#b15">(Dai et al., 2019)</ref>.</p><p>A.3 Why not Positional Encoding? Some
maintain the hidden states at each segment to pass along more context information, as explained in <ref type="bibr" target="#b15">(Dai et al., 2019)</ref>.</p><p>A.3 Why not Positional Encoding? Some
is element-wise product. This idea of incorporating R into the Attn TreeRel function is inspired by <ref type="bibr" target="#b37">Shaw et. al ( 2018)</ref>. where o is a distribution over all possibl
mation gleaned selectively across paths in the code's AST. We include the best-performing technique <ref type="bibr" target="#b35">(Raychev et al., 2016a)</ref> from this line of works in our evaluati previous work (SeqRNN <ref type="bibr" target="#b22">(Hellendoorn and Devanbu, 2017b)</ref>, Deep3 <ref type="bibr" target="#b35">(Raychev et al., 2016a)</ref>, Code2Seq <ref type="bibr" target="#b7" ead><p>In this section, we give details to three previous works for code prediction. We chose Deep3 <ref type="bibr" target="#b35">(Raychev et al., 2016a)</ref> as it is a state-of-the-art technique ( his gives us the green light to explore further. The next questions are motivated by previous work. <ref type="bibr" target="#b35">Raychev et al. (Raychev et al., 2016a)</ref> had found that-for the c SeqRNN ) vs. TravTrans+ ; â¢ from 43.9% to 73.6% when comparing a non-neural tree based model (Deep3 <ref type="bibr" target="#b35">(Raychev et al., 2016a</ref>)) vs. TravTrans+ ; â¢ from 43.6% to 73.6% 1.0"><head n="4.1">Dataset</head><p>We train our models using the py150 dataset (py1, 2016) used in <ref type="bibr" target="#b35">Raychev et al. (2016a)</ref>. The dataset consists of 150k Python 2 s valuate our models on two evaluation datasets:</p><p>â¢ py150: We use the evaluation dataset used in <ref type="bibr" target="#b35">Raychev et al. (2016a)</ref>, which consists of 50k Python ASTs. We p
="bibr" target="#b4">(Allamanis et al., 2016</ref><ref type="bibr">, Brockschmidt et al., 2019</ref><ref type="bibr" target="#b19">, Fernandes et al., 2019)</ref> and open-vocabulary models <ref type=
where permutation equivariance is either learned from data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> or obtained by design <ref type="bibr" target="#b23">[24]</ ]</ref>. With node features acting as identifiers, MPNN were shown to become universal in the limit <ref type="bibr" target="#b22">[23]</ref>, which implies that they can solve the graph isomorphism t d evidence that the power of MPNN grows as a function of depth and width for certain graph problems <ref type="bibr" target="#b22">[23]</ref>, showing that (both anonymous and non-anonymous) MPNN cann ether depth and width needs to grow with the number of nodes solely in the worst-case (as proven in <ref type="bibr" target="#b22">[23]</ref>) or with certain probability over the input distribution.< apacity is an effective generalization of the previously considered product between depth and width <ref type="bibr" target="#b22">[23]</ref>, being able to consolidate more involved properties, as we lower bounds rely on a new technique which renders them applicable not only to worst-case instances <ref type="bibr" target="#b22">[23]</ref>, but in expectation over the input distribution.</p><p>An previous theoretical findings that non-anonymous MPNN are universal and can solve graph isomorphism <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5]</ref>, as well as that the
="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>.</p><p>Roughly tw
target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe mutation equivariant by design. Xu et al. <ref type="bibr" target="#b2">[3]</ref> and Morris et al. <ref type="bibr" target="#b3">[4]</ref> established the equivalence of anonymous MPNN to the 1st-ord en MPNN can also be analyzed by equivalence to the 1-WL test <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. For trees, the 1-WL test requires n iterations because there ng injective aggregation functions (i.e., of unbounded width <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b46">47]</ref>), the equivalence doe _0">1c</ref>). Specifically, methods for isomorphism testing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> that compare graphs G a
studied subcases of isomorphism, such as subgraph freeness <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> or those focused on anonymous networks <ref type="bibr">[3-
studied subcases of isomorphism, such as subgraph freeness <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> or those focused on anonymous networks <ref type="bibr">[3-
can (or perhaps cannot) distinguish between different graphs <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" targe
" target="#b39">40]</ref>, though the results may also be easily extended to account for coarsening <ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" t
ween different graphs <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" targe ures (also known as labels or attributes) and that are permutation equivariant by design. Xu et al. <ref type="bibr" target="#b2">[3]</ref> and Morris et al. <ref type="bibr" target="#b3">[4]</ref> es ls are provided in Appendix A.</p><p>Architecture and training. The networks combined multiple GIN0 <ref type="bibr" target="#b2">[3]</ref> layers with batch normalization and a simple sum readout. Th f the popular graph neural network architectures used today <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" targ lso relevant for the anonymous case, when MPNN can also be analyzed by equivalence to the 1-WL test <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. For trees, the 1-WL t 1-WL only when the former is built using injective aggregation functions (i.e., of unbounded width <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target Figure <ref type="figure" target="#fig_0">1c</ref>). Specifically, methods for isomorphism testing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target
(d) i : v i â V .</formula><p>For simplicity, it is here assumed that no graph pooling is employed <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, though the result
ween different graphs <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" targe ures (also known as labels or attributes) and that are permutation equivariant by design. Xu et al. <ref type="bibr" target="#b2">[3]</ref> and Morris et al. <ref type="bibr" target="#b3">[4]</ref> es ls are provided in Appendix A.</p><p>Architecture and training. The networks combined multiple GIN0 <ref type="bibr" target="#b2">[3]</ref> layers with batch normalization and a simple sum readout. Th f the popular graph neural network architectures used today <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" targ lso relevant for the anonymous case, when MPNN can also be analyzed by equivalence to the 1-WL test <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. For trees, the 1-WL t 1-WL only when the former is built using injective aggregation functions (i.e., of unbounded width <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target Figure <ref type="figure" target="#fig_0">1c</ref>). Specifically, methods for isomorphism testing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target
and its variants <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>, these networks can approximate any smooth function that m
were developed to evaluate overall image quality and not fine-grained lip-sync errors. Although LMD <ref type="bibr" target="#b3">[4]</ref> focuses on the lip region, we found that lip landmarks can b
et audio. A viewer can recognize an out-of-sync video segment as small as just â 0.05 â 0.1 seconds <ref type="bibr" target="#b8">[9]</ref> in duration. Thus, convincingly lip-syncing a real-world vid s derived from three standard test sets. We also propose reliable evaluation metrics using Sync-Net <ref type="bibr" target="#b8">[9]</ref> to precisely evaluate lip sync in unconstrained videos. We a tasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> is the SyncNet <ref type="bibr" target="#b8">[9]</ref> model. We propose to adapt and train a modified version of S pe="bibr" target="#b8">[9]</ref> model. We propose to adapt and train a modified version of SyncNet <ref type="bibr" target="#b8">[9]</ref> for our task.</p></div> <div xmlns="http://www.tei-c.org/ns/ div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Overview of SyncNet.</head><p>SyncNet <ref type="bibr" target="#b8">[9]</ref> inputs a window V of T v consecutive face frames (lower half c accuracy.</p><p>3.3.2 Our expert lip-sync discriminator. We make the following changes to SyncNet <ref type="bibr" target="#b8">[9]</ref> to train an expert lip-sync discriminator that suits our lip ="4.2.1">A Metric to</head><p>Measure the Lip-Sync Error. We propose to use the pre-trained SyncNet <ref type="bibr" target="#b8">[9]</ref> available publicly 3 to measure the lip-sync error between t the randomly chosen speech segment. The accuracy of SyncNet averaged over a video clip is over 99% <ref type="bibr" target="#b8">[9]</ref>. Thus, we believe this can be a good automatic evaluation me pert lip-sync discriminator that we have trained above, but the one released by Chung and Zisserman <ref type="bibr" target="#b8">[9]</ref>, which was trained on a different, non-public dataset. Using video with completely out-of-sync lip movements. Further details can be found in the SyncNet paper <ref type="bibr" target="#b8">[9]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n= s are most likely to be used. Further, given the sensitivity of humans to audio-lip synchronization <ref type="bibr" target="#b8">[9]</ref>, it is necessary to also evaluate our results with the help rated real video results using our new automatic metrics, "LSE-D" and "LSE-C" obtained from SyncNet <ref type="bibr" target="#b8">[9]</ref>. For the human evaluation, we ask 14 evaluators to judge the
, we feed color images. Secondly, our model is significantly deeper, with residual skip connections <ref type="bibr" target="#b14">[15]</ref>. Thirdly, inspired by this public implementation 2 , we us
with a limited set of words such as GRID <ref type="bibr" target="#b9">[10]</ref> (56 words), TIMIT <ref type="bibr" target="#b13">[14]</ref> and LRW <ref type="bibr" target="#b7">[8]</ref> (1000 word
="bibr" target="#b9">[10]</ref> (56 words), TIMIT <ref type="bibr" target="#b13">[14]</ref> and LRW <ref type="bibr" target="#b7">[8]</ref> (1000 words) which significantly hampers a model from learni there is a need for a metric that is designed specifically for measuring lip-sync errors.</p><p>LRW <ref type="bibr" target="#b7">[8]</ref> LRS2  <ref type="table">1</ref>: We propose two new metrics test sets, one each using the test set videos of LRS2 <ref type="bibr" target="#b0">[1]</ref>, LRW <ref type="bibr" target="#b7">[8]</ref>, and LRS3 <ref type="bibr" target="#b2">[3]</ref> respective
et="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> in detecting manipulated video content and their misuse. We
given input audio stream has received considerable attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar speech representations to lip landmarks using several hours of a single speaker. More recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> in this line direc large amount of data of a particular speaker, typically a few hours. A recent work along this line <ref type="bibr" target="#b12">[13]</ref> proposes to seamlessly edit videos of individual speakers
existing works is in terms of the vocabulary. Several works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref> train on datasets w
LRS2 train split (â 29 hours) with a batch size of 64, with T v = 5 frames using the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with an initial learning rate of 1e â3 . Our expert lip-sy ain set <ref type="bibr" target="#b0">[1]</ref>, with a batch size of 80. We use the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with an initial learning rate of 1e â4 and betas Î² 1 = 0.5
syncing talking face videos to match a given input audio stream has received considerable attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ
for that speaker. Another limitation of existing works is in terms of the vocabulary. Several works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" targ

ck(p k ) {At least 3HW F ops.} 12: end for 13: Y = BinaryDense(a L )</formula><p>in both TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and TVM <ref type="bibr" target="#b2">(Chen

ck(p k ) {At least 3HW F ops.} 12: end for 13: Y = BinaryDense(a L )</formula><p>in both TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and TVM <ref type="bibr" target="#b2">(Chen

ck(p k ) {At least 3HW F ops.} 12: end for 13: Y = BinaryDense(a L )</formula><p>in both TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and TVM <ref type="bibr" target="#b2">(Chen
as been evaluated on AlexNet <ref type="bibr" target="#b15">(Krizhevsky et al., 2012)</ref>, VGGNet <ref type="bibr" target="#b10">(He et al., 2015)</ref>, and Resnets <ref type="bibr" target="#b11">(
ck(p k ) {At least 3HW F ops.} 12: end for 13: Y = BinaryDense(a L )</formula><p>in both TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and TVM <ref type="bibr" target="#b2">(Chen
ck(p k ) {At least 3HW F ops.} 12: end for 13: Y = BinaryDense(a L )</formula><p>in both TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and TVM <ref type="bibr" target="#b2">(Chen
sky et al., 2012)</ref>, VGGNet <ref type="bibr" target="#b10">(He et al., 2015)</ref>, and Resnets <ref type="bibr" target="#b11">(He et al., 2016)</ref>. To directly compare against these results, w
vectors into "bitserial" versions that apply bitwise operations on packed bit vectors. For instance <ref type="bibr" target="#b19">(Rastegari et al., 2016)</ref> report convolution layers that use 58Ã space, with most papers introducing modifications to the core algorithm or new training techniques. <ref type="bibr" target="#b19">Rastegari et al. (2016)</ref> introduced XNOR-Net, which improved the
nformation retrieval to capture the exact and soft matches between a query and a candidate document <ref type="bibr" target="#b10">[Xiong et al., 2017]</ref>. Specifically, we apply the basic BERT uni ors are disordered and independent from each other. Thus we adopt a RBF kernel aggregation function <ref type="bibr" target="#b10">[Xiong et al., 2017]</ref> to extract features about the accumulation
a node embedding by aggregating all neighbors' embeddings <ref type="bibr">[Wang et al., 2018;</ref><ref type="bibr" target="#b9">Wu et al., 2019a;</ref><ref type="bibr" target="#b11">Xu et al., 2019] nguish the influence from different neighbors <ref type="bibr" target="#b0">[Cao et al., 2019;</ref><ref type="bibr" target="#b9">Wu et al., 2019a;</ref><ref type="bibr" target="#b11">Xu et al., 2019] ion for e or e â² by aggregating the names/descriptions of all their neighbors as existing works did <ref type="bibr" target="#b9">[Wu et al., 2019a;</ref><ref type="bibr" target="#b11">Xu et al., 2019 he assumption that two relations are more similar if they associate to more similar head-tail pairs <ref type="bibr" target="#b9">[Wu et al., 2019a]</ref>. Specifically, we average C(e) of all the ass lable results or codes. Some methods such as <ref type="bibr" target="#b2">[Chen et al., 2018;</ref><ref type="bibr" target="#b9">Trsedya et al., 2019]</ref> are not compared due to the code implement
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Performance on DWY100K</head><p>Since CEAFF <ref type="bibr" target="#b13">[Zeng et al., 2020]</ref> has already obtained 100% HR1 on the mono-l of the aligned entities are exactly the same, which demands more challenging mono-lingual datasets <ref type="bibr" target="#b13">[Zeng et al., 2020]</ref>.</p></div> <div xmlns="http://www.tei-c.org
een C(e) and C(e â² ). Negative pairs are sampled according to the cosine similarity of two entities <ref type="bibr" target="#b7">[Sun et al., 2018]</ref>.</p><p>For the input of BERT, we give priorit et DBP15K and the mono-lingual dataset DWY100K and use HitRatio@K (K=1,10) and MRR to evaluate (Cf. <ref type="bibr" target="#b7">[Sun et al., 2018]</ref> for details). The dimension of the BERT CLS e the one-hop neighbors take the most important role in aligning two entities. However, as claimed by <ref type="bibr" target="#b7">[Sun et al., 2020]</ref>, the multi-hop neighbors also impact the alig
een C(e) and C(e â² ). Negative pairs are sampled according to the cosine similarity of two entities <ref type="bibr" target="#b7">[Sun et al., 2018]</ref>.</p><p>For the input of BERT, we give priorit et DBP15K and the mono-lingual dataset DWY100K and use HitRatio@K (K=1,10) and MRR to evaluate (Cf. <ref type="bibr" target="#b7">[Sun et al., 2018]</ref> for details). The dimension of the BERT CLS e the one-hop neighbors take the most important role in aligning two entities. However, as claimed by <ref type="bibr" target="#b7">[Sun et al., 2020]</ref>, the multi-hop neighbors also impact the alig
#b15">[Zhu et al., 2017]</ref> to recent emergent graph neural networks such as attention-based GCN <ref type="bibr" target="#b11">[Xu et al., 2019]</ref>, highway <ref type="bibr">GCN [Wu et al., 201 ings <ref type="bibr">[Wang et al., 2018;</ref><ref type="bibr" target="#b9">Wu et al., 2019a;</ref><ref type="bibr" target="#b11">Xu et al., 2019]</ref>. However, since different KGs are highly heter pe="bibr" target="#b0">[Cao et al., 2019;</ref><ref type="bibr" target="#b9">Wu et al., 2019a;</ref><ref type="bibr" target="#b11">Xu et al., 2019]</ref>, essentially, the GCNlike models still mix the of all their neighbors as existing works did <ref type="bibr" target="#b9">[Wu et al., 2019a;</ref><ref type="bibr" target="#b11">Xu et al., 2019]</ref>. This similar idea is widely used in informati
N [Wu et al., 2019b]</ref>, relation-aware <ref type="bibr">GCN [Wu et al., 2019a]</ref> and VR-GCN <ref type="bibr" target="#b12">[Ye et al., 2019]</ref>.</p><p>Despite much effort taken on graph str


dge graphs (KGs), which can benefit many applications such as question answering and recommendation <ref type="bibr" target="#b8">[Tong et al., 2019]</ref>. However, a single KG is far from complete t
a node embedding by aggregating all neighbors' embeddings <ref type="bibr">[Wang et al., 2018;</ref><ref type="bibr" target="#b9">Wu et al., 2019a;</ref><ref type="bibr" target="#b11">Xu et al., 2019] nguish the influence from different neighbors <ref type="bibr" target="#b0">[Cao et al., 2019;</ref><ref type="bibr" target="#b9">Wu et al., 2019a;</ref><ref type="bibr" target="#b11">Xu et al., 2019] ion for e or e â² by aggregating the names/descriptions of all their neighbors as existing works did <ref type="bibr" target="#b9">[Wu et al., 2019a;</ref><ref type="bibr" target="#b11">Xu et al., 2019 he assumption that two relations are more similar if they associate to more similar head-tail pairs <ref type="bibr" target="#b9">[Wu et al., 2019a]</ref>. Specifically, we average C(e) of all the ass lable results or codes. Some methods such as <ref type="bibr" target="#b2">[Chen et al., 2018;</ref><ref type="bibr" target="#b9">Trsedya et al., 2019]</ref> are not compared due to the code implement
the dependency path between the entities might be even more indicative of the relation, as noted by <ref type="bibr" target="#b40">Toutanova et al. (2015)</ref>. It is quite possible that using these
owledge about relations between entities <ref type="bibr" target="#b29">(Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Baldini Soares et al., 2019)</ref>. Regardless of the end task, the kn
="bibr" target="#b14">(Hewitt and Manning, 2019;</ref><ref type="bibr">Tenney et al., 2019a,b;</ref><ref type="bibr" target="#b17">Jawahar et al., 2019;</ref><ref type="bibr" target="#b12">Goldberg, 2

rns of prompts that successfully extract knowledge from LMs. In open information extraction systems <ref type="bibr" target="#b2">(Banko et al., 2007)</ref>, manually defined patterns are often levera
owledge about relations between entities <ref type="bibr" target="#b29">(Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Baldini Soares et al., 2019)</ref>. Regardless of the end task, the kn
owledge about relations between entities <ref type="bibr" target="#b29">(Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Baldini Soares et al., 2019)</ref>. Regardless of the end task, the kn
in downstream language understanding systems <ref type="bibr" target="#b6">(Dai and Le, 2015;</ref><ref type="bibr" target="#b22">Melamud et al., 2016;</ref><ref type="bibr" target="#b27">Peters et a
tations are enhanced with external entity embeddings either at training time or solely at test time <ref type="bibr" target="#b28">(Peters et al., 2019;</ref><ref type="bibr" target="#b30">Poerner et
hoices and picking the most likely one <ref type="bibr" target="#b44">(Zweig and Burges, 2011;</ref><ref type="bibr" target="#b33">Rajani et al., 2019)</ref>. For example, LMs have been used to answer
e="bibr" target="#b43">Yang et al., 2017;</ref><ref type="bibr" target="#b16">IV et al., 2019;</ref><ref type="bibr" target="#b13">Hayashi et al., 2020)</ref>. Similar extensions have been applied to
rial attacks, designing defense mechanisms or building robust variants of GNNs have become critical <ref type="bibr" target="#b25">(Zhu et al. 2019)</ref>.</p><p>In this paper, we propose a new approa nism to attenuate the influence of neighbors with large variance (potentially corrupted). Following <ref type="bibr" target="#b25">(Zhu et al. 2019)</ref>, we set hidden dimensions at 16 and assume a poisoning attacks, UM-GNN consistently outperforms existing methods including the recent Robust GCN <ref type="bibr" target="#b25">(Zhu et al. 2019</ref>);</p><p>â¢ UM-GNN achieves significantly lower tions for specifically defending against adversarial attacks, the recent robust GCN (RGCN) approach <ref type="bibr" target="#b25">(Zhu et al. 2019</ref>) has been the most effective, when compared to h a weighted aggregation of features in a closed neighborhood where the weights are trainable. RGCN <ref type="bibr" target="#b25">(Zhu et al. 2019</ref>): This is a recently proposed ap-proach that e random structural perturbations and its low performance strongly corroborates with the findings in <ref type="bibr" target="#b25">(Zhu et al. 2019</ref>).</p><p>(ii) DICE Attack: In this challenging <ref type="bibr" target="#b26">(ZÃ¼gner, Akbarnejad, and GÃ¼nnemann 2018)</ref>. Recently, Zhu et al. <ref type="bibr" target="#b25">(Zhu et al. 2019</ref>) introduced a robust variant of GCN based on a
amples from the approximate posterior through Monte Carlo sampling. Interestingly, it was showed in <ref type="bibr" target="#b9">(Gal and Ghahramani 2016)</ref> that the dropout inference minimizes t
amples from the approximate posterior through Monte Carlo sampling. Interestingly, it was showed in <ref type="bibr" target="#b9">(Gal and Ghahramani 2016)</ref> that the dropout inference minimizes t
r three benchmark citation networks extensively used in similar studies: Cora, Citeseer, and Pubmed <ref type="bibr" target="#b14">(Sen et al. 2008)</ref>. The documents are represented by nodes, and
n be induced by deleting, adding or re-wiring edges; new nodes could also be injected into the graph<ref type="bibr" target="#b15">(Shanthamallu, Thiagarajan, and Spanias 2020)</ref>. (iv)Attacker's g
onally designed in the graph structure can lead to a non-trivial performance degradation as seen in <ref type="bibr" target="#b26">(ZÃ¼gner, Akbarnejad, and GÃ¼nnemann 2018)</ref>. This limits their app ng, and Leskovec 2017)</ref>. The vulnerability of GNNs to adversarial attacks was first studied in <ref type="bibr" target="#b26">(ZÃ¼gner, Akbarnejad, and GÃ¼nnemann 2018)</ref>. Since then, several g proximation of the given graph and showed that it can defend against specific types of graph attack <ref type="bibr" target="#b26">(ZÃ¼gner, Akbarnejad, and GÃ¼nnemann 2018)</ref>. Recently, Zhu et al. target="#b24">(Xu et al. 2019;</ref><ref type="bibr" target="#b22">Wu et al. 2019)</ref>, gray-box <ref type="bibr" target="#b26">(ZÃ¼gner, Akbarnejad, and GÃ¼nnemann 2018;</ref><ref type="bibr" target
nstant <ref type="bibr" target="#b12">(Kipf and Welling 2017)</ref> or a learnable attention weight <ref type="bibr" target="#b19">(VeliÄkoviÄ et al. 2018</ref>). The update function U is parameterize el, proposed by Kipf &amp; Welling, based on the message passing formulation in eqn. (2).</p><p>GAT <ref type="bibr" target="#b19">(VeliÄkoviÄ et al. 2018</ref>): This model uses a multi-head attentio /p><p>(2), it can be replaced using any other message passing strategy, e.g, graph attention layers <ref type="bibr" target="#b19">(VeliÄkoviÄ et al. 2018)</ref>. Given that all datasets we consider i transductive node classification setup <ref type="bibr" target="#b12">(Kipf and Welling 2017;</ref><ref type="bibr" target="#b19">VeliÄkoviÄ et al. 2018)</ref>, while using the standard train, test,
ed all the baselines and the proposed approach using the Pytorch Deep Graph Library (version 0.5.1) <ref type="bibr" target="#b21">(Wang et al. 2019</ref>). In our implementation of UM-GNN, the GNN mo
sarial attacks on images <ref type="bibr" target="#b10">(Goodfellow, Shlens, and Szegedy 2014;</ref><ref type="bibr" target="#b18">Szegedy et al. 2013)</ref> and their countermeasures <ref type="bibr"
to make GNNs more robust <ref type="bibr" target="#b10">(Goodfellow, Shlens, and Szegedy 2014;</ref><ref type="bibr" target="#b8">Feng et al. 2019</ref>). On the other hand, methods that rely on graph
amples from the approximate posterior through Monte Carlo sampling. Interestingly, it was showed in <ref type="bibr" target="#b9">(Gal and Ghahramani 2016)</ref> that the dropout inference minimizes t
and Wikipedia hyperlinks, has also been explored recently <ref type="bibr">(Min et al., 2019b;</ref><ref type="bibr" target="#b0">Asai et al., 2020)</ref>. The use of dense vector representations for
s selected using Google Suggest API, where the answers are entities in Freebase. CuratedTREC (TREC) <ref type="bibr" target="#b1">(Baudi? and ?ediv?, 2015)</ref>  </p></div> <div xmlns="http://www.tei
ned model <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> and a dual-encoder architecture <ref type="bibr" target="#b3">(Bromley et al., 1994)</ref>, we focus on developing the right trainin
dense encoders have become popular recently <ref type="bibr" target="#b47">(Yih et al., 2011;</ref><ref type="bibr" target="#b14">Huang et al., 2013;</ref><ref type="bibr" target="#b10">Gillick et al
a">1</ref>)) becomes a good ranking function for retrieval is essentially a metric learning problem <ref type="bibr" target="#b20">(Kulis, 2013)</ref>. The goal is to create a vector space such that r
search queries and the answers were spans in Wikipedia articles identified by annotators. TriviaQA <ref type="bibr" target="#b18">(Joshi et al., 2017)</ref> contains a set of trivia questions with an

/ref> and more recently for mini-batch <ref type="bibr" target="#b13">(Henderson et al., 2017;</ref><ref type="bibr" target="#b10">Gillick et al., 2019)</ref>. It has been shown to be an effective str bibr" target="#b47">(Yih et al., 2011;</ref><ref type="bibr" target="#b14">Huang et al., 2013;</ref><ref type="bibr" target="#b10">Gillick et al., 2019)</ref>, with applications to cross-lingual docum
to cosine similarity and L2 distance <ref type="bibr" target="#b29">(Mussmann and Ermon, 2016;</ref><ref type="bibr" target="#b34">Ram and Gray, 2012)</ref>. As our ablation study finds other similari
ch setting <ref type="bibr" target="#b47">(Yih et al., 2011)</ref> and more recently for mini-batch <ref type="bibr" target="#b13">(Henderson et al., 2017;</ref><ref type="bibr" target="#b10">Gillick
a set of trivia questions with answers that were originally scraped from the Web. WebQuestions (WQ) <ref type="bibr" target="#b2">(Berant et al., 2013</ref>) consists of questions selected using Googl
ction of documents. While early QA systems are often complicated and consist of multiple components <ref type="bibr" target="#b9">(Ferrucci (2012)</ref>; <ref type="bibr" target="#b28">Moldovan et al.
(2) a machine reader can thoroughly examine the retrieved contexts and identify the correct answer <ref type="bibr" target="#b5">(Chen et al., 2017)</ref>. Although reducing open-domain QA to machine ent that can select a small set of relevant texts, before applying the reader to extract the answer <ref type="bibr" target="#b5">(Chen et al., 2017)</ref>. <ref type="foot" target="#foot_1">4</ref>Fo e source documents for answering questions. We first apply the pre-processing code released in DrQA <ref type="bibr" target="#b5">(Chen et al., 2017)</ref> to extract the clean, text-portion of articl o-end QA results, measured by exact match with the reference answer after minor normalization as in <ref type="bibr" target="#b5">(Chen et al., 2017;</ref><ref type="bibr" target="#b22">Lee et al., 20 ike TF-IDF or BM25 have been used as the standard method applied broadly to various QA tasks (e.g., <ref type="bibr" target="#b5">Chen et al., 2017;</ref><ref type="bibr">Yang et al., 2019a,b;</ref><r
uct search (MIPS) algorithms (e.g., <ref type="bibr" target="#b38">Shrivastava and Li (2014)</ref>; <ref type="bibr" target="#b11">Guo et al. (2016)</ref>).</p><p>However, it is generally believed tha
n passage or dialogue re-ranking tasks <ref type="bibr" target="#b31">(Nogueira and Cho, 2019;</ref><ref type="bibr" target="#b15">Humeau et al., 2020)</ref>. Finally, a concurrent work <ref type="bib
/ref> and more recently for mini-batch <ref type="bibr" target="#b13">(Henderson et al., 2017;</ref><ref type="bibr" target="#b10">Gillick et al., 2019)</ref>. It has been shown to be an effective str bibr" target="#b47">(Yih et al., 2011;</ref><ref type="bibr" target="#b14">Huang et al., 2013;</ref><ref type="bibr" target="#b10">Gillick et al., 2019)</ref>, with applications to cross-lingual docum
ch setting <ref type="bibr" target="#b47">(Yih et al., 2011)</ref> and more recently for mini-batch <ref type="bibr" target="#b13">(Henderson et al., 2017;</ref><ref type="bibr" target="#b10">Gillick
retrieval methods have thus never be shown to outperform TF-IDF/BM25 for opendomain QA before ORQA <ref type="bibr" target="#b22">(Lee et al., 2019)</ref>, which proposes a sophisticated inverse cloz xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Wikipedia Data Pre-processing</head><p>Following <ref type="bibr" target="#b22">(Lee et al., 2019)</ref>, we use the English Wikipedia dump from Dec. d><p>We use the same five QA datasets and training/dev/testing splitting method as in previous work <ref type="bibr" target="#b22">(Lee et al., 2019)</ref>. Below we briefly describe each dataset and ute differences in exact match accuracy. It is interesting to contrast our results to those of ORQA <ref type="bibr" target="#b22">(Lee et al., 2019)</ref> and also the concurrently developed approach acy compared to <ref type="bibr">ORQA (41.5% vs. 33.3%)</ref> in the open Natural Questions setting <ref type="bibr" target="#b22">(Lee et al., 2019;</ref><ref type="bibr" target="#b21">Kwiatkowski et rticles and thus the distribution of training examples is extremely biased, as argued previously by <ref type="bibr" target="#b22">Lee et al. (2019)</ref>.</p></div> <div xmlns="http://www.tei-c.org/n run an ablation on Natural Questions where the retriever and reader are jointly trained, following <ref type="bibr" target="#b22">Lee et al. (2019)</ref>. This approach obtains a score of 39.8 EM, wh ditional pretraining with the objective that matches surrogates of questions and relevant passages, <ref type="bibr" target="#b22">Lee et al. (2019)</ref> jointly train the question encoder and reader rence answer after minor normalization as in <ref type="bibr" target="#b5">(Chen et al., 2017;</ref><ref type="bibr" target="#b22">Lee et al., 2019)</ref> ). For WQ and TREC in the Multi setting, we f

ch setting <ref type="bibr" target="#b47">(Yih et al., 2011)</ref> and more recently for mini-batch <ref type="bibr" target="#b13">(Henderson et al., 2017;</ref><ref type="bibr" target="#b10">Gillick

improving retrieval.</p><p>Retrieval in open-domain QA is usually implemented using TF-IDF or BM25 <ref type="bibr" target="#b36">(Robertson and Zaragoza, 2009)</ref>, which matches keywords efficien
ghbors into node representation learning and achieve state-ofthe-art performance for recommendation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" ta pervision Signal. Most models approach the recommendation task under a supervised learning paradigm <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta n users and items, where ð¦ ð¢ð indicates that user ð¢ has adopted item ð before. Most existing models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" ta rimental Settings</head><p>We conduct experiments on three widely used benchmark datasets: Yelp2018 <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, Amazon-Book <ref 2018 <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, Amazon-Book <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, and Alibaba-iFash shion <ref type="bibr" target="#b5">[6]</ref> <ref type="foot" target="#foot_0">1</ref> . Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, we use the same 1 et="#b35">[36]</ref>, and PinSage <ref type="bibr" target="#b47">[48]</ref> since the previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" ta type="bibr" target="#b1">[2]</ref>, to GCN that propagates user and item embeddings over the graph <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" ta embedding and item embedding. Here we implement it on a state-of-the-art GCN-based model, LightGCN <ref type="bibr" target="#b16">[17]</ref>. Experimental studies on three benchmark datasets demonstr ibr" target="#b47">48]</ref>, concatenation <ref type="bibr" target="#b43">[44]</ref>, or summation <ref type="bibr" target="#b16">[17]</ref> over the representations of all layers.</p><p>Supervised L rization coefficient ð 2 and the number of GCN layers within the suggested ranges.</p><p>â¢ LightGCN <ref type="bibr" target="#b16">[17]</ref>. This is the state-of-the-art graph-based CF method which between LightGCN and SGL-ED.trainable parameters, the space complexity remains the same as LightGCN<ref type="bibr" target="#b16">[17]</ref>. The time complexity of model inference is also the same,
dditional model parameters.</p><p>SSL has also been applied on graph data. For instance, Info-Graph <ref type="bibr" target="#b32">[33]</ref> and DGI <ref type="bibr" target="#b39">[40]</ref> learns n
ed in CV and NLP tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45]</ref> is infeasible for g and contrastive models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref>. Auto-encoding is t which typically requires multiple different views of samples <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>. There are both pro br" target="#b37">38]</ref> or global-global contrast manner <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>. The former focuses on modeling the relationship between th
target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45]</ref> is infeasible for graph-based recommendation, due to specif
ed in CV and NLP tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45]</ref> is infeasible for g and contrastive models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref>. Auto-encoding is t which typically requires multiple different views of samples <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>. There are both pro br" target="#b37">38]</ref> or global-global contrast manner <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>. The former focuses on modeling the relationship between th
dditional model parameters.</p><p>SSL has also been applied on graph data. For instance, Info-Graph <ref type="bibr" target="#b32">[33]</ref> and DGI <ref type="bibr" target="#b39">[40]</ref> learns n
for recommendation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Despite effe ngs over the graph <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>. Recently, attentio st existing models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref> construct a bipartite graph G = (V, E), where the node set eriments on three widely used benchmark datasets: Yelp2018 <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, Amazon-Book <ref type="bibr" target="#b16">[17,</ref><ref /ref><ref type="bibr" target="#b43">44]</ref>, Amazon-Book <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, and Alibaba-iFashion <ref type="bibr" target="#b5">[6]</re ref> <ref type="foot" target="#foot_0">1</ref> . Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, we use the same 10-core setting for Yelp2018 and Amazon-Bo the previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref> has validated the superiority over the compared ones. Upon on <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48]</ref>, concatenation <ref type="bibr" target="#b43">[44]</ref>, or summation <ref type="bibr" target="#b16">[17]</ref> ov arized in Table <ref type="table" target="#tab_2">2</ref>. We follow the same strategy described in <ref type="bibr" target="#b43">[44]</ref> to split the interactions into training, validation, and t sting with a ratio of 7:1:2.</p><p>For users in the testing set, we follow the all-ranking protocol <ref type="bibr" target="#b43">[44]</ref> to evaluate the top-ð¾ recommendation performance and repor p><p>4.1.1 Compared Methods. We compare the proposed SGL with the following CF models:</p><p>â¢ NGCF <ref type="bibr" target="#b43">[44]</ref>. This is a graph-based CF method largely follows the stand
hrough a Noise Contrastive Estimation (NCE) objective, which can be in either global-local contrast <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38]</ref> or global-global c
click an item and finds it unintersting after consuming it. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>. The neighborhood aggregation scheme in GCNs enlarges the i
type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, and Alibaba-iFashion <ref type="bibr" target="#b5">[6]</ref> <ref type="foot" target="#foot_0">1</ref> . Following <ref t
>[1]</ref>, item co-occurrence <ref type="bibr" target="#b1">[2]</ref>, to user and item attributes <ref type="bibr" target="#b25">[26]</ref>. Recently, Knowledge Graph (KG) is also unified with the u
memory-hierarchy parallelism (MHP). <ref type="foot" target="#foot_0">1</ref> Load Slice Core (LSC) <ref type="bibr" target="#b4">[5]</ref> was the first work to propose an sOoO core; Freeway <ref typ TIVATION</head><p>In this section, we briefly cover the background on the two prior sOoO cores -LSC <ref type="bibr" target="#b4">[5]</ref> and Freeway <ref type="bibr" target="#b9">[10]</ref> -and we FSC.</p><p>Restricted Out-of-Order Microarchitectures. We extensively discussed the Load Slice Core <ref type="bibr" target="#b4">[5]</ref> and Freeway <ref type="bibr" target="#b9">[10]</ref> through
mlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Comparison Against CESP</head><p>Palacharla et al. <ref type="bibr" target="#b12">[13]</ref> propose the complexity-effective superscalar processor (CE oint out the most closely related work.</p><p>Complexity-Effective Architectures. Palacharla et al. <ref type="bibr" target="#b12">[13]</ref> propose the complexity-effective superscalar processors (C
e (LSC) <ref type="bibr" target="#b4">[5]</ref> was the first work to propose an sOoO core; Freeway <ref type="bibr" target="#b9">[10]</ref> builds upon the LSC proposal and exposes more MHP than LSC ueue), while all other instructions execute from the main, arithmetic queue (A-queue). Kumar et al. <ref type="bibr" target="#b9">[10]</ref> observe that, in LSC, an independent load may be stuck behi the background on the two prior sOoO cores -LSC <ref type="bibr" target="#b4">[5]</ref> and Freeway <ref type="bibr" target="#b9">[10]</ref> -and we elaborate on their shortcomings. Figure <ref type=" d cannot issue to the memory hierarchy, hindering the opportunity to expose MHP.</p><p>Kumar et al. <ref type="bibr" target="#b9">[10]</ref> propose Freeway, a core microarchitecture that overcomes LS " target="#tab_2">2</ref>. We evaluate LSC and Freeway following the configurations by Kumar et al. <ref type="bibr" target="#b9">[10]</ref>. The size of the A and B queues in LSC is 16-entries each. s. We extensively discussed the Load Slice Core <ref type="bibr" target="#b4">[5]</ref> and Freeway <ref type="bibr" target="#b9">[10]</ref> throughout the paper. Shioya et al. <ref type="bibr" target
ional OoO core because of frequent dispatch stalls. A similar steering policy is used by Kim et al. <ref type="bibr" target="#b8">[9]</ref> in their Instruction-Level Distributed Processing (ILDP) wor
ional OoO core because of frequent dispatch stalls. A similar steering policy is used by Kim et al. <ref type="bibr" target="#b8">[9]</ref> in their Instruction-Level Distributed Processing (ILDP) wor
ional OoO core because of frequent dispatch stalls. A similar steering policy is used by Kim et al. <ref type="bibr" target="#b8">[9]</ref> in their Instruction-Level Distributed Processing (ILDP) wor
tch stalls when an independent instruction cannot be steered to an empty queue. Salverda and Zilles <ref type="bibr" target="#b13">[14]</ref> evaluate CESP in the context of a realistic baseline and p
</ref>, flea-flicker multipass pipelining <ref type="bibr" target="#b2">[3]</ref>, braid processing <ref type="bibr" target="#b21">[22]</ref> and OUTRIDER <ref type="bibr" target="#b5">[6]</ref> also
execution units. Our experimental results show that FSC outperforms CESP.</p><p>Salverda and Zilles <ref type="bibr" target="#b14">[15]</ref> analyze the fundamental challenges of fusing small in-orde
>[5]</ref> and Freeway <ref type="bibr" target="#b9">[10]</ref> throughout the paper. Shioya et al. <ref type="bibr" target="#b17">[18]</ref> propose the front-end execution architecture which execute
</ref>, flea-flicker multipass pipelining <ref type="bibr" target="#b2">[3]</ref>, braid processing <ref type="bibr" target="#b21">[22]</ref> and OUTRIDER <ref type="bibr" target="#b5">[6]</ref> also
evaluate FSC using the most detailed, cycle-level, and hardwarevalidated core model in Sniper v6.0 <ref type="bibr" target="#b3">[4]</ref>. The configurations for the InO, FSC and OoO cores are provi
ower consumption and chip area using McPAT <ref type="bibr" target="#b10">[11]</ref> and CACTI v6.5 <ref type="bibr" target="#b11">[12]</ref> assuming a 22 nm technology node. Area and per-access powe er and floating-point registers. The MSHR is extended to support 8 outstanding misses. We use CACTI <ref type="bibr" target="#b11">[12]</ref> to estimate chip area. CACTI accounts for the area of circ power consumption. power consumed by the additional FSC hardware structures is modeled using CACTI <ref type="bibr" target="#b11">[12]</ref>. Table <ref type="table" target="#tab_3">3</ref> reports p
ssociative cache with 2/2 read/write ports. We estimate power consumption and chip area using McPAT <ref type="bibr" target="#b10">[11]</ref> and CACTI v6.5 <ref type="bibr" target="#b11">[12]</ref> a div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Power Consumption</head><p>We use McPAT <ref type="bibr" target="#b10">[11]</ref> to calculate InO and OoO core power consumption. power con
bles out-of-order execution capabilities among in-order queues.</p><p>Decoupled Access-Execute. DAE <ref type="bibr" target="#b18">[19]</ref> is the first work to separate access and execute phases of
ional OoO core because of frequent dispatch stalls. A similar steering policy is used by Kim et al. <ref type="bibr" target="#b8">[9]</ref> in their Instruction-Level Distributed Processing (ILDP) wor
ecute phases of a program through coordinated queues. Proposals such as speculative-slice execution <ref type="bibr" target="#b22">[23]</ref>, flea-flicker multipass pipelining <ref type="bibr" target
tructions while buffering non-critical instructions in the front-end. More recently, Alipour et al. <ref type="bibr" target="#b0">[1]</ref> leverage instruction criticality and readiness to bypass the
lative-slice execution <ref type="bibr" target="#b22">[23]</ref>, flea-flicker multipass pipelining <ref type="bibr" target="#b2">[3]</ref>, braid processing <ref type="bibr" target="#b21">[22]</ref>
tch stalls when an independent instruction cannot be steered to an empty queue. Salverda and Zilles <ref type="bibr" target="#b13">[14]</ref> evaluate CESP in the context of a realistic baseline and p
lative-slice execution <ref type="bibr" target="#b22">[23]</ref>, flea-flicker multipass pipelining <ref type="bibr" target="#b2">[3]</ref>, braid processing <ref type="bibr" target="#b21">[22]</ref>
br" target="#b2">[3]</ref>, braid processing <ref type="bibr" target="#b21">[22]</ref> and OUTRIDER <ref type="bibr" target="#b5">[6]</ref> also exploit critical instruction slices <ref type="bibr" ta
dentification by the uniqueness of every individual is an indispensable part of human life nowadays <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, because of t
ntion on various visual tasks such as image classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5]</ref>, semantic segmentatio
stride, and z rounds down z if it is a fraction. BN(?) represents the Batch Normalization operation <ref type="bibr" target="#b55">[56]</ref>, and the nonlinear activation operation is performed by</p
venient for others to implement and compare with the proposed method, five public datasets FANTASIA <ref type="bibr" target="#b56">[57]</ref>,</p><p>CEBSDB <ref type="bibr" target="#b57">[58]</ref>, N
f type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, and frequency based <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> features. However,
ltering to improve the quality of ECG signals. Filtering and segmentation have been widely explored <ref type="bibr" target="#b53">[54]</ref>. Complex pre-processing operations are beyond of the scope
Generally, there are some structural features in ECG signals <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, which promotes the combination of ECG and CNN. Kiranyaz et a
rget="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref>. Different from other biometrics, ECG signals can only be cap
ures from the ECG signals by 1-D CNN and demonstrated the performance of CNN for ECG classification <ref type="bibr" target="#b18">[19]</ref>. The strong capability of CNN for feature extraction of EC suit the one-dimensional ECG signals.</p><p>Several works <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b18">19]</ref> have been studied and demonstrated the availability of modi
ation patterns of the heart, has been proposed and studied <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar
) NN based methods.</p><p>(1) Distance based methods for identification. For distance based methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b24">25]</ref>, assigning the unk
likely fake edges, and assigns less weight to suspicious edges based on network theory of homophily <ref type="bibr" target="#b13">[14]</ref>. The latter components stabilizes the evolution of graph s target="#b39">40]</ref>), GNNGUARD determines importance weights using theory of network homophily <ref type="bibr" target="#b13">[14]</ref>, positing that similar nodes (i.e., nodes with similar fea
h undirected edges and binary features: Cora <ref type="bibr" target="#b44">[45]</ref> and Citeseer <ref type="bibr" target="#b45">[46]</ref>. We also consider a directed graph with numeric node featu
ience and development of technology but can also substantially interfere with human decision making <ref type="bibr" target="#b10">[11]</ref>. For this reason, it is vital to develop GNNs that are rob 2]</ref> and the development of effective defense techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>, adversarial attack
neural networks, are highly fragile to adversarial attacks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. The majority of ex
y of application areas <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. The key to the success odes' local network neighborhoods as well as nodes' features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36]</ref>. The learned embeddin RD can effectively restore state-of-the-art performance of even the strongest and most popular GNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" targ bibr" target="#b35">36]</ref>. The learned embeddings can be used for a variety of downstream tasks <ref type="bibr" target="#b2">[3]</ref>. Let h k u â R D k denote the embedding of node u in the k-t nst adversarial attacks. GNNGUARD works with many GNNs, including Graph Convolutional Network (GCN) <ref type="bibr" target="#b2">[3]</ref>, Graph Attention Network (GAT) <ref type="bibr" target="#b18 Ns. GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. We integrate GNNGUARD with five GNNs (GCN <ref type="bibr" target="#b2">[3]</ref>, GAT <ref type="bibr" target="#b18">[19]</ref>, GIN <ref typ odel architectures, we closely follow original authors' guidelines and relevant papers on GNNs (GCN <ref type="bibr" target="#b2">[3]</ref>, GAT <ref type="bibr" target="#b18">[19]</ref>, GIN <ref typ
not need any external data. Further, recent studies (e.g., <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>) focus on theoretical certificates for GNN robustness inste
f-the-art methods by up to 15.3% in defense performance. Importantly, unlike existing GNN defenders <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta dversarial defense. We briefly overview the state-of-the-art defense methods on graphs. GNN-Jaccard <ref type="bibr" target="#b14">[15]</ref> is a defense approach that pre-processes the adjacency mat ct of fake edges, we prune edges that are likely forged. Building on network homophily and findings <ref type="bibr" target="#b14">[15]</ref> that fake edges tend to connect dissimilar nodes, we prune fense for graphs. We briefly contrast GNNGUARD with existing GNN defenders. Compared to GNN-Jaccard <ref type="bibr" target="#b14">[15]</ref>, which examines fake edges as a GNN preprocessing step, GN line defense algorithms. We compare GNNGUARD to three state-of-the-art graph defenders: GNN-Jaccard <ref type="bibr" target="#b14">[15]</ref>, RobustGCN <ref type="bibr" target="#b16">[17]</ref>, and nd Graph-SAINT <ref type="bibr" target="#b20">[21]</ref>), baseline defense algorithms (GNN-Jaccard <ref type="bibr" target="#b14">[15]</ref>, RobustGCN <ref type="bibr" target="#b16">[17]</ref>, and N defender against poisoning attacks. Recent studies found <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15]</ref> that most damaging attacks add fake edges between nodes tha ion for target node u by manipulating the incident edges of u<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. Here, T = A = {u}. (2) Influence targeted attacks. The att
target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, thereby demonstrat Graph Isomorphism Network (GIN) <ref type="bibr" target="#b6">[7]</ref>, Jumping Knowledge (JK-Net) <ref type="bibr" target="#b19">[20]</ref>, GraphSAINT <ref type="bibr" target="#b20">[21]</ref>, Gra GAT <ref type="bibr" target="#b18">[19]</ref>, GIN <ref type="bibr" target="#b6">[7]</ref>, JK-Net <ref type="bibr" target="#b19">[20]</ref>, and GraphSAINT <ref type="bibr" target="#b20">[21]</ref>) GAT <ref type="bibr" target="#b18">[19]</ref>, GIN <ref type="bibr" target="#b6">[7]</ref>, JK-Net <ref type="bibr" target="#b19">[20]</ref>, and Graph-SAINT <ref type="bibr" target="#b20">[21]</ref>
way that GNNs exchange that information between nodes makes them vulnerable to adversarial attacks <ref type="bibr" target="#b7">[8]</ref>. Adversarial attacks on graphs, which carefully rewire the g ning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks (e.g., Nettack <ref type="bibr" target="#b7">[8]</ref>) that perturb the graph in training-time and evasion attacks "#b28">[29]</ref>. The former deceives the model to misclassify a specific node (i.e., target node) <ref type="bibr" target="#b7">[8]</ref> while the latter degrades the overall performance of the tra e targeted attack where the attacker only manipulates edges of the target node's neighbors. Nettack <ref type="bibr" target="#b7">[8]</ref> generates perturbations by modifying graph structure (i.e., . Also, <ref type="bibr" target="#b15">[16]</ref> is designed specifically for the Nettack attacker <ref type="bibr" target="#b7">[8]</ref> and so is less versatile. Another technique <ref type="bibr" our model to baselines under three kinds of adversarial attacks: direct targeted attack (Nettack-Di <ref type="bibr" target="#b7">[8]</ref>), influence targeted attack (Nettack-In <ref type="bibr" tar attack (Nettack-Di <ref type="bibr" target="#b7">[8]</ref>), influence targeted attack (Nettack-In <ref type="bibr" target="#b7">[8]</ref>), and non-targeted attack (Mettack <ref type="bibr" target=" or all neighbors. In the targeted attack, we select 40 correctly classified target nodes (following <ref type="bibr" target="#b7">[8]</ref>): 10 nodes with the largest classification margin, 20 random taset into training (10%), validation (10%), and test set (80%) following the experimental setup in <ref type="bibr" target="#b7">[8]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Ap ef type="bibr" target="#b15">[16]</ref>), and models for generating adversarial attacks (Nettack-Di <ref type="bibr" target="#b7">[8]</ref>, Nettack-In <ref type="bibr" target="#b7">[8]</ref>, and Met for generating adversarial attacks (Nettack-Di <ref type="bibr" target="#b7">[8]</ref>, Nettack-In <ref type="bibr" target="#b7">[8]</ref>, and Mettack <ref type="bibr" target="#b25">[26]</ref>).</p> y loss using Adam optimizer and learning rate of 0.01. For other parameters, we follow the setup in <ref type="bibr" target="#b7">[8]</ref>.</p></div>			</div> 			<div type="references">  				<listBib attacker finds optimal perturbation A through optimization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8]</ref>:</p><formula xml:id="formula_3">argmin A âP G â L attack (f(A s. The attacker aims to destroy prediction for target node u by manipulating the incident edges of u<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. Here, T = A = {u}.
n abundance of research for image (e.g., <ref type="bibr" target="#b33">[34]</ref>) and text (e.g., <ref type="bibr" target="#b34">[35]</ref>) adversarial defense. We briefly overview the state-of-the
ting a citation network of CS papers published between 1971 and 2014. We use a Disease Pathway (DP) <ref type="bibr" target="#b47">[48]</ref> graph with continuous features describing a system Generat further dataset statistics in Table <ref type="table">7</ref>.</p><p>The new, Disease Pathway (DP) <ref type="bibr" target="#b47">[48]</ref> dataset describes a system of interacting human proteins w
zing GCN, S-GCN, ChebNet and related methods. Our architecture is analogous to the inception module <ref type="bibr" target="#b43">Szegedy et al. (2015)</ref>; <ref type="bibr" target="#b22">Kazi et a ion ( <ref type="formula" target="#formula_6">4</ref>) is analogous to the popular Inception module <ref type="bibr" target="#b43">Szegedy et al. (2015)</ref> for classic CNN architectures (Figure <re
>, modeling of proteins <ref type="bibr" target="#b14">Gainza et al. (2019)</ref> and nucleic acids <ref type="bibr" target="#b37">Rossi et al. (2019)</ref>, and fake news detection on social media <r

.tei-c.org/ns/1.0"><head n="2.3">Convolution-like operators on graphs</head><p>Spectral graph CNNs. <ref type="bibr" target="#b7">Bruna et al. Bruna et al. (2014)</ref> used the graph Fourier transfor as a transfer function Ä(Î»), applied to the Laplacian as Ä(â) = Î¦Ä(Î)Î¦ . Unlike the construction of <ref type="bibr" target="#b7">Bruna et al. (2014)</ref> that does not generalize across graphs, the
017)</ref>, medicine <ref type="bibr" target="#b33">Parisot et al. (2018)</ref>, drug repositioning <ref type="bibr" target="#b54">Zitnik et al. (2018)</ref>, discovery of anti-cancer foods <ref type=
sitioning <ref type="bibr" target="#b54">Zitnik et al. (2018)</ref>, discovery of anti-cancer foods <ref type="bibr" target="#b45">Veselkov et al. (2019)</ref>, modeling of proteins <ref type="bibr" t (and edge features, if available) as a matrix of the form B(X).</p><p>Limitations. Graph attention <ref type="bibr" target="#b45">Veselkov et al. (2019)</ref> and similar mechanisms <ref type="bibr"
al. (2014)</ref> used the graph Fourier transform to generalize convolutional neural networks (CNN) <ref type="bibr" target="#b27">LeCun et al. (1989)</ref> to graphs. This approach has multiple drawb

ng (e.g. graph autoencoders). The latter is a particularly important setting in recommender systems <ref type="bibr" target="#b2">Berg et al. (2018)</ref>.</p><p>While we focused our discussion on und
017)</ref>, medicine <ref type="bibr" target="#b33">Parisot et al. (2018)</ref>, drug repositioning <ref type="bibr" target="#b54">Zitnik et al. (2018)</ref>, discovery of anti-cancer foods <ref type=
atures by means of message passing with the neighbor nodes, and possibly Pooling amounting to fixed <ref type="bibr" target="#b12">Dhillon et al. (2007)</ref> or learnable <ref type="bibr" target="#b5
l., 2018;</ref><ref type="bibr" target="#b12">Gui et al., 2019b)</ref>.</p><p>Recently, Transformer <ref type="bibr" target="#b34">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, l "#b34">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, like machine translation <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>, language modeling <ref type="bibr" targ mlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer</head><p>Transformer was introduced by <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>, which was mainly based on self-attentio ="bibr" target="#b7">Devlin et al., 2018)</ref>. Instead of using the sinusoidal position embedding <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> and learned absolute position embedding, 1">Transformer Encoder Architecture</head><p>We first introduce the Transformer encoder proposed in <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>. The Transformer encoder takes in an mat e Transformer encoder includes layer normalization and Residual connection, we use them the same as <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>.</p></div> <div xmlns="http://www.tei-c. ng it unable to capture the sequential characteristic of languages. In order to solve this problem, <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> suggested to use position embeddings gen two tokens. For any fixed offset k, P E t+k can be represented by a linear transformation of P E t <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>. In TENER, Transformer encoder is used n d in the Transformer is unaware of positions, to avoid this shortage, position embeddings were used <ref type="bibr" target="#b34">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b7">Devlin et a
sm. Therefore, CNN has been proposed by <ref type="bibr" target="#b33">(Strubell et al., 2017;</ref><ref type="bibr" target="#b11">Gui et al., 2019a)</ref> to encode words concurrently. In order to en https://github. com/fastnlp/TENER.  <ref type="bibr" target="#b39">(Zhang and Yang, 2018)</ref> and <ref type="bibr" target="#b11">(Gui et al., 2019a)</ref>, respectively. "w/ scale" means TENER using

Che et al., 2013)</ref>.</p><p>(4) The corpus of the Chinese NER dataset MSRA came from news domain <ref type="bibr" target="#b19">(Levow, 2006)</ref>.</p><p>(5) Weibo NER was built based on text in C
extraction <ref type="bibr" target="#b24">(Miwa and Bansal, 2016)</ref>, and coreference resolution <ref type="bibr" target="#b9">(Fragkou, 2017)</ref>. Since <ref type="bibr" target="#b5">(Collobert
rmer with the ability of direction-and distanceawareness, we adopt the relative positional encoding <ref type="bibr" target="#b31">(Shaw et al., 2018;</ref><ref type="bibr" target="#b15">Huang et al., core can distinguish different directions and distances. The above improvement is based on the work <ref type="bibr" target="#b31">(Shaw et al., 2018;</ref><ref type="bibr" target="#b6">Dai et al., 20 ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> and learned absolute position embedding, <ref type="bibr" target="#b31">Shaw et al. (2018)</ref> argued that the distance between two tokens
(Levow, 2006)</ref>.</p><p>(5) Weibo NER was built based on text in Chinese social media Sina Weibo <ref type="bibr" target="#b25">(Peng and Dredze, 2015)</ref>, and it contained 4 kinds of entities.<


(Levow, 2006)</ref>.</p><p>(5) Weibo NER was built based on text in Chinese social media Sina Weibo <ref type="bibr" target="#b25">(Peng and Dredze, 2015)</ref>, and it contained 4 kinds of entities.<
Che et al., 2013)</ref>.</p><p>(4) The corpus of the Chinese NER dataset MSRA came from news domain <ref type="bibr" target="#b19">(Levow, 2006)</ref>.</p><p>(5) Weibo NER was built based on text in C
model with feature selection techniques to enhance the performance. The magnitude measure technique <ref type="bibr" target="#b2">[3]</ref> uses the absolute value of weights from a fully trained netw presented two techniques to reduce the number of features, namely filter method (magnitude measure) <ref type="bibr" target="#b2">[3]</ref> and embedded method ( -1 norm regularisation) <ref type="bib ues that connects a hidden neuron j in the hidden layer into an output neuron k in the output layer <ref type="bibr" target="#b2">[3]</ref>. The following equation measures the contribution from an in data set described in Sect. 2.1, and removed two features with the lowest Q-values as suggested by <ref type="bibr" target="#b2">[3]</ref> to produce more consistent results. The network was re-train
examined neural network models to perform video-based stress recognition using ANUStressDB data set <ref type="bibr" target="#b5">[6]</ref>. Recent works on video-based stress recognition <ref type="b > <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Set</head><p>The ANUStressDB data set <ref type="bibr" target="#b5">[6]</ref> consists of video samples from 24 subjects. Each video has a d 6-7% accuracy loss</p><p>The methods described in this paper are neural-based model. The paper in <ref type="bibr" target="#b5">[6]</ref> used Support Vector Machine (SVM) based model to perform the DB data set <ref type="bibr" target="#b5">[6]</ref>. Recent works on video-based stress recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref> requires feature eng based stress recognition system incorporates feature engineering process before making a prediction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>. However, this proce
5">[6]</ref>. Recent works on video-based stress recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref> requires feature engineering process, which is time consumi rates feature engineering process before making a prediction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>. However, this process is expensive and timeconsuming. In o
m due to zero gradient during back-propagation. To address overfitting on the model, dropout layers <ref type="bibr" target="#b11">[12]</ref> are used in the LSTM layer (p = 0.1) and both hidden layer
>[9]</ref>. Finally, we compared improved NN models with a recurrent neural network model with LSTM <ref type="bibr" target="#b4">[5]</ref> to perform video-based stress recognition task. The LSTM mod
sure the contribution of input features towards output values. The -1 norm regularisation technique <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> is an embedded feature tude measure) <ref type="bibr" target="#b2">[3]</ref> and embedded method ( -1 norm regularisation) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Magnitude Measu
towards output values. The -1 norm regularisation technique <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> is an embedded feature selection technique used to bring weig #b2">[3]</ref> and embedded method ( -1 norm regularisation) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Magnitude Measure Technique. The magnitude measure tec
from original fully-connected layers. Instead of ReLU activation function, the model uses LeakyReLU <ref type="bibr" target="#b6">[7]</ref> with negative slope 0.1 to avoid dying ReLU problem due to z
e="bibr" target="#b1">[2]</ref> and optimised using Stochastic Gradient Descent (SGD) with momentum <ref type="bibr" target="#b9">[10]</ref> with learning rate 0.1, and momentum term 0.9. The model hy
from original fully-connected layers. Instead of ReLU activation function, the model uses LeakyReLU <ref type="bibr" target="#b6">[7]</ref> with negative slope 0.1 to avoid dying ReLU problem due to z
e="bibr" target="#b1">[2]</ref> and optimised using Stochastic Gradient Descent (SGD) with momentum <ref type="bibr" target="#b9">[10]</ref> with learning rate 0.1, and momentum term 0.9. The model hy
sure the contribution of input features towards output values. The -1 norm regularisation technique <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> is an embedded feature tude measure) <ref type="bibr" target="#b2">[3]</ref> and embedded method ( -1 norm regularisation) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Magnitude Measu
ed in the model is cross-entropy error function. The model was trained using error back-propagation <ref type="bibr" target="#b1">[2]</ref> and optimised using Stochastic Gradient Descent (SGD) with m
from original fully-connected layers. Instead of ReLU activation function, the model uses LeakyReLU <ref type="bibr" target="#b6">[7]</ref> with negative slope 0.1 to avoid dying ReLU problem due to z
sure the contribution of input features towards output values. The -1 norm regularisation technique <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> is an embedded feature tude measure) <ref type="bibr" target="#b2">[3]</ref> and embedded method ( -1 norm regularisation) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Magnitude Measu
from original fully-connected layers. Instead of ReLU activation function, the model uses LeakyReLU <ref type="bibr" target="#b6">[7]</ref> with negative slope 0.1 to avoid dying ReLU problem due to z
5">[6]</ref>. Recent works on video-based stress recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref> requires feature engineering process, which is time consumi rates feature engineering process before making a prediction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>. However, this process is expensive and timeconsuming. In o
from original fully-connected layers. Instead of ReLU activation function, the model uses LeakyReLU <ref type="bibr" target="#b6">[7]</ref> with negative slope 0.1 to avoid dying ReLU problem due to z
from original fully-connected layers. Instead of ReLU activation function, the model uses LeakyReLU <ref type="bibr" target="#b6">[7]</ref> with negative slope 0.1 to avoid dying ReLU problem due to z
e="bibr" target="#b1">[2]</ref> and optimised using Stochastic Gradient Descent (SGD) with momentum <ref type="bibr" target="#b9">[10]</ref> with learning rate 0.1, and momentum term 0.9. The model hy
from original fully-connected layers. Instead of ReLU activation function, the model uses LeakyReLU <ref type="bibr" target="#b6">[7]</ref> with negative slope 0.1 to avoid dying ReLU problem due to z
from original fully-connected layers. Instead of ReLU activation function, the model uses LeakyReLU <ref type="bibr" target="#b6">[7]</ref> with negative slope 0.1 to avoid dying ReLU problem due to z


" target="#b14">(Goyal et al., 2017;</ref><ref type="bibr" target="#b13">Gotmare et al., 2019;</ref><ref type="bibr" target="#b3">Bernstein et al., 2018;</ref><ref type="bibr" target="#b30">Xiao et al
ctification term is orthogonal to other training stabilization techniques such as gradient clipping <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>, smoothing the adaptive learning rate (i.e
fication (i.e., CIFAR10 <ref type="bibr" target="#b19">(Krizhevsky et al., 2009)</ref> and ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>) are presented in Figure <ref type="figure"
en made to accelerate optimization by applying adaptive learning rate.</p><p>In particular, Adagrad <ref type="bibr" target="#b11">(Duchi et al., 2010)</ref> and its variants, e.g., RMSprop <ref type=
head>Performance Comparison.</head><p>The performances on language modeling (i.e., One Billion Word <ref type="bibr" target="#b6">(Chelba et al., 2013)</ref>) and image classification (i.e., CIFAR10 <
hat generations of researchers have been pursuing <ref type="bibr" target="#b12">(Gauss, 1823;</ref><ref type="bibr" target="#b4">Cauchy, 1847)</ref>. Remarkably, stochastic gradient-based optimizatio
head>Performance Comparison.</head><p>The performances on language modeling (i.e., One Billion Word <ref type="bibr" target="#b6">(Chelba et al., 2013)</ref>) and image classification (i.e., CIFAR10 <
fication (i.e., CIFAR10 <ref type="bibr" target="#b19">(Krizhevsky et al., 2009)</ref> and ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>) are presented in Figure <ref type="figure"
head>Performance Comparison.</head><p>The performances on language modeling (i.e., One Billion Word <ref type="bibr" target="#b6">(Chelba et al., 2013)</ref>) and image classification (i.e., CIFAR10 <
ints (Luo et al., 2019)), initialization <ref type="bibr" target="#b1">(Balduzzi et al., 2017;</ref><ref type="bibr" target="#b32">Zhang et al., 2019)</ref> and normalization <ref type="bibr" target="
them to vanilla Adam with and without warmup on the IWSLT'14 German to English translation dataset <ref type="bibr" target="#b5">(Cettolo et al., 2014)</ref>.</p><p>In order to reduce the variance of
ctification term is orthogonal to other training stabilization techniques such as gradient clipping <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>, smoothing the adaptive learning rate (i.e
heuristic -using a small learning rate in the first few epochs of training to mitigate such problem <ref type="bibr" target="#b28">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b25">Popel &amp <head>B.3 NEURAL MACHINE TRANSLATION</head><p>Our experiments are based on the default Transformers <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> implementation from the fairseq package
head>Performance Comparison.</head><p>The performances on language modeling (i.e., One Billion Word <ref type="bibr" target="#b6">(Chelba et al., 2013)</ref>) and image classification (i.e., CIFAR10 <
head>Performance Comparison.</head><p>The performances on language modeling (i.e., One Billion Word <ref type="bibr" target="#b6">(Chelba et al., 2013)</ref>) and image classification (i.e., CIFAR10 <
fication (i.e., CIFAR10 <ref type="bibr" target="#b19">(Krizhevsky et al., 2009)</ref> and ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>) are presented in Figure <ref type="figure"
" target="#b14">(Goyal et al., 2017;</ref><ref type="bibr" target="#b13">Gotmare et al., 2019;</ref><ref type="bibr" target="#b3">Bernstein et al., 2018;</ref><ref type="bibr" target="#b30">Xiao et al
target="#b13">Gotmare et al., 2019;</ref><ref type="bibr" target="#b3">Bernstein et al., 2018;</ref><ref type="bibr" target="#b30">Xiao et al., 2017)</ref>. We notice that r t has a similar form to th
="#b7">(Chen &amp; Gu, 2018)</ref>, or adding range constraints (Luo et al., 2019)), initialization <ref type="bibr" target="#b1">(Balduzzi et al., 2017;</ref><ref type="bibr" target="#b32">Zhang et a at the beginning of the training, since weights are sampled from normal distributions with mean zero<ref type="bibr" target="#b1">(Balduzzi et al., 2017)</ref>, further analysis is conducted in Sectio
="#b7">(Chen &amp; Gu, 2018)</ref>, or adding range constraints (Luo et al., 2019)), initialization <ref type="bibr" target="#b1">(Balduzzi et al., 2017;</ref><ref type="bibr" target="#b32">Zhang et a at the beginning of the training, since weights are sampled from normal distributions with mean zero<ref type="bibr" target="#b1">(Balduzzi et al., 2017)</ref>, further analysis is conducted in Sectio
label field is one of the most effective way to construct the connection between neighboring pixels <ref type="bibr" target="#b4">[5]</ref> . It decreases with the number of pixels having the same lab
image as points in it. Pixels representing the same object naturally cluster in the spectral space <ref type="bibr" target="#b3">[4]</ref> . This property provides us an opportunity to segment pixels
traint in image domain, that neighboring pixels contribute to the segmentation of the central pixel <ref type="bibr" target="#b24">[25]</ref> . Markov Random Model-based FCM (MRF_FCM) used the MRF mod
composes an image into homogeneous regions, is an important task in remote sensing image processing <ref type="bibr" target="#b0">[1]</ref> . The accuracy of image segmentation has an essential influe
d a noisy image segmentation algorithm by integrating guided filter into fuzzy clustering algorithm <ref type="bibr" target="#b16">[17]</ref> .</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head _mFS) is defined as</p><formula xml:id="formula_22">u pq = s pq exp â Î³ D (p : q )</formula><p>r pq <ref type="bibr" target="#b16">(17)</ref> In this paper, we propose four geodesic-kernel function-ba
work (CNN) which can extract contextual information of an image has gain more attention these years <ref type="bibr" target="#b2">[3]</ref> . It learns features of the input image by stacking the conv
f image segmentation has an essential influence on the subsequent image analysis and interpretation <ref type="bibr" target="#b1">[2]</ref> . Convolutional Neural Network (CNN) which can extract conte
traint in image domain, that neighboring pixels contribute to the segmentation of the central pixel <ref type="bibr" target="#b24">[25]</ref> . Markov Random Model-based FCM (MRF_FCM) used the MRF mod
ed an adaptive wavelet shrinkage to restrain noise and outliers and then segment the image with FCM <ref type="bibr" target="#b9">[10]</ref> . The reason caused poor performance on noise and outliers
traint in image domain, that neighboring pixels contribute to the segmentation of the central pixel <ref type="bibr" target="#b24">[25]</ref> . Markov Random Model-based FCM (MRF_FCM) used the MRF mod
composes an image into homogeneous regions, is an important task in remote sensing image processing <ref type="bibr" target="#b0">[1]</ref> . The accuracy of image segmentation has an essential influe
</formula><p>In this paper, the Einstein summation convention r ik Î¸ k i = k r ik Î¸ k i is employed <ref type="bibr" target="#b20">[21]</ref> . Finally, the detected image is mapped to Riemannian mani
Random Field (MRF) model is an efficient way to construct the connection between neighboring pixels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> . It defines the p
traint in image domain, that neighboring pixels contribute to the segmentation of the central pixel <ref type="bibr" target="#b24">[25]</ref> . Markov Random Model-based FCM (MRF_FCM) used the MRF mod
work (CNN) which can extract contextual information of an image has gain more attention these years <ref type="bibr" target="#b2">[3]</ref> . It learns features of the input image by stacking the conv
ction (RBF) is a non-linear function which can map low dimensional data to higher dimensional space <ref type="bibr" target="#b22">[23]</ref> . It is</p><formula xml:id="formula_12">defined as RBF = e
shift is a typical clustering algorithm which shifts the mean of the cluster to its center of mass <ref type="bibr" target="#b5">[6]</ref> . Recently, Yamasaki and Tanaka have studied the properties
composes an image into homogeneous regions, is an important task in remote sensing image processing <ref type="bibr" target="#b0">[1]</ref> . The accuracy of image segmentation has an essential influe
.">Riemannian manifold space</head><p>Image representation is very important for image segmentation <ref type="bibr" target="#b17">[18]</ref> . Riemannian manifold space is based on the characteristic
.">Riemannian manifold space</head><p>Image representation is very important for image segmentation <ref type="bibr" target="#b17">[18]</ref> . Riemannian manifold space is based on the characteristic
way to construct the connection between neighboring pixels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> . It defines the prior probability according to the labels
to overcome the sensitivity to cluster shapes and scales of Euclidean distance in the feature space <ref type="bibr" target="#b23">[24]</ref> . FCM_S algorithm introduces a constraint in image domain,
Peters et al., 2018)</ref>, GPT-2 <ref type="bibr" target="#b26">(Radford et al., 2019)</ref>, BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, <ref type="bibr">XLNet (Yang et al., 2019 as reestablished the new state-ofthe-art baselines across various tasks, such as question answering <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, coreference resolution <ref type="bibr" t in the pre-training stage, such as updating the model using only short sequences in the early stage <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>.</p><p>Common strategies for reducing memo ction. Following the paradigm of language model pre-training and down-stream task fine-tuning, BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> consists of multiple layers of bidirection lti-head self-attention layer and a position-wise feed-forward layer. Using the same notation as in <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, we denote the number of Transformer layer ource-intensive process. For instance, the training of BERT-family models is notoriously expensive. <ref type="bibr" target="#b4">Devlin et al. (2019)</ref> report that it takes four days for pre-trai e compare BlockBERT with the following baselines:</p><p>Google BERT The pre-trained base model from <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>.</p><p>RoBERTa-2seq and RoBERTa-1seq We com "figure">6</ref>.</p><p>For all the pre-trained models, we adopt the same fine-tuning QA setup from <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>.</p><p>The tokenized paragraph (p 1 , â¢ â¢ â¢ n units H leads to significant performance degradation <ref type="bibr">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b4">Devlin et al., 2019)</ref> and does not address the long sequence issu target="#b3">Dai et al., 2019)</ref> and its successful application on language model pre-training <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr" target="#b26">Radford et a
al., 2017)</ref>, TriviaQA <ref type="bibr" target="#b12">(Joshi et al., 2017)</ref> and NaturalQA <ref type="bibr" target="#b16">(Kwiatkowski et al., 2019)</ref> These QA datasets have different par
><p>Recent emergence of the pre-training and fine-tuning paradigm, exemplified by methods like ELMo <ref type="bibr" target="#b25">(Peters et al., 2018)</ref>, GPT-2 <ref type="bibr" target="#b26">(Ra
et="#b22">(Miculicich et al., 2018)</ref>, and also non-NLP tasks such as protein sequence modeling <ref type="bibr" target="#b28">(Rives et al., 2019)</ref>. </p><formula xml:id="formula_11">B</formu
ler et al., 2017)</ref>, SearchQA <ref type="bibr" target="#b5">(Dunn et al., 2017)</ref>, TriviaQA <ref type="bibr" target="#b12">(Joshi et al., 2017)</ref> and NaturalQA <ref type="bibr" target="#b1
><p>Recent emergence of the pre-training and fine-tuning paradigm, exemplified by methods like ELMo <ref type="bibr" target="#b25">(Peters et al., 2018)</ref>, GPT-2 <ref type="bibr" target="#b26">(Ra
><p>Recent emergence of the pre-training and fine-tuning paradigm, exemplified by methods like ELMo <ref type="bibr" target="#b25">(Peters et al., 2018)</ref>, GPT-2 <ref type="bibr" target="#b26">(Ra
><p>Recent emergence of the pre-training and fine-tuning paradigm, exemplified by methods like ELMo <ref type="bibr" target="#b25">(Peters et al., 2018)</ref>, GPT-2 <ref type="bibr" target="#b26">(Ra
al., 2017)</ref>, TriviaQA <ref type="bibr" target="#b12">(Joshi et al., 2017)</ref> and NaturalQA <ref type="bibr" target="#b16">(Kwiatkowski et al., 2019)</ref> These QA datasets have different par
es. The first line of research focuses on attention matrix sparsification, such as Star Transformer <ref type="bibr" target="#b8">(Guo et al., 2019)</ref>, Sparse Transformer <ref type="bibr" target="
ler et al., 2017)</ref>, SearchQA <ref type="bibr" target="#b5">(Dunn et al., 2017)</ref>, TriviaQA <ref type="bibr" target="#b12">(Joshi et al., 2017)</ref> and NaturalQA <ref type="bibr" target="#b1
tional LSTM + CRF), respectively, using public word embedding, character features and word features <ref type="bibr" target="#b8">[6,</ref><ref type="bibr">4]</ref>. We explore existing word embedding

Third, a hybrid NER implements both the rule-based machine and deep learning, for instance, Bio-Ner <ref type="bibr" target="#b17">[15]</ref> that was experimented with a rule-based and classification
s the usage of structured and unstructured techniques, such as CRF that was implemented for DrugNER <ref type="bibr" target="#b13">[11]</ref>. While in deep learning, various kind of neural network wo
RESEARCH</head><p>The existing algorithms for the NER task can be classified into three approaches <ref type="bibr" target="#b11">[9]</ref> : rule-based, machine-deep learning, and hybrid. The rule-b


-based algorithm applies a set of rules in order to extract patterns, i.e., rule base for Malay NER <ref type="bibr" target="#b12">[10]</ref>.</p><p>With the emergence of the machine and deep learning
s the usage of structured and unstructured techniques, such as CRF that was implemented for DrugNER <ref type="bibr" target="#b13">[11]</ref>. While in deep learning, various kind of neural network wo
-based algorithm applies a set of rules in order to extract patterns, i.e., rule base for Malay NER <ref type="bibr" target="#b12">[10]</ref>.</p><p>With the emergence of the machine and deep learning

cation hyperplane <ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref><ref type="bibr" t
et="#b13">[14]</ref>, filter approach <ref type="bibr" target="#b14">[15]</ref> and hybrid approach <ref type="bibr" target="#b15">[16]</ref> according to the correlation of algorithms. This depends o
ion of software behavior and obtains excellent results in UNM dataset and KD98 dataset. Wael et al. <ref type="bibr" target="#b50">[51]</ref> presented a heterogeneous detector which consisted of sequ
<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Aron Laszka et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> investigated and c tance between the sample and its center of the hypersphere in each class is calculated according to <ref type="bibr" target="#b10">(11)</ref>. </p><p>Therefore, membership functions of both positive a
ontinuously refines their knowledge base and is capable of fast incremental learning. Sheraz et al. <ref type="bibr" target="#b47">[48]</ref> implemented intrusion detection in a real environment base
stem call databases of normal behavior by a sliding window with a single length or multiple lengths <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Aron Laszka et al. <
ainance of the normal database for each individual program <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. The Murmurhash <ref type="bibr" target="#b21">[22]</ref><r
ises and outliers <ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref>. FSVM is based on
g sequences-based <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> methods are simple and efficient to implement by removing
ref> proposed a method based on the relation between samples and their cluster center. Zhang et al. <ref type="bibr" target="#b67">[68]</ref> proposed a new FSVM method after considering the imperfect ef type="bibr" target="#b65">[66]</ref>, FSVM2 <ref type="bibr" target="#b64">[65]</ref>, and FSVM3 <ref type="bibr" target="#b67">[68]</ref>, the proposed algorithm is also compared with other algori
dden Markov model <ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref> and a one-class support machine. In addition to satisfacto
side information) of users and items <ref type="bibr" target="#b14">(Jain &amp; Dhillon, 2013;</ref><ref type="bibr" target="#b38">Xu et al., 2013)</ref>. In IMC, a rating is decomposed by r ij = x i



YahooMusic <ref type="bibr" target="#b8">(Dror et al., 2011)</ref>, MovieLens-100K and MovieLens-1M <ref type="bibr" target="#b27">(Miller et al., 2003)</ref>. For ML-100K, we train and evaluate on th
ill only extend the convolution range to unrelated distant nodes and oversmooth the node embeddings <ref type="bibr" target="#b21">(Li et al., 2018)</ref>. This is reflected in that previous node-base

cannot generalize to unseen users/items. A recent inductive graphbased recommender system, PinSage <ref type="bibr" target="#b39">(Ying et al., 2018a)</ref>, uses node content as initial node feature t al., 2017)</ref>, F-EAE <ref type="bibr" target="#b13">(Hartford et al., 2018)</ref>, and PinSage <ref type="bibr" target="#b39">(Ying et al., 2018a)</ref>. Among them, GRALS is a graph regularized y use only one or two message passing layers <ref type="bibr" target="#b1">(Berg et al., 2017;</ref><ref type="bibr" target="#b39">Ying et al., 2018a)</ref>.</p></div> <div xmlns="http://www.tei-c.org

head>GNNs for matrix completion</head><p>The matrix completion problem has been studied using GNNs. <ref type="bibr" target="#b29">Monti et al. (2017)</ref> develop a multi-graph CNN (MGCNN) model to st sets. For Flixster, Douban and YahooMusic we use the preprocessed subsets and splits provided by <ref type="bibr" target="#b29">(Monti et al., 2017)</ref>. Dataset statistics are summarized in Tabl ets, we compare our IGMC with GRALS <ref type="bibr" target="#b31">(Rao et al., 2015)</ref>, sRGCNN <ref type="bibr" target="#b29">(Monti et al., 2017)</ref>, GC-MC <ref type="bibr" target="#b1">(Berg

The use of deep pre-trained transformers has led to remarkable progress in a number of applications <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>. For tasks that make pairwise comparisons asks have been achieved through the use of deep pre-trained language models followed by fine-tuning <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>. In this work we explore improvements to t rohibitively slow.</p><p>The current state-of-the-art focuses on using BERT models for pre-training <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, which employ large text corpora on genera based on large pre-trained transformer models with the same architecture and dimension as BERT-base <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, which has 12 layers, 12 attention heads, >We tried two optimizers: Adam (Kingma &amp; Ba, 2015) with weight decay of 0.01 (as recommended by <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>) and Adamax (Kingma &amp; Ba, 2015) withou ">3</ref> we show validation performance when fine-tuning various layers of the weights provided by <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, using Adam with decay optimizer. Fine-tun and the label is the next utterance.</p><p>When pre-training on Wikipedia and Toronto Books, as in <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>, the input is one sentence and the label th training strategy involves training with a masked language model (MLM) task identical to the one in <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>. In the pre-training on Wikipedia and Toron stigate fine-tuning the Bi-and Cross-encoder architectures initialized with the weights provided by <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>, studying the choice of other hyperparamete
put and label candidate, tend to attain much higher accuracies than their counterparts, Bi-encoders <ref type="bibr" target="#b16">(Mazar? et al., 2018;</ref><ref type="bibr" target="#b6">Dinan et al. y, we pre-train two more transformers from scratch using the exact same architecture as BERT-base.  <ref type="bibr" target="#b16">(Mazar? et al., 2018)</ref>, which is a dataset more adapted to dialo , where cand 1 is the correct label and the others are chosen from the training set. Similar to</p><ref type="bibr" target="#b16">Mazar? et al. (2018)</ref></p>, during training we consider the other

" target="#b20">(Wolf et al., 2019;</ref><ref type="bibr" target="#b19">Vig &amp; Ramea, 2019;</ref><ref type="bibr" target="#b18">Urbanek et al., 2019)</ref>. For the latter, concatenating the two se as every word in the candidate label can attend to every word in the input context, and vice-versa. <ref type="bibr" target="#b18">Urbanek et al. (2019)</ref> employed pre-trained BERT models, and fin
l., 2009;</ref><ref type="bibr" target="#b21">Wu et al., 2018)</ref> and classical siamese networks <ref type="bibr" target="#b1">(Bromley et al., 1994)</ref>. For the next utterance prediction tasks

</ref>, which employ large text corpora on general subjects: Wikipedia and the Toronto Books Corpus <ref type="bibr" target="#b28">(Zhu et al., 2015)</ref>. Two classes of fine-tuned architecture are
</ref>, which employ large text corpora on general subjects: Wikipedia and the Toronto Books Corpus <ref type="bibr" target="#b28">(Zhu et al., 2015)</ref>. Two classes of fine-tuned architecture are
" target="#b20">(Wolf et al., 2019;</ref><ref type="bibr" target="#b19">Vig &amp; Ramea, 2019;</ref><ref type="bibr" target="#b18">Urbanek et al., 2019)</ref>. For the latter, concatenating the two se as every word in the candidate label can attend to every word in the input context, and vice-versa. <ref type="bibr" target="#b18">Urbanek et al. (2019)</ref> employed pre-trained BERT models, and fin

their similarity. We refer to these models as Bi-encoders. Such methods include vector space models <ref type="bibr" target="#b17">(Salton et al., 1975)</ref>, LSI <ref type="bibr" target="#b4">(Deerw
capabilities of trace-based dataflow analysis to explore timeliness and load classification. Zhang <ref type="bibr" target="#b52">[53]</ref> performs dynamic prefetch optimization based on profiling,
lation prefetchers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>, and execution-based prefetchers <ref type="bibr" target="#
lation prefetchers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>, and execution-based prefetchers <ref type="bibr" target="#
et="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> pointing to an element within the same data structure at so
ch as Singly-Nested Loop Nests (SNLNs) <ref type="bibr" target="#b49">[50]</ref> or regular strides <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" ta
get="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref> struggle to address arget="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b45">46]</ref> perform static code analysis to generate prefetch targets.
ollars for large WSC providers. Hardware Techniques: Several hardware mechanisms have been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ
prior research on prefetchers including stream prefetchers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar
get="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" tar get="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" tar un-ahead execution <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, precomputation threads <ref type="bibr" target="#b11">[12,
emory hierarchies with small, low-latency caches. However, given that data set sizes are increasing <ref type="bibr" target="#b27">[28]</ref> and transistor scaling is slowing down <ref type="bibr" ta ave up to 60% of performance on the table in the data center <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>One longstanding challenge for prefetcher design is
get="#b14">15,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar get="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar
to-translations it has already been shown that a suitable structure for Ï is a tensor field network <ref type="bibr" target="#b24">[25]</ref>, explained below. Note that Romero et al. <ref type="bibr" q. <ref type="bibr" target="#b4">(5)</ref>.</p><p>Tensor Field Networks Tensor field networks (TFN) <ref type="bibr" target="#b24">[25]</ref> are neural networks, which map point clouds to point cloud annels, but we omit it here. Weiler et al. <ref type="bibr" target="#b32">[33]</ref>, Thomas et al. <ref type="bibr" target="#b24">[25]</ref> and Kondor <ref type="bibr" target="#b12">[13]</ref> showe duces the kernel to a scalar w multiplied by the identity, W = w I, referred to as self-interaction <ref type="bibr" target="#b24">[25]</ref>. As such we can rewrite the TFN layer as</p><formula xml:i c c = w c c per representation degree, shared across all points.</p><p>As proposed in Thomas et al. <ref type="bibr" target="#b24">[25]</ref>, this is followed by a norm-based non-linearity.</p><p>Att tron-proton simulation.</p><p>Linear DeepSet <ref type="bibr" target="#b39">[40]</ref> Tensor Field <ref type="bibr" target="#b24">[25]</ref> Set Transformer <ref type="bibr" target="#b13">[14]</ref> type="bibr" target="#b13">[14]</ref>, a non-equivariant attention model, and Tensor Field Networks <ref type="bibr" target="#b24">[25]</ref>, which is similar to SE(3)-Transformer but does not levera s to train significantly larger versions of both the SE(3)-Transformer and the Tensor Field network <ref type="bibr" target="#b24">[25]</ref> and to apply these models to real-world datasets.</p><p>Ou be found in many mathematical physics libraries.</p><p>Tensor Field Layers In Tensor Field Networks <ref type="bibr" target="#b24">[25]</ref> and 3D Steerable CNNs <ref type="bibr" target="#b32">[33]< that all the Tensor Field networks we trained were significantly bigger than in the original paper <ref type="bibr" target="#b24">[25]</ref>, mostly enabled by the faster computation of the spherical k to obtain stable training. We used a norm based non-linearity for the Tensor Field network (as in <ref type="bibr" target="#b24">[25]</ref>) and no extra non-linearity (beyond the softmax in the sel he Tensor Field network and the linear baseline are SE(3) equivariant. For the Tensor Field Network <ref type="bibr" target="#b24">[25]</ref> baseline, we used the same hyper parameters as for the SE( linear self-interaction and an additional norm-based nonlinearity in each layer as in Thomas et al. <ref type="bibr" target="#b24">[25]</ref>. For the DeepSet <ref type="bibr" target="#b39">[40]</ref>
ith a cross entropy loss. We trained for 60000 steps with batch size 10. We used the Adam optimizer <ref type="bibr" target="#b10">[11]</ref> with a start learning of 1e-2 and a reduction of the learn raining Details We trained each model for 100,000 steps with batch size 128 using an Adam optimizer <ref type="bibr" target="#b10">[11]</ref>. We used a fixed learning rate throughout training and con d dividing by the standard deviation of the training set.</p><p>We trained for 50 epochs using Adam <ref type="bibr" target="#b10">[11]</ref> at initial learning rate 1e-3 and a single-cycle cosine ra

020 (https://hkust-vgd.github.io/benchmark/): 3DmFV<ref type="bibr" target="#b2">[3]</ref>, PointNet<ref type="bibr" target="#b16">[17]</ref>, SpiderCNN<ref type="bibr" target="#b37">[38]</ref>, Point
><formula xml:id="formula_5">)</formula><p>where Q is an orthogonal, N Ã N , change-of-basis matrix <ref type="bibr" target="#b3">[4]</ref>; each D for = 0, 1, 2, ... is a (2 + 1) Ã (2 + 1) matrix kno <formula xml:id="formula_31">)</formula><p>where Q is an orthogonal, N Ã N , change-of-basis matrix <ref type="bibr" target="#b3">[4]</ref>; and each D J for J = 0, 1, 2, ... is a (2J + 1) Ã (2J + 1) and values individually and then concatenate the results into a single vector of the original shape <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b15">16)</ref>. The keys and querie
ref type="bibr" target="#b2">[3]</ref>, PointNet<ref type="bibr" target="#b16">[17]</ref>, SpiderCNN<ref type="bibr" target="#b37">[38]</ref>, PointNet++<ref type="bibr" target="#b17">[18]</ref>, DGCN
weights. In general, the number of query vectors does not have to equal the number of input points <ref type="bibr" target="#b13">[14]</ref>. In the case of self-attention the query, key, and value v t="#b36">37,</ref><ref type="bibr" target="#b13">14]</ref>. One such example is the Set Transformer <ref type="bibr" target="#b13">[14]</ref>. When applied to object classification on ModelNet40 <ref br" target="#b39">[40]</ref> Tensor Field <ref type="bibr" target="#b24">[25]</ref> Set Transformer <ref type="bibr" target="#b13">[14]</ref>  classification task. Here, the network is confronted with -art results as well as a set of our own baselines. Specifically, we compare to the Set-Transformer <ref type="bibr" target="#b13">[14]</ref>, a non-equivariant attention model, and Tensor Field Netwo pe="bibr" target="#b39">[40]</ref>.</p><p>Set Transformer Baseline We used the same architecture as <ref type="bibr" target="#b13">[14]</ref> in their object classification experiment on ModelNet40 <r idden layer was followed by a LeakyReLU. The learning rate was set to 1e-3. For the Set Transformer <ref type="bibr" target="#b13">[14]</ref>, we used 4 self-attention blocks with 64 hidden units and o point cloud data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14]</ref>. One such example is the Set Transformer <ref type="bibr" t low the common practice from the self-attention literature <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14]</ref>, and chosen a softmax nonlinearity to normalise the attenti
bel>(2)</label></formula><p>where {h Q , h K , h V } are, in the most general case, neural networks <ref type="bibr" target="#b26">[27]</ref>. For us, query q i is associated with a point i in the inp 16]</ref>, graph-based problems <ref type="bibr" target="#b28">[29]</ref>, and relational reasoning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7]</ref>, a recent stream of
rmer <ref type="bibr" target="#b13">[14]</ref>. When applied to object classification on ModelNet40 <ref type="bibr" target="#b35">[36]</ref>, the input to the Set Transformer are the cartesian coordi <ref type="bibr" target="#b39">[40]</ref> for their object classification experiment on ModelNet40 <ref type="bibr" target="#b35">[36]</ref>. However, most likely due to the relatively small number o s <ref type="bibr" target="#b13">[14]</ref> in their object classification experiment on ModelNet40 <ref type="bibr" target="#b35">[36]</ref> with an ISAB (induced set attention block)-based encoder f
]</ref>, a recent stream of work has applied forms of self-attention algorithms to point cloud data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta
rmer <ref type="bibr" target="#b13">[14]</ref>. When applied to object classification on ModelNet40 <ref type="bibr" target="#b35">[36]</ref>, the input to the Set Transformer are the cartesian coordi <ref type="bibr" target="#b39">[40]</ref> for their object classification experiment on ModelNet40 <ref type="bibr" target="#b35">[36]</ref>. However, most likely due to the relatively small number o s <ref type="bibr" target="#b13">[14]</ref> in their object classification experiment on ModelNet40 <ref type="bibr" target="#b35">[36]</ref> with an ISAB (induced set attention block)-based encoder f
ing (AT) procedure <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40]</ref> shows promising res al training (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" tar get="#b37">38,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar unreliable for generating adversarial samples during single-step adversarial training. Madry et al. <ref type="bibr" target="#b21">[22]</ref> demonstrated that models trained using adversarial samples ="bibr" target="#b8">9]</ref> accepted to ICLR 2018. In this direction, adversarial training method <ref type="bibr" target="#b21">[22]</ref>, shows promising results for learning robust deep learning ls trained using EAT are still susceptible to multi-step attacks in white-box setting. Madry et al. <ref type="bibr" target="#b21">[22]</ref> demonstrated that adversarially trained model can be made show that over-fitting effect is the reason for failure to satisfy the criteria.</p><p>Madry et al. <ref type="bibr" target="#b21">[22]</ref> demonstrated that it is possible to learn robust models us rameters (Î¸) should be updated so as to decrease the loss on such adversarial samples. Madry et al. <ref type="bibr" target="#b21">[22]</ref> solves the maximization step by generating adversarial sam raining method <ref type="bibr" target="#b12">[13]</ref> and multi-step adversarial training method <ref type="bibr" target="#b21">[22]</ref>. Column-1 of Fig. <ref type="figure" target="#fig_0">1</re for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by <ref type="bibr" target="#b21">[22]</ref>. At each iteration all the clean samples in the mini-batch d for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by<ref type="bibr" target="#b21">[22]</ref>. At each iteration all the clean samples in the mini-batch s added to the image. In our experiments, we set Î± = /steps.</p><p>Projected Gradient Descent (PGD) <ref type="bibr" target="#b21">[22]</ref>: Initially, a small random noise sampled from Uniform dist
f>), in this direction Adversarial Training (AT) procedure <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar has been shown that models trained using single-step adversarial training methods are pseudo robust <ref type="bibr" target="#b34">[35]</ref>:</p><p>â¢ Although these models appears to be robust to sin r), they are susceptible to single-step attacks (non-iterative methods) in black-box attack setting <ref type="bibr" target="#b34">[35]</ref>.</p><p>â¢ Further, these models are susceptible to multi-st b17">[18]</ref> and black-box setting <ref type="bibr" target="#b9">[10]</ref>.</p><p>Tramer et al. <ref type="bibr" target="#b34">[35]</ref> demonstrated that models trained using single-step adversa gle-step adversarial training methods are susceptible to multi-step attacks. Further, Tramer et al. <ref type="bibr" target="#b34">[35]</ref> demonstrated that these models exhibit gradient masking ef
rget="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar
br" target="#b3">4]</ref>) and input pre-processing (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>) have been proposed. Athalye et al. <ref type="bibr" target get="#b13">14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar
methods) in both white-box setting <ref type="bibr" target="#b17">[18]</ref> and black-box setting <ref type="bibr" target="#b9">[10]</ref>.</p><p>Tramer et al. <ref type="bibr" target="#b34">[35]</r ) <ref type="bibr" target="#b16">[17]</ref>, Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type="bibr" target="#b9">[10]</ref> and Projected Gradient Descent  </p></div> <div xmlns="http enerating adversarial samples, since samples generated using these methods show good transfer rates <ref type="bibr" target="#b9">[10]</ref>. Table <ref type="table" target="#tab_3">4</ref> shows the owed by re-projection.</p><p>x</p><p>x</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type="bibr" target="#b9">[10]</ref>: Introduces a momentum term into the IFGSM formulation. Her arget="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref> have been proposed.
rget="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref> have been proposed. Further, in order to defend against adv
target="#b1">2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>. Further, Szegedy e le to the attacker. Various schemes have been proposed to defend against adversarial attacks (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" ta ef><ref type="bibr" target="#b22">23]</ref>), in this direction Adversarial Training (AT) procedure <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" ta ers in typical setting), trained using normal training and single-step adversarial training methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>. Results for these ng the findings of Szegedy et al. <ref type="bibr" target="#b33">[34]</ref>, various attacks (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" ta in order to defend against adversarial attacks, various schemes such as adversarial training (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta nerated by the model being trained. Adversarial sample generation methods range from simple methods <ref type="bibr" target="#b12">[13]</ref> to complex optimization methods <ref type="bibr" target="# to reduce computational complexity, non-iterative methods such as Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b12">[13]</ref> are typically used for generating adversarial samples. Fur ion. Such manipulations could be achieved by perturbing the samples along the adversarial direction <ref type="bibr" target="#b12">[13]</ref>. A perturbation is said to be an adversarial perturbation ain the plot of R versus iteration for models trained using single-step adversarial training method <ref type="bibr" target="#b12">[13]</ref> and multi-step adversarial training method <ref type="bibr tion over three runs.</p><p>Attacks: For l â based attacks, we use Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b12">[13]</ref>, Iterative Fast Gradient Sign Method (IFGSM) <ref type="bi n of adversarial attacks. Fast Gradient Sign Method (FGSM): Non-iterative attack method proposed by <ref type="bibr" target="#b12">[13]</ref>. This method generates l â norm bounded adversarial pertur
get="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>) and input pre-processi
get="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>) and input pre-processi
rget="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar
get="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>) and input pre-processi
. Limago explores the changes needed to upgrade an existing open-source TCP/IP stack from 10 Gbit/s <ref type="bibr" target="#b10">[11]</ref> to 100 Gbit/s, but maintaining the same high-productivity . The starting point for Limago is a 10 Gbit/s TOE written by Sidler et al. in C++ using Vivado-HLS <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p></div> <div ng data path (Rx Engine), the outgoing data path (Tx Engine), and the state-keeping data structures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The dash boxes
cluding that the hardware solution has less latency and a more deterministic behaviour. The work in <ref type="bibr" target="#b25">[26]</ref> presents a complete TOE implementation supporting jumbo fr
t/s TOE written by Sidler et al. in C++ using Vivado-HLS <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> path (Tx Engine), and the state-keeping data structures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The dash boxes are optional modules that can be enabled a
that characterize modern distributed computing have turned the network into a potential bottleneck <ref type="bibr" target="#b0">[1]</ref>. Besides, in cloud environments, the network also limits the
and architectures.</p><p>A concrete example of these developments is provided by Microsoft Catapult <ref type="bibr" target="#b2">[3]</ref>, a deployment of FPGAs in the cloud that has evolved through
will improve the throughput when packet loss occurs as well as support application level processing <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p></div><figu
alue stores <ref type="bibr" target="#b5">[6]</ref>, or for distributed machine learning algorithms <ref type="bibr" target="#b6">[7]</ref>. Catapult is, by far, not the only possible design. In IBM's
and architectures.</p><p>A concrete example of these developments is provided by Microsoft Catapult <ref type="bibr" target="#b2">[3]</ref>, a deployment of FPGAs in the cloud that has evolved through
t/s TOE written by Sidler et al. in C++ using Vivado-HLS <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head> path (Tx Engine), and the state-keeping data structures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The dash boxes are optional modules that can be enabled a
head><p>The benefits of TCP/IP offloading are well-known <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>: reduced CPU utilization and bypassing of the Operating Sy
e to achieve with the Vivado-HLS version being used (2018.2). The circuit is described in detail in <ref type="bibr" target="#b12">[13]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>
rmance, most of them ignore the interactions' timestamp values. While recent works such as TiSASRec <ref type="bibr" target="#b13">[14]</ref> successfully incorporated time information, their usage of ransformer <ref type="bibr" target="#b21">[22]</ref> and Cloze-task based training method. TiSASRec <ref type="bibr" target="#b13">[14]</ref> enhanced SASRec by merging timestamp information into self st, they don't utilize timestamp values which hold important contextual information. While TiSASRec <ref type="bibr" target="#b13">[14]</ref> successfully addressed this issue, they also used a simple pe="bibr" target="#b31">[32]</ref>,</p><p>SASRec <ref type="bibr" target="#b9">[10]</ref>, TiSASRec <ref type="bibr" target="#b13">[14]</ref>, and BERT4Rec <ref type="bibr" target="#b18">[19]</ref>. T essing procedure from <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>. We convert each da g the custom practice <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>, we discard users a We use the rest for training. We follow the common practice <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref> of letting the mode
target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" t et="#b6">[7]</ref> assume that users' behaviors are affected only by their last few behaviors. FPMC <ref type="bibr" target="#b17">[18]</ref> merges Markovchains with matrix factorization method for n
get="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> brought the success of RNN into item sequence understanding
get="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" tar
e strong order constraint of RNN models, CNN-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> were proposed. Some
target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" t et="#b6">[7]</ref> assume that users' behaviors are affected only by their last few behaviors. FPMC <ref type="bibr" target="#b17">[18]</ref> merges Markovchains with matrix factorization method for n
="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b35">36]</ref>. Despite their excellent performance, most of them ignore t
roposed to better understand the sequential history of users <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target nce the first suggestion by GRU4Rec <ref type="bibr" target="#b8">[9]</ref>, many RNN-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" targ
target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" t et="#b6">[7]</ref> assume that users' behaviors are affected only by their last few behaviors. FPMC <ref type="bibr" target="#b17">[18]</ref> merges Markovchains with matrix factorization method for n
target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> assume that users' behaviors are affected only by their last
e strong order constraint of RNN models, CNN-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> were proposed. Some
usted so that it can fit different models and datasets.</p><p>Inspired by the success of Focal Loss <ref type="bibr" target="#b29">[30]</ref>, we estimate ð (ð¢, ð) with a function of ð ( Å·ð¢ð ) that ta
><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref> and position bias <ref type="bibr" target="#b18">[19]</ref>. Moreover, existing studies <ref type="bibr" target="#b32"
-positive interactions when we train a competitive recommender, Neural Matrix Factorization (NeuMF) <ref type="bibr" target="#b15">[16]</ref>, on two real-world datasets. In particular, we construct a s or Reweighted Loss over three representative recommenders: Generalized Matrix Factorization (GMF) <ref type="bibr" target="#b15">[16]</ref>, NeuMF <ref type="bibr" target="#b15">[16]</ref>, and Coll commenders: Generalized Matrix Factorization (GMF) <ref type="bibr" target="#b15">[16]</ref>, NeuMF <ref type="bibr" target="#b15">[16]</ref>, and Collaborative Denoising Auto-Encoder (CDAE) <ref type raining with standard CE. We selected two representative user-based neural CF models, GMF and NeuMF <ref type="bibr" target="#b15">[16]</ref>, and one item-based model, CDAE <ref type="bibr" target="# ve model of robust recommender which can defend random noises within implicit feedback.</p><p>â¢ GMF <ref type="bibr" target="#b15">[16]</ref>: This is a generalized version of matrix factorization by roduct with the element-wise product and a linear neural layer as the interaction function. â¢ NeuMF <ref type="bibr" target="#b15">[16]</ref>: NeuMF is a representative CF neural model, which models t score over all the items except the positive ones used during training. Following existing studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref>, we reported the r 3">[24]</ref> and SVD++ <ref type="bibr" target="#b22">[23]</ref> due to their inferior performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Parameter S
-grained user-specific or item-specific tuning of these parameters, which can be done automatically <ref type="bibr" target="#b4">[5]</ref>. â¢ Across the recommenders, NeuMF performs worse than GMF an nformation of postinteraction behaviors, e.g., rating score (<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>) &lt; 3, indicating that the interacted item dissatisfies the
pe="bibr" target="#b12">13]</ref>. Overall, the results are consistent with the memorization effect <ref type="bibr" target="#b0">[1]</ref>: deep models will first learn the easy and clean patterns in nteraction is identified by auxiliary information of postinteraction behaviors, e.g., rating score (<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>) &lt; 3, indicating th
previous approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar
target="#b35">[36]</ref>. To build more robust recommender systems, some auto-encoder based models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta k also exists in each interaction. We followed former work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref> to remove users and
actions for recommender training.</p><p>Indeed, some efforts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref> have been dedicated to eliminating the effects of false-positive interactions by: 1) negative experience identification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref> (illustrated in Fi ime for each click, where the clicks with dwell time &lt; 10s are thought of as false-positive ones <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43]</ref>. â¢ Amazon-book: It tention to identify negative experiences in implicit signals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>. Prior work usually collects the various users' feedback (e target="#b20">21]</ref>. Prior work usually collects the various users' feedback (e.g., dwell time <ref type="bibr" target="#b20">[21]</ref>, gaze patterns <ref type="bibr" target="#b45">[46]</ref>,
dwell time <ref type="bibr" target="#b42">[43]</ref>, skip <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45]</ref>, and adding to favorites) into training directly. For insta
t impression of users and other factors such as caption bias <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref> and position bias <
" target="#b28">[29]</ref>, news portals <ref type="bibr" target="#b31">[32]</ref> and social media <ref type="bibr" target="#b2">[3]</ref>. As the clue to user choices, implicit feedback (e.g., click
ly discussed in one-class collaborative filtering (OCCF) <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Given user u, I + u = {i â I|r ui = 1} is the set of item imilarity matrices between users and items to predict drug-target interaction. Moreover, Yao et al. <ref type="bibr" target="#b18">[19]</ref> proposed dual regularization by combining the weighted-and

balance between effectiveness and efficiency. In this paper, we employ knowledge distillation (KD) <ref type="bibr" target="#b9">[10]</ref> which is a network compression technique by transferring th ulate the top-N recommendation problem. Then, we explain the concept of knowledge distillation (KD) <ref type="bibr" target="#b9">[10]</ref> and present rank distillation (RD) <ref type="bibr" target= r proposed training strategies.</p><p>Temperature in the KD loss. One key factor of the original KD <ref type="bibr" target="#b9">[10]</ref> is to find a proper balance between the soft targets and ha ]</ref> is to find a proper balance between the soft targets and hard labels. To tackle this issue, <ref type="bibr" target="#b9">[10]</ref> introduces the notion of a temperature T . Although the sof
consider the degrees of importance of items based on their rankings.</p><p>Recently, Tang and Wang <ref type="bibr" target="#b10">[11]</ref> proposed a KD model to address the ranking problem, called ledge distillation (KD) <ref type="bibr" target="#b9">[10]</ref> and present rank distillation (RD) <ref type="bibr" target="#b10">[11]</ref> that applies knowledge distillation to recommender models. ommendation problem because Fig. <ref type="figure">1</ref>. Illustration of rank distillation (RD) <ref type="bibr" target="#b10">[11]</ref>. The teacher model transfers manipulated top-k items as th t may have worse performance than the original student model. Rank distillation (RD). Tang and Wang <ref type="bibr" target="#b10">[11]</ref> proposed ranking distillation (RD) that applies KD for ran e rated by less than 5 users. Table I reports the detailed statistics of these datasets.</p><p>â¢ RD <ref type="bibr" target="#b10">[11]</ref>: To define the KD loss in equation ( <ref type="formula" t een Î» that appears in RD and CD. Specifically, we used the following parameter settings.</p><p>â¢ RD <ref type="bibr" target="#b10">[11]</ref> and RD-Rank: We set Ï to be 0.5. For CDAE, the number of i er, we used the public PyTorch implementation <ref type="foot" target="#foot_5">6</ref> provided in <ref type="bibr" target="#b10">[11]</ref>. All experiments were conducted on a desktop with 128 GB m KD. Also, the gain indicates how additional accuracy achieved by the proposed model over that of RD <ref type="bibr" target="#b10">[11]</ref>.</p><p>Based on this evaluation, we found several interest rform RD over all datasets. Note that the improvement gap for RD is somewhat different from that in <ref type="bibr" target="#b10">[11]</ref>. It is because we used leave-one-out evaluation while <ref in <ref type="bibr" target="#b10">[11]</ref>. It is because we used leave-one-out evaluation while <ref type="bibr" target="#b10">[11]</ref> used cross-validation evaluation. Our models are consisten el size and efficiency. The model size is proportional to the accuracy of our model, as observed in <ref type="bibr" target="#b10">[11]</ref> as well. The same tendency consistently holds in different oot" n="4" xml:id="foot_3">http://dawenl.github.io/data/gowalla pro.zip Competitive models. Since RD<ref type="bibr" target="#b10">[11]</ref> is the state-of-the-art KD model for top-N recommendation,
tivation boundaries, and thus suggested a hinge loss. <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b22">[23]</ref> employed adversarial learning into the KD framework. Recen
reduce the memory size and enhance efficiency. Second, the pruning and sharing method presented in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> removes or bind
et="#b16">[17]</ref> leveraged side information to construct user-item similarity, and Zheng et al. <ref type="bibr" target="#b17">[18]</ref> employed multiple similarity matrices between users and it
to recommender models involves several challenges: (1) Implicit user feedback is extremely sparse. <ref type="bibr" target="#b1">(2)</ref> As users only provide positive feedback in implicit datasets
imputationbased methods. First, the weight-based method <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref> regards all missing feedback as negative ones. For instanc as negative ones. For instance, Hu et al. <ref type="bibr" target="#b11">[12]</ref> and Pan et al. <ref type="bibr" target="#b15">[16]</ref> controlled weights as the confidence of negative values wi
imputationbased methods. First, the weight-based method <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref> regards all missing feedback as negative ones. For instanc as negative ones. For instance, Hu et al. <ref type="bibr" target="#b11">[12]</ref> and Pan et al. <ref type="bibr" target="#b15">[16]</ref> controlled weights as the confidence of negative values wi
ased loss is inappropriate for transferring activation boundaries, and thus suggested a hinge loss. <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b22">[23]</ref> employed adv
alternative direction, several algorithms focused on analyzing the choice of loss functions for KD. <ref type="bibr" target="#b26">[27]</ref> observed that the distance-based loss is inappropriate for
istent with existing KD studies.</p><p>Evaluation protocol. We adopted the leave-one-out evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Specifically, we h and the rest of user-item interactions are used for training data. Unlike sampling-based evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> that randomly chose etrics, hit rate (HR) and normalized discounted cumulative gain (NDCG), as done in existing studies <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. The size N of the
eedback as optimization variables and imputed missing feedback via optimization. Besides, Li et al. <ref type="bibr" target="#b16">[17]</ref> leveraged side information to construct user-item similari
ased loss is inappropriate for transferring activation boundaries, and thus suggested a hinge loss. <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b22">[23]</ref> employed adv
aluation protocol. We adopted the leave-one-out evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Specifically, we held-out the last timestamp useritem inter e used for training data. Unlike sampling-based evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> that randomly chose 100 items from the set of unrated items, ounted cumulative gain (NDCG), as done in existing studies <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. The size N of the ranked list was chosen to be 50 for HR@N
el responses from the teacher model as additional information to educate the student model. Net2Net <ref type="bibr" target="#b33">[34]</ref> employed model parameters of the teacher model directly to

bibr" target="#b22">[23]</ref> employed adversarial learning into the KD framework. Recently, KDGAN <ref type="bibr" target="#b20">[21]</ref> bypassed the convergence step of adversarial learning by e

eferences. Such ambiguity has been explicitly discussed in one-class collaborative filtering (OCCF) <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Given user u, ized into weight-based, sampling-based, and imputationbased methods. First, the weight-based method <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref> regards all mis br" target="#b15">[16]</ref> regards all missing feedback as negative ones. For instance, Hu et al. <ref type="bibr" target="#b11">[12]</ref> and Pan et al. <ref type="bibr" target="#b15">[16]</ref> c
eedback as optimization variables and imputed missing feedback via optimization. Besides, Li et al. <ref type="bibr" target="#b16">[17]</ref> leveraged side information to construct user-item similari
istent with existing KD studies.</p><p>Evaluation protocol. We adopted the leave-one-out evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Specifically, we h and the rest of user-item interactions are used for training data. Unlike sampling-based evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> that randomly chose etrics, hit rate (HR) and normalized discounted cumulative gain (NDCG), as done in existing studies <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. The size N of the
reduce the memory size and enhance efficiency. Second, the pruning and sharing method presented in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> removes or bind
eferences. Such ambiguity has been explicitly discussed in one-class collaborative filtering (OCCF) <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Given user u, ized into weight-based, sampling-based, and imputationbased methods. First, the weight-based method <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref> regards all mis br" target="#b15">[16]</ref> regards all missing feedback as negative ones. For instance, Hu et al. <ref type="bibr" target="#b11">[12]</ref> and Pan et al. <ref type="bibr" target="#b15">[16]</ref> c
n and suggested using the output of intermediate layers as additional matching criteria. Similarly, <ref type="bibr" target="#b25">[26]</ref> utilized the gram matrix of the channel responses from the
aluation protocol. We adopted the leave-one-out evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Specifically, we held-out the last timestamp useritem inter e used for training data. Unlike sampling-based evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> that randomly chose 100 items from the set of unrated items, ounted cumulative gain (NDCG), as done in existing studies <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. The size N of the ranked list was chosen to be 50 for HR@N
et="#b16">[17]</ref> leveraged side information to construct user-item similarity, and Zheng et al. <ref type="bibr" target="#b17">[18]</ref> employed multiple similarity matrices between users and it
eters, and (3) knowledge distillation (KD).</p><p>First, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> proposed the binary encoding of model parameters. Under th
eters, and (3) knowledge distillation (KD).</p><p>First, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> proposed the binary encoding of model parameters. Under th
r model to a simple student model. Many existing studies <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b27">[28]</ref> have utilized KD to compress deep neural networks as well >[21]</ref> bypassed the convergence step of adversarial learning by employing a triple-player game <ref type="bibr" target="#b27">[28]</ref>. In this study, we develop an improved loss function for K

istent with existing KD studies.</p><p>Evaluation protocol. We adopted the leave-one-out evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Specifically, we h and the rest of user-item interactions are used for training data. Unlike sampling-based evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> that randomly chose etrics, hit rate (HR) and normalized discounted cumulative gain (NDCG), as done in existing studies <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. The size N of the
eedback as optimization variables and imputed missing feedback via optimization. Besides, Li et al. <ref type="bibr" target="#b16">[17]</ref> leveraged side information to construct user-item similari
chose two state-ofthe-art recommender models-CDAE <ref type="bibr" target="#b7">[8]</ref> and Caser <ref type="bibr" target="#b6">[7]</ref>. (This paper focuses on top-N recommender models with point- e Adagrad optimizer with learning rate = 0.2, l2-regularizer = 0.001, and batch size = 256. â¢ Caser <ref type="bibr" target="#b6">[7]</ref>: The latent dimensions for the teacher and the student model
aluation protocol. We adopted the leave-one-out evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Specifically, we held-out the last timestamp useritem inter e used for training data. Unlike sampling-based evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> that randomly chose 100 items from the set of unrated items, ounted cumulative gain (NDCG), as done in existing studies <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. The size N of the ranked list was chosen to be 50 for HR@N

to recommender models involves several challenges: (1) Implicit user feedback is extremely sparse. <ref type="bibr" target="#b1">(2)</ref> As users only provide positive feedback in implicit datasets
istent with existing KD studies.</p><p>Evaluation protocol. We adopted the leave-one-out evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Specifically, we h and the rest of user-item interactions are used for training data. Unlike sampling-based evaluation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> that randomly chose etrics, hit rate (HR) and normalized discounted cumulative gain (NDCG), as done in existing studies <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. The size N of the
imputationbased methods. First, the weight-based method <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref> regards all missing feedback as negative ones. For instanc as negative ones. For instance, Hu et al. <ref type="bibr" target="#b11">[12]</ref> and Pan et al. <ref type="bibr" target="#b15">[16]</ref> controlled weights as the confidence of negative values wi
chose two state-ofthe-art recommender models-CDAE <ref type="bibr" target="#b7">[8]</ref> and Caser <ref type="bibr" target="#b6">[7]</ref>. (This paper focuses on top-N recommender models with point- e Adagrad optimizer with learning rate = 0.2, l2-regularizer = 0.001, and batch size = 256. â¢ Caser <ref type="bibr" target="#b6">[7]</ref>: The latent dimensions for the teacher and the student model
alternative direction, several algorithms focused on analyzing the choice of loss functions for KD. <ref type="bibr" target="#b26">[27]</ref> observed that the distance-based loss is inappropriate for
retion, (2) pruning and sharing model parameters, and (3) knowledge distillation (KD).</p><p>First, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> proposed the bi
odel training. To validate the proposed model, we chose two state-ofthe-art recommender models-CDAE <ref type="bibr" target="#b7">[8]</ref> and Caser <ref type="bibr" target="#b6">[7]</ref>. (This pap tion N (0, 1). Specifically, each baseline CF model had the following hyperparameters.</p><p>â¢ CDAE <ref type="bibr" target="#b7">[8]</ref>: The latent dimensions for the teacher and the student model
mentation <ref type="bibr" target="#b32">[33]</ref>), and deep contextual language models from BERT <ref type="bibr" target="#b14">[15]</ref> and XLNet <ref type="bibr" target="#b40">[41]</ref> in a v GloVe <ref type="bibr" target="#b30">[31]</ref>, to contextual embeddings as the ones used in BERT <ref type="bibr" target="#b14">[15]</ref> and XL-Net <ref type="bibr" target="#b40">[41]</ref>. The tations based on the Transformer architecture <ref type="bibr" target="#b36">[37]</ref>, named BERT <ref type="bibr" target="#b14">[15]</ref> and XLNet <ref type="bibr" target="#b40">[41]</ref>. The t r Siamese</figDesc><table /><note>XLNet-512 (most complex Transformer architecture). As suggested in<ref type="bibr" target="#b14">[15]</ref>, the Transformer training is performed with batch size b =
, and deep contextual language models from BERT <ref type="bibr" target="#b14">[15]</ref> and XLNet <ref type="bibr" target="#b40">[41]</ref> in a vanilla and Siamese architecture <ref type="bibr" tar contextual embeddings as the ones used in BERT <ref type="bibr" target="#b14">[15]</ref> and XL-Net <ref type="bibr" target="#b40">[41]</ref>. The Transformer architecture allowed the efficient unsupe ype="bibr" target="#b36">[37]</ref>, named BERT <ref type="bibr" target="#b14">[15]</ref> and XLNet <ref type="bibr" target="#b40">[41]</ref>. The two Transformer models are originally designed to sol <ref type="bibr" target="#b42">[43]</ref> alone, XLNet uses additional Web corpora for pretraining <ref type="bibr" target="#b40">[41]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head pected is that BERT generally achieves slightly better results than XLNet. According to Yang et al. <ref type="bibr" target="#b40">[41]</ref>, XLNet surpasses BERT on the related GLUE benchmark <ref t may be attributed to two reasons, pretraining on different corpora, and smaller models compared to <ref type="bibr" target="#b40">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained 0">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained models used by Yang et al. <ref type="bibr" target="#b40">[41]</ref>. Furthermore, the published XLNet BASE model we considered ublished XLNet BASE model we considered is pretrained on different data than the one in Yang et al. <ref type="bibr" target="#b40">[41]</ref>  <ref type="foot" target="#foot_13">15</ref> . In contrast
ased document embeddings from GloVe <ref type="bibr" target="#b30">[31]</ref> and Paragraph Vectors <ref type="bibr" target="#b22">[23]</ref> (as Doc2vec implementation <ref type="bibr" target="#b32"> they encode the segments with GloVe <ref type="bibr" target="#b30">[31]</ref> and Paragraph Vectors <ref type="bibr" target="#b22">[23]</ref> and compute their similarity to determine whether papers a ref type="bibr" target="#b34">35]</ref> but unable to represent entire documents. Paragraph Vectors <ref type="bibr" target="#b22">[23]</ref> (also known as Doc2vec), extends word2vec to learn embeddi
ntations of sentences and their similarity <ref type="bibr" target="#b25">[26]</ref>. In prior work <ref type="bibr" target="#b31">[32]</ref>, we also utilized a Siamese BERT model to determine the di
ype="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>. Akkalyoncu Yilmaz et al. <ref type="bibr" target="#b1">[2]</ref> apply BERT to an information retrieval system for an end-to-
nt pairs. Second, we implement six different models using word-based document embeddings from GloVe <ref type="bibr" target="#b30">[31]</ref> and Paragraph Vectors <ref type="bibr" target="#b22">[23]< papers into background, purpose, mechanism, and findings. Next, they encode the segments with GloVe <ref type="bibr" target="#b30">[31]</ref> and Paragraph Vectors <ref type="bibr" target="#b22">[23]< >[37]</ref> neural language models introduced a shift from context-free word embeddings, like GloVe <ref type="bibr" target="#b30">[31]</ref>, to contextual embeddings as the ones used in BERT <ref ty raining the classifier. 8 https://radimrehurek.com/gensim/models/doc2vec.html 3.5.2 AvgGloVe. GloVe <ref type="bibr" target="#b30">[31]</ref> also produce dense embedding representations, but unlike w
c relations that would underpin such a system. While other NLP tasks, like relation extraction (RE) <ref type="bibr" target="#b41">[42]</ref>, deal with relations, they are not concerned with semantic 8">[39]</ref>, word analogies <ref type="bibr" target="#b24">[25]</ref>, entity relation extraction <ref type="bibr" target="#b41">[42]</ref>), or similarity between text pairs (i.e., binary classific
even if they are explicitly mentioned in the text. An analysis of the inner Transformer components <ref type="bibr" target="#b10">[11]</ref> is a subject for future work.</p></div> <div xmlns="http:/
beddings. However, XLNet integrates the relative positional encoding, as proposed in Transformer-XL <ref type="bibr" target="#b13">[14]</ref>. Therefore, XLNet's architecture is, in theory, not bound
fect of domain-specific pretraining on the performance of the language model has already been shown <ref type="bibr" target="#b7">[8]</ref>.</p><p>Our evaluation also shows that the Siamese networks c -score of 0.65, which was achieved by SciBERT on the related task of citation intent classification <ref type="bibr" target="#b7">[8]</ref>. While the effort for the unsupervised pretraining of a lang
nt pairs. Second, we implement six different models using word-based document embeddings from GloVe <ref type="bibr" target="#b30">[31]</ref> and Paragraph Vectors <ref type="bibr" target="#b22">[23]< papers into background, purpose, mechanism, and findings. Next, they encode the segments with GloVe <ref type="bibr" target="#b30">[31]</ref> and Paragraph Vectors <ref type="bibr" target="#b22">[23]< >[37]</ref> neural language models introduced a shift from context-free word embeddings, like GloVe <ref type="bibr" target="#b30">[31]</ref>, to contextual embeddings as the ones used in BERT <ref ty raining the classifier. 8 https://radimrehurek.com/gensim/models/doc2vec.html 3.5.2 AvgGloVe. GloVe <ref type="bibr" target="#b30">[31]</ref> also produce dense embedding representations, but unlike w
g â Ã¬ w Queen = Ã¬ w Man â Ã¬ w Woman <ref type="bibr" target="#b24">[25]</ref>. Allen and Hospedales <ref type="bibr" target="#b2">[3]</ref> give a mathematical description of analogies as linear relat
fect of domain-specific pretraining on the performance of the language model has already been shown <ref type="bibr" target="#b7">[8]</ref>.</p><p>Our evaluation also shows that the Siamese networks c -score of 0.65, which was achieved by SciBERT on the related task of citation intent classification <ref type="bibr" target="#b7">[8]</ref>. While the effort for the unsupervised pretraining of a lang
nt pairs. Second, we implement six different models using word-based document embeddings from GloVe <ref type="bibr" target="#b30">[31]</ref> and Paragraph Vectors <ref type="bibr" target="#b22">[23]< papers into background, purpose, mechanism, and findings. Next, they encode the segments with GloVe <ref type="bibr" target="#b30">[31]</ref> and Paragraph Vectors <ref type="bibr" target="#b22">[23]< >[37]</ref> neural language models introduced a shift from context-free word embeddings, like GloVe <ref type="bibr" target="#b30">[31]</ref>, to contextual embeddings as the ones used in BERT <ref ty raining the classifier. 8 https://radimrehurek.com/gensim/models/doc2vec.html 3.5.2 AvgGloVe. GloVe <ref type="bibr" target="#b30">[31]</ref> also produce dense embedding representations, but unlike w
and led to significant improvements in many NLP benchmarks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. Reimers and Gurevy arget="#b28">[29]</ref>), relations between sentences or entities (e.g., natural language inference <ref type="bibr" target="#b38">[39]</ref>, word analogies <ref type="bibr" target="#b24">[25]</ref>, t al. <ref type="bibr" target="#b40">[41]</ref>, XLNet surpasses BERT on the related GLUE benchmark <ref type="bibr" target="#b38">[39]</ref>, so we were expecting a similar outcome. We hypothesize th
15]</ref> and XLNet <ref type="bibr" target="#b40">[41]</ref> in a vanilla and Siamese architecture <ref type="bibr" target="#b8">[9]</ref>. Each system is evaluated under specific configurations rega vych <ref type="bibr" target="#b33">[34]</ref> proposed to combine BERT with a Siamese architecture <ref type="bibr" target="#b8">[9]</ref> for semantic representations of sentences and their similari ese Transformer. We combine the two Transformers (BERT and XLNet) in a Siamese network architecture <ref type="bibr" target="#b8">[9]</ref>. In Siamese networks, two inputs are fed through identical s
ype="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>. Akkalyoncu Yilmaz et al. <ref type="bibr" target="#b1">[2]</ref> apply BERT to an information retrieval system for an end-to-
e embedding space. Word2vec is widely applied in NLP tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref> but unable to represent entire documents. Paragraph Vectors
ical queries. To evaluate the presented techniques, we build a dataset using Wikipedia and Wikidata <ref type="bibr" target="#b37">[38]</ref> repositories to illustrate our problem. Wikipedia articles
g â Ã¬ w Queen = Ã¬ w Man â Ã¬ w Woman <ref type="bibr" target="#b24">[25]</ref>. Allen and Hospedales <ref type="bibr" target="#b2">[3]</ref> give a mathematical description of analogies as linear relat
f type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. Reimers and Gurevych <ref type="bibr" target="#b33">[34]</ref> proposed to combine BERT with a Siamese architecture <ref , the Transformers), and then passed to a classifier or a similarity function. Reimers and Gurevych <ref type="bibr" target="#b33">[34]</ref> have shown that Siamese BERT networks are suitable for tex her fixed nor frozen, but continually learned during the training of the classifier. Different than <ref type="bibr" target="#b33">[34]</ref>, our implemented Siamese architecture is applied to a mult [u; v; |u âv |; u * v]</formula><p>). Furthermore, we confirmed the results of Reimers and Gurevych <ref type="bibr" target="#b33">[34]</ref>, i.e., the most crucial component is the element-wise diff
e ever-emerging information overload, digital libraries employ literature recommender systems (LRS) <ref type="bibr" target="#b6">[7]</ref>. These systems recommend related documents with the help of 55% of publications using content-based filtering, it accounts for the majority of the LRS research <ref type="bibr" target="#b6">[7]</ref>. Structure and style are not actively being accounted for. T
ar words end up close to each other in the embedding space. Word2vec is widely applied in NLP tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref> but unable to repr
upervised pretraining of language models and led to significant improvements in many NLP benchmarks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta f type="bibr" target="#b8">[9]</ref> for semantic representations of sentences and their similarity <ref type="bibr" target="#b25">[26]</ref>. In prior work <ref type="bibr" target="#b31">[32]</ref>,
ical queries. To evaluate the presented techniques, we build a dataset using Wikipedia and Wikidata <ref type="bibr" target="#b37">[38]</ref> repositories to illustrate our problem. Wikipedia articles
as successfully solved various document classification tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>. Akkalyoncu Yilmaz et al. <ref type="bibr" target="#b1">[2] Use case</head><p>Existing datasets provide either classifications of single documents (e.g., topic <ref type="bibr" target="#b28">[29]</ref>), relations between sentences or entities (e.g., natural l
e ever-emerging information overload, digital libraries employ literature recommender systems (LRS) <ref type="bibr" target="#b6">[7]</ref>. These systems recommend related documents with the help of 55% of publications using content-based filtering, it accounts for the majority of the LRS research <ref type="bibr" target="#b6">[7]</ref>. Structure and style are not actively being accounted for. T
. Especially for complex information needs, the formulation of analogical queries is more intuitive <ref type="bibr" target="#b23">[24]</ref>. A system that supports analogical queries would be partic s C is to ?" is a fundamental aspect of human intelligence <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>. Chan et al. <ref type="bibr" target="#b9">[10]</ref> empha
e ever-emerging information overload, digital libraries employ literature recommender systems (LRS) <ref type="bibr" target="#b6">[7]</ref>. These systems recommend related documents with the help of 55% of publications using content-based filtering, it accounts for the majority of the LRS research <ref type="bibr" target="#b6">[7]</ref>. Structure and style are not actively being accounted for. T
ype="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>. Akkalyoncu Yilmaz et al. <ref type="bibr" target="#b1">[2]</ref> apply BERT to an information retrieval system for an end-to-
xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformers</head><p>Recently, Transformer-based <ref type="bibr" target="#b36">[37]</ref> neural language models introduced a shift from context-fre two language models for deep contextual text representations based on the Transformer architecture <ref type="bibr" target="#b36">[37]</ref>, named BERT <ref type="bibr" target="#b14">[15]</ref> and
and led to significant improvements in many NLP benchmarks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. Reimers and Gurevy arget="#b28">[29]</ref>), relations between sentences or entities (e.g., natural language inference <ref type="bibr" target="#b38">[39]</ref>, word analogies <ref type="bibr" target="#b24">[25]</ref>, t al. <ref type="bibr" target="#b40">[41]</ref>, XLNet surpasses BERT on the related GLUE benchmark <ref type="bibr" target="#b38">[39]</ref>, so we were expecting a similar outcome. We hypothesize th
ry in which recommender system methodologies can be tested <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>. In <ref type="bibr" target="#b35">[36]</ref>, we compared r Wikipedia articles have been addressed in the literature <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>  <ref type="foot" target="#foot_2">3</ref> is connected to can be tested <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>. In <ref type="bibr" target="#b35">[36]</ref>, we compared text-and link-based document similarity measu . It remains unknown how the length of the processed sequence affects the classification task. From <ref type="bibr" target="#b35">[36]</ref>, we know that the performance of similarity measures peaks and the second-highest with 256 tokens. One could think this outcome is to be expected. However, in <ref type="bibr" target="#b35">[36]</ref>, the performance of text-and linkbased document similarity
ype="bibr" target="#b41">[42]</ref>), or similarity between text pairs (i.e., binary classification <ref type="bibr" target="#b15">[16]</ref>). Our task is defined as as multi-class classification of
recently published survey <ref type="bibr" target="#b4">[5]</ref>. To our knowledge, Hassan et al. <ref type="bibr" target="#b26">[27]</ref> are one of the first to use BERT to recommend research pap
e ever-emerging information overload, digital libraries employ literature recommender systems (LRS) <ref type="bibr" target="#b6">[7]</ref>. These systems recommend related documents with the help of 55% of publications using content-based filtering, it accounts for the majority of the LRS research <ref type="bibr" target="#b6">[7]</ref>. Structure and style are not actively being accounted for. T
dbow over the distributed memory training model is due to its results in semantic similarity tasks <ref type="bibr" target="#b21">[22]</ref>. It is important to mention that even though the embedding
15]</ref> and XLNet <ref type="bibr" target="#b40">[41]</ref> in a vanilla and Siamese architecture <ref type="bibr" target="#b8">[9]</ref>. Each system is evaluated under specific configurations rega vych <ref type="bibr" target="#b33">[34]</ref> proposed to combine BERT with a Siamese architecture <ref type="bibr" target="#b8">[9]</ref> for semantic representations of sentences and their similari ese Transformer. We combine the two Transformers (BERT and XLNet) in a Siamese network architecture <ref type="bibr" target="#b8">[9]</ref>. In Siamese networks, two inputs are fed through identical s
e embedding space. Word2vec is widely applied in NLP tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref> but unable to represent entire documents. Paragraph Vectors
any NLP benchmarks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. Reimers and Gurevych <ref type="bibr" target="#b33">[34]</ are pretrained with different data. While BERT is trained on English Wikipedia and the BooksCorpus <ref type="bibr" target="#b42">[43]</ref> alone, XLNet uses additional Web corpora for pretraining < n contrast to BERT, XLNet is pretrained on Web corpora in addition to Wikipedia and the BooksCorpus <ref type="bibr" target="#b42">[43]</ref>. The almost exclusive pretraining on Wikipedia most likely
any NLP benchmarks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. Reimers and Gurevych <ref type="bibr" target="#b33">[34]</ are pretrained with different data. While BERT is trained on English Wikipedia and the BooksCorpus <ref type="bibr" target="#b42">[43]</ref> alone, XLNet uses additional Web corpora for pretraining < n contrast to BERT, XLNet is pretrained on Web corpora in addition to Wikipedia and the BooksCorpus <ref type="bibr" target="#b42">[43]</ref>. The almost exclusive pretraining on Wikipedia most likely
beddings. However, XLNet integrates the relative positional encoding, as proposed in Transformer-XL <ref type="bibr" target="#b13">[14]</ref>. Therefore, XLNet's architecture is, in theory, not bound
ype="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>. Akkalyoncu Yilmaz et al. <ref type="bibr" target="#b1">[2]</ref> apply BERT to an information retrieval system for an end-to-
c relations that would underpin such a system. While other NLP tasks, like relation extraction (RE) <ref type="bibr" target="#b41">[42]</ref>, deal with relations, they are not concerned with semantic 8">[39]</ref>, word analogies <ref type="bibr" target="#b24">[25]</ref>, entity relation extraction <ref type="bibr" target="#b41">[42]</ref>), or similarity between text pairs (i.e., binary classific
eam generates about 1.2TB intermediate data to count 4-motif on the MiCo graph with 1 million edges <ref type="bibr" target="#b17">[18]</ref>.</p><p>Recently, specialized systems have been developed f thms in these specialized pattern matching systems can be described with nested loops, and AutoMine <ref type="bibr" target="#b17">[18]</ref> and GraphZero <ref type="bibr" target="#b11">[12]</ref> re ate-of-the-art singlemachine pattern matching systems. GraphZero is an upgraded version of AutoMine <ref type="bibr" target="#b17">[18]</ref>, and it outperforms AutoMine by up to 40Ã. Fractal is a JV ef>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Automine <ref type="bibr" target="#b17">[18]</ref> is built upon a set-based representation and uses compilat
ef type="bibr" target="#b34">[35]</ref>, SEED <ref type="bibr" target="#b35">[36]</ref> and QKCount <ref type="bibr" target="#b36">[37]</ref>) and general-purpose systems (Arabesque <ref type="bibr" t

sed in many fields, such as social networks <ref type="bibr" target="#b0">[1]</ref>, bioinformatics <ref type="bibr" target="#b1">[2]</ref>, and fraud detection <ref type="bibr" target="#b2">[3]</ref> computation engines and systems have been designed to estimate an approximate number of embeddings <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" es of n vertices in an embedding (e.g., when n = 5, they are <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target= target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target= target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target=
r even several days to mine a pattern with a size of 6 on an unlabeled graph with millions of edges <ref type="bibr" target="#b12">[13]</ref>.</p><p>Recently, researchers have proposed several general M Co-authorship Patents <ref type="bibr" target="#b26">[27]</ref> 3.8M 16.5M US Patents LiveJournal <ref type="bibr" target="#b12">[13]</ref> 4.0M 34.7M Social network Orkut <ref type="bibr" target="#
rms several JVM-based specialized algorithms (MRSUB <ref type="bibr" target="#b34">[35]</ref>, SEED <ref type="bibr" target="#b35">[36]</ref> and QKCount <ref type="bibr" target="#b36">[37]</ref>) and
ve proposed several general-purpose graph mining systems <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>, such as Arabesque and RStream, which provide high-level a API for implementing various graph mining algorithms and an efficient runtime engine.</p><p>G-Miner <ref type="bibr" target="#b16">[17]</ref> models the processing of a graph mining job as an independ
r even several days to mine a pattern with a size of 6 on an unlabeled graph with millions of edges <ref type="bibr" target="#b12">[13]</ref>.</p><p>Recently, researchers have proposed several general M Co-authorship Patents <ref type="bibr" target="#b26">[27]</ref> 3.8M 16.5M US Patents LiveJournal <ref type="bibr" target="#b12">[13]</ref> 4.0M 34.7M Social network Orkut <ref type="bibr" target="#
">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Automine <ref
br" target="#b20">[21]</ref>, approximate pattern mining <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, and frequent subgraph mining (FSM) <ref type="bibr" targe ng (FSM) <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. ASAP <ref type="bibr" target="#b22">[23]</ref> is a distributed approximate pattern matching system for e ]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. ASAP <ref type="bibr" target="#b22">[23]</ref> is the state-of-the-art one among them. ASAP is a distribu
r">([4,5,6,7,3]</ref>, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target= " target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref> denotes the function id
">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> to automatically designed neural architectures <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" t es human experts to frequently try and evaluate numerous different operation and connection options <ref type="bibr" target="#b3">[4]</ref>. In contrast to architectures that are manually designed, th NAS-generated architectures have shown promising results in many domains, such as image recognition <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" t rates a total search space of itive training procedure of each selected architecture can be avoided <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref> so that researche
. They brought great advancements in many applications of neural network, such as visual perception <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bib
="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In its early stage f type="bibr" target="#b5">[6]</ref> and sequence modeling <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Recently, a f type="bibr" target="#b23">[24]</ref>, language modelling <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, etc. Despite their efines operation candidates on the node, whereas we associate operations on the edge as inspired by <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" t y Search Space S t . The topology search space is inspired by the popular cell-based NAS algorithms <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" t e API. The implementation difference between DARTS <ref type="bibr" target="#b7">[8]</ref> and GDAS <ref type="bibr" target="#b6">[7]</ref> is only less than 20 lines of code. Our library reduces the order DARTS (DARTS-V1) <ref type="bibr" target="#b7">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type="bibr" target="#b6">[7]</ref>, SETN <ref type="bibr" target="#b16">[17]</ref>, TAS <ref ty cy. Such strategy is more robust than using the arg max over the learned architecture parameters in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr" t
rchitectures which may be insufficient to comprehensively evaluate NAS algorithms. NAS-Bench-1shot1 <ref type="bibr" target="#b19">[20]</ref> reuses the NAS-Bench-101 dataset with some modification to ibility and generalization ability of the NAS algorithms <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bib thm-agnostic while NAS-Bench-101 without any modification is only applicable to selected algorithms <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The original c evoted their effort to building a fair comparison and development environments for NAS. Zela et al. <ref type="bibr" target="#b19">[20]</ref> proposed a general framework for oneshot NAS methods and r ensive computational cost. We bring some results from <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b19">[20]</ref> to provide some preliminary evidence of generalization. In
de ground truth label for hypernetwork-based NAS methods <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, which learn to generate parameters of an architecture. Ot
lgorithms, such as platformaware NAS <ref type="bibr" target="#b11">[12]</ref>, accuracy prediction <ref type="bibr" target="#b37">[38]</ref>, mutation-based NAS <ref type="bibr" target="#b38">[39]</r to predict the final accuracy of an architecture based on the results of few early training epochs <ref type="bibr" target="#b37">[38]</ref>. These algorithms can be trained faster and the performanc 5">[6]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, which learn to search based on an approximation of the pe
eport the performance, e.g., different data augmentation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, different regularization <ref type="bibr" target="#b10">[
[15]</ref>, and different selections of hyper-parameters <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr" target="#b2">(3)</ref> The validation se ="#b7">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type="bibr" target="#b6">[7]</ref>, SETN <ref type="bibr" target="#b16">[17]</ref>, TAS <ref type="bibr" target="#b20">[21]</ref>, FBNet-V2 <
structure has shown a surprising robustness and accuracy <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Regarding the parameters vs. the accuracy, the candidates
y 12 epochs, which can be used in banditbased algorithms <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Since 12 epochs are not sufficient to evaluate the relati
="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In its early stages, the great success of deep learning was sequence modeling <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Recently, a variety of NAS algorithms have been incre of the selected architecture is not split in the same way <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>. These discrepancies cause a problem when comparing the perf anguage modelling <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, etc. Despite their success, many researchers have raised co whereas we associate operations on the edge as inspired by <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>. We summarize cha orithms, such as NAS algorithms based on parameter sharing <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Therefore, many NAS algorithms cannot be directly evaluated art as used in the recent neural cell-based NAS algorithms <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>. As shown in the space is inspired by the popular cell-based NAS algorithms <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Since all cells and test sets to provide a consistent training and evaluation settings for previous NAS algorithms <ref type="bibr" target="#b7">[8]</ref>. Most NAS methods use the validation set to evaluate archite he process has been unified through an easy-to-use API. The implementation difference between DARTS <ref type="bibr" target="#b7">[8]</ref> and GDAS <ref type="bibr" target="#b6">[7]</ref> is only les e="bibr" target="#b4">[5]</ref>. (IV) Differentiable algorithms. e.g., first order DARTS (DARTS-V1) <ref type="bibr" target="#b7">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type="bibr" target ng the arg max over the learned architecture parameters in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr" target="#b2">(3)</ref> The searched archit
y 12 epochs, which can be used in banditbased algorithms <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Since 12 epochs are not sufficient to evaluate the relati
going a transition from hand-designed neural architectures <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> to automatically de the number of nodes and including all possible edges so that our search space is algorithmagnostic. <ref type="bibr" target="#b1">(2)</ref> We provide extra diagnostic information, such as architectur
lgorithms, such as platformaware NAS <ref type="bibr" target="#b11">[12]</ref>, accuracy prediction <ref type="bibr" target="#b37">[38]</ref>, mutation-based NAS <ref type="bibr" target="#b38">[39]</r to predict the final accuracy of an architecture based on the results of few early training epochs <ref type="bibr" target="#b37">[38]</ref>. These algorithms can be trained faster and the performanc 5">[6]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, which learn to search based on an approximation of the pe
e employed to train this architecture and report the performance, e.g., different data augmentation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, different regu
k for oneshot NAS methods and reused NAS-Bench-101 to benchmark different NAS algorithms. Yu et al. <ref type="bibr" target="#b28">[29]</ref> designed a novel evaluation framework to evaluate the sear
de ground truth label for hypernetwork-based NAS methods <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, which learn to generate parameters of an architecture. Ot
de ground truth label for hypernetwork-based NAS methods <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, which learn to generate parameters of an architecture. Ot
eters for each architecture. This can provide ground truth label for hypernetwork-based NAS methods <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, which learn to
going a transition from hand-designed neural architectures <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> to automatically de the number of nodes and including all possible edges so that our search space is algorithmagnostic. <ref type="bibr" target="#b1">(2)</ref> We provide extra diagnostic information, such as architectur
><p>T HE deep learning community is undergoing a transition from hand-designed neural architectures <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" t ss of deep learning was promoted by the introductions of novel neural architectures, such as ResNet <ref type="bibr" target="#b0">[1]</ref>, Inception <ref type="bibr" target="#b2">[3]</ref>, VGGNet < s the same topology. The intermediate residual block is the basic residual block with a stride of 2 <ref type="bibr" target="#b0">[1]</ref>, which serves to down-sample the spatial size and double the ur NATS-Bench, we follow previous literature to set up the hyper-parameters and training strategies <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" iginal ImageNet to 16Ã16 pixels to form ImageNet16Ã16, from which we select all images with label â <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">120]</ref> to construct ImageNet-16-120. In
t scheduler <ref type="bibr" target="#b14">[15]</ref>, and different selections of hyper-parameters <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. <ref type="bib ing procedure of each selected architecture can be avoided <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref> so that researchers can target on the essence of NAS, i.e.
information retrieval community and search engine industry as the next generation search technology <ref type="bibr" target="#b12">[13]</ref>.</p><p>In general, a search engine comprises a recall laye l in Facebook search is not a text embedding problem, as is actively researched in the IR community <ref type="bibr" target="#b12">[13]</ref>. Instead it is a more complex problem that requires unders
ed representation learning, has been proven to be successful techniques contributing to the success <ref type="bibr" target="#b1">[2]</ref>. In essence, embedding is a way to represent a sparse vector
ors into coarse clusters typically through K-means algorithm, and the other is product quantization <ref type="bibr" target="#b7">[8]</ref> which does a fine-grained quantization to enable efficient c
useful to transform data prior to applying the quantization. We experimented with both PCA and OPQ <ref type="bibr" target="#b4">[5]</ref> to transform the data, and observed that OPQ is generally mo
returned by the first stage model. We shared the same spirit as the cascaded embedding training in <ref type="bibr" target="#b17">[18]</ref>, which ensembled a set of models trained with different le
ors into coarse clusters typically through K-means algorithm, and the other is product quantization <ref type="bibr" target="#b7">[8]</ref> which does a fine-grained quantization to enable efficient c
useful to transform data prior to applying the quantization. We experimented with both PCA and OPQ <ref type="bibr" target="#b4">[5]</ref> to transform the data, and observed that OPQ is generally mo
ade significant progress in speech recognition, computer vision, and natural language understanding <ref type="bibr" target="#b9">[10]</ref>. Among them embedding, which is also called representation
ade significant progress in speech recognition, computer vision, and natural language understanding <ref type="bibr" target="#b9">[10]</ref>. Among them embedding, which is also called representation
returned by the first stage model. We shared the same spirit as the cascaded embedding training in <ref type="bibr" target="#b17">[18]</ref>, which ensembled a set of models trained with different le
arity function, we choose cosine similarity as it is one of the commonly used in embedding learning <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_3">S(Q, D) = cos(E Q , E D ) = nt features that contributed to the major model improvements.</p><p>Text features. Character n-gram <ref type="bibr" target="#b6">[7]</ref> is a common approach to represent text for text embedding. I
ors into coarse clusters typically through K-means algorithm, and the other is product quantization <ref type="bibr" target="#b7">[8]</ref> which does a fine-grained quantization to enable efficient c
based retrieval into our serving stack, we implemented first-class support for NN search in Unicorn <ref type="bibr" target="#b2">[3]</ref>, a retrieval engine powering most search products at Faceboo as realtime updates, efficient query planning and execution, and support for multi-hop queries (see <ref type="bibr" target="#b2">[3]</ref>).</p><p>The latter allows us to support top-K NN queries, wh
zation. There are different algorithms for coarse quantization. It is useful to compare between IMI <ref type="bibr" target="#b10">[11]</ref> and IVF <ref type="bibr" target="#b14">[15]</ref> algorith
arity function, we choose cosine similarity as it is one of the commonly used in embedding learning <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_3">S(Q, D) = cos(E Q , E D ) = nt features that contributed to the major model improvements.</p><p>Text features. Character n-gram <ref type="bibr" target="#b6">[7]</ref> is a common approach to represent text for text embedding. I
arity function, we choose cosine similarity as it is one of the commonly used in embedding learning <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_3">S(Q, D) = cos(E Q , E D ) = nt features that contributed to the major model improvements.</p><p>Text features. Character n-gram <ref type="bibr" target="#b6">[7]</ref> is a common approach to represent text for text embedding. I
based retrieval into our serving stack, we implemented first-class support for NN search in Unicorn <ref type="bibr" target="#b2">[3]</ref>, a retrieval engine powering most search products at Faceboo as realtime updates, efficient query planning and execution, and support for multi-hop queries (see <ref type="bibr" target="#b2">[3]</ref>).</p><p>The latter allows us to support top-K NN queries, wh
useful to transform data prior to applying the quantization. We experimented with both PCA and OPQ <ref type="bibr" target="#b4">[5]</ref> to transform the data, and observed that OPQ is generally mo
useful to transform data prior to applying the quantization. We experimented with both PCA and OPQ <ref type="bibr" target="#b4">[5]</ref> to transform the data, and observed that OPQ is generally mo
ors into coarse clusters typically through K-means algorithm, and the other is product quantization <ref type="bibr" target="#b7">[8]</ref> which does a fine-grained quantization to enable efficient c
ade significant progress in speech recognition, computer vision, and natural language understanding <ref type="bibr" target="#b9">[10]</ref>. Among them embedding, which is also called representation
e vectors, on which we use cosine similarity as the distance metric. We propose to use triplet loss <ref type="bibr" target="#b13">[14]</ref> to approximate the recall objective to learn the neural ne e from computer vision field and for the classification task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar
lculation.</p><p>Input: PSL Rules R, Prediction Å·i , and Probability P(y|s i ), i = {1, 2, 3}; BERT <ref type="bibr" target="#b17">(Devlin et al. 2018)</ref>, to derive the sentence representation v i etails</head><p>In the framework of CTRL-PG, any contextualized word embedding method, such as BERT <ref type="bibr" target="#b17">(Devlin et al. 2018)</ref>, ELMo <ref type="bibr" target="#b41">(Pete nd RoBERTa <ref type="bibr" target="#b37">(Liu et al. 2019b)</ref>, can be utilized. We choose BERT <ref type="bibr" target="#b17">(Devlin et al. 2018)</ref> to derive contextualized sentence embeddin ch tokenized sequence and learns an embedding vector for it. We follow the experimental settings in <ref type="bibr" target="#b17">(Devlin et al. 2018)</ref> to use 12 Transformer layers and attention #b29">(Kingma and Ba 2014)</ref> to optimize the parameters. We follow the experimental settings in <ref type="bibr" target="#b17">(Devlin et al. 2018)</ref> to set the dropout rate, and batch size as
was before f. Some recent studies <ref type="bibr" target="#b32">(Leeuwenberg and Moens 2017;</ref><ref type="bibr" target="#b40">Ning, Feng, and Roth 2017;</ref><ref type="bibr" target="#b27">Han, Z ref type="bibr">Han et al. 2019;</ref><ref type="bibr" target="#b25">Han, Ning, and Peng 2019;</ref><ref type="bibr" target="#b40">Ning, Feng, and Roth 2017)</ref> formulate the problem as a structure
ibr" target="#b16">(Deng and Wiebe 2015;</ref><ref type="bibr" target="#b12">Chen et al. 2019;</ref><ref type="bibr" target="#b28">Hu et al. 2016</ref>) have explored Probabilistic Soft Logic (PSL) <r ess <ref type="bibr" target="#b18">(Farnadi, Babaki, and Getoor 2019)</ref>, Model Interpretability <ref type="bibr" target="#b28">(Hu et al. 2016)</ref>, Probabilistic Reasoning (Augustine, Rekatsina
ibr" target="#b16">(Deng and Wiebe 2015;</ref><ref type="bibr" target="#b12">Chen et al. 2019;</ref><ref type="bibr" target="#b28">Hu et al. 2016</ref>) have explored Probabilistic Soft Logic (PSL) <r ess <ref type="bibr" target="#b18">(Farnadi, Babaki, and Getoor 2019)</ref>, Model Interpretability <ref type="bibr" target="#b28">(Hu et al. 2016)</ref>, Probabilistic Reasoning (Augustine, Rekatsina
<p>In recent years, PSL rules have been applied to various machine learning topics such as Fairness <ref type="bibr" target="#b18">(Farnadi, Babaki, and Getoor 2019)</ref>, Model Interpretability <ref
r" target="#b3">(Bethard et al. 2015</ref><ref type="bibr" target="#b4">(Bethard et al. , 2016</ref><ref type="bibr" target="#b5">(Bethard et al. , 2017) )</ref> are some great efforts of building cli
)</ref>.</p><p>Recently, some researchers <ref type="bibr" target="#b16">(Deng and Wiebe 2015;</ref><ref type="bibr" target="#b12">Chen et al. 2019;</ref><ref type="bibr" target="#b28">Hu et al. 2016< ellert 2020), Knowledge Graph Construction <ref type="bibr" target="#b42">(Pujara et al. 2013;</ref><ref type="bibr" target="#b12">Chen et al. 2019)</ref> and Sentiment Analysis <ref type="bibr" targe

2-2012 <ref type="bibr" target="#b46">(Sun, Rumshisky, and Uzuner 2013)</ref> and Clinical TempEval <ref type="bibr" target="#b3">(Bethard et al. 2015</ref><ref type="bibr" target="#b4">(Bethard et al
red knowledge, i.e. extract important clinical named entities and relationships from the narratives <ref type="bibr" target="#b1">(Aronson and Lang 2010;</ref><ref type="bibr" target="#b44">Savova et
was before f. Some recent studies <ref type="bibr" target="#b32">(Leeuwenberg and Moens 2017;</ref><ref type="bibr" target="#b40">Ning, Feng, and Roth 2017;</ref><ref type="bibr" target="#b27">Han, Z ref type="bibr">Han et al. 2019;</ref><ref type="bibr" target="#b25">Han, Ning, and Peng 2019;</ref><ref type="bibr" target="#b40">Ning, Feng, and Roth 2017)</ref> formulate the problem as a structure
private-attribute inference attack can be naturally formulated as a problem of adversarial learning <ref type="bibr" target="#b18">[19]</ref>. In our proposed RAP, there are two components: a Bayesian
type="bibr" target="#b28">[29]</ref>. These systems build profiles that represent user's interests <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref> and recommend releva ate-attribute attacker âÎ± max {Î¸ t P } T t =1</formula><p>L D P privacy-aware recommendation system <ref type="bibr" target="#b7">(8)</ref> The inner part learns the most determined adversary which ad
p of works exploits both friend and behavioral information <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref>. Gong et al. <ref t /ref><ref type="bibr" target="#b24">25]</ref>. Gong et al. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> make a social-behavior-attribute network in which all users
e growth of the Web has raised numerous challenges for online users including disinformation spread <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" targe
tems toward personalized ranking. Inspired by recent success of Bayesian Personalized Ranking (BPR) <ref type="bibr" target="#b41">[42]</ref> in image and friend recommendation systems <ref type="bibr
ings and preferences (i.e., perturbation based techniques) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Another priv type="bibr" target="#b44">45]</ref> and perturbation based <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref> approaches. Some me proposes to add or remove items and ratings from user profiles minimize privacy risk. Polat et al. <ref type="bibr" target="#b37">[38]</ref> use a randomized perturbation technique by sharing disguis s of the users within the same group is then used to estimate a group preference vector. Similar to <ref type="bibr" target="#b37">[38]</ref>, randomness is then added to the preference vector to be s
target="#b27">28]</ref> and recommend relevant items to the users based on the constructed profiles <ref type="bibr" target="#b39">[40]</ref>. Despite the effectiveness of recommendation systems, they
arget="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref> and community membership information <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">44]</ref> to infer target's
e of a single rating or an entire user data is masked (i.e., differential privacy based techniques) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" ta stigate if the target is in the database. They could be categorized into differential privacy based <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" ta ions and generates differentially private average of users' preferences in each cluster. Hua et al. <ref type="bibr" target="#b22">[23]</ref> propose a private matrix factorization which adds noise to
iends' information <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref> and community membership information <ref type="bibr" targe
ayer. The dimension of the hidden layer is determined through grid search. We note that Gong et al. <ref type="bibr" target="#b35">[36]</ref> also proposed an attribute inference attack which leverage
dustry practitioners have turned to search-based compilation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" targ nually-written assembly code on large matrix multiplications <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b43">45]</ref>, as the code has bee tation definition of matrix multiplication. ple compiler techniques have been introduced (e.g., TVM <ref type="bibr" target="#b9">[10]</ref>, Halide <ref type="bibr" target="#b36">[38]</ref>, Tensor C ically, the compiler partitions the large computational graph of a DNN into several small subgraphs <ref type="bibr" target="#b9">[10]</ref>. This partition has a negligible effect on the performance arch and learned cost model performs the best among them, which is also used in our evaluation. TVM <ref type="bibr" target="#b9">[10]</ref> utilizes a similar scheduling language and includes a templ search for GPU code automatically, but it is not yet meant to be used for compute-bounded problems <ref type="bibr" target="#b9">[10]</ref>. It cannot outperform TVM on operators like conv2d and matm f type="bibr" target="#b9">[10]</ref>. It cannot outperform TVM on operators like conv2d and matmul <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">44]</ref>. This is because of graph level include layout optimizations <ref type="bibr" target="#b27">[29]</ref>, operator fusion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">35]</ref>, constant folding <
of several DNNs, which include ResNet-50 <ref type="bibr" target="#b18">[20]</ref> and MobileNet-V2 <ref type="bibr" target="#b38">[40]</ref> for image classification, 3D-ResNet-18 <ref type="bibr">[1
different techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b31">33]</ref>. The latest one with beam search and learned cost model per
f DNNs' performance, researchers and industry practitioners have turned to search-based compilation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe e performance of programs during the search. We adopt a learned cost model similar to related works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> with newly designed ion and automatic search. Halide has three versions of auto-scheduler based on different techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" targ >) or aggressive pruning by inaccurately evaluating incomplete programs (e.g. Halide auto-scheduler <ref type="bibr" target="#b1">[2]</ref>), which prevents them from covering a large enough search sp <ref type="bibr" target="#b29">[31]</ref> to search for good decisions (e.g., Halide auto-scheduler <ref type="bibr" target="#b1">[2]</ref>). In this approach, the compiler constructs a tensor program limited search space heuristically.</p><p>(2) Aggressive early pruning (e.g., Halide auto-scheduler <ref type="bibr" target="#b1">[2]</ref>). Aggressive early pruning based on evaluating incomplete pr ntel CPU.</p><p>We include PyTorch <ref type="bibr" target="#b34">[36]</ref>, Halide auto-scheduler <ref type="bibr" target="#b1">[2]</ref>, Flex-Tensor <ref type="bibr" target="#b51">[53]</ref> and A for complete programs but fails to accurately predict the final performance of incomplete programs. <ref type="bibr" target="#b1">(2)</ref> The fixed order of sequential decisions limits the design of
o-tuning. More recent works NeuroVectorizer <ref type="bibr" target="#b16">[17]</ref> and AutoPhase <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">23]</ref> use deep reinforce
ardware instruction sequences, while Ansor generates tensor programs with nests of loops. OpenTuner <ref type="bibr" target="#b3">[4]</ref> is a general framework for program auto-tuning based on mult
ion and auto-tuning have already shown its effectiveness in domains other than deep learning. Stock <ref type="bibr" target="#b39">[41]</ref> is a super-optimizer based on random search. Stock searche
t cannot outperform TVM on operators like conv2d and matmul <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">44]</ref>. This is because of the lack of certain optimizations and t
ion and auto-tuning have already shown its effectiveness in domains other than deep learning. Stock <ref type="bibr" target="#b39">[41]</ref> is a super-optimizer based on random search. Stock searche
ad><p>We benchmark the end-to-end inference execution time of several DNNs, which include ResNet-50 <ref type="bibr" target="#b18">[20]</ref> and MobileNet-V2 <ref type="bibr" target="#b38">[40]</ref>
xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evolutionary Search</head><p>Evolutionary search <ref type="bibr" target="#b47">[49]</ref> is a generic meta-heuristic algorithm inspired by biologic
g, and orange NAS components. Top: gradientbased meta-learning with fixed architecture such as MAML <ref type="bibr" target="#b15">[16]</ref> or REPTILE <ref type="bibr" target="#b36">[37]</ref>. Midd are as follows:</p><p>1. We show that model-agnostic, gradient-based metalearning methods (such as <ref type="bibr" target="#b15">[16]</ref>) can very naturally be combined with recently proposed gra rchitecture search because of conceptual similarities to gradient-based meta-learning, such as MAML <ref type="bibr" target="#b15">[16]</ref>, which will allow us to combine the two kinds of methods.< rning of Neural Architectures</head><p>Similar to MAML's meta-learning strategy in the weight space <ref type="bibr" target="#b15">[16]</ref>, our goal is to meta-learn an architecture with correspond rning algorithm ? not only for w but also for the architecture ?. As an example, one could use MAML <ref type="bibr" target="#b15">[16]</ref> as a meta-learning algorithm, which runs SGD on the meta-o head>MiniImagenet</head><p>Omniglot Method # params 1-shot, 5-way 5-shot, 5-way 1-shot, 20 way MAML <ref type="bibr" target="#b15">[16]</ref> 30k 48. which might be of independent interest. Empirical ng, and orange NAS components. Top: gradientbased meta-learning with fixed architecture such as MAML<ref type="bibr" target="#b15">[16]</ref> or REPTILE<ref type="bibr" target="#b36">[37]</ref>. Middl ew examples. Prior work has proposed meta-learning methods for this problem that are model-agnostic <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref> and allow meta-lea >In this work, we focus on a particular class of approaches denoted as model-agnostic meta-learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" ta task-learner, where k refers to k iterations of learning/ weight updates (e.g., by SGD). Prior work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta eta 3. This is in contrast to prior work where the architecture is always fixed during meta-testing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref>. Also prior work u itting when only very little data is available. Prior work <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar not improve performance due to overfitting as reported by <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Results are presented in Table <ref type="table" tar
ef> or meta-learning a subset of weights that is shared across tasks but fixed during task learning <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>In this wor
type="bibr" target="#b19">[20]</ref>, semantic segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref>, and disparity esti
ref>, weight sharing <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>, or multifidelity optimization <ref type="bibr" target="#b2">
more thorough literature overview. Researchers often frame NAS as a reinforcement learning problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" targ
loping more efficient methods, e.g., via network morphisms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targe
lated tasks is known as meta-learning or learning to learn <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b20">21]</ref>. Here, we consider s few-shot learning via meta-learning or learning to learn <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar
e="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>, or multifidelity optimization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targ
ef><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7]</ref>, object detection <ref type="bibr" target="#b19">[20]</ref>, semantic segmentation <ref type="bibr" target="#b7">[8,</
a variety of tasks <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">44]</ref>. We briefly review th
ef><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7]</ref>, object detection <ref type="bibr" target="#b19">[20]</ref>, semantic segmentation <ref type="bibr" target="#b7">[8,</
rotect the proprietary data. However, recent work by Zhu et al., "Deep Leakage from Gradient" (DLG) <ref type="bibr" target="#b0">[1]</ref> showed the possibility to steal the private training data fr nables us to always extract the ground-truth labels and significantly simplify the objective of DLG <ref type="bibr" target="#b0">[1]</ref> in order to extract good-quality data. Hence, we name our ap extraction with better fidelity.</p><p>â¢ We empirically demonstrate the advantages of iDLG over DLG <ref type="bibr" target="#b0">[1]</ref> via comparing the accuracy of extracted labels and the fidel <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Recent work by Zhu et al. <ref type="bibr" target="#b0">[1]</ref> presents an approach (DLG) to steal the proprietary data pro s</head><p>In this section, we empirically demonstrate the advantages of our (iDLG) method over DLG <ref type="bibr" target="#b0">[1]</ref>. We perform experiments on the classification task over thre " target="#b9">[10]</ref> with 10, 100, and 5749 categories respectively. Following the settings in <ref type="bibr" target="#b0">[1]</ref>, we use the randomly initialized LeNet for all experiments. is used as the optimizer. For fast training, we resize all images in LFW to 32 Ã 32.</p><p>For DLG <ref type="bibr" target="#b0">[1]</ref>, as described by the authors, we start the procedure with th % 100.0% LFW 79.1% 100.0% Table <ref type="table">1</ref>: Accuracy of the extracted labels for DLG <ref type="bibr" target="#b0">[1]</ref> and iDLG. Note that iDLG always extracts the correct label a
arget="#b0">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type="bibr" target="#b7">[8]</ref>, CIFAR-100 <ref type="bibr" target="#b8">[9]</ref>, and LFW
pe="bibr" target="#b0">[1]</ref>, we use the randomly initialized LeNet for all experiments. L-BFGS <ref type="bibr" target="#b10">[11]</ref> with learning rate 1 is used as the optimizer. For fast tr
pe="bibr" target="#b0">[1]</ref>, we use the randomly initialized LeNet for all experiments. L-BFGS <ref type="bibr" target="#b10">[11]</ref> with learning rate 1 is used as the optimizer. For fast tr
arget="#b0">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type="bibr" target="#b7">[8]</ref>, CIFAR-100 <ref type="bibr" target="#b8">[9]</ref>, and LFW
distributed learning systems such as Collaborative Learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> and Federated Learning loss (difference between gradients). 6:</p><p>x â â x â Î·â x LG Update the dummy datum. 7: end for <ref type="bibr" target="#b2">3</ref> </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Expe
arget="#b0">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type="bibr" target="#b7">[8]</ref>, CIFAR-100 <ref type="bibr" target="#b8">[9]</ref>, and LFW
ref type="bibr" target="#b3">4]</ref> and Federated Learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, it is widely believed
ref type="bibr" target="#b3">4]</ref> and Federated Learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, it is widely believed
pe="bibr" target="#b0">[1]</ref>, we use the randomly initialized LeNet for all experiments. L-BFGS <ref type="bibr" target="#b10">[11]</ref> with learning rate 1 is used as the optimizer. For fast tr
arget="#b0">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type="bibr" target="#b7">[8]</ref>, CIFAR-100 <ref type="bibr" target="#b8">[9]</ref>, and LFW
Normalization (GN) as a simple alternative to BN. We notice that many classical features like SIFT <ref type="bibr" target="#b13">[14]</ref> and HOG <ref type="bibr" target="#b14">[15]</ref> are grou ><p>The channels of visual representations are not entirely independent. Classical features of SIFT <ref type="bibr" target="#b13">[14]</ref>, HOG <ref type="bibr" target="#b14">[15]</ref>, and GIST < more abstract and their behaviors are not as intuitive. However, in addition to orientations (SIFT <ref type="bibr" target="#b13">[14]</ref>, HOG <ref type="bibr" target="#b14">[15]</ref>, or <ref ty
lt, many recent models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target=
when testing. The pre-computed statistics may also change when the target data distribution changes <ref type="bibr" target="#b31">[32]</ref>. These issues lead to inconsistency at training, transferr
" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> are trained with non-tr
ing along the batch dimension. These methods are effective for training sequential models (RNN/LSTM <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>) or generative mod
in neuroscience is to normalize across the cell responses <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" tar
ks including detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, segmentation <ref type="bibr" target="#b10">[11,</ref><ref /ref><ref type="bibr" target="#b9">10]</ref>, segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>, video recognition <ref type="bibr" target="#b11">[12,</ref> Mask R-CNN frameworks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> use a batch size of 1 or 2 images because of higher resoluti ice (1 or 2 images/GPU <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b57">58]</ref>). As a result, BN is with a batch size of 2), so we ignore this variant.</p><p>We experiment on the Mask R-CNN baselines <ref type="bibr" target="#b9">[10]</ref>, implemented in the publicly available codebase of Detectro et="#tab_4">4</ref> shows the comparison of GN vs. BN * on Mask R-CNN using a conv 4 backbone ("C4" <ref type="bibr" target="#b9">[10]</ref>). This C4 variant uses ResNet's layers of up to conv 4 to e
ks including detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, segmentation <ref type="bibr" target="#b10">[11,</ref><ref /ref><ref type="bibr" target="#b9">10]</ref>, segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>, video recognition <ref type="bibr" target="#b11">[12,</ref> Mask R-CNN frameworks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> use a batch size of 1 or 2 images because of higher resoluti ice (1 or 2 images/GPU <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b57">58]</ref>). As a result, BN is with a batch size of 2), so we ignore this variant.</p><p>We experiment on the Mask R-CNN baselines <ref type="bibr" target="#b9">[10]</ref>, implemented in the publicly available codebase of Detectro et="#tab_4">4</ref> shows the comparison of GN vs. BN * on Mask R-CNN using a conv 4 backbone ("C4" <ref type="bibr" target="#b9">[10]</ref>). This C4 variant uses ResNet's layers of up to conv 4 to e
alization variants <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Moreover, although the batch size may change, GN can natur malization methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar type="figure" target="#fig_1">2</ref>). Instead of operating on features, Weight Normalization (WN) <ref type="bibr" target="#b18">[19]</ref> proposes to normalize the filter weights. These methods do
ods are effective for training sequential models (RNN/LSTM <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>) or generative models (GANs <ref type="bibr" target="#b23">
<ref type="bibr" target="#b13">[14]</ref>, HOG <ref type="bibr" target="#b14">[15]</ref>, and GIST <ref type="bibr" target="#b40">[41]</ref> are group-wise representations by design, where each group
ss on non-autoregressive sequence generation <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b20">Ghazvininejad et al., 2019)</ref>. 1  Specifically, the Levenshtein T maximum number of decoding steps is reached <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b20">Ghazvininejad et al., 2019)</ref>.<ref type="foot" target="#foot_3">5 n et al., 2018)</ref> or multi-pass decoding <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b20">Ghazvininejad et al., 2019;</ref><ref type="bibr" target="#b22">Gu et s by iteratively editing the outputs from previous iterations. Edit operations such as substitution <ref type="bibr" target="#b20">(Ghazvininejad et al., 2019)</ref> and insertion-deletion <ref type="
slation quality on par or better than both EDITOR and Levenshtein Transformer with hard constraints <ref type="bibr" target="#b47">(Susanto et al., 2020)</ref>.</p></div> <div xmlns="http://www.tei-c. r" target="#b16">(Dinu et al., 2019;</ref><ref type="bibr" target="#b36">Post and Vilar, 2018;</ref><ref type="bibr" target="#b47">Susanto et al., 2020)</ref>. Compared to <ref type="bibr" target="#b3 constraints (Table <ref type="table" target="#tab_6">6</ref>). Consistent with previous findings by <ref type="bibr" target="#b47">Susanto et al. (2020)</ref>, incorporating soft constraints in LevT i s in LevT improves BLEU by +0.3 on Wiktionary and by +0.4 on IATE. Enforcing hard constraints as in <ref type="bibr" target="#b47">Susanto et al. (2020)</ref> increases the term usage by +8-10% and im they help close the small gap to reach 100% term usage and do not 14 We use our implementations of <ref type="bibr" target="#b47">Susanto et al. (2020)</ref>'s technique for a more controlled compari b47">Susanto et al. (2020)</ref>'s technique for a more controlled comparison. The LevT baseline in <ref type="bibr" target="#b47">Susanto et al. (2020)</ref> achieves higher BLEU than ours on the sma
h Repositioning (EDITOR), which builds on recent progress on non-autoregressive sequence generation <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b20">Ghazvininejad et al., 2018;</ref><ref type="bibr" target="#b46">Stern et al., 2018)</ref> or multi-pass decoding <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b20">Ghazvininejad "bibr" target="#b22">(Gu et al., 2019)</ref>, or 2) the maximum number of decoding steps is reached <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b20">Ghazvininejad s widely used in nonautoregressive generation <ref type="bibr" target="#b21">(Gu et al., 2018;</ref><ref type="bibr" target="#b26">Lee et al., 2018;</ref><ref type="bibr" target="#b22">Gu et al., 2019
daptation for MT <ref type="bibr" target="#b23">(Hokamp and Liu, 2017)</ref> and caption generation <ref type="bibr" target="#b1">(Anderson et al., 2017)</ref>, they suffer from several issues: constr

/ref>): Romanian-English (Ro-En) from WMT16 (Bojar et al., 2016), English-German (En-De) from WMT14 <ref type="bibr" target="#b6">(Bojar et al., 2014)</ref>, and English-Japanese (En-Ja) from WAT2017
t="#b52">Wang et al., 2019)</ref>. These issues have been addressed via partially parallel decoding <ref type="bibr" target="#b51">(Wang et al., 2018;</ref><ref type="bibr" target="#b46">Stern et al.,
standard for many sequence generation tasks <ref type="bibr" target="#b12">(Cho et al., 2014;</ref><ref type="bibr" target="#b13">Chorowski et al., 2015;</ref><ref type="bibr" target="#b50">Vinyals a
tence <ref type="bibr" target="#b4">(Bangalore et al., 2007)</ref>, or the Operation Sequence Model <ref type="bibr" target="#b17">(Durrani et al., 2015;</ref><ref type="bibr" target="#b44">Stahlberg
orporate domain-specific knowledge and lexicons which is particularly helpful in low-resource cases <ref type="bibr" target="#b2">(Arthur et al., 2016;</ref><ref type="bibr" target="#b48">Tang et al.,
els and boldface the top scores among NAR models based on the paired bootstrap test with p &lt; 0.05<ref type="bibr" target="#b14">(Clark et al., 2011)</ref>. EDITOR decodes 6-7% faster than LevT on R
s/1.0"><head n="3.2">Dual-Path Imitation Learning</head><p>We train EDITOR using imitation learning <ref type="bibr" target="#b15">(DaumÃ© III et al., 2009;</ref><ref type="bibr" target="#b39">Ross et
in Gu et al. ( <ref type="formula">2019</ref>))<ref type="foot" target="#foot_7">9</ref> and RIBES <ref type="bibr" target="#b24">(Isozaki et al., 2010)</ref>, which is more sensitive to word order d
e-level models that generate a bag of target words that is reordered to construct a target sentence <ref type="bibr" target="#b4">(Bangalore et al., 2007)</ref>, or the Operation Sequence Model <ref t
seful for interactive machine translation <ref type="bibr" target="#b19">(Foster et al., 2002;</ref><ref type="bibr" target="#b5">Barrachina et al., 2009)</ref> and domain adaptation <ref type="bibr"
t="#b52">Wang et al., 2019)</ref>. These issues have been addressed via partially parallel decoding <ref type="bibr" target="#b51">(Wang et al., 2018;</ref><ref type="bibr" target="#b46">Stern et al.,
)</ref>, or the Operation Sequence Model <ref type="bibr" target="#b17">(Durrani et al., 2015;</ref><ref type="bibr" target="#b44">Stahlberg et al., 2018)</ref>, which views translation as a sequence



"#b41">(Sennrich et al., 2016a;</ref><ref type="bibr" target="#b18">Ficler and Goldberg, 2017;</ref><ref type="bibr" target="#b40">Scarton and Specia, 2018)</ref>, which condition generation on users'
ndependence assumptions between target tokens <ref type="bibr" target="#b28">(Ma et al., 2019;</ref><ref type="bibr" target="#b52">Wang et al., 2019)</ref>. These issues have been addressed via partia
n addressed via partially parallel decoding <ref type="bibr" target="#b51">(Wang et al., 2018;</ref><ref type="bibr" target="#b46">Stern et al., 2018)</ref> or multi-pass decoding <ref type="bibr" tar
ps. <ref type="foot" target="#foot_5">7</ref>We select the best checkpoint based on validation BLEU <ref type="bibr" target="#b35">(Papineni et al., 2002)</ref>. All models are trained on 8 NVIDIA V10
y generating a sequence of tokens in parallel <ref type="bibr" target="#b21">(Gu et al., 2018;</ref><ref type="bibr" target="#b33">van den Oord et al., 2018;</ref><ref type="bibr" target="#b28">Ma et
br">(Koehn et al., 2007)</ref> and have been shown to be useful for interactive machine translation <ref type="bibr" target="#b19">(Foster et al., 2002;</ref><ref type="bibr" target="#b5">Barrachina e
models that decode from left-to-right are the de facto standard for many sequence generation tasks <ref type="bibr" target="#b12">(Cho et al., 2014;</ref><ref type="bibr" target="#b13">Chorowski et a

ps. <ref type="foot" target="#foot_5">7</ref>We select the best checkpoint based on validation BLEU <ref type="bibr" target="#b35">(Papineni et al., 2002)</ref>. All models are trained on 8 NVIDIA V10
standard for many sequence generation tasks <ref type="bibr" target="#b12">(Cho et al., 2014;</ref><ref type="bibr" target="#b13">Chorowski et al., 2015;</ref><ref type="bibr" target="#b50">Vinyals a
ining NMT models with constraints as inputs <ref type="bibr" target="#b43">(Song et al., 2019;</ref><ref type="bibr" target="#b16">Dinu et al., 2019)</ref> or with constrained beam search that drastic ses in both the source and target sequences <ref type="bibr" target="#b43">(Song et al., 2019;</ref><ref type="bibr" target="#b16">Dinu et al., 2019)</ref>, or 2) constrained decoding where beam searc e constraints and apply BPE to the constraint sequence. Different from the terminology test sets in <ref type="bibr" target="#b16">Dinu et al. (2019)</ref> which contain only several hundred sentences T with Terminology Constraints</head><p>We evaluate EDITOR on the terminology test sets released by <ref type="bibr" target="#b16">Dinu et al. (2019)</ref> to test its ability to incorporate terminolo nto et al., 2020)</ref>. Compared to <ref type="bibr" target="#b36">Post and Vilar (2018)</ref> and <ref type="bibr" target="#b16">Dinu et al. (2019)</ref>, EDITOR with soft constraints achieves highe OR and Sockeye-based</cell></row></table><note>De test sets with terminology constraints released by<ref type="bibr" target="#b16">Dinu et al. (2019)</ref>. Machine Translation Results. For each metri vided order, EDI-TOR's reposition operation helps generate a more fluent and adequate translation.  <ref type="bibr" target="#b16">(Dinu et al., 2019)</ref> provided with correct terminology entries ( o test its ability to incorporate terminology constraints and to further compare it with prior work <ref type="bibr" target="#b16">(Dinu et al., 2019;</ref><ref type="bibr" target="#b36">Post and Vila
een introduced by designing complex architectures tailored to specific content or style constraints <ref type="bibr" target="#b0">(Abu Sheikha and Inkpen, 2011;</ref><ref type="bibr" target="#b29">Mei
l machine translation (MT) architectures <ref type="bibr" target="#b3">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b49">Vaswani et al., 2017)</ref> make it difficult for users to specify pr . By contrast, autoregressive NMT models <ref type="bibr" target="#b3">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b49">Vaswani et al., 2017)</ref> do not explicitly separate lexical choice y distribution P (A) over the action space A. Our model is based on the Transformer encoder-decoder <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref> and we extract the decoder representa- F
is critical to realism.</p><p>In contrast to methods that use a parametric model of the human face <ref type="bibr" target="#b0">[1]</ref>, we directly predict the positions of face mesh vertices in
parate networks that are chained together.</p><p>We use an architecture similar to one described in <ref type="bibr" target="#b6">[7]</ref>, where the authors build a network that is robust to the ini
ious human faces, we apply Laplacian mesh editing to morph a canonical mesh into the predicted mesh <ref type="bibr" target="#b2">[3]</ref>. This lets us use the blend shape coefficients for different
parate networks that are chained together.</p><p>We use an architecture similar to one described in <ref type="bibr" target="#b6">[7]</ref>, where the authors build a network that is robust to the ini
proach by employing region-specific heads that transform the feature maps with spatial transformers <ref type="bibr" target="#b3">[4]</ref>, while being up to 30 percent faster during inference. We te atures that are used by the attention mechanism. Specifically, we use a spatial transformer mod-ule <ref type="bibr" target="#b3">[4]</ref> to extract 24 Ã 24 region features from the 64 Ã 64 feature and hard) have been developed for visual feature extraction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. These attention mechanisms sample a grid of 2D points in fea
parate networks that are chained together.</p><p>We use an architecture similar to one described in <ref type="bibr" target="#b6">[7]</ref>, where the authors build a network that is robust to the ini
proach by employing region-specific heads that transform the feature maps with spatial transformers <ref type="bibr" target="#b3">[4]</ref>, while being up to 30 percent faster during inference. We te atures that are used by the attention mechanism. Specifically, we use a spatial transformer mod-ule <ref type="bibr" target="#b3">[4]</ref> to extract 24 Ã 24 region features from the 64 Ã 64 feature and hard) have been developed for visual feature extraction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. These attention mechanisms sample a grid of 2D points in fea
ious human faces, we apply Laplacian mesh editing to morph a canonical mesh into the predicted mesh <ref type="bibr" target="#b2">[3]</ref>. This lets us use the blend shape coefficients for different
parate networks that are chained together.</p><p>We use an architecture similar to one described in <ref type="bibr" target="#b6">[7]</ref>, where the authors build a network that is robust to the ini
ious human faces, we apply Laplacian mesh editing to morph a canonical mesh into the predicted mesh <ref type="bibr" target="#b2">[3]</ref>. This lets us use the blend shape coefficients for different
proach by employing region-specific heads that transform the feature maps with spatial transformers <ref type="bibr" target="#b3">[4]</ref>, while being up to 30 percent faster during inference. We te atures that are used by the attention mechanism. Specifically, we use a spatial transformer mod-ule <ref type="bibr" target="#b3">[4]</ref> to extract 24 Ã 24 region features from the 64 Ã 64 feature and hard) have been developed for visual feature extraction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. These attention mechanisms sample a grid of 2D points in fea
quence Models for Peptide Binding Prediction</head><p>The approach builds on the UDSMProt-framework <ref type="bibr" target="#b11">[12]</ref> and related work in natural language processing <ref type= with a concat pooling layer and two fully connected layers. The setup closely follows that used in <ref type="bibr" target="#b11">[12]</ref>, where protein properties were predicted. The smaller data of the number of hidden units from 1150 to 64 and of the embedding size from 400 to 50. Similar to <ref type="bibr" target="#b11">[12]</ref>, the training procedure included 1-cycle learning rate sch e potential of unlabeled peptide data in order to observe similar improvements as seen for proteins <ref type="bibr" target="#b11">[12]</ref> in particular for small datasets.  Turning to MHC Class II uarcy of 0.137, which is is considerably lower than the accuracy of 0.41 reported in the literature <ref type="bibr" target="#b11">[12]</ref>. This effect is a direct consequence of the considerably s
10; 11]</ref> and we discuss in more detail how USMPep stands out from these approaches. MHCnuggets <ref type="bibr" target="#b9">[10]</ref> is rather similar to the proposed approach (apart from the
alleles, which is the default evaluation metric for related tasks such as remote homology detection <ref type="bibr" target="#b19">[20]</ref> or transcription factor binding site prediction <ref type=
ed sequences. We filtered the SwissProt release 2018 10 for the human proteome and employed NetChop <ref type="bibr" target="#b22">[23]</ref> to obtain proteasome cleavage sites for these proteins. Th
6]</ref>, available on the Immune Epitope Database (IEDB)<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b16">[17]</ref>, and is split into a non-overlapping training (BD2009) and
detection <ref type="bibr" target="#b19">[20]</ref> or transcription factor binding site prediction <ref type="bibr" target="#b20">[21]</ref>. The difference between both evaluation approaches is rela

6]</ref>, available on the Immune Epitope Database (IEDB)<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b16">[17]</ref>, and is split into a non-overlapping training (BD2009) and


l performance. We provide, amongst others, evaluation results on the recently published HPV dataset <ref type="bibr" target="#b8">[9]</ref>, demonstrating an excellent performance, which strongly sugg tides derived from two human paillomavirus 16 (HPV16) proteins binding to seven HLA class I alleles <ref type="bibr" target="#b8">[9]</ref>. Peptides were considered as binders if they had IC 50 -valu The scores for literature approaches were calculated based on peptide-wise predictions provided in <ref type="bibr" target="#b8">[9]</ref>. NetMHC tools, MHCFlurry, SMMPMBEC and consensus and our sco used to train MHC prediction tools. This applies to the recently released HPV binding affinity data <ref type="bibr" target="#b8">[9]</ref>. However, in this benchmark, it is not possible to disentang predictive performance only based on AUC ROC. We report the performance of all models considered in <ref type="bibr" target="#b8">[9]</ref> and our tools measured by AUC ROC in Tab. III, where we used >[9]</ref> and our tools measured by AUC ROC in Tab. III, where we used the predictions provided by <ref type="bibr" target="#b8">[9]</ref>. Our USMPep tools show an excellent prediction performance.
reported in Appendix A.1.</p><p>End-to-End Entity Linking (EL) For EL, we reproduce the setting of <ref type="bibr" target="#b27">Kolitsas et al. (2018)</ref> using the same in-domain and out-of-doma ntity mentions (a span contained in d j ) and e i â E its corresponding entity in the KB. Following <ref type="bibr" target="#b27">Kolitsas et al. (2018)</ref>, we considered only mentions that have e tics for 10k steps and we do model selection on the validation set. Again, following previous works <ref type="bibr" target="#b27">(Kolitsas et al., 2018)</ref>, we considered only mentions that have

rget="#b30">Le &amp; Titov, 2018;</ref><ref type="bibr" target="#b37">Logeswaran et al., 2019;</ref><ref type="bibr" target="#b4">Broscheit, 2019;</ref><ref type="bibr" target="#b64">Wu et al., 2019</
pensive when E is very large (e.g., Wikipedia has â¼6M entities). Hence, we exploit Beam Search (BS, <ref type="bibr" target="#b60">Sutskever et al., 2014)</ref>, an established approximate decoding st
rget="#b30">Le &amp; Titov, 2018;</ref><ref type="bibr" target="#b37">Logeswaran et al., 2019;</ref><ref type="bibr" target="#b4">Broscheit, 2019;</ref><ref type="bibr" target="#b64">Wu et al., 2019</
a standard seq2seq objective, i.e., maximizing the output sequence likelihood with teacher forcing <ref type="bibr" target="#b59">(Sutskever et al., 2011;</ref><ref type="bibr">2014)</ref> and regula
statistics for 10k steps and we do model selection on the validation set. Following previous works <ref type="bibr" target="#b66">(Yamada et al., 2016;</ref><ref type="bibr" target="#b16">Ganea &amp;
g using Natural Questions <ref type="bibr" target="#b29">(Kwiatkowski et al., 2019)</ref>, HotpotQA <ref type="bibr" target="#b70">(Yang et al., 2018c)</ref>, TriviaQA <ref type="bibr" target="#b24">(
a standard seq2seq objective, i.e., maximizing the output sequence likelihood with teacher forcing <ref type="bibr" target="#b59">(Sutskever et al., 2011;</ref><ref type="bibr">2014)</ref> and regula
discourse representation structure parsing <ref type="bibr" target="#b36">(Liu et al., 2018)</ref>  <ref type="bibr" target="#b28">(Konstas et al., 2017)</ref>. In these works a structured representat
sequence-to-sequence problems has been explored for different problems, including semantic parsing <ref type="bibr" target="#b54">(Rongali et al., 2020)</ref>, semantic role labelling <ref type="bibr
ef type="bibr" target="#b19">(Hoffart et al., 2012)</ref>, N3-Reuters-128 (R128), N3-RSS-500 (R500) <ref type="bibr" target="#b51">(RÃ¶der et al., 2014)</ref>, and OKE challenge 2015 and 2016 (OKE15 an

i et al., 2019)</ref>, HotpotQA <ref type="bibr" target="#b70">(Yang et al., 2018c)</ref>, TriviaQA <ref type="bibr" target="#b24">(Joshi et al., 2017)</ref>, ELI5 <ref type="bibr" target="#b12">(Fan
t al., 2020)</ref>.</p><p>Although there has been extensive previous work on entity retrieval (e.g. <ref type="bibr" target="#b18">Hoffart et al., 2011;</ref><ref type="bibr" target="#b48">Piccinno &a -entity from Wikipedia. Then, for the in-domain scenario, we fine-tune using the AIDA-CoNLL dataset <ref type="bibr" target="#b18">(Hoffart et al., 2011)</ref>. For the out-of-domain scenario, we eval e KB (i.e., Wikipedia) and we used their candidate sets with the additions of the table computed by <ref type="bibr" target="#b18">Hoffart et al. (2011)</ref>.</p><p>Training We pre-trained GENRE on a
generate entity names. This architecture has been shown to retain factual knowledge to some extent <ref type="bibr" target="#b45">(Petroni et al., 2019)</ref> and language translation skills <ref typ
rget="#b30">Le &amp; Titov, 2018;</ref><ref type="bibr" target="#b37">Logeswaran et al., 2019;</ref><ref type="bibr" target="#b4">Broscheit, 2019;</ref><ref type="bibr" target="#b64">Wu et al., 2019</
sequence-to-sequence problems has been explored for different problems, including semantic parsing <ref type="bibr" target="#b54">(Rongali et al., 2020)</ref>, semantic role labelling <ref type="bibr
ists of five tasks that use the same Wikipedia dump as a knowledge source: fact checking with FEVER <ref type="bibr" target="#b62">(Thorne et al., 2018)</ref>; open domain question answering using Nat
a standard seq2seq objective, i.e., maximizing the output sequence likelihood with teacher forcing <ref type="bibr" target="#b59">(Sutskever et al., 2011;</ref><ref type="bibr">2014)</ref> and regula
i et al., 2019)</ref>, HotpotQA <ref type="bibr" target="#b70">(Yang et al., 2018c)</ref>, TriviaQA <ref type="bibr" target="#b24">(Joshi et al., 2017)</ref>, ELI5 <ref type="bibr" target="#b12">(Fan
r" target="#b21">Huang et al., 2015;</ref><ref type="bibr" target="#b30">Le &amp; Titov, 2018;</ref><ref type="bibr" target="#b37">Logeswaran et al., 2019;</ref><ref type="bibr" target="#b4">Broscheit oftmax over all entities is very expensive, hence current solutions need to subsample negative data <ref type="bibr" target="#b37">(Logeswaran et al., 2019;</ref><ref type="bibr" target="#b25">Karpukh
rchy which have shown an advantageous performance over paradigms using user-item interactions alone <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>.</p><p>Generall representation involves extensive and unscalable computation with the adjacent matrix of the graph. <ref type="bibr" target="#b18">[19]</ref> learns a hierarchical representation of graphs by decompos nd becomes prevailing in several scenarios such as link prediction, e-commerce recommendation, etc, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>. There are some of our proposed method, which fixes the number of user levels to 2. The parameter of CGNN refers to <ref type="bibr" target="#b18">[19]</ref>. â¢ DIN: A popular deep neural network method without graph
ed learning for computing meaningful and interpretable clusters on input graphs. On the other hand, <ref type="bibr" target="#b21">[22]</ref> proposes an approach that automatically constructs an easy such as link prediction, e-commerce recommendation, etc, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>. There are some recent works that learn hierarchical graph raph representation is e-commerce taxonomy for offering a personalized dynamic shopping navigation. <ref type="bibr" target="#b21">[22]</ref> illstrates a topic-driven hierarchical taxonomy based on u </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments and Results</head><p>SHOAL <ref type="bibr" target="#b21">[22]</ref> is Alibaba's current topic-driven taxonomy solution deploy iveness, we compare our proposed method with Alibaba's current topic-driven taxonomy solution SHOAL <ref type="bibr" target="#b21">[22]</ref>. In the parameter setting, we set the level number of the
ently flat and do not learn hierarchical representations of graphs. On one hand, it demonstrates in <ref type="bibr" target="#b19">[20]</ref> that hierarchical representations of graphs can be combine ng GNNs with different clustering processes. In particular, the recently proposed approach DIFFPOOL <ref type="bibr" target="#b19">[20]</ref>, a differentiable graph pooling module that can generate h ing to be effective in graph classification tasks, in addition to a user's individual embedding. In <ref type="bibr" target="#b19">[20]</ref>, authors make some efforts in effectively co-training two world e-commerce tasks of such large scale, including <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b19">[20]</ref>. Our baseline algorithms are as follows:</p><p>â¢ CGNN: A g

node embeddings by passing, transforming, and aggregating node feature information across the graph <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b25">[26]</ref>. The generated
ollaborative Filtering</head><p>Another line of research <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> exploits the user-item interaction graph to infer user pre

node embeddings of non-linear interactions in tasks such as node classification and link prediction <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b16">[17]</ref>. In particular, <
rmance over paradigms using user-item interactions alone <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>.</p><p>Generally speaking, GNN methods are inherently flat



rmance over paradigms using user-item interactions alone <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>.</p><p>Generally speaking, GNN methods are inherently flat
on Rate (CVR), personalized recommendation list, and so on <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Precisely predicting a user preference in such a complex en

rmance over paradigms using user-item interactions alone <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>.</p><p>Generally speaking, GNN methods are inherently flat


s in tasks such as node classification and link prediction <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b16">[17]</ref>. In particular, <ref type="bibr" target="#b17">[18]</ref>
g assumes that behaviorally similar users would exhibit similar preference on items, and vice versa <ref type="bibr" target="#b6">[7]</ref>. As a result, users and items are vectorized as embeddings t
rmance over paradigms using user-item interactions alone <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>.</p><p>Generally speaking, GNN methods are inherently flat

t="#b29">[30]</ref>. On the other hand, some researchers <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> illustrate a user's community-level embedding to be effect
node embeddings of non-linear interactions in tasks such as node classification and link prediction <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b16">[17]</ref>. In particular, <
node embeddings by passing, transforming, and aggregating node feature information across the graph <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b25">[26]</ref>. The generated



tiveness of a query q for a topic t k can be derived from r(q, t k ) = pop(q, t k ) â¢ con(q, t k ), <ref type="bibr" target="#b13">(14)</ref> in which pop(q, t k ) and con(q, t k ) are the popularity

number of queries. To make the topic more interpretive, we follow the similar strategy described in <ref type="bibr" target="#b36">[37]</ref> to find the most representative query as the description f
s in tasks such as node classification and link prediction <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b16">[17]</ref>. In particular, <ref type="bibr" target="#b17">[18]</ref>

e="bibr" target="#b7">Garg et al., 2017;</ref><ref type="bibr" target="#b19">May et al., 2010;</ref><ref type="bibr" target="#b38">Zhao et al., 2018;</ref><ref type="bibr" target="#b27">Rudinger et al
ef type="bibr" target="#b4">(Collins, 2005;</ref><ref type="bibr" target="#b17">Madison, 2009;</ref><ref type="bibr" target="#b9">Gillespie, 2016)</ref>, they are asked to tag the example with the sin
anguage processing research has recently been driven by the use of large pretrained language models <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b16">Liu et al., ww.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate three widely used MLMs: BERT Base <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, RoBERTa Large <ref type="bibr" target="#b metric bias aligns well with crowd judgements. <ref type="bibr" target="#b26">Rozado (2020)</ref>   <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> and ELMo <ref type="bibr" target="#b24">(P which U.S. stereotypical biases are present in large pretrained masked language models such as BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>. The dataset consists of 1,508 examples th

ontribution. learn and use these biases <ref type="bibr" target="#b2">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan et al., 2017;</ref><ref type="bibr" target="#b7">Garg et al., </head><p>Measuring Bias Bias in natural language processing has gained visibility in recent years. <ref type="bibr" target="#b3">Caliskan et al. (2017)</ref> introduce a dataset for evaluating gender
arget="#b2">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan et al., 2017;</ref><ref type="bibr" target="#b7">Garg et al., 2017;</ref><ref type="bibr" target="#b19">May et al., 201
b36">(Williams et al., 2018)</ref> or 2-3 sentence story openings taken from examples in ROCStories <ref type="bibr" target="#b20">(Mostafazadeh et al., 2016)</ref>. To encourage crowdworkers to write awn from either MultiNLI <ref type="bibr" target="#b36">(Williams et al., 2018)</ref> or ROCStories <ref type="bibr" target="#b20">(Mostafazadeh et al., 2016)</ref>.</p></div> <div xmlns="http://www.t


embeddings extracted from them, have been shown to * Equal contribution. learn and use these biases <ref type="bibr" target="#b2">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan e >Mitigating Bias There has been prior work investigating methods for mitigating bias in NLP models. <ref type="bibr" target="#b2">Bolukbasi et al. (2016)</ref> propose reducing gender bias in word emb l., 2018)</ref>.</p><p>Discussing Bias Upon surveying 146 NLP papers that analyze or mitigate bias, <ref type="bibr" target="#b2">Blodgett et al. (2020)</ref> provide recommendations to guide such res
bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b16">Liu et al., 2019;</ref><ref type="bibr" target="#b14">Lan et al., 2020)</ref>. However, these models are trained on minimal 19)</ref>, RoBERTa Large <ref type="bibr" target="#b16">(Liu et al., 2019)</ref>, and ALBERT XXL-v2 <ref type="bibr" target="#b14">(Lan et al., 2020)</ref>. These models have shown good performance on
ce and is not structured for human communication (e.g., unlike words).</p><p>Several recent studies <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" ta be used with various pretext tasks. In this paper, we follow a simple instance discrimination task <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" ta 8">[29]</ref>. Contrastive learning is at the core of several recent works on unsupervised learning <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" ta </ref>. Overall, all three mechanisms benefit from a larger K. A similar trend has been observed in <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b55">56]</ref> under the memory b t tasks can be based on some form of contrastive loss functions. The instance discrimination method <ref type="bibr" target="#b60">[61]</ref> is related to the exemplar-based task <ref type="bibr" tar +/Ï ) K i=0 exp(qâ¢ki/Ï )<label>(1)</label></formula><p>where Ï is a temperature hyper-parameter per <ref type="bibr" target="#b60">[61]</ref>. The sum is over one positive and K negative samples. Intu these networks to downstream tasks.</p><p>Another mechanism is the memory bank approach proposed by <ref type="bibr" target="#b60">[61]</ref> (Figure <ref type="figure">2b</ref>). A memory bank consis ver the past epoch and thus are less consistent. A momentum update is adopted on the memory bank in <ref type="bibr" target="#b60">[61]</ref>. Its momentum update is on the representations of the same igning a new pretext task, we use a simple one mainly following the instance discrimination task in <ref type="bibr" target="#b60">[61]</ref>, to which some recent works <ref type="bibr" target="#b62" ="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b1">2]</ref> are related.</p><p>Following <ref type="bibr" target="#b60">[61]</ref>, we consider a query and a key as a positive pair if they ose last fully-connected layer (after global average pooling) has a fixed-dimensional output (128-D <ref type="bibr" target="#b60">[61]</ref>). This output vector is normalized by its L2-norm <ref typ (128-D <ref type="bibr" target="#b60">[61]</ref>). This output vector is normalized by its L2-norm <ref type="bibr" target="#b60">[61]</ref>. This is the representation of the query or key. The tempe or key. The temperature Ï in Eqn.( <ref type="formula" target="#formula_0">1</ref>) is set as 0.07 <ref type="bibr" target="#b60">[61]</ref>. The data augmentation setting follows <ref type="bibr" ta f>) is set as 0.07 <ref type="bibr" target="#b60">[61]</ref>. The data augmentation setting follows <ref type="bibr" target="#b60">[61]</ref>: a 224Ã224-pixel crop is taken from a randomly resized ima ate of 0.03. We train for 200 epochs with the learning rate multiplied by 0.1 at 120 and 160 epochs <ref type="bibr" target="#b60">[61]</ref>, taking â¼53 hours training ResNet-50. For IG-1B, we use a ecause the positive key is in the same mini-batch). The network is ResNet-50.</p><p>The memory bank <ref type="bibr" target="#b60">[61]</ref> mechanism can support a larger dictionary size. But it is We hope an advanced pretext task will improve this. Beyond the simple instance discrimination task <ref type="bibr" target="#b60">[61]</ref>, it is possible to adopt MoCo for pretext tasks like maske 1">Here 58.0% is with InfoNCE and K=65536. We reproduce 54.3% when using NCE and K=4096 (the same as<ref type="bibr" target="#b60">[61]</ref>), close to 54.0% in<ref type="bibr" target="#b60">[61]</re when using NCE and K=4096 (the same as<ref type="bibr" target="#b60">[61]</ref>), close to 54.0% in<ref type="bibr" target="#b60">[61]</ref>.</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place sed on other forms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b35">36]</ref>, such as margin-bas specific pretext task. The input x q and x k can be images <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63]</ref>, patches <ref type= backbone, by default used in existing ResNet-based results <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar
his paper, we follow a simple instance discrimination task <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b1">2]</ref>: a query matches a ke et="#b28">[29,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" targe x k can be images <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63]</ref>, patches <ref type="bibr" target="#b45">[46]</ref>, or cont k can be identical <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63]</ref>, partially shared <ref type="bibr" target="#b45">[46,</ref> stance discrimination task in <ref type="bibr" target="#b60">[61]</ref>, to which some recent works <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b1">2]</ref> are related.</p><p>F tive pair if they originate from the same image, and otherwise as a negative sample pair. Following <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b1">2]</ref>, we take two random
tream tasks (e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref>). Next we compare M www.tei-c.org/ns/1.0"><head>A.5. Implementation: Semantic segmentation</head><p>We use an FCN-based <ref type="bibr" target="#b42">[43]</ref> structure. The backbone consists of the convolutional laye N and ReLU, and then a 1Ã1 convolution for perpixel classification. The total stride is 16 (FCN-16s <ref type="bibr" target="#b42">[43]</ref>). We set dilation = 6 in the two extra 3Ã3 convolutions, f
features, so our experiments are on controlled schedules, e.g., the 1Ã (â¼12 epochs) or 2Ã schedules <ref type="bibr" target="#b21">[22]</ref> for COCO, in contrast to 6Ãâ¼9Ã in <ref type="bibr" target= n the train2017 set (â¼118k images) and evaluate on val2017. The schedule is the default 1Ã or 2Ã in <ref type="bibr" target="#b21">[22]</ref>.</p><p>Results. Table <ref type="table" target="#tab_7">5<
derings <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">45]</ref>, tracking <ref type="bibr" target="#b58">[59]</ref> or segmenting objects <ref type="bibr" target="#b46">[47]< ontrastive loss functions can also be based on other forms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" tar b45">[46]</ref>. The networks f q and f k can be identical <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63]</ref>, partially shared <
e initialization for finetuning in downstream tasks (e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" tar
pre-defined categories (e.g., eight positions <ref type="bibr" target="#b12">[13]</ref>, color bins <ref type="bibr" target="#b63">[64]</ref>) by cross-entropy or margin-based losses. Other alternativ tive multiview coding (CMC) <ref type="bibr" target="#b55">[56]</ref> it is related to colorization <ref type="bibr" target="#b63">[64]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head toencoders <ref type="bibr" target="#b47">[48]</ref>, or cross-channel auto-encoders (colorization) <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>. Some pretext task

features, so our experiments are on controlled schedules, e.g., the 1Ã (â¼12 epochs) or 2Ã schedules <ref type="bibr" target="#b21">[22]</ref> for COCO, in contrast to 6Ãâ¼9Ã in <ref type="bibr" target= n the train2017 set (â¼118k images) and evaluate on val2017. The schedule is the default 1Ã or 2Ã in <ref type="bibr" target="#b21">[22]</ref>.</p><p>Results. Table <ref type="table" target="#tab_7">5<

s ResNeXt-50-32Ã8d <ref type="bibr" target="#b61">[62]</ref>. â  : Pre-training uses FastAutoAugment <ref type="bibr" target="#b39">[40]</ref> that is supervised by ImageNet labels.</p><p>patchified in
e initialization for finetuning in downstream tasks (e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" tar
t="#b23">[24]</ref>) between generative adversarial networks and noise-contrastive estimation (NCE) <ref type="bibr" target="#b27">[28]</ref>.</p><p>Pretext tasks. A wide range of pretext tasks have b >[61]</ref> is related to the exemplar-based task <ref type="bibr" target="#b16">[17]</ref> and NCE <ref type="bibr" target="#b27">[28]</ref>. The pretext task in contrastive predictive coding (CPC) <
s ResNeXt-50-32Ã8d <ref type="bibr" target="#b61">[62]</ref>. â  : Pre-training uses FastAutoAugment <ref type="bibr" target="#b39">[40]</ref> that is supervised by ImageNet labels.</p><p>patchified in
ng is most influential when serving as the initialization for finetuning in downstream tasks (e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" ta
Faster R-CNN <ref type="bibr" target="#b51">[52]</ref> with a backbone of R50-dilated-C5 or R50-C4 <ref type="bibr" target="#b31">[32]</ref> (details in appendix), with BN tuned, implemented in <ref 1.0"><head n="4.2.2">COCO Object Detection and Segmentation</head><p>Setup. The model is Mask R-CNN <ref type="bibr" target="#b31">[32]</ref> with the FPN <ref type="bibr" target="#b40">[41]</ref> or epochs) and 2Ã schedules on COCO. These schedules were inherited from the original Mask R-CNN paper <ref type="bibr" target="#b31">[32]</ref>, which could be suboptimal given later advance in the fiel
" target="#b46">[47]</ref> in videos, or clustering features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Contrastive learning vs. pretext tasks. Various pretex ssification.</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><ref type="bibr" target="#b3">4</ref> Our w2Ã and w4Ã models correspond to the "Ã8" and "Ã16" cases
="#b32">[33]</ref>, we also report its variants that are 2Ã and 4Ã wider (more channels), following <ref type="bibr" target="#b37">[38]</ref>. <ref type="foot" target="#foot_3">4</ref> We set K = 6553 ef type="bibr" target="#b3">4</ref> Our w2Ã and w4Ã models correspond to the "Ã8" and "Ã16" cases in<ref type="bibr" target="#b37">[38]</ref>, because the standard-sized ResNet is referred to as "Ã4" ef type="bibr" target="#b37">[38]</ref>, because the standard-sized ResNet is referred to as "Ã4" in<ref type="bibr" target="#b37">[38]</ref>.</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place
ormalization during fine-tuning: we fine-tune with BN that is trained (and synchronized across GPUs <ref type="bibr" target="#b48">[49]</ref>), instead of freezing it by an affine layer <ref type="bib
e initialization for finetuning in downstream tasks (e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" tar
t="#b23">[24]</ref>) between generative adversarial networks and noise-contrastive estimation (NCE) <ref type="bibr" target="#b27">[28]</ref>.</p><p>Pretext tasks. A wide range of pretext tasks have b >[61]</ref> is related to the exemplar-based task <ref type="bibr" target="#b16">[17]</ref> and NCE <ref type="bibr" target="#b27">[28]</ref>. The pretext task in contrastive predictive coding (CPC) <
/ref>; Rv50 is a reversible net <ref type="bibr" target="#b22">[23]</ref>, RX50 is ResNeXt-50-32Ã8d <ref type="bibr" target="#b61">[62]</ref>. â  : Pre-training uses FastAutoAugment <ref type="bibr" ta
academic communities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" tar /head><p>GCNs are showing great potential in various tasks <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" tar ted by recursively aggregating and transforming the representation vectors of its neighbor vertices <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" ta to sample a subset from the neighbor vertices of each vertex <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b18">18]</ref> as the new neighbors, specifically,</p><formula xml:id="for ling to alleviate receptive field expansion that effectively trades off accuracy and execution time <ref type="bibr" target="#b18">[18]</ref>. It is formulated as</p><formula xml:id="formula_4">a k v ing preprocessing <ref type="bibr" target="#b20">[20]</ref> or with random selection during runtime <ref type="bibr" target="#b18">[18]</ref>. Aggregation aggregates the features from its 1-hop neighb he execution time breakdown of GCN (GCN) <ref type="bibr" target="#b25">[25]</ref>, GraphSage (GSC) <ref type="bibr" target="#b18">[18]</ref>, and GINConv (GIN) <ref type="bibr" target="#b39">[39]</re
a synchronization after the processing of all vertices, the degree of parallelism will be degraded <ref type="bibr" target="#b17">[17]</ref>. On the contrary, the gather-based method can control the thermore, further challenges exist with 1) inefficient memory subsystem due to workload-agnosticism <ref type="bibr" target="#b17">[17]</ref>, 2) difficulty in data reuse like systolic arrays <ref typ ="bibr" target="#b22">22,</ref><ref type="bibr" target="#b33">33]</ref>. For example, Graphicionado <ref type="bibr" target="#b17">[17]</ref> is tailored for graph analtyics; while TPU <ref type="bibr lerate these workloads <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" tar
get="#b20">20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref> to solve problems including node classification <ref type=" <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b16">16]</ref>, graph clustering <ref type="bibr" target="#b44">[44]</ref>, and recommendation <ref type="bibr" target="#b12">[12]</r </p><formula xml:id="formula_1">S(v) = Sample k N(v) .</formula><p>(2) Sometimes, the Pool function <ref type="bibr" target="#b44">[44]</ref> follows the Combine function to transform the original gra s a general tool to realize hierarchical graph-level transformation for a broad set of input graphs <ref type="bibr" target="#b44">[44]</ref>. It can be inserted after the Combine function of any GCNs
a between computing units in CPUs as like TPU <ref type="bibr" target="#b22">[22]</ref> and Eyeriss <ref type="bibr" target="#b8">[8]</ref>. Thus, the energyhungry data accesses to cache introduce hig et="#b8">[8]</ref>. Thus, the energyhungry data accesses to cache introduce high energy consumption <ref type="bibr" target="#b8">[8]</ref>. For GPUs, although they are well optimized for neural netwo . Besides, it is hard to efficiently reuse the highly reusable parameter data between compute units <ref type="bibr" target="#b8">[8]</ref>.</p><p>GPUs are inherently optimized for compute-intensive w hitecture designs are proposed to accelerate these workloads <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" targe
applied before the Aggregate function to sample a subset from the neighbor vertices of each vertex <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b18">18]</ref> as the new neighbors ynamic generation while the latter ones are predefined and can be read from off-chip memory like in <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b20">20]</ref>. To reduce the laten
plement the reuse of the highly reusable parameter data between computing units in CPUs as like TPU <ref type="bibr" target="#b22">[22]</ref> and Eyeriss <ref type="bibr" target="#b8">[8]</ref>. Thus, to improve the processing parallelism and data reuse, we adopt the well-known systolic array design <ref type="bibr" target="#b22">[22]</ref> and modify it to be compatible with GCNs. A Weight Buffer sticism <ref type="bibr" target="#b17">[17]</ref>, 2) difficulty in data reuse like systolic arrays <ref type="bibr" target="#b22">[22]</ref>, and 3) expensive on-line preprocessing for workload reorg Graphicionado <ref type="bibr" target="#b17">[17]</ref> is tailored for graph analtyics; while TPU <ref type="bibr" target="#b22">[22]</ref> focuses on the acceleration of neural networks. However, G target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b33">33]</ref>. For example, Graph
tching techniques to offset the processor-memory disparity by exploiting the regular access pattern <ref type="bibr" target="#b11">[11]</ref>, they fail to address the abundant dynamic and irregular d
</p><p>GPUs are inherently optimized for compute-intensive workloads with regular execution pattern <ref type="bibr" target="#b29">[29]</ref> such as neural networks, but handling the Aggregation phas
ive category of models to represent and process graph data <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" tar
able 4 :</head><label>4</label><figDesc>Dataset information<ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b42">42]</ref>.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#V
al efforts from both the industrial and academic communities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" tar e problems including node classification <ref type="bibr" target="#b25">[25]</ref>, link prediction <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b16">16]</ref>, graph clustering
the support of better branch prediction, which could potentially offer more IPC gains. Prior works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> have tried to a hose address is very predictable. Moreover, we do not make any modifications to the ISA. Gao et al. <ref type="bibr" target="#b12">[13]</ref> proposed a closely related work. They correlate the branch
level of accuracy.</p><p>Energy Per Access (EPA) for IMLI and LDBP were calculated using CACTI 6.0 <ref type="bibr" target="#b22">[23]</ref>. For IMLI, we model an ideal structure with a single port.
tween the outcome of the current branch and the history of previous branch outcomes.</p><p>PPM-like <ref type="bibr" target="#b26">[27]</ref> and TAGE <ref type="bibr" target="#b4">[5]</ref> achieve h

type="bibr" target="#b3">[4]</ref> or TAGEbased predictors <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b5">[6]</ref>. These predictors may use global and local history, and a st
e GAP benchmark suite <ref type="bibr" target="#b15">[16]</ref> and the SPEC2006 integer benchmarks <ref type="bibr" target="#b16">[17]</ref> having less than 95% prediction accuracy on our IMLI basel
hich could potentially offer more IPC gains. Prior works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> have tried to address different types of hardto-predict br sing data values as an input to the branch predictor improves the misprediction rate. Farooq et al. <ref type="bibr" target="#b13">[14]</ref> note that some hard-to-predict data-dependent branches man
hich could potentially offer more IPC gains. Prior works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> have tried to address different types of hardto-predict br sing data values as an input to the branch predictor improves the misprediction rate. Farooq et al. <ref type="bibr" target="#b13">[14]</ref> note that some hard-to-predict data-dependent branches man
pture the history of such branches competently, even with an unusually large predictor. Prior works <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> show that using
level of accuracy.</p><p>Energy Per Access (EPA) for IMLI and LDBP were calculated using CACTI 6.0 <ref type="bibr" target="#b22">[23]</ref>. For IMLI, we model an ideal structure with a single port.
CPU designs use either perceptron-based predictors <ref type="bibr" target="#b0">[1]</ref> [2] [3] <ref type="bibr" target="#b3">[4]</ref> or TAGEbased predictors <ref type="bibr" target="#b4">[5]</r
cal works include XLA <ref type="bibr" target="#b8">[9]</ref> (applicable to training as well), TVM <ref type="bibr" target="#b9">[10]</ref>, Glow <ref type="bibr" target="#b10">[11]</ref>, Tensor Com gebra operations and calls into backend-specific libraries for execution on different backends. TVM <ref type="bibr" target="#b9">[10]</ref> is an end-to-end compiler framework with Halide at the core
amming models. Typical DSL compilers include Halide <ref type="bibr" target="#b17">[18]</ref>, DLVM <ref type="bibr" target="#b18">[19]</ref>, Diesel <ref type="bibr" target="#b19">[20]</ref>, TIRAMIS
bservation representation. For 2D convolutions that are basically most time-consuming in CNN models <ref type="bibr" target="#b28">[29]</ref>, the observation O conv is 17-dimensional and defined as</
e <ref type="bibr" target="#b17">[18]</ref>, DLVM <ref type="bibr" target="#b18">[19]</ref>, Diesel <ref type="bibr" target="#b19">[20]</ref>, TIRAMISU <ref type="bibr" target="#b20">[21]</ref> and Tr
network (CNN) models <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> usually have high com
<ref type="bibr" target="#b5">[6]</ref>, Mxnet <ref type="bibr" target="#b6">[7]</ref> and PyTorch <ref type="bibr" target="#b7">[8]</ref> all provide built-in support for GPUs. However, these framew
ilar workflow to TVM, but was further extended to support encypted data with homomorphic encryption <ref type="bibr" target="#b32">[33]</ref>. Some other compiling frameworks (e.g. Tensor Comprehensio
e <ref type="bibr" target="#b17">[18]</ref>, DLVM <ref type="bibr" target="#b18">[19]</ref>, Diesel <ref type="bibr" target="#b19">[20]</ref>, TIRAMISU <ref type="bibr" target="#b20">[21]</ref> and Tr
e <ref type="bibr" target="#b17">[18]</ref>, DLVM <ref type="bibr" target="#b18">[19]</ref>, Diesel <ref type="bibr" target="#b19">[20]</ref>, TIRAMISU <ref type="bibr" target="#b20">[21]</ref> and Tr
e gradient update per data sample, PPO enables training with mini-batch updates. In addition, RLlib <ref type="bibr" target="#b27">[28]</ref> is used to implement our search algorithm (see Figure <ref ormula_9">L V F t the square-error loss V Î¸ (s t ) â V target t</formula><p>. Please refer to RLlib <ref type="bibr" target="#b27">[28]</ref> for more implementation details.</p></div> <div xmlns="htt
ion. PPO is a new family of policy gradient methods for RL. Unlike standard policy gradient methods <ref type="bibr" target="#b26">[27]</ref> performing one gradient update per data sample, PPO enable
udy of the distinguishing power of some GNN variants has been initiated. In two independent studies <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., ee-aware MPNNs that do use degree information. The former class of MPNNs covers the GNNs studied in <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., s bounded by the WL algorithm. This result can be seen as a slight generalisation of the results in <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., and degree-aware MPNNs matches that of the WL algorithm.</p><p>For anonymous MPNNs related to GNNs <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., or short) is well understood. Indeed, as we will shortly see, it follows from two independent works <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., L , as is indicated in Figure <ref type="figure" target="#fig_1">1</ref>. Proposition 5.2 (Based on <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., ymous MPNN by using an injection h : A s â Q. What follows is in fact an adaptation of Lemma 5 from <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> itself based on <ref type="bibr">[Zaheer et a gue that M anon is weaker than M WL . The proof is a trivial adaptation of the proofs of Lemma 2 in <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> and Theorem 5 in <ref type="bibr" target="#b1 t) w = (â â â (t) M ) w ,</formula><p>as desired.</p><p>We remark that we cannot use the results in <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> and <ref type="bibr" target="#b11">[Morris et x because the class M anon is more general than the class considered in those papers. The proofs in <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> and <ref type="bibr" target="#b11">[Morris et can be written in the form g (t) uâNG(v) h (t) (â â â (tâ1) u</formula><p>) , based on Lemma 5 from <ref type="bibr" target="#b15">[Xu et al., 2019]</ref>.</p><p>Suppose that Î½ Î½ Î½ : V â A s0 . It now
has been initiated. In two independent studies <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> the distinguishing power of GNNs is link rmer class of MPNNs covers the GNNs studied in <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref>, the latter class covers the GCNs <ref t thm.</p><p>For anonymous MPNNs related to GNNs <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> and degree-aware MPNNs related to GCNs < er the graph neural network architectures <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> defined by:</p><formula xml:id="formula_ tly see, it follows from two independent works <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> that the distinguishing power of aMPNNs et="#fig_1">1</ref>. Proposition 5.2 (Based on <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref>). The classes M anon and M WL are equall h between anonymous graph neural networks <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> and degree-aware graph neural networks < n as a slight generalisation of the results in <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019</ref>]. (ii) The distinguishing power of degree ep-by-step, by GNNs that use ReLU or sign as activation function. This result refines the result in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref> in that their simulation using the ReLU f of aMPNNs which are of special interest: those arising from the graph neural networks considered in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>. In Example 3.1 we established that such of the proofs of Lemma 2 in <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> and Theorem 5 in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>. We show, by induction on the number of r remark that we cannot use the results in <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> and <ref type="bibr" target="#b11">[Morris et al., 2019]</ref> as a black box because the class M anon i onsidered in those papers. The proofs in <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> and <ref type="bibr" target="#b11">[Morris et al., 2019]</ref> relate to graph neural networks which, in nd M WL , and thus also M anon , are equally strong. The following results are known. Theorem 5.5 ( <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>). (i) The classes M sign GNN and M WL are side effect, we obtain a simpler aMPNN M in M GNN , satisfying M WL M , than the one constructed in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>. The proof strategy is inspired by that o ef type="bibr" target="#b11">[Morris et al., 2019]</ref>. The proof strategy is inspired by that of <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>. Crucial in the proof is the notion of ro of two, at the cost of introducing an extra parameter p â A. Furthermore, the aMPNN constructed in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref> uses two distinct weight matrices in A (s By the induction hypothesis, these rows are linearly independent. Following the same argument as in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>, this implies that there exists an (s tâ1 ure the labelling "refines" â â â (t) MWL . To do so, we again follow closely the proof strategy of <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>. More specifically, we will need an analo entries having value 1 and whose size will be determined from the context. Lemma 5.9 (Lemma 9 from <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>). Let C â A mÃw be a matrix in which all ns of the non-zero entries in Âµ Âµ Âµ Regarding future work, we point out that, following the work of <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>, we fix the input graph in our analysis.
rt by describing message passing neural networks (MPNNs) for deep learning on graphs, introduced by <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref>. Roughly speaking, in MPNNs, vertex labels ave noticed that we use a different formalisation of MPNNs than the one given in the original paper <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref>. The first difference is that our MPNNs ar ied to v and u â N G (v). We add this function to avoid a certain ambiguity in the formalisation in <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref> on what precisely the message functions ca n what precisely the message functions can depend on. More specifically, only a dependence on â â â <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref>. In contrast, the examples given in <ref t â â â <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref>. In contrast, the examples given in <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref> use more information, such as the degree o explicit the information that message functions can use. It is readily verified that every MPNN of <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref> corresponds to an MPNN in our formalism.</ 4">(tâ1) v and â â â (tâ1) u is specified in</formula><p>The second difference is that the MPNNs in <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref> work on graphs that carry both vertex and lity by parameterising the update functions with the current label of the vertex itself, just as in <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref>.</p></div> <div xmlns="http://www.tei-c.or
</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref>, the latter class covers the GCNs <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, among others.</p><p>Contributions. For ><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> and degree-aware MPNNs related to GCNs <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, our main results are the following (se ) In contrast, we show that the WL algorithm cannot be simulated by popular GCNs such as those from <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>. This observation is somewhat contradic ef type="formula" target="#formula_134">18</ref>) at the end of Section 6) was already suggested in <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> based on empirical results. Our simulat er to capture numbers used by popular graph neural network architectures, such as roots of integers <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, we will work with algebraic numbers. A Example 3.3 (GCNs by <ref type="bibr">Kipf and Welling)</ref>. We consider the GCN architecture by <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, which in round t â¥ 1 computes</p><form ntial examples of degree-aware MPNNs are the popular graph convolutional networks, as introduced by <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>. These are of the form:</p><formula xml "formula_95">(t) MWL â â â â (t)</formula><p>M holds. We construct such an M originating from a GCN <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> defined in Example 3.3. That is, M is a ))</p><p>for some functions g : N + â A + and h : N + â A + . Example 6.5. The GCN architecture of <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> corresponds to graph neural networks of ormula><p>In particular, the class M dGNN 4 , corresponding to the popular graph neural networks of <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, is not stronger than M WL . We also re follow from our next result that a similar extension suffices to make the graph neural networks of <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> stronger than M WL .</p><p>We will now 0 â¤ p, q â¤ 1, is stronger than M WL . The introduction of the parameter p was already suggested in <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>. The proof of Proposition 6.2 shows tha p><p>Table <ref type="table">1</ref>: Various graph neural network formalisms, as reported in e.g., <ref type="bibr" target="#b8">[Kipf and Welling, 2017</ref><ref type="bibr" target="#b13">, Wu et al ><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> and degree-aware graph neural networks <ref type="bibr" target="#b8">[Kipf and</ref><ref type="bibr">Welling, 2017, Meltzer et al., 2019]</
tion-aware MPNNs using close connections with the LOCAL model for distributed graph computations of <ref type="bibr" target="#b0">[Angluin, 1980]</ref>. As such, MPNNs from <ref type="bibr" target="#b
ll need are indeed computable for algebraic numbers encoded using such a representation (see, e.g., <ref type="bibr" target="#b12">[Ouaknine and Worrell, 2014]</ref>).</p><p>Labelled graphs. Let G = (
5.</p><p>Example 3.1 (GNN architectures). We first consider the graph neural network architectures <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et rchitectures of graph neural networks. We again distinguish between anonymous graph neural networks <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et

cially rely on the fact that the set A of algebraic numbers is countable (see e.g., Theorem 2.2. in <ref type="bibr" target="#b6">[Jarvis, 2014]</ref>). As a consequence, also A s is countable.</p><p> of the Weisfeiler-Lehman algorithm for edge-labelled graphs as is done for graph neural networks in <ref type="bibr" target="#b6">[Jaume et al., 2019]</ref>.</p><p>We also want to compare our formalis

network formalisms, as reported in e.g., <ref type="bibr" target="#b8">[Kipf and Welling, 2017</ref><ref type="bibr" target="#b13">, Wu et al., 2019a</ref><ref type="bibr" target="#b10">, Meltzer et a

5.</p><p>Example 3.1 (GNN architectures). We first consider the graph neural network architectures <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et rchitectures of graph neural networks. We again distinguish between anonymous graph neural networks <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et
5.</p><p>Example 3.1 (GNN architectures). We first consider the graph neural network architectures <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et rchitectures of graph neural networks. We again distinguish between anonymous graph neural networks <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et
â â â (tâ1) is the same, the WL algorithm terminates. Termination is guaranteed in at most n steps <ref type="bibr" target="#b5">[Immerman and Lander, 1990]</ref>.</p></div> <div xmlns="http://www.te
</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref>, the latter class covers the GCNs <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, among others.</p><p>Contributions. For ><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> and degree-aware MPNNs related to GCNs <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, our main results are the following (se ) In contrast, we show that the WL algorithm cannot be simulated by popular GCNs such as those from <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>. This observation is somewhat contradic ef type="formula" target="#formula_134">18</ref>) at the end of Section 6) was already suggested in <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> based on empirical results. Our simulat er to capture numbers used by popular graph neural network architectures, such as roots of integers <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, we will work with algebraic numbers. A Example 3.3 (GCNs by <ref type="bibr">Kipf and Welling)</ref>. We consider the GCN architecture by <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, which in round t â¥ 1 computes</p><form ntial examples of degree-aware MPNNs are the popular graph convolutional networks, as introduced by <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>. These are of the form:</p><formula xml "formula_95">(t) MWL â â â â (t)</formula><p>M holds. We construct such an M originating from a GCN <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> defined in Example 3.3. That is, M is a ))</p><p>for some functions g : N + â A + and h : N + â A + . Example 6.5. The GCN architecture of <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> corresponds to graph neural networks of ormula><p>In particular, the class M dGNN 4 , corresponding to the popular graph neural networks of <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, is not stronger than M WL . We also re follow from our next result that a similar extension suffices to make the graph neural networks of <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> stronger than M WL .</p><p>We will now 0 â¤ p, q â¤ 1, is stronger than M WL . The introduction of the parameter p was already suggested in <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>. The proof of Proposition 6.2 shows tha p><p>Table <ref type="table">1</ref>: Various graph neural network formalisms, as reported in e.g., <ref type="bibr" target="#b8">[Kipf and Welling, 2017</ref><ref type="bibr" target="#b13">, Wu et al ><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> and degree-aware graph neural networks <ref type="bibr" target="#b8">[Kipf and</ref><ref type="bibr">Welling, 2017, Meltzer et al., 2019]</
5.</p><p>Example 3.1 (GNN architectures). We first consider the graph neural network architectures <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et rchitectures of graph neural networks. We again distinguish between anonymous graph neural networks <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et
ll need are indeed computable for algebraic numbers encoded using such a representation (see, e.g., <ref type="bibr" target="#b12">[Ouaknine and Worrell, 2014]</ref>).</p><p>Labelled graphs. Let G = (
5.</p><p>Example 3.1 (GNN architectures). We first consider the graph neural network architectures <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et rchitectures of graph neural networks. We again distinguish between anonymous graph neural networks <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et

â â â (tâ1) is the same, the WL algorithm terminates. Termination is guaranteed in at most n steps <ref type="bibr" target="#b5">[Immerman and Lander, 1990]</ref>.</p></div> <div xmlns="http://www.te
â â â (tâ1) is the same, the WL algorithm terminates. Termination is guaranteed in at most n steps <ref type="bibr" target="#b5">[Immerman and Lander, 1990]</ref>.</p></div> <div xmlns="http://www.te
ent obfuscation based defenses have proven vulnerable. In their recent seminal work, Athalye et al. <ref type="bibr" target="#b1">[2]</ref> presented a suite of strategies for estimating network gradi range of possible attacks, including those having successfully circumvented many previous defenses <ref type="bibr" target="#b1">[2]</ref>. Under these attacks, we compare the worst-case robustness o b32">33]</ref>.</p><p>Unfortunately, many of these methods have proven vulnerable by Athalye et al. <ref type="bibr" target="#b1">[2]</ref>, who introduced a set of attacking strategies, including a m .</p><p>Thus far, gradient obfuscation is generally considered vulnerable (and at least incomplete) <ref type="bibr" target="#b1">[2]</ref>. We revisit gradient obfuscation, and our defense demonstrat ased defense mechanisms seem plausible. Yet they are all fragile. As demonstrated by Athalye et al. <ref type="bibr" target="#b1">[2]</ref>, with random input transformation, adversarial examples can -removal transformation is also ineffective. One can use Backward Pass Differentiable Approximation <ref type="bibr" target="#b1">[2]</ref> to easily construct effective adversarial examples. In short ation (BPDA).</head><p>To circumvent the defense using non-differentiable operators, Athalye et al. <ref type="bibr" target="#b1">[2]</ref> introduced a strategy called Backward Pass Differentiable Ap attacking strategy. It causes the adversary to suffer from either exploding or vanishing gradients <ref type="bibr" target="#b1">[2]</ref>. Figure <ref type="figure" target="#fig_4">3</ref>-left show #b32">33]</ref>. Yet those defenses have been proven vulnerable under a reparameterization strategy <ref type="bibr" target="#b1">[2]</ref>. This strategy aims to find some differentiable function h(â¢ those defenses, the transformed image g(x) remain similar to the input x. Consequently, as shown in <ref type="bibr" target="#b1">[2]</ref>, those defenses can be easily circumvented by replacing g(â¢) mlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">BPDA Attack and the Variants</head><p>BPDA attack <ref type="bibr" target="#b1">[2]</ref>, as reviewed in Sec. 3.3, is a powerful way to estimate netw ck-box attacks, including the black-box transfer attack <ref type="bibr" target="#b29">[30]</ref>   <ref type="bibr" target="#b1">[2]</ref>. We evaluate other methods using the code provided in the or er all tested attacks. The methods indicated by a star (*) are those circumvented by Athalye et al. <ref type="bibr" target="#b1">[2]</ref>. We include their results therein as a reference. The other nsformations to the input. But they have been circumvented by Expectation Over Transformation (EOT) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. EOT attack first esti rsarial examples of f b directly (so that g(h(â¢)) = h(â¢)), without solving the optimization problem <ref type="bibr" target="#b1">(2)</ref>. We argue that finding such an h(â¢) is extremely hard. If h(
erate its training speed or further improve its robustness <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar ificantly lower training cost than the adversarial training<ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b50">51]</ref>, while offering str ess under all attacks.</p><p>based on adversarial training <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b53">54]</ref>. For all those methods, we use the implementation code prov
arget="#b48">49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>, motivated by the fact that the gradient information is ess rget="#b48">[49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Unfortunately, many of these methods have proven vul d previous methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>. In contrast to those aiming to purge input images of the a oding gradients have been exploited as a defense mechanism <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref>. Yet those defenses have been proven vulnerable under a rep that aims to erase adversarial noise from the input image <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>. In those defenses, the transformed image g(x) remain simil put transformation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref> have a performance overhead at inference time. For example, om a potentially adversarial input, by projecting it on a GAN-or PixelCNNrepresented image manifold <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref> or regularizing th c. 3.3). We refer this attack as BPDA-I attack. Under this attack, several previous defenses (e.g., <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta
arget="#b13">14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>, motivated by the f erators in the model <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Unfortunatel approach opposite to the intuition behind previous methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>. In contrast to tho et="#b13">[14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b6">7]</ref> that implement gradie ttack. Under this attack, several previous defenses (e.g., <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14]</ref>) have been nullifie ersarial defense methods that rely on input transformation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref> have a performance ojecting it on a GAN-or PixelCNNrepresented image manifold <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref> or regularizing the input image through total variation min ). Reparameterization. Vanishing and exploding gradients have been exploited as a defense mechanism <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref>. Yet those defense br" target="#b32">33]</ref> have a performance overhead at inference time. For example, PixelDefend <ref type="bibr" target="#b38">[39]</ref> projects the input to a pre-trained PixelCNN-represented m
arget="#b13">14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>, motivated by the f erators in the model <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Unfortunatel approach opposite to the intuition behind previous methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>. In contrast to tho et="#b13">[14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b6">7]</ref> that implement gradie ttack. Under this attack, several previous defenses (e.g., <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14]</ref>) have been nullifie ersarial defense methods that rely on input transformation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref> have a performance ojecting it on a GAN-or PixelCNNrepresented image manifold <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref> or regularizing the input image through total variation min ). Reparameterization. Vanishing and exploding gradients have been exploited as a defense mechanism <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref>. Yet those defense br" target="#b32">33]</ref> have a performance overhead at inference time. For example, PixelDefend <ref type="bibr" target="#b38">[39]</ref> projects the input to a pre-trained PixelCNN-represented m
get="#b53">54,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar
rget="#b43">[44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b15">16]</ref>) are proposed to improve the transferability of the adversa
. Another type of black-box attacking methods is query-based <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" targe
deed, a long line of works aims to obfuscate the network model's gradient with respect to its input <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" ta ef>. To this end, there exist a long line of works that apply random transformation to input images <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b13">14]</ref>, or employ stochas xample, by transforming the input image with certain randomness such as random resizing and padding <ref type="bibr" target="#b49">[50]</ref>, the network gradients become hard to estimate.</p><p>Anot DA has circumvented a handful of recent defense techniques <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar
et="#b49">[50,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" targe 9">[10]</ref> and nondifferentiable operators in the model <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" targe get="#b49">50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b6">7]</ref> that implement gradient obfuscation, in many defenses resulti
target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20]</ref>: they execute the model many times with different input in
ly work mostly deals with simple graphs with unlabeled edges, recently proposed relation-aware GNNs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> consider multi-rel pe="bibr" target="#b38">[39]</ref> consider direction and relation types, respectively. Also, R-GCN <ref type="bibr" target="#b37">[38]</ref> considers direction and relation types simultaneously. Rec e="bibr" target="#b44">45]</ref>. 5) R-GCN. This is a GNN-based method for modeling relational data <ref type="bibr" target="#b37">[38]</ref>. 6) MEAN. 7) LAN. These are GNN models for a out-of-knowle get="#b14">[15]</ref>. 3) R-GCN. The same model used in the entity prediction on KG completion task <ref type="bibr" target="#b37">[38]</ref>. 4) I-GEN. Inductive GEN, which only uses feature represen nds the graph convolutional network to consider multi-relational structure, by Schlichtkrull et al. <ref type="bibr" target="#b37">[38]</ref>.</p><p>6) MEAN. This model computes the embedding of entit s W r and W r to prevent the excessive increase in the model size, proposed in Schlichtkrull et al. <ref type="bibr" target="#b37">[38]</ref>: W r = B b=1 a r b V b , where B is the number of basis, a twork based methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref>. While they require ggested by several recent works on multi-relational graphs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref>, where directed rel ne in previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>, we measure the ranks in a filtered setting where we do not
tic matching based <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b44">45]</ref> and deep neural network based methods <ref type="bibr" targ ult. 4) ComplEx. Semantic matching based embedding methods <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b44">45]</ref>. 5) R-GCN. This is a GNN-based method for modeling relation ider asymmetric relations, where scores are differently measured based on the order of the entities <ref type="bibr" target="#b44">[45]</ref>.</p><p>5) R-GCN. This is a GNN-based method for modeling r
e to minimize the distance between correct and instance embeddings. Also, gradient-based approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref> learn shared param
recent works propose graph-based neural architectures, referred to as Graph Neural Networks (GNNs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. While early work mo
#b4">[5]</ref> .   Evaluation Metrics For evaluation, we use the ranking procedure by Bordes et al. <ref type="bibr" target="#b3">[4]</ref>. For a triplet with an unseen head entity, we replace its co
ckle this issue by learning few relations of seen entities <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b6">7]</ref>. Nonetheless, the problem becomes more difficult as knowledge
all the experiment, we use PyTorch <ref type="bibr" target="#b34">[35]</ref> and PyTorch geometric <ref type="bibr" target="#b10">[11]</ref> frameworks on a single Titan XP or a single GeForce RTX 20
evolving nature with new emerging entities. Several models <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b51">52]</ref> tackle this problem by utilizing extra information about th
<ref type="bibr" target="#b21">[22]</ref>, learnable attention-weighted combination of the features <ref type="bibr" target="#b46">[47]</ref>, to name a few. While most of the existing models work wit
large links to entities with only few links. To this end, we follow a scheme similar to Wang et al. <ref type="bibr" target="#b49">[50]</ref> and start to learn the model with many shot cases, then gr y simulating the data-rich circumstance under the meta-learning framework, motivated by Wang et al. <ref type="bibr" target="#b49">[50]</ref>. Specifically, we firstly meta-train our GENs with many sh
he nodes by aggregating the features from the neighboring nodes, that use recurrent neural networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref>, mean pooling with
ven their complementary strengths and weaknesses <ref type="bibr">(d'Avila Garcez et al. 2015;</ref><ref type="bibr" target="#b39">RocktÃ¤schel and Riedel 2017;</ref><ref type="bibr" target="#b47">Yang nowledge is compiled into a neural network architecture <ref type="bibr">(BoÅ¡njak et al. 2017;</ref><ref type="bibr" target="#b39">RocktÃ¤schel and Riedel 2017;</ref><ref type="bibr">Evans and Grefenst retability and generalisation, thereby inheriting the best of both worlds. Among such systems, NTPs <ref type="bibr" target="#b39">(RocktÃ¤schel and Riedel 2017;</ref><ref type="bibr" target="#b32">Min div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-end Differentiable Proving</head><p>NTPs <ref type="bibr" target="#b39">(RocktÃ¤schel and Riedel 2017)</ref> recursively build a neural networ l atoms in the body need to be proven, and because Z is a free variable with many possible bindings <ref type="bibr" target="#b39">(RocktÃ¤schel and Riedel 2017)</ref>. We consider two problems -given (F) log[1 â ntp K Î¸ ( F, d)]<label>(4)</label></formula><p>NTPs can also learn interpretable rules. <ref type="bibr" target="#b39">RocktÃ¤schel and Riedel (2017)</ref> show that it is possible to learn tes. Although NTPs can be used for learning interpretable rules from data, the solution proposed by <ref type="bibr" target="#b39">RocktÃ¤schel and Riedel (2017)</ref> can be quite inefficient, as the <ref type="bibr" target="#b21">(Kemp et al. 2006</ref>) -following the same evaluation protocols as <ref type="bibr" target="#b39">RocktÃ¤schel and Riedel (2017)</ref>. Furthermore, since GNTPs allows Prediction Results. We compare GNTPs and NTPs on a set of link prediction benchmarks, also used in <ref type="bibr" target="#b39">RocktÃ¤schel and Riedel (2017)</ref>. Results, presented in Table 1, s predicates, 111 unary predicates, 14 constants and 2565 true facts. We follow the protocol used by <ref type="bibr" target="#b39">RocktÃ¤schel and Riedel (2017)</ref> and split every dataset into trai ww.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">For consistency, we use the same notation as<ref type="bibr" target="#b39">RocktÃ¤schel and Riedel (2017)</ref>.</note> 			<note xmlns="http://ww s parallel inference to be implemented very efficiently on GPU. This optimisation is also present in<ref type="bibr" target="#b39">RocktÃ¤schel and Riedel (2017)</ref>.</note> 			<note xmlns="http://ww <note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">Results reported in<ref type="bibr" target="#b39">RocktÃ¤schel and Riedel (2017)</ref> were calculated with an incorrect 158 facts about the neighbourhood of countries, and the location of countries and subregions. As in <ref type="bibr" target="#b39">RocktÃ¤schel and Riedel (2017)</ref>, we randomly split countries into
till very effective Bag of Embeddings model <ref type="bibr" target="#b45">(White et al. 2015;</ref><ref type="bibr" target="#b0">Arora, Liang, and Ma 2017)</ref> showing that, even in this case, A re
" target="#b47">Yang, Yang, and Cohen 2017;</ref><ref type="bibr">Evans and Grefenstette 2018;</ref><ref type="bibr" target="#b43">Weber et al. 2019)</ref>. While symbolic models can generalise well f

nd latent features of the graph <ref type="bibr" target="#b34">(Nickel, Jiang, and Tresp 2014;</ref><ref type="bibr" target="#b30">Minervini et al. 2016)</ref>. Lastly, our work is related to <ref typ
ets and Evaluation Protocols. We report the results of experiments on benchmark datasets -Countries <ref type="bibr" target="#b3">(Bouchard, Singh, and Trouillon 2015)</ref>, Nations, UMLS, and Kinshi .org/ns/1.0"><head>Countries, UMLS, Nations</head><p>Countries Countries is a dataset introduced by <ref type="bibr" target="#b3">Bouchard, Singh, and Trouillon (2015)</ref> for testing reasoning capa
in a variety of frameworks, including Natural Logic (MacCartney and Manning 2007), Semantic Parsing <ref type="bibr" target="#b2">(Bos 2008)</ref>, Natural Language Inference and Recognising Textual E
nd latent features of the graph <ref type="bibr" target="#b34">(Nickel, Jiang, and Tresp 2014;</ref><ref type="bibr" target="#b30">Minervini et al. 2016)</ref>. Lastly, our work is related to <ref typ
architectures, for this work we opted for a simple but still very effective Bag of Embeddings model <ref type="bibr" target="#b45">(White et al. 2015;</ref><ref type="bibr" target="#b0">Arora, Liang,
n approximate nearest neighbour data structures for sparsifying read operations in memory networks. <ref type="bibr" target="#b38">Riedel et al. (2013)</ref> pioneered the idea of jointly embedding KB
d to be incomplete, ambiguous, and noisy, impairing the application of standard deductive reasoners <ref type="bibr" target="#b17">(Huang, van Harmelen, and ten Teije 2005)</ref>.</p><p>A rich and bro

y with the set of rules released by <ref type="bibr" target="#b15">Guo et al. (2016)</ref>. WordNet <ref type="bibr" target="#b29">(Miller 1995</ref>) is a lexical knowledge base for the English langu
stette 2018)</ref>. Orthogonally, even when accurate, such methods cannot explain given predictions <ref type="bibr" target="#b26">(Lipton 2018)</ref>.</p><p>A promising strategy for overcoming these
stette 2018)</ref>. Orthogonally, even when accurate, such methods cannot explain given predictions <ref type="bibr" target="#b26">(Lipton 2018)</ref>.</p><p>A promising strategy for overcoming these
nce GNTPs allows to experiment on significantly larger datasets, we also report results on the WN18 <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref> Baselines. On benchmark datasets, we compar ecall Curve <ref type="bibr" target="#b9">(Davis and Goadrich 2006)</ref> (AUC-PR), MRR, and HITS@m <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref>.</p></div> <div xmlns="http://www.tei-c.org Y). children(X, Y) :parents(Y, X). spouse(X, Y) :spouse(Y, X).</p><p>We also evaluate GNTP on WN18 <ref type="bibr" target="#b1">(Bordes et al. 2013) and</ref><ref type="bibr">WN18RR (Dettmers et al.
nce GNTPs allows to experiment on significantly larger datasets, we also report results on the WN18 <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref> Baselines. On benchmark datasets, we compar ecall Curve <ref type="bibr" target="#b9">(Davis and Goadrich 2006)</ref> (AUC-PR), MRR, and HITS@m <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref>.</p></div> <div xmlns="http://www.tei-c.org Y). children(X, Y) :parents(Y, X). spouse(X, Y) :spouse(Y, X).</p><p>We also evaluate GNTP on WN18 <ref type="bibr" target="#b1">(Bordes et al. 2013) and</ref><ref type="bibr">WN18RR (Dettmers et al.
" target="#b47">Yang, Yang, and Cohen 2017;</ref><ref type="bibr">Evans and Grefenstette 2018;</ref><ref type="bibr" target="#b43">Weber et al. 2019)</ref>. While symbolic models can generalise well f
rule induction mechanisms, and to approaches combining observable and latent features of the graph <ref type="bibr" target="#b34">(Nickel, Jiang, and Tresp 2014;</ref><ref type="bibr" target="#b30">M


<p>The main focus of Artificial Intelligence is building systems that exhibit intelligent behaviour <ref type="bibr" target="#b25">(Levesque 2014)</ref>. Notably, Natural Language Understanding (NLU)
get="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. The recognition process has become a necessary part of man aved first, then the recognition tasks are conducted on text <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31]</ref>. Given the plain text of an academic homepage, we aim to re ding, i.e., S â R nÃd e . Following state-of-the-art methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31]</ref>, we use GloVe <ref type="bibr" target="#b18">[19]</ref> to ods have been developed to address these problems. The state-of-the-art for publication recognition <ref type="bibr" target="#b30">[31]</ref> uses a Bi-LSTM-CRF based model to learn the page-level and l language processing methods. For example, state-of-the-art techniques for publication recognition <ref type="bibr" target="#b30">[31]</ref> and for person names recognition <ref type="bibr" target=" ifferent methods to capture the position patterns. The state-of-the-art for publication recognition <ref type="bibr" target="#b30">[31]</ref> trains webpage-level and line-level models together to cap nd Preprocessing. We use the same datasets used by the state-of-the-art for publication recognition <ref type="bibr" target="#b30">[31]</ref> and person name recognition <ref type="bibr" target="#b0"> ref type="table" target="#tab_0">2</ref> summarises the dataset statistics.</p><p>â¢ HomePub dataset <ref type="bibr" target="#b30">[31]</ref> contains the plain text of 2,087 homepages from different formation about the position patterns and person names. PAM also outperforms the hierarchical PubSE <ref type="bibr" target="#b30">[31]</ref> model, which can capture the positional diversity, by 3.64 a manual inspection of recognition results of state-ofthe-art models for the two tasks (i.e., PubSE <ref type="bibr" target="#b30">[31]</ref> and CogNN <ref type="bibr" target="#b0">[1]</ref>) and our ublications and 5,542 person names.</p><p>For publication string recognition, we observe that PubSE <ref type="bibr" target="#b30">[31]</ref> misrecognises strings about patents, grants, and research
arget="#b1">[2]</ref> based controller to update the memory, while Working Memory Network (W-MemNN) <ref type="bibr" target="#b16">[17]</ref> uses a multi-head attention <ref type="bibr" target="#b25" <ref type="bibr" target="#b25">[26]</ref>, which is similar to that used in Working Memory Network <ref type="bibr" target="#b16">[17]</ref>. Multi-head attention allows the model to jointly attend t
ne for person name recognition and the other for publication recognition. Specifically, we use LSTM <ref type="bibr" target="#b6">[7]</ref> as the RNN unit:</p><formula xml:id="formula_0">N = LST M (S
Our proposed PAM model outperforms the baselines that use standard NER models, such as Stanford NER <ref type="bibr" target="#b4">[5]</ref> and Bi-LSTM-CRF <ref type="bibr" target="#b8">[9]</ref>, by
emory updating process. Studies have exploited relative token position and importance in a sentence <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, whereas our algor nÃd z and emb is applied to reduce the space complexity when computing U l in multi-head attention <ref type="bibr" target="#b20">[21]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
Dynamic Memory Network (DMN) <ref type="bibr" target="#b11">[12]</ref> uses a gated recurrent unit <ref type="bibr" target="#b1">[2]</ref> based controller to update the memory, while Working Memory
e two tasks or simply concatenating the representation of publication and person name when training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>. However, our experi minimising the total loss of the two tasks, or using simple concatenation procedures when training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ ation procedure similar to Ma et al. <ref type="bibr" target="#b12">[13]</ref> and Hashimoto et al. <ref type="bibr" target="#b5">[6]</ref>. This model has a pipeline architecture for two jointly trai
emory updating process. Studies have exploited relative token position and importance in a sentence <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, whereas our algor nÃd z and emb is applied to reduce the space complexity when computing U l in multi-head attention <ref type="bibr" target="#b20">[21]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
et="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Such a recognition problem is simpler since the text in re
ne for person name recognition and the other for publication recognition. Specifically, we use LSTM <ref type="bibr" target="#b6">[7]</ref> as the RNN unit:</p><formula xml:id="formula_0">N = LST M (S
oited relative token position and importance in a sentence <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, whereas our algorithm focuses on relative line position an
lleviated by recognising information from academic homepages.</p><p>Models based on memory networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> are proposed for q ><head n="3.2">Alternatingly Updated Memory</head><p>Different from the traditional Memory Networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, which updates a m
representation of publication and person name when training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>. However, our experimental study shows that such a straight asks, or using simple concatenation procedures when training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>. However, our exper s resulted from replacing the AM and PM modules with a concatenation procedure similar to Ma et al. <ref type="bibr" target="#b12">[13]</ref> and Hashimoto et al. <ref type="bibr" target="#b5">[6]</re
Dynamic Memory Network (DMN) <ref type="bibr" target="#b11">[12]</ref> uses a gated recurrent unit <ref type="bibr" target="#b1">[2]</ref> based controller to update the memory, while Working Memory
target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar
revious studies on academic homepages usually use rule-based <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref> or a hybrid of machine learning and rule-based methods <ref ased methods <ref type="bibr" target="#b2">[3]</ref> on the HTML DOM trees of webpages. Yang and Ho <ref type="bibr" target="#b29">[30]</ref> use heuristic rules to locate the publications in a DOM tr
target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar
representation of publication and person name when training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>. However, our experimental study shows that such a straight asks, or using simple concatenation procedures when training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>. However, our exper s resulted from replacing the AM and PM modules with a concatenation procedure similar to Ma et al. <ref type="bibr" target="#b12">[13]</ref> and Hashimoto et al. <ref type="bibr" target="#b5">[6]</re
et="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Such a recognition problem is simpler since the text in re
Our proposed PAM model outperforms the baselines that use standard NER models, such as Stanford NER <ref type="bibr" target="#b4">[5]</ref> and Bi-LSTM-CRF <ref type="bibr" target="#b8">[9]</ref>, by
e two tasks or simply concatenating the representation of publication and person name when training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>. However, our experi minimising the total loss of the two tasks, or using simple concatenation procedures when training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ ation procedure similar to Ma et al. <ref type="bibr" target="#b12">[13]</ref> and Hashimoto et al. <ref type="bibr" target="#b5">[6]</ref>. This model has a pipeline architecture for two jointly trai
lleviated by recognising information from academic homepages.</p><p>Models based on memory networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> are proposed for q ><head n="3.2">Alternatingly Updated Memory</head><p>Different from the traditional Memory Networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, which updates a m
t-flips in <ref type="bibr" target="#b7">[8]</ref>. However, a newly proposed Bit-Flip Attack (BFA) <ref type="bibr" target="#b16">[17]</ref> whose progressive bit searching algorithm can successfully ial Weight Attack</head><p>The bit-flip based adversarial weight attack, aka. Bit-Flip Attack (BFA) <ref type="bibr" target="#b16">[17]</ref>, is an adversarial attack variant which performs weight fa loss increment. Thus, the bit searching in iteration i can be formulated as an optimization process <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_0">max { Bi l } L f x; { Bi l e acceleration of modern AI applications. To clarify, we use the same threat model as in prior work <ref type="bibr" target="#b16">[17]</ref>, which is listed in Table <ref type="table">1</ref>.</p></ cted in Fig. <ref type="figure" target="#fig_2">2</ref>, the progressive bit search proposed in BFA <ref type="bibr" target="#b16">[17]</ref> is prone to identify vulnerable bit in the weight whose ab .   BFA Configuration. To evaluate the effectiveness of the proposed defense methods, the code from <ref type="bibr" target="#b16">[17]</ref> is utilized with further modification. The number of bit-f trials. Note that, all the quantized DNN reported hereafter still uses the uniform quantizer as in <ref type="bibr" target="#b16">[17]</ref>, but with quantization-aware training instead of post-trai ns/1.0"><head n="5.3.">Comparison of Alternative Defense Methods</head><p>Adversarial weight attack <ref type="bibr" target="#b16">[17]</ref> is a recently developed security threat model for modern D Trained adversarial defense <ref type="bibr" target="#b14">[15]</ref> with strong weight attack BFA <ref type="bibr" target="#b16">[17]</ref>. Again, adversarial input defense fails to defend BFA, req
(||W + l â E(W + l )|| 2 + ||W â l â E(W â l )|| 2 )</formula><p>piece-wise clustering penalty term <ref type="bibr" target="#b5">(6)</ref> where Î» is the clustering coefficient to tune the strength o les for training and test respectively. The data augmentation technique is identical as reported in <ref type="bibr" target="#b5">[6]</ref>. The ResNet-20 <ref type="bibr" target="#b5">[6]</ref> and V tation technique is identical as reported in <ref type="bibr" target="#b5">[6]</ref>. The ResNet-20 <ref type="bibr" target="#b5">[6]</ref> and VGG-11 <ref type="bibr" target="#b19">[20]</ref> are the re xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ResNet-20<ref type="bibr" target="#b5">[6]</ref> with 8-bit weight. Bit-flips NBF for BFA (meanÂ±std):11.2 Â± 1 xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effect of Network Width. The ResNet-20<ref type="bibr" target="#b5">[6]</ref> with different width configuration (1Ã,2Ã and 4Ã) are report
(||W + l â E(W + l )|| 2 + ||W â l â E(W â l )|| 2 )</formula><p>piece-wise clustering penalty term <ref type="bibr" target="#b5">(6)</ref> where Î» is the clustering coefficient to tune the strength o les for training and test respectively. The data augmentation technique is identical as reported in <ref type="bibr" target="#b5">[6]</ref>. The ResNet-20 <ref type="bibr" target="#b5">[6]</ref> and V tation technique is identical as reported in <ref type="bibr" target="#b5">[6]</ref>. The ResNet-20 <ref type="bibr" target="#b5">[6]</ref> and VGG-11 <ref type="bibr" target="#b19">[20]</ref> are the re xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ResNet-20<ref type="bibr" target="#b5">[6]</ref> with 8-bit weight. Bit-flips NBF for BFA (meanÂ±std):11.2 Â± 1 xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effect of Network Width. The ResNet-20<ref type="bibr" target="#b5">[6]</ref> with different width configuration (1Ã,2Ã and 4Ã) are report
ion on the identified vulnerable weight bits can be physically conducted by Row-Hammer Attack (RHA) <ref type="bibr" target="#b18">[19]</ref>. Meanwhile, the DNN under defense has higher resistance ag ances have brought up the vulnerability issue of data stored in DRAM, where Row-Hammer Attack (RHA) <ref type="bibr" target="#b18">[19]</ref> has been shown to maliciously flip the memory bits in DRAM
tion technique, such as dropout, is also used to enhance the resistance against adversarial example <ref type="bibr" target="#b22">[23]</ref>. As shown in Table <ref type="table" target="#tab_3">4</re
ue, which converts the weights from 32-bit floating-point to {-1,+1} binary format encoded by 1-bit <ref type="bibr" target="#b17">[18]</ref>. Here, the binarizationaware training is leveraged as a de
on technique.</p><p>Increasing model capacity. Prior works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref> have experimentally confirmed the resistance improvement agai >Analysis</head><p>Effect of Network Width. In prior works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>, enhance the capacity of target DNN via increasing the layer sion, similar to the observations from adversarial example <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref>, increasing the network capacity by a large amount will enhan
-Normalization (BN) layer in the deep neural network is customary to accelerate the training of DNN <ref type="bibr" target="#b10">[11]</ref>, by normalizing the hidden features that forwarded along t
-Normalization (BN) layer in the deep neural network is customary to accelerate the training of DNN <ref type="bibr" target="#b10">[11]</ref>, by normalizing the hidden features that forwarded along t
on technique.</p><p>Increasing model capacity. Prior works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref> have experimentally confirmed the resistance improvement agai >Analysis</head><p>Effect of Network Width. In prior works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>, enhance the capacity of target DNN via increasing the layer sion, similar to the observations from adversarial example <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref>, increasing the network capacity by a large amount will enhan

preprocessing step and performs standard logistic regression to remove redundant computation. PPRGo <ref type="bibr" target="#b3">[4]</ref> uses Personalized PageRank to capture multi-hop neighborhood cency matrix Ã and feature matrix X in the precomputation phase, which requires O(LmF ) time. PPRGo <ref type="bibr" target="#b3">[4]</ref> calculates approximate the Personalized PageRank (PPR) matri in APPNP and PPRGo <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4]</ref>; 2) w = 0 for = 0, . . . , L â 1 and w L = 1, in which case P br" target="#b36">[37]</ref>, SGC and PPRGo (linear model) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>We implement GBP in PyTorch and C++, and employ initia
as social analysis <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>, biology <ref type="bibr" target="#b9">[10,</ref><ref type=
type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>, recommendation systems <ref type="bibr" target="#b35">[36]</ref>, and computer vision <ref type="bibr" target="#b38">[39,</
type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>, recommendation systems <ref type="bibr" target="#b35">[36]</ref>, and computer vision <ref type="bibr" target="#b38">[39,</
raph diffusion matrix, such as the Heat Kernel PageRank or the Personalized PageRank matrix. Mixhop <ref type="bibr" target="#b0">[1]</ref> mixes higher-order information to learn a wider class of rep
as social analysis <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>, biology <ref type="bibr" target="#b9">[10,</ref><ref type=
use seven open graph datasets with different size: three citation networks Cora, Citeser and Pubmed <ref type="bibr" target="#b24">[25]</ref>, a Protein-Protein interaction network PPI <ref type="bibr
rsonalized PageRank to capture multi-hop neighborhood information and uses a forward push algorithm <ref type="bibr" target="#b1">[2]</ref> to accelerate computation.</p><p>While the above methods sig ulates approximate the Personalized PageRank (PPR) matrix â =0 Î±(1 â Î±) Ã by forward push algorithm <ref type="bibr" target="#b1">[2]</ref> and then applies the PPR matrix to the feature matrix X to d
get="#b0">[1]</ref> mixes higher-order information to learn a wider class of representations. JKNet <ref type="bibr" target="#b31">[32]</ref> explores the relationship between node influence and rando /ref><ref type="bibr" target="#b36">37]</ref>, and the reverse transition probability matrix D â1 A <ref type="bibr" target="#b31">[32]</ref>, respectively. We can also manipulate the weights w to sim
type="bibr" target="#b35">[36]</ref>, and computer vision <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref>. Graph Convolutional
ommunity detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref>. For each node, we generate a sparse random feature by rand
n or essential characteristics. However, there is no such wide-accepted formal definition. Paulheim <ref type="bibr" target="#b5">[6]</ref> defined four criteria for knowledge graphs. Ehrlinger and W statistical relational learning <ref type="bibr" target="#b8">[9]</ref>, knowledge graph refinement <ref type="bibr" target="#b5">[6]</ref>, Chinese knowledge graph construction <ref type="bibr" targe
ns and proposed Definition 1 which emphasizes the reasoning engine of knowledge graphs. Wang et al. <ref type="bibr" target="#b7">[8]</ref> proposed a definition as a multi-relational graph in Definit ormation into an ontology and applies a reasoner to derive new knowledge. Definition 2 (Wang et al. <ref type="bibr" target="#b7">[8]</ref>). A knowledge graph is a multirelational graph composed of e "#b5">[6]</ref>, Chinese knowledge graph construction <ref type="bibr" target="#b9">[10]</ref>, KGE <ref type="bibr" target="#b7">[8]</ref> or KRL <ref type="bibr" target="#b10">[11]</ref>. The latter </ref> presented KRL in a linear manner, with a concentration on quantitative analysis. Wang et al. <ref type="bibr" target="#b7">[8]</ref> categorized KRL according to scoring functions, and specific s of auxiliary information for KRL such as attributes, relation path and logical rules. Wang et al. <ref type="bibr" target="#b7">[8]</ref> gave a detailed review on these information. This paper disc
/head><p>Previous survey papers on knowledge graphs mainly focus on statistical relational learning <ref type="bibr" target="#b8">[9]</ref>, knowledge graph refinement <ref type="bibr" target="#b5">[6
edge graph refinement <ref type="bibr" target="#b5">[6]</ref>, Chinese knowledge graph construction <ref type="bibr" target="#b9">[10]</ref>, KGE <ref type="bibr" target="#b7">[8]</ref> or KRL <ref ty
mentions may express the same relation under the supervision of a relational database. Mintz et al. <ref type="bibr" target="#b105">[106]</ref> adopted the distant supervision for relation classificat named entity tags, and conjunctive features. Traditional methods rely highly on feature engineering <ref type="bibr" target="#b105">[106]</ref>, with a recent approach exploring the inner correlation
wledge representation firstly dated back to 1956 as the concept of semantic net proposed by Richens <ref type="bibr" target="#b0">[1]</ref>, while the symbolic logic knowledge can go back to the Gener
"bibr" target="#b100">[101]</ref> capturing the correlation between cross-lingual attributes, KDCoE <ref type="bibr" target="#b101">[102]</ref> embedding multi-lingual entity descriptions via co-train
n example, h has a real part Re(h) and an imaginary part Im(h), i.e., h = Re(h) + i Im(h). Com-plEx <ref type="bibr" target="#b17">[18]</ref> firstly introduces complex vector space shown in Fig. <ref f type="bibr" target="#b33">[34]</ref>, DistMult <ref type="bibr" target="#b25">[26]</ref>, ComplEx <ref type="bibr" target="#b17">[18]</ref>, and ANALOGY <ref type="bibr" target="#b16">[17]</ref>.</p " target="#b15">[16]</ref>, manifold space <ref type="bibr" target="#b22">[23]</ref>, complex space <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bib sors. With the multiplicative operations, RESCAL <ref type="bibr" target="#b41">[42]</ref>, ComplEx <ref type="bibr" target="#b17">[18]</ref>, and SimplE <ref type="bibr" target="#b40">[41]</ref> also ="bibr" target="#b25">[26]</ref> can only model symmetric relations, while its extension of ComplEx <ref type="bibr" target="#b17">[18]</ref> managed to preserve antisymmetric relations, but involves c relations, but involves redundant computations <ref type="bibr" target="#b40">[41]</ref>. ComplEx <ref type="bibr" target="#b17">[18]</ref>, SimplE <ref type="bibr" target="#b40">[41]</ref>, and Tuc dependent </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex vector</head><p>ComplEx <ref type="bibr" target="#b17">[18]</ref> h Recently, two adversarial sampling are further proposed. ling strategies are summarized in Table <ref type="table" target="#tab_8">6</ref>. Trouillon et al. <ref type="bibr" target="#b17">[18]</ref> studied the number of negative samples generated per posit
contextualized GCN model over pruned dependency tree of sentences after path-centric pruning. AGGCN <ref type="bibr" target="#b122">[123]</ref> also applies GCN over dependency tree, but utilizes mult b123">[124]</ref> Pre-training + GCN + CNN + attention position embedding, relation hierarchy AGGCN <ref type="bibr" target="#b122">[123]</ref> GCN + multi-head attention + dense layers dependency tre
n example, h has a real part Re(h) and an imaginary part Im(h), i.e., h = Re(h) + i Im(h). Com-plEx <ref type="bibr" target="#b17">[18]</ref> firstly introduces complex vector space shown in Fig. <ref f type="bibr" target="#b33">[34]</ref>, DistMult <ref type="bibr" target="#b25">[26]</ref>, ComplEx <ref type="bibr" target="#b17">[18]</ref>, and ANALOGY <ref type="bibr" target="#b16">[17]</ref>.</p " target="#b15">[16]</ref>, manifold space <ref type="bibr" target="#b22">[23]</ref>, complex space <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bib sors. With the multiplicative operations, RESCAL <ref type="bibr" target="#b41">[42]</ref>, ComplEx <ref type="bibr" target="#b17">[18]</ref>, and SimplE <ref type="bibr" target="#b40">[41]</ref> also ="bibr" target="#b25">[26]</ref> can only model symmetric relations, while its extension of ComplEx <ref type="bibr" target="#b17">[18]</ref> managed to preserve antisymmetric relations, but involves c relations, but involves redundant computations <ref type="bibr" target="#b40">[41]</ref>. ComplEx <ref type="bibr" target="#b17">[18]</ref>, SimplE <ref type="bibr" target="#b40">[41]</ref>, and Tuc dependent </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex vector</head><p>ComplEx <ref type="bibr" target="#b17">[18]</ref> h Recently, two adversarial sampling are further proposed. ling strategies are summarized in Table <ref type="table" target="#tab_8">6</ref>. Trouillon et al. <ref type="bibr" target="#b17">[18]</ref> studied the number of negative samples generated per posit
l></formula><p>within the projected translation of P Ï (h) + P Ï (r) â P Ï (t). GarcÃ­a-DurÃ¡n et al. <ref type="bibr" target="#b137">[138]</ref> concatenated predicate token sequence and temporal token
type="bibr" target="#b95">[96]</ref> for the joint embedding of entity and text. Ganea and Hofmann <ref type="bibr" target="#b96">[97]</ref> proposed an attentive neural model over local context wind
n separately. These two tasks, however, can also be integrated into a unified framework. Han et al. <ref type="bibr" target="#b63">[64]</ref> proposed a joint learning framework with mutual attention ated knowledge acquisition KGC and relation extraction separately with different models. Han et al. <ref type="bibr" target="#b63">[64]</ref> put them under the same roof and proposed a joint learning
target="#b105">[106]</ref>, with a recent approach exploring the inner correlation between features <ref type="bibr" target="#b106">[107]</ref>. Deep neural networks is changing the representation lea
generation. By leveraging text corpus with the sentence bag of current entity denoted as b et , CPL <ref type="bibr" target="#b75">[76]</ref> proposes collaborative policy learning for path finding an â Î³) fr q (es, e T ) ht = LST M (htâ1, atâ1) M-Walk <ref type="bibr" target="#b74">[75]</ref> stâ1 <ref type="bibr" target="#b75">[76]</ref> Extractor be t , et r , e (et,r ,e ) â be t step-wise dela
rms reinforcement policyguided path reasoning over knowledge-graph-based useritem interaction. KGAT <ref type="bibr" target="#b168">[169]</ref> applies graph attention network over the collaborative k
noise to word embeddings for CNN-and RNN-based relation extraction under the MIML learning setting <ref type="bibr" target="#b124">[125]</ref>. DSGAN <ref type="bibr" target="#b125">[126]</ref> denoi "#b122">[123]</ref> GCN + multi-head attention + dense layers dependency tree Adversarial Wu et al. <ref type="bibr" target="#b124">[125]</ref> AT + PCNN/RNN + selective attention indicator encoding D
type="bibr" target="#b191">[192]</ref>, Freebase <ref type="bibr" target="#b192">[193]</ref>, NELL <ref type="bibr" target="#b193">[194]</ref> and Wikidata <ref type="bibr" target="#b194">[195]</ref> f type="bibr" target="#b192">[193]</ref> -â¼125,000,000 https://developers.google.com/freebase/ NELL <ref type="bibr" target="#b193">[194]</ref> -242,453 http://rtw.ml.cmu.edu/rtw/ Wikidata <ref type="
and phrase masking to integrate knowledge into language model, and is further improved by ERNIE 2.0 <ref type="bibr" target="#b152">[153]</ref> via continual multi-task learning. Rethinking about larg
. Side information such as class ties <ref type="bibr" target="#b111">[112]</ref> and relation path <ref type="bibr" target="#b112">[113]</ref> is also utilized. RNNs are also introduced, for example, " target="#b111">[112]</ref> CNN/PCNN + pairwise ranking position embedding, class ties Zeng et al. <ref type="bibr" target="#b112">[113]</ref> CNN + max pooling position embedding, relation path RNNs
o initialize the neural model by transfer learning from entity classification. The cooperative CORD <ref type="bibr" target="#b132">[133]</ref> ensembles text corpus and knowledge graph with external " target="#b131">[132]</ref> Transfer learning + sub-tree parse + attention position embedding CORD <ref type="bibr" target="#b132">[133]</ref> BiGRU + hierarchical attention + cooperative module posi
ning to noisy relation extraction and found that 9-layer CNNs have improved performance. Liu et al. <ref type="bibr" target="#b131">[132]</ref> proposed to initialize the neural model by transfer lear " target="#b130">[131]</ref> Residual convolution block + max pooling position embedding Liu et al. <ref type="bibr" target="#b131">[132]</ref> Transfer learning + sub-tree parse + attention position
elation extraction. Noticing that current NRE methods do not use very deep networks, Huang and Wang <ref type="bibr" target="#b130">[131]</ref> applied deep residual learning to noisy relation extract rmula_56">)</formula></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Others</head><p>ResCNN-x <ref type="bibr" target="#b130">[131]</ref> Residual convolution block + max pooling position embedd

er entity and provides more capable inference. Instead of using a binary reward function, Multi-Hop <ref type="bibr" target="#b73">[74]</ref> proposes a soft reward mechanism. To enable more effective "#b72">[73]</ref> (et, es, rq, eq) {(et, r, v)} I {et = eq} ht = LST M (htâ1, [atâ1; ot]) Multi-Hop <ref type="bibr" target="#b73">[74]</ref> (et, (es, rq)) r , e | et, r , e â G Î³ + (1 â Î³) fr q (es,
ibr" target="#b39">[40]</ref> explored connections among several bilinear models. Chandrahas et al. <ref type="bibr" target="#b170">[171]</ref> explored the geometric understanding of additive and mul
dge graph retrieval and graph attention mechanism under an encoder-decoder framework. DialKG Walker <ref type="bibr" target="#b186">[187]</ref> traverses symbolic knowledge graph to learn contextual t
er entity and provides more capable inference. Instead of using a binary reward function, Multi-Hop <ref type="bibr" target="#b73">[74]</ref> proposes a soft reward mechanism. To enable more effective "#b72">[73]</ref> (et, es, rq, eq) {(et, r, v)} I {et = eq} ht = LST M (htâ1, [atâ1; ot]) Multi-Hop <ref type="bibr" target="#b73">[74]</ref> (et, (es, rq)) r , e | et, r , e â G Î³ + (1 â Î³) fr q (es,
lation, head and the conjugate of tail. Inspired by Euler's identity e iÎ¸ = cos Î¸ + i sin Î¸, RotatE <ref type="bibr" target="#b18">[19]</ref> proposes a rotational model taking relation as a rotation <ref type="bibr" target="#b22">[23]</ref>, complex space <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and Gaussian di o, the manifold collapses into a point. With the introduction of rotational Hadmard product, RotatE <ref type="bibr" target="#b18">[19]</ref> can also capture inversion and composition patterns as wel /label></formula><p>where f G (h, r, t) is the scoring function of generator. Similarly, Sun et al. <ref type="bibr" target="#b18">[19]</ref> proposed self-adversarial negative sampling based on self /ref> generator embedding exp f G (h i ,r,t i )</p><p>j=1 exp f G h j ,r,t j</p><p>Self-adversarial <ref type="bibr" target="#b18">[19]</ref> current embedding exp Î±f h j ,r,t j i exp Î±f (h i ,r,t i )
ral Logical Reasoning</head><p>Logical rules are also studied for temporal reasoning. Chekol et al. <ref type="bibr" target="#b144">[145]</ref> explored Markov logic network and probabilistic soft log
edge graph refinement <ref type="bibr" target="#b5">[6]</ref>, Chinese knowledge graph construction <ref type="bibr" target="#b9">[10]</ref>, KGE <ref type="bibr" target="#b7">[8]</ref> or KRL <ref ty
of Markov logic networks and KRL methods, while handling the uncertainty of logic rules. ExpressGNN <ref type="bibr" target="#b84">[85]</ref> generalizes pLogicNet by tuning graph networks and embeddi combination of logic rules and embeddings, recent works <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref> combines Markov logic networks with KGE, aiming to leverag ted by simple bruteforce search, making it insufficient on large-scale knowledge graphs. ExpressGNN <ref type="bibr" target="#b84">[85]</ref> attempts to use NeuralLP <ref type="bibr" target="#b82">[8

Mix, a data augmentation method for generating sub-sequences along with their labels based on mixup <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref>. Under the active sequence labeling framew ns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequence Mixup in the Embedding Space</head><p>Mixup <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref> is a data augmentation method that impleme lation-based Regularizations Mixup implements interpolation in the input space to regularize models <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref>. Recently, the Mixup variants <ref type="b r limit rather than a too narrow score range setting.</p><p>For the mixing coefficient Î», we follow <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref> to sample it from Beta(Î±, Î±) and explore Î±
eling datasets for the named entity recognition (NER) and event detection tasks.</p><p>(1) CoNLL-03 <ref type="bibr" target="#b37">(Tjong Kim Sang and De Meulder, 2003)</ref> is a corpus for NER task.
also perform statistical significance tests for the above results. We use Wilcoxon Signed Rank Test <ref type="bibr" target="#b42">(Wilcoxon, 1992)</ref>, a non-parametric alternative to the paired t-
> measure the uncertainty of sequence models by the most likely predicted sequence. For a CRF model <ref type="bibr" target="#b21">(Lafferty et al., 2001)</ref>, we calculate Î³ with the predicted sequ
get="#b40">(Verma et al., 2019;</ref><ref type="bibr" target="#b36">Summers and Dinneen, 2019;</ref><ref type="bibr" target="#b14">Guo et al., 2019b)</ref> turn to perform interpolation in the hidden
proving model performance. For example, <ref type="bibr" target="#b38">Tomanek et al. (2007)</ref>; <ref type="bibr" target="#b34">Shen et al. (2017)</ref> select query samples based on data uncertain >Zhang et al., 2019;</ref><ref type="bibr" target="#b45">Yu et al., 2020)</ref> and active learning <ref type="bibr" target="#b34">(Shen et al., 2017;</ref><ref type="bibr" target="#b15">Hazra et al.,
This significance test fits our task as F-score is generally assumed to be not normally distributed <ref type="bibr" target="#b10">(Dror et al., 2018)</ref>, and nonparametric significance tests shoul
="bibr" target="#b25">Liu et al., 2018;</ref><ref type="bibr" target="#b11">Fang et al., 2017;</ref><ref type="bibr" target="#b12">Gao et al., 2019)</ref>. In this study, we mainly focus on active lea
recognition (NER) <ref type="bibr" target="#b22">(Lample et al., 2016)</ref>, and event extraction <ref type="bibr" target="#b44">(Yang et al., 2019)</ref>. Recently, neural sequential models <ref ty
ery policies Î³(â¢). Below, we introduce several representative policies.</p><p>Least Confidence (LC) <ref type="bibr" target="#b7">Culotta and McCallum (2005)</ref> measure the uncertainty of sequence sign. So far, various uncertainty-based <ref type="bibr" target="#b31">(Scheffer et al., 2001;</ref><ref type="bibr" target="#b7">Culotta and McCallum, 2005;</ref><ref type="bibr" target="#b19">Kim et
2018)</ref>. Recently, the Mixup variants <ref type="bibr" target="#b40">(Verma et al., 2019;</ref><ref type="bibr" target="#b36">Summers and Dinneen, 2019;</ref><ref type="bibr" target="#b14">Guo et
//www.tei-c.org/ns/1.0"><head>Introduction</head><p>Large-scale knowledge graphs (KGs) such as YAGO <ref type="bibr" target="#b10">(Suchanek, Kasneci, and Weikum 2007)</ref>, NELL <ref type="bibr" tar e="bibr" target="#b0">(Carlson et al. 2010), and</ref><ref type="bibr">Wikidata (VrandeÄiÄ and</ref><ref type="bibr" target="#b10">KrÃ¶tzsch 2014)</ref> usually represent facts in the form of relations ="bibr" target="#b9">Socher et al. 2013;</ref><ref type="bibr" target="#b11">Yang et al. 2015;</ref><ref type="bibr" target="#b10">Trouillon et al. 2016;</ref><ref type="bibr" target="#b8">Schlichtkru sume available, sufficient training instances for all relations.</p><p>In light of the above issue, <ref type="bibr" target="#b10">Xiong et al. (2018)</ref> proposed GMatching which introduces a local )</ref> have been proposed to learn entity embeddings by using relational information, Xiong et al. <ref type="bibr" target="#b10">(Xiong et al. 2018</ref>) demonstrated that explicitly encoding graph spectively. In order to measure the similarity between two vectors, we employ a recurrent processor <ref type="bibr" target="#b10">(Vinyals et al. 2016</ref>) f Âµ to perform multiple steps matching. T










unctions. To exploit the sparsity of DNNs to improve energy efficiency, many architectures, such as <ref type="bibr" target="#b24">[25]</ref>, are proposed to detect and skip the multiplications assoc
ge domain, time domain accumulation also can be realized by delay line based pulse width modulation <ref type="bibr" target="#b7">[8]</ref>. The idea of inmemory actually means replacing traditional d type="bibr" target="#b30">[31]</ref> Twin-8T <ref type="bibr" target="#b31">[32]</ref> Sandwich-RAM <ref type="bibr" target="#b7">[8]</ref> Wang et al. <ref type="bibr" target="#b32">[33]</ref> Thinke
mance is highly sensitive to data communication bandwidth and memory access latency. Reconfigurable <ref type="bibr" target="#b13">[14]</ref> and sparsity-aware <ref type="bibr" target="#b14">[15]</re ef type="bibr" target="#b7">[8]</ref> Wang et al. <ref type="bibr" target="#b32">[33]</ref> Thinker <ref type="bibr" target="#b13">[14]</ref> iFPNA <ref type="bibr" target="#b27">[28]</ref> This Work
nificant. Recently, algorithm researchers have turned more attention to 2?4-bit quantization models <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" t uantized ResNet-18 models. It can be found that the learned stepsize based 3-bit quantization (LSQ) <ref type="bibr" target="#b20">[21]</ref> on both activations and weights achieves the same accuracy 2">[23]</ref>.</p><p>The design of the LSQ quantizer in our implementation follows Esser's practice <ref type="bibr" target="#b20">[21]</ref>, but there are some differences in Most of the previous lo but there are some differences in Most of the previous low-bitwidth quantization schemes, including <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" t
have turned more attention to 2?4-bit quantization models <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>, which have the p the previous low-bitwidth quantization schemes, including <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>, do not quantize
N neg . To address these both steps, the proposed design adopts a content addressable memory (CAM, <ref type="bibr" target="#b25">[26]</ref>) based digital implementation instead of analog MAC circui a. Therefore, the XOR function is embedded in the 10T hit cell. In conventional NAND-type CAM cells <ref type="bibr" target="#b25">[26]</ref>, the pass transistors are NMOS only, making the high volta
N neg . To address these both steps, the proposed design adopts a content addressable memory (CAM, <ref type="bibr" target="#b25">[26]</ref>) based digital implementation instead of analog MAC circui a. Therefore, the XOR function is embedded in the 10T hit cell. In conventional NAND-type CAM cells <ref type="bibr" target="#b25">[26]</ref>, the pass transistors are NMOS only, making the high volta
essing-in-memory (PIM) architectures <ref type="bibr" target="#b3">[4]</ref> and silicon prototypes <ref type="bibr" target="#b4">[5]</ref> stand out recently due to their ultra good energy efficiency
method solving such an issue is known as tensorization or dataflows, whose details are discussed in <ref type="bibr" target="#b27">[28]</ref>. It is discovered that local storage and movement of parti . <ref type="bibr" target="#b32">[33]</ref> Thinker <ref type="bibr" target="#b13">[14]</ref> iFPNA <ref type="bibr" target="#b27">[28]</ref> This Work  power consuming part is the clock tree. The clo
essing-in-memory (PIM) architectures <ref type="bibr" target="#b3">[4]</ref> and silicon prototypes <ref type="bibr" target="#b4">[5]</ref> stand out recently due to their ultra good energy efficiency
lowbitwidth quantization. Binary/ternary quantization and binary weight quantization appeared first <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. However, the accu
><ref type="bibr" target="#b5">6]</ref>, previous model bias <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and position bias < niform data directly <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. In this paper, we f a recommender system, and that explicitly handling of the biases may help improve the performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" ta f>.</p><p>A recent work has shown that a uniform data can alleviate the previous model bias problem <ref type="bibr" target="#b15">[16]</ref>. But the uniform data is always few and expensive to colle us on how to solve the bias problems in a recommender system with a uniform data. Along the line of <ref type="bibr" target="#b15">[16]</ref>, we conduct empirical studies on a real advertising system /p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION</head><p>In a recent work <ref type="bibr" target="#b15">[16]</ref>, it is shown that a uniform (i.e., unbiased) data can alle
f stable feature distillation, which consists of a deep global balancing regression (DGBR) algorithm<ref type="bibr" target="#b12">[13]</ref>, a teacher network and a student network. The DGBR algorit ner is another promising direction.</p><p>Feature-Based Module. The current stable feature approach <ref type="bibr" target="#b12">[13]</ref> needs much time and computing resources. For implementing
t works are no longer limited to model structure, but considers sample-based knowledge distillation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. In this paper, we tudy rather than the past knowledge distillation approaches such as considering the level of sample <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref> and model structur
em may suffer from the bias problems such as popularity bias <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, previous model bias <ref type="bibr" target="#b8">[9,</ref><
posed framework is implemented using a lowrank model. We use biased matrix factorization (biasedMF) <ref type="bibr" target="#b11">[12]</ref> as the baseline, which is one of the most classic basic mo
knowledge distillation and recommender systems has also attracted the attention of the researchers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" ta
><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and position bias <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>. Previous studies ha .e., SVM PropDCG and DeepPropDCG. Some position bias estimation methods for ranking are proposed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>. IPS is one of the m
. Moreover, a uniform data is useful for counterfactual learning, such as imputation model learning <ref type="bibr" target="#b32">[33]</ref>, propensity computation <ref type="bibr" target="#b23">[24 methods, such as the doubly robust method <ref type="bibr" target="#b6">[7]</ref> and its variants <ref type="bibr" target="#b32">[33]</ref>. Moreover, they are also related to the types of knowledge st set (ð ð¡ð ). Following the settings of the previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref>, we employ two evaluation metrics that are widely used in i
et="#b23">[24]</ref> and modeling with uniform data directly <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar
rget="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. In this paper, we would like to study methods for better u
rget="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. In this paper, we would like to study methods for better u
knowledge distillation and recommender systems has also attracted the attention of the researchers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" ta
adaptive, collective and integrative) in transfer learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>In addition, we must keep in mind that the different
><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and position bias <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>. Previous studies ha .e., SVM PropDCG and DeepPropDCG. Some position bias estimation methods for ranking are proposed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>. IPS is one of the m
ntroduce the samples from ð ð¡ to help ð ð ? Inspired by modeling of heterogeneous implicit feedback <ref type="bibr" target="#b19">[20]</ref>, we add a confidence parameter to each sample of ð ð and ð
previous model bias <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and position bias <ref type="bibr" target="#b2">[3,</ref><r umptions about the data being missing not at random (MNAR) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>A recent work has shown that a uniform data can alle the statistics are described in Table <ref type="table" target="#tab_3">3</ref>.</p><p>â¢ Yahoo! R3 <ref type="bibr" target="#b16">[17]</ref>: This dataset contains ratings collected from two differen
previous model bias <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and position bias <ref type="bibr" target="#b2">[3,</ref><r umptions about the data being missing not at random (MNAR) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>A recent work has shown that a uniform data can alle the statistics are described in Table <ref type="table" target="#tab_3">3</ref>.</p><p>â¢ Yahoo! R3 <ref type="bibr" target="#b16">[17]</ref>: This dataset contains ratings collected from two differen
knowledge distillation and recommender systems has also attracted the attention of the researchers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" ta
em may suffer from the bias problems such as popularity bias <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, previous model bias <ref type="bibr" target="#b8">[9,</ref><
<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, previous model bias <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" targ
><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and position bias <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>. Previous studies ha .e., SVM PropDCG and DeepPropDCG. Some position bias estimation methods for ranking are proposed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>. IPS is one of the m
nsity computation <ref type="bibr" target="#b23">[24]</ref> and modeling with uniform data directly <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ idation set (ð ð£ð ), and the rest as test set (ð ð¡ð ). Following the settings of the previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref>, we employ two evalu istillation can be realized.</p><p>â¢ Causal Embedding Strategy (CausE). The causal embedding method <ref type="bibr" target="#b4">[5]</ref> first considers the scenario of training ð ð and ð ð¡ simulta
allenge to process massive remote sensing images. In our work, two lightweight attention mechanisms <ref type="bibr" target="#b16">[17]</ref> which contains spatial attention and channel attention are ts a convolution operation with 7 Ã 7 kernel size.</p><p>In this study, we follow the method of Woo <ref type="bibr" target="#b16">[17]</ref>to integrate the two attention mechanisms. First, we use ch



lassification, and threshold algorithms to segment an image <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>. However, these methods depend heavily on artificial design
ition by weighted sum the features of all other positions <ref type="bibr" target="#b14">[15]</ref> <ref type="bibr" target="#b15">[16]</ref>. Thus, it can model the longrange context information for
he difference of ground objects becomes further notable. To segment HRRSIs effectively, Chen et al. <ref type="bibr" target="#b7">[8]</ref> used a shuffle convolution neural network and found that the
nd then used conditional random fields to perform postclassification refinement. Kampffmeyer et al. <ref type="bibr" target="#b10">[11]</ref> used a weighted loss function to solve the problem of cate f the ISPRS Vaihingen dataset. We adopt the practice of <ref type="bibr" target="#b8">[9]</ref>[10] <ref type="bibr" target="#b10">[11]</ref> and do not report the accuracy of the clutter/background c
lassification, and threshold algorithms to segment an image <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>. However, these methods depend heavily on artificial design
ed methods have been regarded as a promising approach to solve image semantic segmentation problems <ref type="bibr" target="#b2">[3]</ref>[4] <ref type="bibr" target="#b4">[5]</ref>. For example, met
ution neural network and found that the method is effective in segmenting small objects. Liu et al. <ref type="bibr" target="#b8">[9]</ref> designed an hourglass-shaped network (HSN) network based on >Table I reports the semantic segmentation of the ISPRS Vaihingen dataset. We adopt the practice of <ref type="bibr" target="#b8">[9]</ref>[10] <ref type="bibr" target="#b10">[11]</ref> and do not rep
<p>Identifying compound-protein interaction (CPI) plays an import role in discovering hit compounds <ref type="bibr" target="#b39">(Vamathevan et al., 2019)</ref>. Conventional methods, such as struct
hitecture of TransformerCPI</head><p>The model we proposed is based on the transformer architecture <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref>, which was originally devised for neural
hed, such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, GPT-2 , Transformer-XL <ref type="bibr" target="#b6">(Dai et al., 2019)</ref> and XLnet <ref type="bibr" target="#b49">(Yan
l performance. From another perspective that regards compound structure as molecular graph, CPI-GNN <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref> and GraphDTA <ref type="bibr" target="#b pared our model on three previous benchmark datasets, Human dataset, Caenorhabditis elegans dataset <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref> and BindingDB dataset <ref type="bibr" t unique compounds and 2504 unique proteins and the training, valid and test sets are randomly split <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref>. BindingDB dataset contains 39 747 posit RF), L2-logistic (L2), support vector  machines (SVM), newly reported sequence-based models CPI-GNN <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref> and DrugVQA <ref type="bibr" target="#b5 ormation for these two datasets. We followed the same training and evaluating strategies as CPI-GNN <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref> and repeated with three different random _4">4 and 5</ref>. Since the implementation of KNN, RF, L2, SVM are not mentioned in the literature <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref>, these models are not compared on Bindin singer et al., 2012)</ref> and Human dataset <ref type="bibr" target="#b20">(Liu et al., 2015;</ref><ref type="bibr" target="#b38">Tsubaki et al., 2019)</ref>, where DUD-E dataset was collected with t
in the manner as expected. The hidden ligand bias issue has been reported in DUD-E and MUV datasets <ref type="bibr" target="#b34">(Sieg et al., 2019)</ref>, raising extensive concerns in the field of s <ref type="bibr" target="#b4">(Chen et al., 2019)</ref> and other models trained on DUD-E dataset <ref type="bibr" target="#b34">(Sieg et al., 2019)</ref> have been pointed out to make predictions m
in the manner as expected. The hidden ligand bias issue has been reported in DUD-E and MUV datasets <ref type="bibr" target="#b34">(Sieg et al., 2019)</ref>, raising extensive concerns in the field of s <ref type="bibr" target="#b4">(Chen et al., 2019)</ref> and other models trained on DUD-E dataset <ref type="bibr" target="#b34">(Sieg et al., 2019)</ref> have been pointed out to make predictions m
019)</ref>, GPT-2 , Transformer-XL <ref type="bibr" target="#b6">(Dai et al., 2019)</ref> and XLnet <ref type="bibr" target="#b49">(Yang et al., 2019)</ref>. Transformer is also applied in chemical re
pe="bibr" target="#b11">Gonen, 2012;</ref><ref type="bibr" target="#b14">Jacob and Vert, 2008;</ref><ref type="bibr" target="#b40">van Laarhoven et al., 2011;</ref><ref type="bibr" target="#b44">Wang
t="#b15">(Karimi et al., 2019)</ref>, and <ref type="bibr" target="#b9">Gao et al. (2018)</ref> and <ref type="bibr" target="#b51">Zheng et al. (2020)</ref> also treated compounds and proteins as sequ quence-based models CPI-GNN <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref> and DrugVQA <ref type="bibr" target="#b51">(Zheng et al., 2020)</ref> have been evaluated on these datasets. Gra 38">(Tsubaki et al., 2019)</ref> and repeated with three different random seeds followed by DrugVQA <ref type="bibr" target="#b51">(Zheng et al., 2020)</ref> to evaluate TransformerCPI, and Area Under
uch as DUD-E dataset <ref type="bibr" target="#b26">(Mysinger et al., 2012)</ref> and Human dataset <ref type="bibr" target="#b20">(Liu et al., 2015;</ref><ref type="bibr" target="#b38">Tsubaki et al. one class, and negative samples were generated by algorithms that may introduce undetectable noise <ref type="bibr" target="#b20">(Liu et al., 2015;</ref><ref type="bibr" target="#b26">Mysinger et al 007)</ref> and highly credible negative CPI samples obtained using a systematic screening framework <ref type="bibr" target="#b20">(Liu et al., 2015)</ref>. In detail, the human dataset contains 3369
uch as DUD-E dataset <ref type="bibr" target="#b26">(Mysinger et al., 2012)</ref> and Human dataset <ref type="bibr" target="#b20">(Liu et al., 2015;</ref><ref type="bibr" target="#b38">Tsubaki et al. one class, and negative samples were generated by algorithms that may introduce undetectable noise <ref type="bibr" target="#b20">(Liu et al., 2015;</ref><ref type="bibr" target="#b26">Mysinger et al 007)</ref> and highly credible negative CPI samples obtained using a systematic screening framework <ref type="bibr" target="#b20">(Liu et al., 2015)</ref>. In detail, the human dataset contains 3369
eins and 15 343 CPI among them.</p><p>Second, we constructed a Kinase dataset based on KIBA dataset <ref type="bibr" target="#b36">(Tang et al., 2014)</ref>. KIBA score was developed to combine variou remove inconsistency between different bioactivity types, which greatly reduced bias in the dataset <ref type="bibr" target="#b36">(Tang et al., 2014)</ref>. The KIBA dataset contains 467 targets and ed the suggested threshold KIBA value of 12.1 <ref type="bibr" target="#b14">(He et al., 2017;</ref><ref type="bibr" target="#b36">Tang et al., 2014)</ref> to divide dataset into a positive set and a
ibr" target="#b44">Wang et al., 2011;</ref><ref type="bibr" target="#b45">Wang and Zeng, 2013;</ref><ref type="bibr" target="#b47">Yamanishi et al., 2008)</ref>.</p><p>With the rapid development of de
eins and 15 343 CPI among them.</p><p>Second, we constructed a Kinase dataset based on KIBA dataset <ref type="bibr" target="#b36">(Tang et al., 2014)</ref>. KIBA score was developed to combine variou remove inconsistency between different bioactivity types, which greatly reduced bias in the dataset <ref type="bibr" target="#b36">(Tang et al., 2014)</ref>. The KIBA dataset contains 467 targets and ed the suggested threshold KIBA value of 12.1 <ref type="bibr" target="#b14">(He et al., 2017;</ref><ref type="bibr" target="#b36">Tang et al., 2014)</ref> to divide dataset into a positive set and a
ensive concerns in the field of drug design. Structure-based virtual screening, 3D-CNN-based models <ref type="bibr" target="#b4">(Chen et al., 2019)</ref> and other models trained on DUD-E dataset <r
learning task, and many novel and powerful pre-training models have been established, such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, GPT-2 , Transformer-XL <ref type="bibr" t
in the manner as expected. The hidden ligand bias issue has been reported in DUD-E and MUV datasets <ref type="bibr" target="#b34">(Sieg et al., 2019)</ref>, raising extensive concerns in the field of s <ref type="bibr" target="#b4">(Chen et al., 2019)</ref> and other models trained on DUD-E dataset <ref type="bibr" target="#b34">(Sieg et al., 2019)</ref> have been pointed out to make predictions m
pe="bibr" target="#b11">Gonen, 2012;</ref><ref type="bibr" target="#b14">Jacob and Vert, 2008;</ref><ref type="bibr" target="#b40">van Laarhoven et al., 2011;</ref><ref type="bibr" target="#b44">Wang
s, which were transformed into negative logarithm, pIC 50 , pK i and pEC 50 . Following early works <ref type="bibr" target="#b22">(Liu et al., 2007;</ref><ref type="bibr" target="#b43">Wan et al., 20
sets <ref type="bibr" target="#b30">(Qiu et al., 2020)</ref>, we used a gated convolutional network <ref type="bibr" target="#b7">(Dauphin et al., 2016)</ref> with Conv1D and gated linear unit instead and k is the patch size, r is the sigmoid function and is the element-wise product between matrices <ref type="bibr" target="#b7">(Dauphin et al., 2016)</ref>. The output of the gated convolutional ne
o calculate the final output. WideDTA <ref type="bibr">(O Â¨ztu Â¨rk et al., 2019)</ref> and Conv-DTI <ref type="bibr" target="#b19">(Lee et al., 2019)</ref> followed the similar idea, and WideDTA utili
ich are fixed during training process and contain less information than that of end-to-end learning <ref type="bibr" target="#b13">(Hamanaka et al., 2017;</ref><ref type="bibr" target="#b37">Tian et a
nitial design space and the output is a refined design space of simpler or better models. Following <ref type="bibr" target="#b20">[21]</ref>, we characterize the quality of a design space by sampling gn, elevated to the population level and guided via distribution estimates of network design spaces <ref type="bibr" target="#b20">[21]</ref>.</p><p>As a testbed for this paradigm, our focus is on exp essential to use a reliable comparison metric to guide our design process. Recently, the authors of <ref type="bibr" target="#b20">[21]</ref> proposed a methodology for comparing and analyzing populat c scenario).</p><p>We rely on the concept of network design spaces introduced by Radosavovic et al. <ref type="bibr" target="#b20">[21]</ref>. A design space is a large, possibly infinite, population esign space is a large, possibly infinite, population of model architectures. The core insight from <ref type="bibr" target="#b20">[21]</ref> is that we can sample models from a design space, giving r ce design. To evaluate and compare design spaces, we use the tools introduced by Radosavovic et al. <ref type="bibr" target="#b20">[21]</ref>, who propose to quantify the quality of a design space by a single ResNet-50 <ref type="bibr" target="#b7">[8]</ref> model at 4GF for 100 epochs.</p><p>As in <ref type="bibr" target="#b20">[21]</ref>, our primary tool for analyzing design space quality is th i-c.org/ns/1.0"><head>Appendix C: Optimization Settings</head><p>Our basic training settings follow <ref type="bibr" target="#b20">[21]</ref> as discussed in Â§3. To tune the learning rate lr and weigh initial design space and the output is a refined design space of simpler or better models. Following<ref type="bibr" target="#b20">[21]</ref>, we characterize the quality of a design space by sampling tp://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use the term design space following<ref type="bibr" target="#b20">[21]</ref>, rather than search space, to emphasize that we are not se ://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">Our training setup in Â§3 exactly follows<ref type="bibr" target="#b20">[21]</ref>. We use SGD with momentum of 0.9, mini-batch size of 128 o
while most mobile networks use longer schedules with various enhancements, such as deep supervision <ref type="bibr" target="#b15">[16]</ref>, Cutout <ref type="bibr" target="#b3">[4]</ref>, DropPath
<ref type="bibr" target="#b15">[16]</ref>, Cutout <ref type="bibr" target="#b3">[4]</ref>, DropPath <ref type="bibr" target="#b13">[14]</ref>, AutoAugment <ref type="bibr" target="#b1">[2]</ref>, and el><figDesc>Training enhancements to EFFICIENTNET-B0. Our EFFICIENTNET-B0 reproduction with DropPath<ref type="bibr" target="#b13">[14]</ref> and a 250 epoch training schedule (third row), achieves re
good models, e.g., <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" tar >19]</ref> and NAS <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>We emphasize
while most mobile networks use longer schedules with various enhancements, such as deep supervision <ref type="bibr" target="#b15">[16]</ref>, Cutout <ref type="bibr" target="#b3">[4]</ref>, DropPath
k design, popularized by NAS. NAS has proven to be an effective tool for finding good models, e.g., <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta ">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19]</ref> and NAS <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta
f type="bibr" target="#b25">[26]</ref>, ResNet <ref type="bibr" target="#b7">[8]</ref>, and ResNeXt <ref type="bibr" target="#b30">[31]</ref>. We start with a relatively unconstrained design space we f><ref type="bibr" target="#b27">28]</ref>, ResNet <ref type="bibr" target="#b7">[8]</ref>, ResNeXt <ref type="bibr" target="#b30">[31]</ref>, DenseNet <ref type="bibr" target="#b10">[11]</ref>, and M t.</p><p>Most of our experiments use the standard residual bottlenecks block with group convolution <ref type="bibr" target="#b30">[31]</ref>, shown in Figure <ref type="figure" target="#fig_2">4</ref ><p>Next, we compare REGNETX to standard RESNET <ref type="bibr" target="#b7">[8]</ref> and RESNEXT <ref type="bibr" target="#b30">[31]</ref> models. All of the models in this experiment come from the Desc>Figure 4. The X block is based on the standard residual bottleneck block with group convolution<ref type="bibr" target="#b30">[31]</ref>. (a) Each X block consists of a 1Ã1 conv, a 3Ã3 group conv ls lead to considerable improvements over standard RESNE(X)T <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref> models in all metrics. We highlight the improvements for fi
k design, popularized by NAS. NAS has proven to be an effective tool for finding good models, e.g., <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta ">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19]</ref> and NAS <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta
et <ref type="bibr" target="#b10">[11]</ref>, and Mo-bileNet <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref>. The design process behind these networks was largely manua design choices. Modern mobile networks often employ the inverted bottleneck (b &lt; 1) proposed in <ref type="bibr" target="#b24">[25]</ref> along with depthwise conv <ref type="bibr" target="#b0">[1 ork on finding better mobile networks via both manual design <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19]</ref> and NAS <ref type="
visual recognition tasks. Examples include LeNet <ref type="bibr" target="#b14">[15]</ref>, AlexNet <ref type="bibr" target="#b12">[13]</ref>, VGG <ref type="bibr" target="#b25">[26]</ref>, and ResNet c.org/ns/1.0"><head n="2.">Related Work</head><p>Manual network design. The introduction of AlexNet <ref type="bibr" target="#b12">[13]</ref> catapulted network design into a thriving research area. I


been proposed to this end, which compare networks' internal layers in addition to final predictions <ref type="bibr" target="#b15">(Jiao et al. 2019;</ref><ref type="bibr" target="#b21">Sun et al. 202 e teacher layers into m buckets with approximately the same sizes and pick only one layer from each <ref type="bibr" target="#b15">(Jiao et al. 2019;</ref><ref type="bibr">Sun et al. 2019)</ref>. Ther PKD is not the only model that utilizes internal layers' information. Other models such as TinyBERT <ref type="bibr" target="#b15">(Jiao et al. 2019)</ref> and MobileBERT <ref type="bibr" target="#b21
rnal layers in addition to final predictions <ref type="bibr" target="#b15">(Jiao et al. 2019;</ref><ref type="bibr" target="#b21">Sun et al. 2020</ref><ref type="bibr">Sun et al. , 2019))</ref>, but ther models such as TinyBERT <ref type="bibr" target="#b15">(Jiao et al. 2019)</ref> and MobileBERT <ref type="bibr" target="#b21">(Sun et al. 2020</ref>) also found it crucial for training competitiv
the student model may fail to provide high-quality results.</p><p>To tackle this problem, Wu et al. <ref type="bibr" target="#b1">(2020)</ref> proposed a combinatorial technique, called CKD. In their ation produced by the function F c given a subset of teacher layers indicated by A(j). In Wu et al. <ref type="bibr" target="#b1">(2020)</ref>, F c is implemented via a simple concatenation. Depending <p>The application of KD in NLP and NLU is not limited to the aforementioned models. Aguilar et al. <ref type="bibr" target="#b1">(2020)</ref> followed the same architecture as PKD but they introduced To this end, we run a grid search similar to <ref type="bibr">Sun et al. (2019)</ref> and Wu et al. <ref type="bibr" target="#b1">(2020)</ref>. For our experiments, the batch size is set to 32 and the



b22">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type="bibr" target="#b25">Wei et al. (2019)</ref> introduced a novel training procedure where t


the student model may fail to provide high-quality results.</p><p>To tackle this problem, Wu et al. <ref type="bibr" target="#b1">(2020)</ref> proposed a combinatorial technique, called CKD. In their ation produced by the function F c given a subset of teacher layers indicated by A(j). In Wu et al. <ref type="bibr" target="#b1">(2020)</ref>, F c is implemented via a simple concatenation. Depending <p>The application of KD in NLP and NLU is not limited to the aforementioned models. Aguilar et al. <ref type="bibr" target="#b1">(2020)</ref> followed the same architecture as PKD but they introduced To this end, we run a grid search similar to <ref type="bibr">Sun et al. (2019)</ref> and Wu et al. <ref type="bibr" target="#b1">(2020)</ref>. For our experiments, the batch size is set to 32 and the


layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type="bibr" target="#b16">Liu et al. (2019)</ref> investigated KD from another perspective. Ins
b22">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type="bibr" target="#b25">Wei et al. (2019)</ref> introduced a novel training procedure where t
onsider it as a complementary and generic add-on to enrich the training process of any neural model <ref type="bibr" target="#b11">(Furlanello et al. 2018)</ref>.</p><p>In KD, a student network (S) is
et al. ( <ref type="formula">2019</ref>) squeezed multiple translation engines into one transformer <ref type="bibr" target="#b22">(Vaswani et al. 2017</ref>) and showed that knowledge can be distille

y> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge distillation (KD) <ref type="bibr" target="#b5">(BuciluÇ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" tar w.tei-c.org/ns/1.0"><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type="bibr" target="#b5">(BuciluÇ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" tar

b22">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type="bibr" target="#b25">Wei et al. (2019)</ref> introduced a novel training procedure where t
et al. ( <ref type="formula">2019</ref>) squeezed multiple translation engines into one transformer <ref type="bibr" target="#b22">(Vaswani et al. 2017</ref>) and showed that knowledge can be distille
y> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge distillation (KD) <ref type="bibr" target="#b5">(BuciluÇ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" tar w.tei-c.org/ns/1.0"><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type="bibr" target="#b5">(BuciluÇ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" tar
y> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge distillation (KD) <ref type="bibr" target="#b5">(BuciluÇ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" tar w.tei-c.org/ns/1.0"><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type="bibr" target="#b5">(BuciluÇ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" tar


b22">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type="bibr" target="#b25">Wei et al. (2019)</ref> introduced a novel training procedure where t

layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type="bibr" target="#b16">Liu et al. (2019)</ref> investigated KD from another perspective. Ins
y> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge distillation (KD) <ref type="bibr" target="#b5">(BuciluÇ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" tar w.tei-c.org/ns/1.0"><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type="bibr" target="#b5">(BuciluÇ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" tar
b22">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type="bibr" target="#b25">Wei et al. (2019)</ref> introduced a novel training procedure where t
y> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge distillation (KD) <ref type="bibr" target="#b5">(BuciluÇ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" tar w.tei-c.org/ns/1.0"><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type="bibr" target="#b5">(BuciluÇ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" tar
the student model may fail to provide high-quality results.</p><p>To tackle this problem, Wu et al. <ref type="bibr" target="#b1">(2020)</ref> proposed a combinatorial technique, called CKD. In their ation produced by the function F c given a subset of teacher layers indicated by A(j). In Wu et al. <ref type="bibr" target="#b1">(2020)</ref>, F c is implemented via a simple concatenation. Depending <p>The application of KD in NLP and NLU is not limited to the aforementioned models. Aguilar et al. <ref type="bibr" target="#b1">(2020)</ref> followed the same architecture as PKD but they introduced To this end, we run a grid search similar to <ref type="bibr">Sun et al. (2019)</ref> and Wu et al. <ref type="bibr" target="#b1">(2020)</ref>. For our experiments, the batch size is set to 32 and the
s on Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b9">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018b;</ref><ref type="bibr" target="#b8">Gao et al., 201 large graphs, layer sampling techniques <ref type="bibr" target="#b9">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018b;</ref><ref type="bibr" target="#b26">Ying et al., 2 ly a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. <ref type="bibr" target="#b4">Chen et al. (2018b)</ref> and <ref type="bibr" target="#b14">Huang et he above edge sampler to perform layer sampling. Under the independent layer sampling assumption of <ref type="bibr" target="#b4">Chen et al. (2018b)</ref>, one would sample a connection u ( ) , v ( + b3">Chen et al. (2018a)</ref>. Point (2) is due to the better interlayer connectivity compared with <ref type="bibr" target="#b4">Chen et al. (2018b)</ref>, and unbiased minibatch estimator compared w s to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN <ref type="bibr" target="#b4">(Chen et al., 2018b)</ref> performs sampling from another perspective. , 2016)</ref>, 2. GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017)</ref>, 3. FastGCN <ref type="bibr" target="#b4">(Chen et al., 2018b)</ref>, 4. S-GCN <ref type="bibr" target="#b3">(Ch of each layer independently. This is similar to the treatment of layers independently by prior work <ref type="bibr" target="#b4">(Chen et al., 2018b;</ref><ref type="bibr" target="#b14">Huang et al., ip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods <ref type="bibr" target="#b4">(Chen et al., 2018b;</ref><ref type="bibr" target="#b14">Huang et al.,

s for PPI (0.995) and Reddit (0.970). * Equal contribution * The skip-connection design proposed by <ref type="bibr" target="#b14">Huang et al. (2018)</ref> does not have such "subset" requirement, an ve the softmax step which normalizes attention values within the same neighborhood, as suggested by <ref type="bibr" target="#b14">Huang et al. (2018)</ref>. See Appendix C.3.</p></div> 			</abstract> selected by one node in the next layer. <ref type="bibr" target="#b4">Chen et al. (2018b)</ref> and <ref type="bibr" target="#b14">Huang et al. (2018)</ref> further propose samplers to restrict the ne ize in all layers. However, the minibatches potentially become too sparse to achieve high accuracy. <ref type="bibr" target="#b14">Huang et al. (2018)</ref> improves FastGCN by an additional sampling (3) is due to the simple and trivially parallelizable pre-processing compared with the sampling of <ref type="bibr" target="#b14">Huang et al. (2018)</ref> and clustering of <ref type="bibr" target=" its neighbors in the training graph. The removal of softmax is also seen in the attention design of <ref type="bibr" target="#b14">Huang et al. (2018)</ref>. Note that during the minibatch training, G e="bibr" target="#b4">Chen et al., 2018b;</ref><ref type="bibr" target="#b8">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Huang et al., 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2 e="bibr" target="#b3">Chen et al., 2018a;</ref><ref type="bibr" target="#b8">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Huang et al., 2018)</ref> have been proposed for efficient minibatch tment of layers independently by prior work <ref type="bibr" target="#b4">(Chen et al., 2018b;</ref><ref type="bibr" target="#b14">Huang et al., 2018)</ref>. Consider a layer-( + 1) node v and a layer other hand, for some layer sampling methods <ref type="bibr" target="#b4">(Chen et al., 2018b;</ref><ref type="bibr" target="#b14">Huang et al., 2018)</ref>, extra modification to their samplers is re et al., 2018b)</ref>, 4. S-GCN <ref type="bibr" target="#b3">(Chen et al., 2018a)</ref>, 5. AS-GCN <ref type="bibr" target="#b14">(Huang et al., 2018)</ref>, and 6. ClusterGCN <ref type="bibr" target

r" target="#b9">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018b;</ref><ref type="bibr" target="#b8">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Huang et al., 20 bibr" target="#b26">Ying et al., 2018a;</ref><ref type="bibr" target="#b3">Chen et al., 2018a;</ref><ref type="bibr" target="#b8">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Huang et al., 20

r" target="#b9">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018b;</ref><ref type="bibr" target="#b8">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Huang et al., 20 bibr" target="#b26">Ying et al., 2018a;</ref><ref type="bibr" target="#b3">Chen et al., 2018a;</ref><ref type="bibr" target="#b8">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Huang et al., 20
Bruna et al. (2013)</ref>. Further, <ref type="bibr" target="#b16">Kipf &amp; Welling (2016)</ref>; <ref type="bibr" target="#b6">Defferrard et al. (2016)</ref> speed up graph convolution computation
"#b19">Leskovec &amp; Faloutsos, 2006;</ref><ref type="bibr" target="#b12">Hu &amp; Lau, 2013;</ref><ref type="bibr" target="#b20">Li et al., 2015)</ref>. In the experiments, we implement a regular ra
El-Haija et al., 2019)</ref> or even more complicated networks for the task of graph classification <ref type="bibr" target="#b27">(Ying et al., 2018b)</ref>, we replace the full adjacency matrix A wi
r" target="#b9">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018b;</ref><ref type="bibr" target="#b8">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Huang et al., 20 bibr" target="#b26">Ying et al., 2018a;</ref><ref type="bibr" target="#b3">Chen et al., 2018a;</ref><ref type="bibr" target="#b8">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Huang et al., 20
nd pursued in natural language processing <ref type="bibr" target="#b9">[10]</ref>, computer vision <ref type="bibr" target="#b16">[17]</ref>, and other domains. To date, the most powerful solution is e how to define (dis)similar instances.</p><p>Q2: Define (dis)similar instances. In computer vision <ref type="bibr" target="#b16">[17]</ref>, two random data augmentations (e.g., random crop, random d by the graph encoder, the final d-dimensional output vectors are then normalized by their L2-Norm <ref type="bibr" target="#b16">[17]</ref>.</p><p>A running example. We illustrate a running example ffectively build and maintain the dictionary, such as end-to-end (E2E) and momentum contrast (MoCo) <ref type="bibr" target="#b16">[17]</ref>. We discuss the two strategies as follows.</p><p>E2E sampl propagation. The parameters of f k (denoted by Î¸ k ) are not updated by gradient descent. He et al. <ref type="bibr" target="#b16">[17]</ref> propose a momentum-based update rule for Î¸ k . More formal the dictionary, such as memory bank <ref type="bibr" target="#b58">[59]</ref>. Recently, He et al. <ref type="bibr" target="#b16">[17]</ref> show that MoCo is a more effective option than memory bank >Contrastive loss mechanisms. The common belief is that MoCo has stronger expression power than E2E <ref type="bibr" target="#b16">[17]</ref>, and a larger dictionary size K always helps. We also obse r, the effect of a large dictionary size is not as significant as reported in computer vision tasks <ref type="bibr" target="#b16">[17]</ref>. For example, MoCo (K = 16384) merely outperforms MoCo (K in Table <ref type="table" target="#tab_6">5</ref> in the Appendix. Momentum. As mentioned in MoCo <ref type="bibr" target="#b16">[17]</ref>, momentum m plays a subtle role in learning high-quality r tasets. For US-Airport, the best performance is reached by m = 0.999, which is the desired value in <ref type="bibr" target="#b16">[17]</ref>, showing that building a consistent dictionary is importan brings better performance. Moreover, we do not observe the "training loss oscillation" reported in <ref type="bibr" target="#b16">[17]</ref> when setting m = 0. GCC (MoCo) converges well, but the acc ings.</p><p>In computer vision, a large collection of work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" tar objectives for graph structured data. Inspired by the recent success of contrastive learning in CV <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59]</ref> and NLP <ref type= ats each instance as a distinct class of its own and learns to discriminate between these instances <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59]</ref>. The promise is th
s from NetRep <ref type="bibr" target="#b43">[44]</ref>, as well as a LiveJournal dataset from SNAP <ref type="bibr" target="#b2">[3]</ref>. Table <ref type="table" target="#tab_0">1</ref> presents th
get="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>. We first introduce the self-supervised pre-training settin from unlabeled graph data and then feed them into logistic regression or SVM. Examples include DGK <ref type="bibr" target="#b60">[61]</ref>, Struc2vec <ref type="bibr" target="#b42">[43]</ref>, Grap n="4.2.2">Graph Classification.</head><p>Setup. We use five datasets from Yanardag and Vishwanathan <ref type="bibr" target="#b60">[61]</ref> -COLLAB, IMDB-BINARY, IMDB-MULTI, REDDITBINARY and REDDIT- re GCC with several recent developed graph classification models, including Deep Graph Kernel (DGK) <ref type="bibr" target="#b60">[61]</ref>, graph2vec <ref type="bibr" target="#b32">[33]</ref>, Info tasks, we follow Struc2vec to use logistic regression. For graph classification tasks, we follow DGK<ref type="bibr" target="#b60">[61]</ref> and GIN<ref type="bibr" target="#b59">[60]</ref> to use SV .</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Graph Classification.</head><p>DGK <ref type="bibr" target="#b60">[61]</ref>, graph2vec <ref type="bibr" target="#b32">[33]</ref>, Info
e same r -ego network as a similar instance pair and define the data augmentation as graph sampling <ref type="bibr" target="#b26">[27]</ref>. Graph sampling is a technique to derive representative su algorithms, such as neighborhood sampling <ref type="bibr" target="#b15">[16]</ref> and forest fire <ref type="bibr" target="#b26">[27]</ref>.</p><p>Discussion on anonymization. Now we discuss the int
7">[58]</ref>, motif distribution <ref type="bibr" target="#b30">[31]</ref>, community organization <ref type="bibr" target="#b33">[34]</ref>, and core-periphery structure <ref type="bibr" target="#b5
ir degree distributions follow a power law <ref type="bibr" target="#b0">[1]</ref>. Leskovec et al. <ref type="bibr" target="#b27">[28]</ref> discover that a wide range of real graphs satisfy the dens
/ref>, k-core <ref type="bibr" target="#b1">[2]</ref>, motif <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>, etc. Consequently, models of this genre, such as Struc2vec ork science literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref>. One illustrative s
type="bibr" target="#b11">[12]</ref>, ProNE <ref type="bibr" target="#b64">[65]</ref> and Struc2vec <ref type="bibr" target="#b42">[43]</ref> which are trained directly on the US-Airport graph, empiri ref type="bibr" target="#b31">32]</ref>, etc. Consequently, models of this genre, such as Struc2vec <ref type="bibr" target="#b42">[43]</ref> and RolX <ref type="bibr" target="#b17">[18]</ref>, usuall gistic regression or SVM. Examples include DGK <ref type="bibr" target="#b60">[61]</ref>, Struc2vec <ref type="bibr" target="#b42">[43]</ref>, GraphWave <ref type="bibr" target="#b11">[12]</ref>, grap ntation is fed into an output layer to predict the node label. As for datasets, we adopt US-Airport <ref type="bibr" target="#b42">[43]</ref> and H-index <ref type="bibr" target="#b63">[64]</ref>. US- bibr" target="#b64">[65]</ref>, Graph-Wave <ref type="bibr" target="#b11">[12]</ref>, and Struc2vec <ref type="bibr" target="#b42">[43]</ref>. Table <ref type="table" target="#tab_1">2</ref> represent embedding dimension to 64.</p><p>Code: https://github.com/snap-stanford/graphwave/.</p><p>Struc2vec <ref type="bibr" target="#b42">[43]</ref> We download the authors' official source code and use defa fication Datasets.</p><p>US-Airport 8 We obtain the US-Airport dataset directly from Ribeiro et al. <ref type="bibr" target="#b42">[43]</ref>.</p><p>H-index 9 We create the H-index dataset, a co-autho been commonly used to benchmark graph learning algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" tar
ego-network (i.e., r ) which GCC conducts data augmentation on. In this work, we follow Qiu et al. <ref type="bibr" target="#b41">[42]</ref> to use 0.8 as the restart probability. The proposed GCC fr vertex degrees <ref type="bibr" target="#b59">[60]</ref> and the binary indicator of the ego vertex <ref type="bibr" target="#b41">[42]</ref> as vertex features. After encoded by the graph encoder, th

ain GIN and report the detailed results in  Setup. We adopt the co-author dataset from Zhang et al. <ref type="bibr" target="#b65">[66]</ref>, which are the conference co-author graphs of KDD, ICDM, S of different methods. We compare GCC with RolX <ref type="bibr" target="#b17">[18]</ref>, Panther++ <ref type="bibr" target="#b65">[66]</ref> and GraphWave <ref type="bibr" target="#b11">[12]</ref>. W top-k similarity search task in three co-author networks. We can see that, compared with Panther++ <ref type="bibr" target="#b65">[66]</ref> and GraphWave <ref type="bibr" target="#b11">[12]</ref> wh Top-k Similarity Search.</head><p>Random, RolX <ref type="bibr" target="#b17">[18]</ref>, Panther++ <ref type="bibr" target="#b65">[66]</ref> We obtainÃÄthe experimental results for these baselines fr target="#b65">[66]</ref> We obtainÃÄthe experimental results for these baselines from Zhang et al. <ref type="bibr" target="#b65">[66]</ref>.</p><p>Code: https://github.com/yuikns/panther/.</p><p>Gra
milar semantics.</p><p>Sharing a similar philosophy, there have been works on contrastive attention <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref>. MGCAM <ref type=" attention <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref>. MGCAM <ref type="bibr" target="#b30">[31]</ref> uses the contrastive feature between persons and backgroun for context modeling; instead of using extra supervision to localize regions to compare as in MGCAM <ref type="bibr" target="#b30">[31]</ref>, ACM automatically learns to focus on meaningful regions t
abnormal location <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref> without the text reports on the location of the disease. Som get="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref>. The dataset contains 112,120 images from 30,805 unique pati
opments of medical image recognition models have shown potentials for growth in diagnostic accuracy <ref type="bibr" target="#b25">[26]</ref>.</p><p>With the recent presence of large-scale chest X-ray
et="#b40">[41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" targe
ays every day. Several studies regarding radiologic errors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9]</ref> have reported that 20-30% of exams are misdiagnosed. To compe
nd thoracic diseases from chest X-rays using deep learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar ays. It is used as a benchmark dataset in previous studies <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" tar ty.</p><p>To address the issue of localizing diseases using only class-level labels, Guendel et al. <ref type="bibr" target="#b11">[12]</ref> propose an auxiliary localization task where the ground tr
chest X-ray tasks and natural image tasks.</p><p>Experimental Setting Following the previous study <ref type="bibr" target="#b1">[2]</ref> on multi-label classification with chest X-Rays, we mainly a
ists usually read tens or hundreds of X-rays every day. Several studies regarding radiologic errors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9]</ref> have reported that 2
chest X-ray tasks and natural image tasks.</p><p>Experimental Setting Following the previous study <ref type="bibr" target="#b1">[2]</ref> on multi-label classification with chest X-Rays, we mainly a
est X-ray datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3]</ref>, there has been a lon est X-ray datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3]</ref> showed that commonly
ns for diagnosing chest diseases. In the US, more than 35 million chest X-rays are taken every year <ref type="bibr" target="#b19">[20]</ref>. It is primarily used to screen diseases such as lung canc
n this paper, we introduce TOAD-GAN as a solution to these problems. Our work is inspired by SinGAN <ref type="bibr" target="#b15">(Shaham, Dekel, and Michaeli 2019)</ref>, a recent Generative Adversa s of TOAD-GAN on Super Mario Bros. level 1-2. The architecture is adapted from SinGAN (cf. Fig. 4 of<ref type="bibr" target="#b15">(Shaham, Dekel, and Michaeli 2019)</ref>). We use a downsampling meth e training process.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>SinGAN</head><p>SinGAN <ref type="bibr" target="#b15">(Shaham, Dekel, and Michaeli 2019</ref>) is a novel GAN architecture p at the lowest scale. For a more in-depth explanation please refer to the original SinGAN paper by <ref type="bibr" target="#b15">Shaham, Dekel, and Michaeli (2019)</ref>.</p></div> <div xmlns="http:
sed using an embedding of the level slices that is inspired by the FrÃ©chet Inception Distance (FID) <ref type="bibr" target="#b8">(Heusel et al. 2017</ref>). In the second part, we show the generality
ether and can result in repeating patterns.</p><p>Tile Pattern KL-Divergence We use the TPKL-Div by <ref type="bibr" target="#b13">Lucas and Volz (2019)</ref> to evaluate the similarity of our generat "#fig_3">4(i)-(l)</ref>.</p><p>Tab. 2 shows our resulting divergences compared to those reported by <ref type="bibr" target="#b13">Lucas and Volz (2019)</ref> and <ref type="bibr" target="#b6">Green e ously generated samples and found that an average of 90.62% of them were unique.</p><p>2 Results by <ref type="bibr" target="#b13">Lucas and Volz (2019)</ref>, on level 1-1 averaged over 2 Ã 2, 3 Ã 3,
rrence of tokens (e.g. a single enemy or ground block) in existing game levels to identify patterns <ref type="bibr" target="#b3">(Dahlskog and Togelius 2012</ref>) and combined them using simple stat ref type="bibr" target="#b11">Khalifa et al. (2019)</ref>.</p><p>Super Mario Bros. Level Generation <ref type="bibr" target="#b3">Dahlskog and Togelius (2012)</ref> identified and analyzed patterns wi lined how those patterns could be combined and varied to create new levels. In their continued work <ref type="bibr" target="#b3">(Dahlskog and Togelius 2014)</ref>, they additionally defined micro-(v
t of previous tokens in the unrolled level.</p><p>Recently, GANs were used to create SMB levels. In <ref type="bibr" target="#b21">(Volz et al. 2018)</ref> the authors train a GAN on slices of the ori he closest to a convincing overworld level is (p) which was also created using a GAN-based approach <ref type="bibr" target="#b21">(Volz et al. 2018)</ref>. However, this method relies on small sample xtreme regime of learning from only one single training level. Similar to other recent publications <ref type="bibr" target="#b21">(Volz et al. 2018;</ref><ref type="bibr" target="#b20">Torrado et al.
sed using an embedding of the level slices that is inspired by the FrÃ©chet Inception Distance (FID) <ref type="bibr" target="#b8">(Heusel et al. 2017</ref>). In the second part, we show the generality
t of previous tokens in the unrolled level.</p><p>Recently, GANs were used to create SMB levels. In <ref type="bibr" target="#b21">(Volz et al. 2018)</ref> the authors train a GAN on slices of the ori he closest to a convincing overworld level is (p) which was also created using a GAN-based approach <ref type="bibr" target="#b21">(Volz et al. 2018)</ref>. However, this method relies on small sample xtreme regime of learning from only one single training level. Similar to other recent publications <ref type="bibr" target="#b21">(Volz et al. 2018;</ref><ref type="bibr" target="#b20">Torrado et al.

" target="#b3">(Dahlskog and Togelius 2012</ref>) and combined them using simple statistical models <ref type="bibr" target="#b17">(Snodgrass and OntanÃ³n 2013)</ref>. The quality of these algorithms c lations, which have to be defined manually. Recent approaches used PCG via Machine Learning (PCGML) <ref type="bibr" target="#b17">(Summerville et al. 2018)</ref> to learn the patterns and relations f 17">(Summerville et al. 2018)</ref> to learn the patterns and relations from the data automatically <ref type="bibr" target="#b17">(Summerville and Mateas 2016;</ref><ref type="bibr" target="#b22">Vol cts the height of a token in a level slice, given the heights of all tokens in the previous slices. <ref type="bibr" target="#b17">Summerville and Mateas (2016)</ref> trained their model on levels by generation of SMB levels. There are 15 original SMB levels provided by the Video Game Level Corpus <ref type="bibr" target="#b17">(Summerville et al. 2016)</ref>, each with different characteristics.
rrence of tokens (e.g. a single enemy or ground block) in existing game levels to identify patterns <ref type="bibr" target="#b3">(Dahlskog and Togelius 2012</ref>) and combined them using simple stat ref type="bibr" target="#b11">Khalifa et al. (2019)</ref>.</p><p>Super Mario Bros. Level Generation <ref type="bibr" target="#b3">Dahlskog and Togelius (2012)</ref> identified and analyzed patterns wi lined how those patterns could be combined and varied to create new levels. In their continued work <ref type="bibr" target="#b3">(Dahlskog and Togelius 2014)</ref>, they additionally defined micro-(v
sky, Chintala, and Bottou 2017)</ref> and penalizing the norm of the gradients of the discriminator <ref type="bibr" target="#b7">(Gulrajani et al. 2017)</ref>. The resulting Wasserstein GAN with Grad
y are unfortunately not the only weak spot in machine learning systems.</p><p>Recently, Xiao et al. <ref type="bibr" target="#b34">[35]</ref> have demonstrated that data preprocessing used in machine ns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image-Scaling Attacks</head><p>Recently, Xiao et al. <ref type="bibr" target="#b34">[35]</ref> have shown that scaling algorithms are vulnerable to attac aling algorithm. Both matrices can be computed in advance and are reusable. We refer to Xiao et al. <ref type="bibr" target="#b34">[35]</ref> for a description how to calculate L and R.</p><p>Based on assignment.</p><p>We implement image-scaling attacks in the strong variant proposed by Xiao et al. <ref type="bibr" target="#b34">[35]</ref>. We make a slight improvement to the original attacks: Ins on rectangular blocks instead of columns and rows. As a result, the original attack by Xiao et al. <ref type="bibr" target="#b34">[35]</ref> is not applicable to this scaling algorithm. To attack are
ehensive analysis of image-scaling attacks and derive defenses for prevention. In a concurrent work <ref type="bibr" target="#b23">[24]</ref>, we study the application for the poisoning scenario. More
ling is a standard procedure in computer vision and a common preprocessing step in machine learning <ref type="bibr" target="#b20">[21]</ref>. A scaling algorithm takes a source image S and resizes it
puter vision [e.g., <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> and natural language processing [e.g., <ref type="bibr" tar
ling is a standard procedure in computer vision and a common preprocessing step in machine learning <ref type="bibr" target="#b20">[21]</ref>. A scaling algorithm takes a source image S and resizes it

experiments along with these two objectives.</p><p>Dataset &amp; Setup. We use the ImageNet dataset <ref type="bibr" target="#b24">[25]</ref> with a pre-trained VGG19 model <ref type="bibr" target="#b

, such as techniques based on wavelets and shearlets [e.g., <ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. These involved approaches, however, are difficult to analy
puter vision [e.g., <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> and natural language processing [e.g., <ref type="bibr" tar

the relationship between the preference factors and the disentangled embeddings.</p><p>According to <ref type="bibr" target="#b30">(Yang et al., 2018)</ref>, the mutual information maximization can be
th different weights. However, these methods <ref type="bibr" target="#b27">(Wu et al., 2019b;</ref><ref type="bibr" target="#b31">Zhu et al., 2019;</ref><ref type="bibr" target="#b0">An et al., 2019) set of documents given a query. Some works <ref type="bibr" target="#b24">(Wang et al., 2018;</ref><ref type="bibr" target="#b31">Zhu et al., 2019)</ref> propose to improve news representations via e eddings. These embeddings can be pre-trained from a large corpus or randomly initialized. Following <ref type="bibr" target="#b31">(Zhu et al., 2019)</ref>, we define the profile embedding</p><formula ed news representations would be taken as initial input embeddings of our model GNUD. Following DAN <ref type="bibr" target="#b31">(Zhu et al., 2019)</ref>, we use two parallel convolutional neural ne sa-10week, which respectively collect news click logs as long as 1 week and 10 weeks. Following DAN <ref type="bibr" target="#b31">(Zhu et al., 2019)</ref>, we just select user id, news id, time-stamp ws title and profile as semantic-level and knowledge-level representations, respectively.</p><p>DAN <ref type="bibr" target="#b31">(Zhu et al., 2019)</ref>, a deep attention neural network for news re
informative user and news representations <ref type="bibr" target="#b20">(Okura et al., 2017;</ref><ref type="bibr" target="#b24">Wang et al., 2018)</ref>. For instance, DKN <ref type="bibr" target=" Okura et al., 2017;</ref><ref type="bibr" target="#b24">Wang et al., 2018)</ref>. For instance, DKN <ref type="bibr" target="#b24">(Wang et al., 2018)</ref> learns knowledge-aware news representation )</ref>, a CF based deep matrix factorization model without considering the news content.</p><p>DKN <ref type="bibr" target="#b24">(Wang et al., 2018)</ref>, a deep content based news recommendation f </ref>) is a content-based deep neural network to rank a set of documents given a query. Some works <ref type="bibr" target="#b24">(Wang et al., 2018;</ref><ref type="bibr" target="#b31">Zhu et al., 2
twork. We also use the concatenation of news title and profile embeddings as features.</p><p>DeepFM <ref type="bibr" target="#b5">(Guo et al., 2017)</ref>, a general model that combines factorization
="bibr" target="#b27">(Wu et al., 2019b;</ref><ref type="bibr" target="#b31">Zhu et al., 2019;</ref><ref type="bibr" target="#b0">An et al., 2019)</ref> usually focus on news contents, and seldom cons ref> exploited different types of news information with an attentive multi-view learning framework. <ref type="bibr" target="#b0">An et al. (2019)</ref> considered both titles and topic categories of
increasingly important role for mining users' reading interest and providing personalized contents <ref type="bibr" target="#b11">(IJntema et al., 2010;</ref><ref type="bibr" target="#b17">Liu et al.
rest and providing personalized contents <ref type="bibr" target="#b11">(IJntema et al., 2010;</ref><ref type="bibr" target="#b17">Liu et al., 2010)</ref>.</p><p>A core problem in news recommendation

s is a central task for news recommendation. Traditional collaborative filtering (CF) based methods <ref type="bibr" target="#b23">(Wang and Blei, 2011)</ref> often utilize historical interactions bet
on of a user by aggregating her clicked news history with different weights. However, these methods <ref type="bibr" target="#b27">(Wu et al., 2019b;</ref><ref type="bibr" target="#b31">Zhu et al., 20 al knowledge, and learn representations of users from their browsed news using an attention module. <ref type="bibr" target="#b27">Wu et al. (2019b)</ref> applied attention mechanism at both word-and
p for DeepFM.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Adressa DMF <ref type="bibr" target="#b29">(Xue et al., 2017)</ref>, a CF based deep matrix factorization model
rest and providing personalized contents <ref type="bibr" target="#b11">(IJntema et al., 2010;</ref><ref type="bibr" target="#b17">Liu et al., 2010)</ref>.</p><p>A core problem in news recommendation
the user's clicked news as the query and the candidate news as the documents.</p><p>Wide &amp; Deep <ref type="bibr" target="#b2">(Cheng et al., 2016)</ref>, a deep model for recommendation which comb
tion method, with the concatenation of TF-IDF vectors of news title and profile as input.</p><p>CNN <ref type="bibr" target="#b13">(Kim, 2014)</ref>, applying two parallel CNNs to word sequences in ne

on of a user by aggregating her clicked news history with different weights. However, these methods <ref type="bibr" target="#b27">(Wu et al., 2019b;</ref><ref type="bibr" target="#b31">Zhu et al., 20 al knowledge, and learn representations of users from their browsed news using an attention module. <ref type="bibr" target="#b27">Wu et al. (2019b)</ref> applied attention mechanism at both word-and
increasingly important role for mining users' reading interest and providing personalized contents <ref type="bibr" target="#b11">(IJntema et al., 2010;</ref><ref type="bibr" target="#b17">Liu et al.

the user's clicked news as the query and the candidate news as the documents.</p><p>Wide &amp; Deep <ref type="bibr" target="#b2">(Cheng et al., 2016)</ref>, a deep model for recommendation which comb
tion method, with the concatenation of TF-IDF vectors of news title and profile as input.</p><p>CNN <ref type="bibr" target="#b13">(Kim, 2014)</ref>, applying two parallel CNNs to word sequences in ne
al Settings</head><p>Datasets. We conduct experiments on the realworld online news datasets Adressa <ref type="bibr" target="#b4">(Gulla et al., 2017)</ref> 2 from a Norwegian news portal to evaluate
tructure characteristics and alleviate the sparsity, thus improving the rec-ommendation performance <ref type="bibr" target="#b25">(Wang et al., 2019)</ref>. For example, as shown in Figure <ref type= bedding because of its powerful representation learning based on node features and graph structure. <ref type="bibr" target="#b25">Wang et al. (2019)</ref> explored the GNN to capture high-order conne
fline model lack fine-grained estimation and customized models are not general as desired. Timeloop <ref type="bibr" target="#b21">[21]</ref> and Eyeriss <ref type="bibr" target="#b22">[22]</ref> use
a modeling framework which is similar to that of <ref type="bibr" target="#b13">[14]</ref>. MAESTRO <ref type="bibr" target="#b26">[26]</ref> is the very first to adopt a data-centric approach.</p></d
eriss<ref type="bibr" target="#b29">[29]</ref> and our Predictor, for the CONV1 and CONV5 of AlexNet<ref type="bibr" target="#b30">[30]</ref>.</figDesc><table><row><cell>Layer</cell><cell>comp.</cell>
n intensive studies of DNN accelerators. For example, the first well-optimized FPGA DNN accelerator <ref type="bibr" target="#b16">[16]</ref> uses loop tiling; the DianNao series <ref type="bibr" targ ds have been developed for predict-ing or simulating DNN accelerators' performance. Roofline models <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18]</ref> and customized ana
of different hardware platforms, such as FPGAs and ASICs, for improving DNN acceleration efficiency <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe 18]</ref> and customized analytical models which are closely tied to the specific design attributes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" targ
s FPGAs and ASICs, for improving DNN acceleration efficiency <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targ
" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Moreover, DNN-based ap
eriss<ref type="bibr" target="#b29">[29]</ref> and our Predictor, for the CONV1 and CONV5 of AlexNet<ref type="bibr" target="#b30">[30]</ref>.</figDesc><table><row><cell>Layer</cell><cell>comp.</cell>
rget="#b2">[3]</ref>. However, their powerful performance often comes with a prohibitive complexity <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target
"bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and natural language processing <ref type="bibr" target="#b2">[3]</ref>. However, their powerful performance often comes with a proh
Ns) have achieved record-breaking performance in various applications, such as image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and natural language p br" target="#b15">[15]</ref>, developing DNN accelerators presents significant challenges, because: <ref type="bibr" target="#b0">(1)</ref> mainstream DNNs have millions of parameters and billions of
m, let q i , k i , v i stand for the i-th row in Q, K, V respectively. Following the formulation in <ref type="bibr" target="#b28">(Tsai et al. 2019)</ref>, the i-th query's attention is defined as a
d architecture efficient, as well as maintain higher prediction capacity?</p><p>Vanilla Transformer <ref type="bibr" target="#b29">(Vaswani et al. 2017</ref>) has three significant limitations when so tei-c.org/ns/1.0"><head>Efficient Self-attention Mechanism</head><p>The canonical self-attention in <ref type="bibr" target="#b29">(Vaswani et al. 2017</ref>) is defined on receiving the tuple input ( ing long sequential outputs through one forward procedure</p><p>We use a standard decoder structure <ref type="bibr" target="#b29">(Vaswani et al. 2017)</ref> in Fig.</p><p>(2), and it is composed of
-series forecasting is a critical ingredient across many domains, such as sensor network monitoring <ref type="bibr" target="#b19">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, forecasting (LSTF) problem. Some significant real-world applications are sensor network monitoring <ref type="bibr" target="#b19">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management,
ver, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b16" ng techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b16"

e-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet <ref type="bibr" target="#b27">(Taylor and Letham 2018)</ref>, LSTMa <ref type="bibr" target="#b1">(
ef type="bibr" target="#b23">Ray 1990;</ref><ref type="bibr" target="#b24">Seeger et al. 2017;</ref><ref type="bibr" target="#b25">Seeger, Salinas, and Flunkert 2016)</ref>, and deep learning techniqu
2018)</ref>, LSTMa <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2015)</ref> and LSTnet <ref type="bibr" target="#b14">(Lai et al. 2018</ref>) and DeepAR <ref type="bibr" target="#b10">(Fl
e-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet <ref type="bibr" target="#b27">(Taylor and Letham 2018)</ref>, LSTMa <ref type="bibr" target="#b1">(
on blocks in Fig. <ref type="figure" target="#fig_3">(3</ref>). Inspired by the dilated convolution <ref type="bibr" target="#b32">(Yu, Koltun, and Funkhouser 2017;</ref><ref type="bibr" target="#b11"
2018)</ref>, LSTMa <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2015)</ref> and LSTnet <ref type="bibr" target="#b14">(Lai et al. 2018</ref>) and DeepAR <ref type="bibr" target="#b10">(Fl

t="#b19">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, economics and finance <ref type="bibr" target="#b33">(Zhu and Shasha 2002)</ref>, and disease propagation analysis <ref ty
2018)</ref>, LSTMa <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2015)</ref> and LSTnet <ref type="bibr" target="#b14">(Lai et al. 2018</ref>) and DeepAR <ref type="bibr" target="#b10">(Fl
2018)</ref>, LSTMa <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2015)</ref> and LSTnet <ref type="bibr" target="#b14">(Lai et al. 2018</ref>) and DeepAR <ref type="bibr" target="#b10">(Fl
-series forecasting is a critical ingredient across many domains, such as sensor network monitoring <ref type="bibr" target="#b19">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, forecasting (LSTF) problem. Some significant real-world applications are sensor network monitoring <ref type="bibr" target="#b19">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management,
reliable workhorse for time-series forecasting <ref type="bibr" target="#b3">(Box et al. 2015;</ref><ref type="bibr" target="#b23">Ray 1990;</ref><ref type="bibr" target="#b24">Seeger et al. 2017;</re
reliable workhorse for time-series forecasting <ref type="bibr" target="#b3">(Box et al. 2015;</ref><ref type="bibr" target="#b23">Ray 1990;</ref><ref type="bibr" target="#b24">Seeger et al. 2017;</re
2018)</ref>, LSTMa <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2015)</ref> and LSTnet <ref type="bibr" target="#b14">(Lai et al. 2018</ref>) and DeepAR <ref type="bibr" target="#b10">(Fl
-series forecasting is a critical ingredient across many domains, such as sensor network monitoring <ref type="bibr" target="#b19">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, forecasting (LSTF) problem. Some significant real-world applications are sensor network monitoring <ref type="bibr" target="#b19">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management,
ver, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b16" ng techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b16"

the goal of humancomputer interaction popular, and it has been a research hotspot in recent decades <ref type="bibr" target="#b0">[1]</ref>. Automatic Speech Recognition (ASR) refers to the task of an
. . . , 0.003] (6)</p><p>Feature-space Maximum Likelihood Linear Regression (FMLLR) was explored in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> for speaker ada
function and the regularization term is the sum of the square of all feature weights like Formula. <ref type="bibr" target="#b18">19</ref>.</p><formula xml:id="formula_18">loss_function = loss + Ï *
"bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p><p>Data augmentation <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> has been propos
is the End-toend models, such as Encoder-decoder structure <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref> and Neural Network structure with Connectionist temporal cla
function and the regularization term is the sum of the square of all feature weights like Formula. <ref type="bibr" target="#b18">19</ref>.</p><formula xml:id="formula_18">loss_function = loss + Ï *
NLP) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and so on <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p><p>Data aug
="#b3">[4]</ref> and Neural Network structure with Connectionist temporal classification (CTC) loss <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The hybrid hidden by the acoustic model, and finally get better results. y * = arg max y log p(y|x) + Î» log P LM (y) <ref type="bibr" target="#b4">(5)</ref> where P LM (y) is provided by the LM, y * denotes the final
method of changing speed has the lowest implementation cost and achieve stateof-the-art performance <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, A new metho

nt compares the results with L2 regularization, which is the-state-of-art method. L2 regularization <ref type="bibr" target="#b34">[35]</ref> is a technique to discourage the complexity of the model.
ginal distribution. It has shown promising results in several areas, including Computer Vision (CV) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b40">[41]</ref>- <ref type="bib
nt compares the results with L2 regularization, which is the-state-of-art method. L2 regularization <ref type="bibr" target="#b34">[35]</ref> is a technique to discourage the complexity of the model.
="#b3">[4]</ref> and Neural Network structure with Connectionist temporal classification (CTC) loss <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The hybrid hidden by the acoustic model, and finally get better results. y * = arg max y log p(y|x) + Î» log P LM (y) <ref type="bibr" target="#b4">(5)</ref> where P LM (y) is provided by the LM, y * denotes the final
ransfer learning has been proposed for a long time, and SJ Pan et al. made a complete summary of it <ref type="bibr" target="#b8">[9]</ref>. It can make full use of the data in the non-target domain t
method of changing speed has the lowest implementation cost and achieve stateof-the-art performance <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, A new metho
n many tasks such as image recognition <ref type="bibr" target="#b9">[10]</ref>, speech recognition <ref type="bibr" target="#b10">[11]</ref>, etc. Unsupervised pre-training also uses additional data
n many tasks such as image recognition <ref type="bibr" target="#b9">[10]</ref>, speech recognition <ref type="bibr" target="#b10">[11]</ref>, etc. Unsupervised pre-training also uses additional data
using a shared representation; what is learned for each task can help other tasks be learned better <ref type="bibr" target="#b29">[30]</ref>. We set up a second task to make both context-dependent (C </p><p>To solve these problems, we investigated a new network structure based on multitask learning <ref type="bibr" target="#b29">[30]</ref>. Our proposed structure is not only trained to optimize a
"bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p><p>Data augmentation <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> has been propos
nt compares the results with L2 regularization, which is the-state-of-art method. L2 regularization <ref type="bibr" target="#b34">[35]</ref> is a technique to discourage the complexity of the model.
rain a better initial model. It has shown promising results in many tasks such as image recognition <ref type="bibr" target="#b9">[10]</ref>, speech recognition <ref type="bibr" target="#b10">[11]</re
xt to speech, TTS) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref> and speech recognit et="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> and ASR <ref type="bibr" target="#b5">[6,</ref><ref type="b , we re-sample it to 16kHZ and convert the raw waveform into mel-spectrograms following Shen et al. <ref type="bibr" target="#b34">[35]</ref> with 50ms frame size, 12.5ms hop size. For the text, we us
embeddings and speaker embeddings following the practice in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, and then fine-tune all parameters. It can help prevent the T
g. The data setting in Lithuanian is similar to that in English. We select a subset of Liepa corpus <ref type="bibr" target="#b19">[20]</ref> and only use the characters as the raw texts. The D h cont

to build ASR models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>As can be seen, a large amount of data resources are
TRODUCTION</head><p>Speech synthesis (text to speech, TTS) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar setting, both TTS <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> and ASR <ref type=" is less likely to cause word skipping. Attention Diagonal Ratio. As demonstrated by previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref>, the attention ali randomly select 200 sentences for IR test and 20 sentences for MOS test, following the practice in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref>  <ref type="foot"
et="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>, compared to that in the richresource setting. Additionally ech. â¢ There are also some related works focusing on low-resource TTS and ASR, such as Speech Chain <ref type="bibr" target="#b38">[39]</ref>, Almost Unsup <ref type="bibr" target="#b30">[31]</ref>, a
embeddings and speaker embeddings following the practice in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, and then fine-tune all parameters. It can help prevent the T
rget="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref>, the pronunciation
-quality paired data are reduced to dozens of minutes in TTS <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar
rget="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref> while the multi-speaker low-quality paired data is reduced and ASR.</p><p>Multi-Speaker TTS Synthesis. Different from <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> that only support a single speaker in both TTS and ASR mode formation. Considering the dual nature between TTS and ASR, we further leverage dual transformation <ref type="bibr" target="#b30">[31]</ref> to boost the accuracy of each other with unpaired speech a of each other, especially in the lowresource scenarios. Therefore, we leverage dual transformation <ref type="bibr" target="#b30">[31]</ref> between TTS and ASR to improve the ability to transform be el is that we replace the feed-forward network with a one-dimensional convolution network following <ref type="bibr" target="#b30">[31]</ref>, in order to better capture the dependencies in a long spe former model to support ASR and TTS, we need different input and output modules for speech and text <ref type="bibr" target="#b30">[31]</ref>. For the TTS model: 1) The input module of the encoder is -resource TTS and ASR, such as Speech Chain <ref type="bibr" target="#b38">[39]</ref>, Almost Unsup <ref type="bibr" target="#b30">[31]</ref>, and SeqRQ-AE <ref type="bibr" target="#b22">[23]</ref>. H ems and thus cannot achieve reasonable accuracy in the extremely low-resource setting. For example, <ref type="bibr" target="#b30">[31]</ref> requires a pronunciation lexicon to convert the character he accuracy, which are costly and not available in the extremely low-resource setting. As a result, <ref type="bibr" target="#b30">[31]</ref> cannot synthesize reasonable speech in TTS and achieves hi
MI alone, and the choice of encoder and MI estimators have a significant impact on the performance <ref type="bibr" target="#b52">(Tschannen et al., 2020)</ref>.</p><p>Figure <ref type="figure">1</re


xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph neural networks (GNN) <ref type="bibr" target="#b31">(Li et al., 2015;</ref><ref type="bibr" target="#b11">Gilmer et al.,
o et al., 2017)</ref>. Also, they are limited to transductive settings and cannot use node features <ref type="bibr" target="#b68">(You et al., 2019)</ref>. Graph kernels <ref type="bibr" target="#b4"
Reg) <ref type="bibr" target="#b3">(Belkin et al., 2006)</ref>, semi-supervised embedding (SemiEmb) <ref type="bibr" target="#b60">(Weston et al., 2012)</ref>, Planetoid <ref type="bibr" target="#b66"
get="#b50">Tang et al., 2015;</ref><ref type="bibr" target="#b13">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b15">Hamilton et al., 2017)</ref> flatten graphs into sequences by taking rvised GNNs reported in <ref type="bibr" target="#b64">(Xu et al., 2019b)</ref> including GraphSAGE <ref type="bibr" target="#b15">(Hamilton et al., 2017)</ref>, GCN, GAT, and two variants of GIN: GIN

n matrix <ref type="bibr" target="#b26">(Klicpera et al., 2019b)</ref>, whereas, late-fusion models <ref type="bibr" target="#b53">(Tsitsulin et al., 2018;</ref><ref type="bibr" target="#b25">Klicpera #b23">(Kipf &amp; Welling, 2016)</ref>, a variant of DGI with a GDC encoder, and a variant of VERSE <ref type="bibr" target="#b53">(Tsitsulin et al., 2018)</ref> by minimizing KL-divergence between no
e the diffusion is used. Early-fusion models <ref type="bibr" target="#b62">(Xu et al., 2019a;</ref><ref type="bibr" target="#b20">Jiang et al., 2019)</ref> use graph diffusion to decide the neighbors

thor. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type="bibr" target="#b13">(Zhang et al. 2018)</ref>, paper-author network <ref type="bibr" targ periments:</p><p>â¢ AMiner-AND<ref type="foot" target="#foot_0">1</ref> . The dataset is released by <ref type="bibr" target="#b13">(Zhang et al. 2018)</ref>, which contains 500 author names for traini e.g., paper network <ref type="bibr" target="#b13">(Zhang et al. 2018)</ref>, paper-author network <ref type="bibr" target="#b13">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature (Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type="bibr" target="#b13">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of me fully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type="bibr" target="#b13">(Zhang et al. 2018</ref>): This model designs a supervised global sta D, we use 100 names for testing and compare the result with the results of other models reported in <ref type="bibr" target="#b13">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample
get="#b3">(Han et al. 2004;</ref><ref type="bibr" target="#b4">Huang, Ertekin, and Giles 2006;</ref><ref type="bibr" target="#b12">Yoshida et al. 2010</ref>) usually leverage supervised learning algor t="#b4">Huang, Ertekin, and Giles 2006;</ref><ref type="bibr" target="#b8">Louppe et al. 2016;</ref><ref type="bibr" target="#b12">Yoshida et al. 2010)</ref>, which usually solve the problem in a disc name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type="bibr" target="#b12">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655
ence evaluating and mentor recommendation, which raises the necessity of author name disambiguation <ref type="bibr" target="#b9">(Smalheiser and Torvik 2009)</ref>.</p><p>Author name disambiguation i e generator is guided to find the rules to select the homogeneous papers. To update Î¸ G , we follow <ref type="bibr" target="#b9">(Schulman et al. 2015)</ref> to compute the gradient of V(G, D) by pol
resent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type="bibr" target="#b2">(Grover and Leskovec 2016)</ref> to represent these features by v i â



ncludes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type="bibr" target="#b3">(Han et al. 2004;</ref><ref type="bibr" target="#b4">Huang, Ertekin, a tegories according to the information they focus on. The first are based on the content information <ref type="bibr" target="#b3">(Han et al. 2004;</ref><ref type="bibr" target="#b4">Huang, Ertekin, a
ncludes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type="bibr" target="#b3">(Han et al. 2004;</ref><ref type="bibr" target="#b4">Huang, Ertekin, a tegories according to the information they focus on. The first are based on the content information <ref type="bibr" target="#b3">(Han et al. 2004;</ref><ref type="bibr" target="#b4">Huang, Ertekin, a
les and complicated feature engineering to some extent. Inspired by generative adversarial networks <ref type="bibr" target="#b1">(Goodfellow et al. 2014)</ref>, we may combine the two categories in a
ncludes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type="bibr" target="#b3">(Han et al. 2004;</ref><ref type="bibr" target="#b4">Huang, Ertekin, a tegories according to the information they focus on. The first are based on the content information <ref type="bibr" target="#b3">(Han et al. 2004;</ref><ref type="bibr" target="#b4">Huang, Ertekin, a
ey cannot measure the high-order connections among papers. Methods focusing on relation information <ref type="bibr" target="#b5">(Kanani, McCallum, and Pal 2007;</ref><ref type="bibr" target="#b0">Be
ocusing on relation information <ref type="bibr" target="#b5">(Kanani, McCallum, and Pal 2007;</ref><ref type="bibr" target="#b0">Bekkerman and McCallum 2005)</ref> usually solve the problem on the bi


ncludes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type="bibr" target="#b3">(Han et al. 2004;</ref><ref type="bibr" target="#b4">Huang, Ertekin, a tegories according to the information they focus on. The first are based on the content information <ref type="bibr" target="#b3">(Han et al. 2004;</ref><ref type="bibr" target="#b4">Huang, Ertekin, a
resent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type="bibr" target="#b2">(Grover and Leskovec 2016)</ref> to represent these features by v i â
resent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type="bibr" target="#b2">(Grover and Leskovec 2016)</ref> to represent these features by v i â
resent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type="bibr" target="#b2">(Grover and Leskovec 2016)</ref> to represent these features by v i â
ey cannot measure the high-order connections among papers. Methods focusing on relation information <ref type="bibr" target="#b5">(Kanani, McCallum, and Pal 2007;</ref><ref type="bibr" target="#b0">Be


tured data. In particular, the Information Bottleneck (IB) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> provides a critical principle for representation learning:
target="#b2">3,</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> have demonstrated impressive performance, by learning to fus
arget="#b45">[46]</ref>, finances <ref type="bibr" target="#b46">[47]</ref> and recommender systems <ref type="bibr" target="#b47">[48]</ref>, because of their flexibility in modeling both the relatio
t="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, in a sense that they can fit more complex graph-structure Appendix E). Permutation invariance is known to be important for structural representation learning <ref type="bibr" target="#b12">[13]</ref>. X ) as in Eq. ( <ref type="formula">2</ref>), and further
target="#b36">[37]</ref>, between representations of sub-structures and the hidden feature vectors <ref type="bibr" target="#b37">[38]</ref>, between representations of graphs and their sub-structure graph-structured data. Furthermore, several works on GNNs <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref> leverage informat
A GeForce RTX 2080 GPUs, together with Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GH CPUs. We use PyTorch <ref type="bibr" target="#b49">[50]</ref> and PyTorch Geometric <ref type="bibr" target="#b50">[51]<
zation trick for Steps 3 and 7: Step 3 uses Gumbel-softmax <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> while Step 7 uses</p><formula xml:id="formula_17">áº(l) X,v </label></formula><p>A ) is a non-informative distribution <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Specifically, we use the uniform distribution for the cate . For the reparameterization in AIB, we use Gumbel-softmax <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> with temperature Ï . For GIB-Cat, the number of neighbors k
f the graph also makes it prone to noise and adversarial attacks that target at the graph structure <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Here we add ur models extremely robust to structural perturbations/attacks where traditional GNNs are sensitive <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Both our models a ar from robust and can be easily attacked by malicious manipulation on either features or structure <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Accordingly, seve periment, we compare the robustness of different models against adversarial attacks. We use Nettack <ref type="bibr" target="#b14">[15]</ref>, a strong targeted attack technique on graphs that attacks attack happens after or before the model is trained, respectively. We follow the setting of Nettack <ref type="bibr" target="#b14">[15]</ref>: for each dataset, select (i) 10 nodes with highest margin e attacking style of Nettack, as the most effective attack is typically via structural perturbation <ref type="bibr" target="#b14">[15]</ref>, as is also confirmed in Appendix J. Therefore, next we fu Additional Details for Adversarial Attack Experiment</head><p>We use the implementation of Nettack <ref type="bibr" target="#b14">[15]</ref> in the repository https://github.com/DSE-MSU/ DeepRobust w d evaluate its performance on the target node in both evasive and poisoning setting. Different from <ref type="bibr" target="#b14">[15]</ref> that only keeps the largest connected component of the gra s, we still use the full graph and standard split, which makes the defense even harder than that in <ref type="bibr" target="#b14">[15]</ref>. For each dataset and each number of perturbations (1, 2,
resentations <ref type="bibr" target="#b32">[33]</ref>, removing suspicious and uninformative edges <ref type="bibr" target="#b33">[34]</ref>, low-rank approximation of the adjacency matrix <ref type= state-of-the-art graph defense models specifically designed against adversarial attacks: GCNJaccard <ref type="bibr" target="#b33">[34]</ref> that pre-processes the graph by deleting the edges between fferent class with very different features, which exactly matches the assumption used by GCNJaccard <ref type="bibr" target="#b33">[34]</ref>. GCNJaccard proceeds to delete edges with dissimilar node cks by connecting nodes with different classes. This exactly satisfies the assumption of GCNJaccard <ref type="bibr" target="#b33">[34]</ref>. GCNJaccard proceeds by deleting edges with low feature si
training the model, we adopt reparameterization trick for Steps 3 and 7: Step 3 uses Gumbel-softmax <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> while Step 7 uses< =1 {u â V vt |u iid â¼ Bernoulli(Ï<label>(</label></formula><p>A ) is a non-informative distribution <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Specifically, we te the true marginal distribution P(Z X ). For the reparameterization in AIB, we use Gumbel-softmax <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> with temperature Ï
ng more powerful GNNs <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta
hts into global model behavior.</p><p>Triggers are a new form of universal adversarial perturbation <ref type="bibr" target="#b18">(Moosavi-Dezfooli et al., 2017)</ref> adapted to discrete textual inp or anyone to fool machine learning models. Moreover, universal attacks often transfer across models <ref type="bibr" target="#b18">(Moosavi-Dezfooli et al., 2017)</ref>, which further decreases attack e adversarial threat is higher if an attack is universal: using the exact same attack for any input <ref type="bibr" target="#b18">(Moosavi-Dezfooli et al., 2017;</ref><ref type="bibr" target="#b4">Br

to evaluate reading comprehension models <ref type="bibr" target="#b13">(Jia and Liang, 2017;</ref><ref type="bibr" target="#b26">Ribeiro et al., 2018)</ref> and stress test neural machine translatio
wo text classification datasets.</p><p>Sentiment Analysis We use binary Stanford Sentiment Treebank <ref type="bibr" target="#b29">(Socher et al., 2013)</ref>. We consider Bi-LSTM models (Graves and S
wo text classification datasets.</p><p>Sentiment Analysis We use binary Stanford Sentiment Treebank <ref type="bibr" target="#b29">(Socher et al., 2013)</ref>. We consider Bi-LSTM models (Graves and S
ed to any input whereas SEARs is only applicable when one its rule applies.</p><p>In parallel work, <ref type="bibr" target="#b0">Behjati et al. (2019)</ref> consider universal adversarial attacks on
wo text classification datasets.</p><p>Sentiment Analysis We use binary Stanford Sentiment Treebank <ref type="bibr" target="#b29">(Socher et al., 2013)</ref>. We consider Bi-LSTM models (Graves and S
to evaluate reading comprehension models <ref type="bibr" target="#b13">(Jia and Liang, 2017;</ref><ref type="bibr" target="#b26">Ribeiro et al., 2018)</ref> and stress test neural machine translatio
ocher et al., 2013)</ref>. We consider Bi-LSTM models (Graves and Schmidhuber, 2005) using word2vec <ref type="bibr" target="#b17">(Mikolov et al., 2018)</ref> or ELMo <ref type="bibr" target="#b22">(
tions. For example, adversarially-modified inputs are used to evaluate reading comprehension models <ref type="bibr" target="#b13">(Jia and Liang, 2017;</ref><ref type="bibr" target="#b26">Ribeiro et ain type. We focus on why, who, when, and where questions. We use sentences of length ten following <ref type="bibr" target="#b13">Jia and Liang (2017)</ref> and sum the cross-entropy of the start and SQuAD; adversarial evaluation in-stead highlights erroneous model behaviors on a per-example basis <ref type="bibr" target="#b13">(Jia and Liang, 2017)</ref>. Here, we analyze the SQuAD triggers to s
tei-c.org/ns/1.0"><head n="4">Attacking Reading Comprehension</head><p>We create triggers for SQuAD <ref type="bibr" target="#b25">(Rajpurkar et al., 2016)</ref>. We use an intentionally simple baseli
the rest of the application using multiple parallel IQs <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. This is effectively a limited form of OoO scheduling. How ependence chains may impede the exploitation of ILP and MLP in such parallel InO scheduling windows <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>To sum u thereby eliminating the need for associative LQ searches <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, this approach could significantly degrade perfor ws the performance of Load Slice Core (LSC) <ref type="bibr" target="#b14">[15]</ref>, Freeway core <ref type="bibr" target="#b15">[16]</ref>, CASINO core, and OoO core, normalized to that of an InO c ependent slice blocks the issue of younger, independent slices. To address this limitation, Freeway <ref type="bibr" target="#b15">[16]</ref> introduces a dependence-aware slice scheduling policy. In ues built upon an energy-efficient stall-on-use InO core <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, the various shapes and sizes of dependence chain ever, the various shapes and sizes of dependence chains could restrict their ability to exploit ILP <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>2) Energ
e comparisons <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>. Also, we need
nd chip area overhead, we use modified versions of McPAT <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref> and CACTI 6.5 <ref type="bibr" target="#b44">[44]</ref>, c
se structures <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b68">[68]</ref>, <ref type="bibr" target="#b69">[69]</ref>. We adopt the latter approach because it can easily be imp
memories (CAMs) and random access memories (RAMs), and accessed multiple times by each instruction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" t rmance improvements, but they are less than those of SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> models. This is because SpecInO <ref type="bibr" target="#b1" "bibr" target="#b0">1]</ref> models. This is because SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> has more opportunities for speculative issue. SpecInO <ref ty omes ready in the next cycle, which can be caught by SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. Instructions exiting SpecInO <ref type="bibr" target="#b1">[ instructions are renamed at the S-IQ head, assuming SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. To provide a better understanding, we mark newly added featu sue), and performance with various IQ sizes assuming SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> with unlimited other resources. As the IQ size increases, mor nclude that the optimal configuration of the S-IQ is SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>F. f any IQ. Each of the S-IQ and IQ are configured with SpecInO<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. Conditional register renaming is disabled in these experimen
cient by addressing the complexity of the scheduling logic <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref> or reducing the accesses to powerhungry structures <ref type
es to the LSQ <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b66">[66]</ref>, <ref type="bibr" target="#b67">[67]</ref>. Another approach is to eliminate the LQ or SQ, as well as
f>, <ref type="bibr" target="#b75">[75]</ref>, <ref type="bibr" target="#b76">[76]</ref>. Recently, <ref type="bibr" target="#b77">[77]</ref>, <ref type="bibr" target="#b78">[78]</ref> go one step fur ">[74]</ref>, <ref type="bibr" target="#b75">[75]</ref>, <ref type="bibr" target="#b76">[76]</ref>, <ref type="bibr" target="#b77">[77]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>
]</ref> ConD <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b13">14]</ref> ConV <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b24">24]</ref> Counts  shows slig all issue rate and an 6% improvement in performance by applying a conditional renaming scheme. ConV <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b24">24]</ref> shows similar perf "#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref>, and OoO cores are with 4 or more ways <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bib
">[61]</ref>, <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b65">[65]</ref> or to reduce th
oads ahead of the older stores with unresolved addresses <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b27">[27]</ref>. Memory disambiguation is supported by the LSU which consi
also trigger side effects, adverse reactions, and even serious toxicity, leading patients in danger <ref type="bibr" target="#b1">[2]</ref>. As there exists increasing needs of multi-drug treatments, unannotated DDIs, and cannot give alerts to potential DDIs before a combinational treatment is made <ref type="bibr" target="#b1">[2]</ref>. In contrast, machine learning-based methods provide a promi presents drugs in a form of feature vector according to drug properties, such as chemical structure <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" targe 2]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, targets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" targe "bibr" target="#b11">[12]</ref>, SVM <ref type="bibr" target="#b11">[12]</ref>, logistic regression <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target s, then deduces potential DDIs with the well-trained model. Most methods utilize a single predictor <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" targe
arget="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t arget="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, Anatomical Therapeutic Chemical classification (ATC) code t="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, medication and/or clinical observations <ref type="bibr" target="#b10">[11]</ref>.</p><p>The supervised predictor is usually implemented by sed method (named as LP) <ref type="bibr" target="#b12">[13]</ref> and Zhang's method (named as CE) <ref type="bibr" target="#b10">[11]</ref> in 5-CV test. Vilar et al <ref type="bibr" target="#b5">[6 nlabeled nodes by computing drug similarityderived weights of edges on the DDI network. Zhang et al <ref type="bibr" target="#b10">[11]</ref> collects a variety of drug-related data (e.g., known drug- ion with an ensemble (CE) classifier model.</p><p>To ensure a fair comparison, the DB2 dataset from <ref type="bibr" target="#b10">[11]</ref> is adopted. In the DB2 dataset, all unlabeled drug pairs a ore, respectively. Although the AUC and ACC of DPDDI are slightly lower than that of Zhang's method <ref type="bibr" target="#b10">[11]</ref>, the AUPR and F 1 of DPDDI are higher. AUPR is often belie rug-drug interaction, F 1 should be more effective measure than ACC.</p><p>In addition, Zhang et al <ref type="bibr" target="#b10">[11]</ref> used 9 drug-related data sources, while our DPDDI just use drug transporter, drug pathway, drug indication, drug side effect and drug off side effect used in <ref type="bibr" target="#b10">[11]</ref>) to construct the dug-drug similarity network, using DPDDI compare with other state-of-the-art methods, a smaller dataset (named as DB2) built by Zhang et al. <ref type="bibr" target="#b10">[11]</ref> was adopted to evaluate the performance of our DPDDI. DB2 0</cell></row></table><note><p><p><p><p><p>a</p>The results are taken from Table</p>5</p>in Ref.</p><ref type="bibr" target="#b10">[11]</ref> </p></note></figure> <figure xmlns="http://www.tei-c.org/n 2]</ref>, side effects <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar r" target="#b7">[8]</ref>, label propagation <ref type="bibr" target="#b12">[13]</ref>, random walk <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, probabilistic sof
l activity. As 138 of 1562 drugs in DB1 have no ATC code, we adopted their predicted codes by SPACE <ref type="bibr" target="#b34">[35]</ref>, which deduce ATC codes from chemical structures. To feed
l activity. As 138 of 1562 drugs in DB1 have no ATC code, we adopted their predicted codes by SPACE <ref type="bibr" target="#b34">[35]</ref>, which deduce ATC codes from chemical structures. To feed
st and q-fold cross-validation (CV) test are often used to examine the effectiveness of a predictor <ref type="bibr" target="#b23">[24]</ref>. Of the two test methods, the jackknife test is deemed the
results in small-scale pruned datasets and thus is not pragmatic and suitable in the real scenario <ref type="bibr" target="#b16">[17]</ref>. In addition, some handcrafted drug features may not be pr features in building machine learning models for various downstream tasks, such as link prediction <ref type="bibr" target="#b16">[17]</ref>. Recently, the GCN has been applied to the field of drug d
tor, in this study, we adopted the 5-fold cross-validation (5CV) test as done by most investigators <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" t
tructure according to Pubchem fingerprints. ATC codes are released by the World Health Organization <ref type="bibr" target="#b33">[34]</ref>, and they categorize drug substances at different levels a
ion <ref type="bibr" target="#b12">[13]</ref>, random walk <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, probabilistic soft logic <ref type="bibr" target="#b8">[9, et="#b7">[8]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, while some of th
in the training phase. Many former works in similar areas <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> adopted the same nu
t="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, targets <ref typ f><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, label propagation <ref type="bibr" target="#b12">[13]</ref>, random walk <ref type="bibr" target="#b10">[11,</ref><ref target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" t ="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, label propagationbased method (named as LP) <ref type="bibr" target="#b12">[13]</ref> and Zhang's method (named as CE) <ref type="bibr" target=" ion profile fingerprints (IPFs) to measure similarity for predicting DDIs. Label propagation method <ref type="bibr" target="#b12">[13]</ref> applies label propagation to assign labels from known DDIs target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, medication and/or
ackground knowledge <ref type="bibr" target="#b0">[1]</ref>. Although Neural Attention Models (NAM) <ref type="bibr" target="#b1">[2]</ref> are endowed with a certain degree of interpretability in vis 8]</ref>), (c) discover inherent bias in the model's predictive strategy (e.g., contextual modeling <ref type="bibr" target="#b1">[2]</ref>[9]), (d) prevent prediction errors in unintuitive scenarios e-infusion during the model learning using information-theoretic loss function (e.g., KL divergence <ref type="bibr" target="#b1">[2]</ref>) can check conceptual drifting at the representational level ge in DL models can be categorized into the shallow infusion, semi-deep infusion, and deep infusion <ref type="bibr" target="#b1">[2]</ref>. In shallow infusion, both the external information and meth here sentences are purposefully paraphrased to elicit meaningful responses from an agent (or user); <ref type="bibr" target="#b1">(2)</ref> The clinical conversation contains implicit references to he
blem is a low resource (insufficient benchmark datasets and unlabeled corpus for transfer learning) <ref type="bibr" target="#b10">[11]</ref>. Also, knowledge-infusion during the model learning using
g, interpreting, and explaining results (e.g., identifying emerging sub-events in natural disasters <ref type="bibr" target="#b7">[8]</ref>), (c) discover inherent bias in the model's predictive strat
blem is a low resource (insufficient benchmark datasets and unlabeled corpus for transfer learning) <ref type="bibr" target="#b10">[11]</ref>. Also, knowledge-infusion during the model learning using
a and relations between concepts that plays an important role in tracing student's concepts mastery <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b15">[16]</ref>, and (b) Engagem "#b17">[18]</ref>. Assessments or diagnostic tests are also created using such behavioral profiling <ref type="bibr" target="#b16">[17]</ref>. Student's tendency to answer a question without comprehen
, instructors, public health experts) to conceptually understand the working of involved AI systems <ref type="bibr" target="#b19">[20]</ref>. There is also a pressing requirement for benchmark datase
r conceptual (using either generic or domain-specific KG <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref>) in nature pertaining to its inner functioning."</p><p>Expla
xceptionally challenging to probe the model's mechanism without the support of background knowledge <ref type="bibr" target="#b0">[1]</ref>. Although Neural Attention Models (NAM) <ref type="bibr" tar
r conceptual (using either generic or domain-specific KG <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref>) in nature pertaining to its inner functioning."</p><p>Expla
1">[2]</ref>[9]), (d) prevent prediction errors in unintuitive scenarios (e.g. adversarial examples <ref type="bibr" target="#b9">[10]</ref>, CHECKLIST <ref type="bibr" target="#b3">[4]</ref>), (e) ma rchangeably in the prior research without clear distinctions and the different roles that they play <ref type="bibr" target="#b9">[10]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>I
r conceptual (using either generic or domain-specific KG <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref>) in nature pertaining to its inner functioning."</p><p>Expla
t information for making matching decisions. The last technique, data augmentation, is adapted from <ref type="bibr" target="#b30">[31]</ref> for EM to help D learn "harder" to understand the data inv address this issue, D applies MixDA, a recently proposed data augmentation technique for NLP tasks <ref type="bibr" target="#b30">[31]</ref> illustrated in Figure <ref type="figure" target="#fig_3">3 h the entry_swap operator. We compare the different combinations and report the best one. Following <ref type="bibr" target="#b30">[31]</ref>, we apply MixDA with the interpolation parameter Î» sampled arget="#b58">59</ref>]. We designed a set of DA operators suitable for EM and apply them with MixDA <ref type="bibr" target="#b30">[31]</ref>, a recently proposed DA strategy based on convex interpola nd span_shuffle. These two operators are used in NLP tasks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b30">31]</ref> and shown to be effective for text classification. For span DA) has been extensively studied in computer vision and has recently received more attention in NLP <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" ta
models by fine-tuning pre-trained LMs in a simpler architecture.</p><p>Pre-trained LMs such as BERT <ref type="bibr" target="#b12">[13]</ref> and GPT-2 <ref type="bibr" target="#b40">[41]</ref> have d task. See Appendix A for the model architecture. In D , we fine-tune the popular 12layer BERT model <ref type="bibr" target="#b12">[13]</ref>, RoBERTa <ref type="bibr" target="#b28">[29]</ref>, and a currently support 4 pre-trained models: Distil-BERT <ref type="bibr" target="#b44">[45]</ref>, BERT <ref type="bibr" target="#b12">[13]</ref>, RoBERTa <ref type="bibr" target="#b28">[29]</ref>, and XL Figure <ref type="figure">6</ref> shows the model architecture of D 's language models such as BERT <ref type="bibr" target="#b12">[13]</ref>, DistilBERT <ref type="bibr" target="#b44">[45]</ref>, and best performing pre-trained model among DistilBERT <ref type="bibr" target="#b44">[45]</ref>, BERT <ref type="bibr" target="#b12">[13]</ref>, XLNet <ref type="bibr" target="#b60">[61]</ref>, and RoBE
s usually not at the beginning of the sequences.</p><p>There are many ways to perform summarization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" ta
ef>, crowdsourcing <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b51">52]</ref>, or machine learning <ref type="bibr" target="#b45">[46,</r
ndix E. External knowledge is known to be effective in improving neural network models in NLP tasks <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" targ
ratio of 3:1:1. The same split of the datasets is also used in the evaluation of other EM solutions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta ">[25]</ref>, DeepER <ref type="bibr" target="#b13">[14]</ref>, and follow-up works of Deep-Matcher <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>. We also compare w n the ER-Magellan EM datasets. The numbers of DeepMatcher+ (DM+) are the highest available found in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta type="bibr" target="#b22">[23]</ref> slightly outperforms Deep-Matcher in the DBLP-ACM dataset and <ref type="bibr" target="#b16">[17]</ref> achieves better F1 in the Walmart-Amazon and Amazon-Google [25]</ref>, DeepMatcher <ref type="bibr" target="#b33">[34]</ref>, and DeepMatcher's follow-up work <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b22">[23]</ref>.</p><p>We su olutions used deep learning and achieved promising results <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar bibr" target="#b33">[34]</ref> and the two follow-up works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref>   . We also plot the score of DeepMatcher+ on the full data
ef>, crowdsourcing <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b51">52]</ref>, or machine learning <ref type="bibr" target="#b45">[46,</r
ef><ref type="bibr" target="#b52">53]</ref>, crowdsourcing <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b51">52]</ref>, or machine learnin
n 7.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>There is a concurrent work <ref type="bibr" target="#b5">[6]</ref>, which applies a similar idea. See Appendix E for a detailed large, we use DistilBERT across all settings for faster training.</p><p>There is a concurrent work <ref type="bibr" target="#b5">[6]</ref>, which also applies pre-trained LM to the entity matching pr ef> reports the best epoch on the test set, instead of the validation set), the reported results in <ref type="bibr" target="#b5">[6]</ref> is not directly comparable. We summarize in Appendix E the d "#b5">[6]</ref> is not directly comparable. We summarize in Appendix E the difference between D and <ref type="bibr" target="#b5">[6]</ref> and explain why the reported results are different.</p></div and observed that DeepMatcher improved by as much as 5.2% on some datasets.</p><p>A concurrent work <ref type="bibr" target="#b5">[6]</ref> also applies pre-trained LMs to the entity matching problem ined LMs to the entity matching problem and achieves good performance. While the proposed method in <ref type="bibr" target="#b5">[6]</ref> is similar to the baseline version of D , D can be further o benchmarks using a more standard evaluation method. We provide a detailed comparison between D and <ref type="bibr" target="#b5">[6]</ref> in Appendix E. External knowledge is known to be effective i .0"><head>E. THE DIFFERENCE BETWEEN DITTO AND A CONCURRENT WORK</head><p>There is a concurrent work <ref type="bibr" target="#b5">[6]</ref> which also applies pre-trained LMs to entity matching and ob h also applies pre-trained LMs to entity matching and obtained good results. The method proposed in <ref type="bibr" target="#b5">[6]</ref> is essentially identical to the baseline version of D which on all the 13 ER-Magellan datasets, the WDC product benchmark, and a company matching dataset while <ref type="bibr" target="#b5">[6]</ref> experimented in 5/13 of the ER-Magellan datasets.</p><p>On t llan datasets.</p><p>On these 5 evaluated datasets, one might notice that the reported F1 scores in <ref type="bibr" target="#b5">[6]</ref> are slightly higher compared to the baseline's F1 scores sho the baseline's F1 scores shown in Table <ref type="table">5</ref>. The reason is that according to <ref type="bibr" target="#b5">[6]</ref>, for each run on each dataset, the F1 score is computed as t It is not difficult to see that over the same set of model snapshots, the F1 score computed by the <ref type="bibr" target="#b5">[6]</ref>'s evaluation method would be greater or equal to the F1 scor ore computed using our method, which explains the differences in the reported values between us and <ref type="bibr" target="#b5">[6]</ref>.</p><p>Table <ref type="table" target="#tab_3">12</ref> summ get="#tab_3">12</ref> summarizes the detailed comparison of the baseline D , the proposed method in <ref type="bibr" target="#b5">[6]</ref>, and the full D . Recall that we construct the baseline by t pe="bibr" target="#b60">[61]</ref>, and RoBERTa <ref type="bibr" target="#b28">[29]</ref> following <ref type="bibr" target="#b5">[6]</ref>. Although the baseline D does not outperform <ref type="bibr ref> following <ref type="bibr" target="#b5">[6]</ref>. Although the baseline D does not outperform <ref type="bibr" target="#b5">[6]</ref> because of the different evaluation method, the optimized D t="#b5">[6]</ref> because of the different evaluation method, the optimized D is able to outperform <ref type="bibr" target="#b5">[6]</ref> in 4/5 of the evaluated datasets.</p><p>Table <ref type="tab
s usually not at the beginning of the sequences.</p><p>There are many ways to perform summarization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" ta
self-trained to perform auxiliary tasks such as missing token and next-sentence prediction. Studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">50]</ref> have shown that the
rg/ns/1.0"><head n="4.2">Implementation and experimental setup</head><p>We implemented D in PyTorch <ref type="bibr" target="#b35">[36]</ref> and the Transformers library <ref type="bibr" target="#b57
arget="#b53">54</ref>] and the matching problem with rules <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" tar
get="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b63">64]</ref>. DeepER <ref type="bibr" target="#b13">[14]</ref> trains EM ERT <ref type="bibr" target="#b42">[43]</ref>, described in Section 5, is inspired by this. Auto-EM <ref type="bibr" target="#b63">[64]</ref> improves deep learning-based EM models by pre-training the
target="#b34">35,</ref><ref type="bibr" target="#b53">54</ref>] and the matching problem with rules <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" ta
rg/ns/1.0"><head n="4.2">Implementation and experimental setup</head><p>We implemented D in PyTorch <ref type="bibr" target="#b35">[36]</ref> and the Transformers library <ref type="bibr" target="#b57
such as missing token and next-sentence prediction. Studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">50]</ref> have shown that the shallow layers capture lexical meaning
cision. D 's architecture is much simpler when compared to many state-of-the-art EM solutions today <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b13">14]</ref>. Even though the b <ref type="bibr" target="#b13">[14]</ref> or DeepMatcher<ref type="foot" target="#foot_1">3</ref>  <ref type="bibr" target="#b33">[34]</ref>. Furthermore, D can also ingest and match hierarchically s head><p>We experimented with all the 13 publicly available datasets used for evaluating DeepMatcher <ref type="bibr" target="#b33">[34]</ref>. These datasets are from the ER Benchmark datasets <ref ty pany datasets are textheavy meaning that at least one attributes contain long text. Also, following <ref type="bibr" target="#b33">[34]</ref>, we use the dirty version of the DBLP-ACM, DBLP-Scholar, i . We report the average F1 of 5 repeated runs in all the settings.</p><p>â¢ DeepMatcher: DeepMatcher <ref type="bibr" target="#b33">[34]</ref> is the SOTA matching solution. Compared to D , DeepMatcher ]</ref> to train the word embeddings. When reporting DeepMatcher's F1 scores, we use the numbers in <ref type="bibr" target="#b33">[34]</ref> for the ER-Magellan datasets and numbers in <ref type="bib "#b16">[17]</ref> achieves better F1 in the Walmart-Amazon and Amazon-Google datasets. According to <ref type="bibr" target="#b33">[34]</ref>, the Magellan system ( <ref type="bibr" target="#b24">[25] pe="bibr" target="#b13">[14]</ref>, Magellan <ref type="bibr" target="#b24">[25]</ref>, DeepMatcher <ref type="bibr" target="#b33">[34]</ref>, and DeepMatcher's follow-up work <ref type="bibr" target= rent (e.g., they used k-fold cross-validation while we use the train/valid/test splits according to <ref type="bibr" target="#b33">[34]</ref>). In our experiments, we implemented DeepER with LSTM as t have summarized DM in Section 4.2. In addition to simply taking the numbers from the original paper <ref type="bibr" target="#b33">[34]</ref>, we also ran their open-source version (DM (reproduced)) w 2 and 15 epochs). The reproduced results are in general lower than the original reported numbers in <ref type="bibr" target="#b33">[34]</ref> (the 3rd column) because we did not try the other model va e Abt-Buy dataset. Others: We obtained the results for Magellan by taking the reported results from <ref type="bibr" target="#b33">[34]</ref> and the two follow-up works <ref type="bibr" target="#b22" er 4.6.5 of <ref type="bibr" target="#b32">[33]</ref>) and is also used by DeepMatcher and Magellan <ref type="bibr" target="#b33">[34]</ref>. It is not difficult to see that over the same set of mode other EM solutions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>. We list the size of each dataset in Table <ref type="table available found in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>     The use of a pre-trained LM contributes to a large port et="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b63">64]</ref>. DeepER <ref type="
s usually not at the beginning of the sequences.</p><p>There are many ways to perform summarization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" ta
rg/ns/1.0"><head n="4.2">Implementation and experimental setup</head><p>We implemented D in PyTorch <ref type="bibr" target="#b35">[36]</ref> and the Transformers library <ref type="bibr" target="#b57
type="bibr" target="#b12">[13]</ref>, RoBERTa <ref type="bibr" target="#b28">[29]</ref>, and XLNet <ref type="bibr" target="#b60">[61]</ref>. We use the base uncased variant of each model in all our RT <ref type="bibr" target="#b44">[45]</ref>, BERT <ref type="bibr" target="#b12">[13]</ref>, XLNet <ref type="bibr" target="#b60">[61]</ref>, and RoBERTa <ref type="bibr" target="#b28">[29]</ref> fol
s usually not at the beginning of the sequences.</p><p>There are many ways to perform summarization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" ta
ces a channel-attention mechanism by adaptively recalibrating the channel feature responses. SK-Net <ref type="bibr" target="#b37">[38]</ref> brings the feature-map attention across two network branch /ref>: Comparing our ResNeSt block with SE-Net <ref type="bibr" target="#b29">[30]</ref> and SK-Net <ref type="bibr" target="#b37">[38]</ref>. A detailed view of Split-Attention unit is shown in Figur -Net operates on top of the entire block regardless of multiple groups. Previous models like SK-Net <ref type="bibr" target="#b37">[38]</ref> introduced feature attention between two network branches, type="bibr" target="#b28">[29]</ref>, ResNet-D <ref type="bibr" target="#b25">[26]</ref> and SKNet <ref type="bibr" target="#b37">[38]</ref>. Remarkably, our ResNeSt-50 achieves 80.64 top-1 accuracy, ach group is Split Attention in Cardinal Groups. Following <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>, a combined representation for each cardinal group can be o obal average pooling across spatial dimensions s k â R C/K <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref>. Here the c-th component is calculated as:</p><formula xml: Our method generalizes prior work on feature-map attention <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref> within a cardinal group setting <ref type="bibr" target="#b
"#b51">[52]</ref>, in which each network block consists of different convolutional kernels. ResNeXt <ref type="bibr" target="#b60">[61]</ref> adopts group convolution <ref type="bibr" target="#b33">[3 ight) depicts an overview of a Split-Attention Block.</p><p>Feature-map Group. As in ResNeXt blocks <ref type="bibr" target="#b60">[61]</ref>, the feature can be divided into several groups, and the n
the previous methods, our Fig. <ref type="figure">1</ref>: Comparing our ResNeSt block with SE-Net <ref type="bibr" target="#b29">[30]</ref> and SK-Net <ref type="bibr" target="#b37">[38]</ref>. A de then the intermediate representation of each group is Split Attention in Cardinal Groups. Following <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>, a combined repres ead of the 1 Ã 1 layer to better preserve such information <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. Convolutional layers require handling featuremap boundarie
type="bibr" target="#b31">[32]</ref> is used after each convolutional layer before ReLU activation <ref type="bibr" target="#b43">[44]</ref>. Network weights are initialized using Kaiming Initializat
p://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Modern CNN Architectures. Since AlexNet <ref type="bibr" target="#b33">[34]</ref>, deep convolutional neural networks <ref type="bibr" targe t convolutional kernels. ResNeXt <ref type="bibr" target="#b60">[61]</ref> adopts group convolution <ref type="bibr" target="#b33">[34]</ref> in the ResNet bottle block, which converts the multi-path

ugmented example. Regularization. Very deep neural networks tend to overfit even for large datasets <ref type="bibr" target="#b67">[68]</ref>. To prevent this, dropout regularization randomly masks ou t network ensemble <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b67">68]</ref>. A dropout layer with the dropout probability of 0.2 is app
batch with its reversed order sample <ref type="bibr" target="#b25">[26]</ref>. Batch Normalization <ref type="bibr" target="#b31">[32]</ref> is used after each convolutional layer before ReLU activat
ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b72">73]</ref> and pose estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b57">58]</ref>. Recent work has s
CNN operators.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Deformable <ref type="bibr" target="#b71">[72]</ref>   Beyond the paper contributions, we empirically find seve ade-Mask-RCNN on COCO val set. The ResNeSt-101 is applied with and without deformable convolution v2<ref type="bibr" target="#b71">[72]</ref>. It shows that our split-attention module is compatible wi nce segmentation, shown in  We also evaluate our ResNeSt with and without deformable convolution v2 <ref type="bibr" target="#b71">[72]</ref>. With its help, we are able to obtain a higher performance
ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b72">73]</ref> and pose estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b57">58]</ref>. Recent work has s

type="bibr" target="#b31">[32]</ref> is used after each convolutional layer before ReLU activation <ref type="bibr" target="#b43">[44]</ref>. Network weights are initialized using Kaiming Initializat

type="bibr" target="#b43">[44]</ref>. Network weights are initialized using Kaiming Initialization <ref type="bibr" target="#b23">[24]</ref>. A drop layer is inserted before the final classification

gt; 0. This mitigates network overconfidence and overfitting.</p><p>Auto Augmentation. Auto-Augment <ref type="bibr" target="#b10">[11]</ref> is a strategy that augments the training data with transfo
p><p>Multi-path and Feature-map Attention. Multi-path representation has shown success in GoogleNet <ref type="bibr" target="#b51">[52]</ref>, in which each network block consists of different convolu d training crop size of 224, while the Inception-Net family<ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref> uses a training c
arget="#b59">60]</ref> usually use a fixed training crop size of 224, while the Inception-Net family<ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" t
target="#b7">[8,</ref><ref type="bibr" target="#b68">69]</ref> or introduce long-range connections <ref type="bibr" target="#b55">[56]</ref> or use cross-channel feature-map attention <ref type="bibr sks at the same time? Cross-channel information has demonstrated success in downstream applications <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" ta
p><p>Multi-path and Feature-map Attention. Multi-path representation has shown success in GoogleNet <ref type="bibr" target="#b51">[52]</ref>, in which each network block consists of different convolu d training crop size of 224, while the Inception-Net family<ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref> uses a training c
type="bibr" target="#b31">[32]</ref> is used after each convolutional layer before ReLU activation <ref type="bibr" target="#b43">[44]</ref>. Network weights are initialized using Kaiming Initializat
can be directly applied on many existing downstream models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" tar ages (aka. minival) using the standard COCO AP metric of single scale. We train all models with FPN <ref type="bibr" target="#b40">[41]</ref>, synchronized batch normalization <ref type="bibr" target= f> models with ResNeSt-50 and ResNeSt-101 as their backbones. All models are trained along with FPN <ref type="bibr" target="#b40">[41]</ref> and synchronized batch normalization. For data augmentatio
ef type="bibr" target="#b72">73]</ref> and pose estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b57">58]</ref>. Recent work has significantly boosted image classification ><p>We investigate the effect of backbone on pose estimation task. The baseline model is SimplePose <ref type="bibr" target="#b57">[58]</ref> with ResNet50 and ResNet101 implemented in Glu-onCV <ref t

f>, while recent image classification networks have focused more on group or depth-wise convolution <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" ta er well to other tasks as their isolated representations cannot capture cross-channel relationships <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Therefore, a netw
ugmented example. Regularization. Very deep neural networks tend to overfit even for large datasets <ref type="bibr" target="#b67">[68]</ref>. To prevent this, dropout regularization randomly masks ou t network ensemble <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b67">68]</ref>. A dropout layer with the dropout probability of 0.2 is app
can be directly applied on many existing downstream models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" tar ages (aka. minival) using the standard COCO AP metric of single scale. We train all models with FPN <ref type="bibr" target="#b40">[41]</ref>, synchronized batch normalization <ref type="bibr" target= f> models with ResNeSt-50 and ResNeSt-101 as their backbones. All models are trained along with FPN <ref type="bibr" target="#b40">[41]</ref> and synchronized batch normalization. For data augmentatio
b55">[56]</ref> or use cross-channel feature-map attention <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b64">65]</ref>. While these approaches do improve the transfer learning pe tream applications <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref>, while recent image classification networks have focused mo ain all models with FPN <ref type="bibr" target="#b40">[41]</ref>, synchronized batch normalization <ref type="bibr" target="#b64">[65]</ref> and image scale augmentation (short size of a image is pic is applied to the backbone network, resulting in a stride-8 model. Synchronized Batch Normalization <ref type="bibr" target="#b64">[65]</ref> is used during training, along with a polynomial-like lear xel cross entropy loss against the ground truth labels. We use multi-scale evaluation with flipping <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" ta
ficantly boosted image classification accuracy through large scale neural architecture search (NAS) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b54">55]</ref>. Despite their sta ed CNN architectures that achieved state-of-the-art classification performance, such as: Amoe-baNet <ref type="bibr" target="#b44">[45]</ref>, MNASNet <ref type="bibr" target="#b53">[54]</ref>, and Ef
p://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Modern CNN Architectures. Since AlexNet <ref type="bibr" target="#b33">[34]</ref>, deep convolutional neural networks <ref type="bibr" targe t convolutional kernels. ResNeXt <ref type="bibr" target="#b60">[61]</ref> adopts group convolution <ref type="bibr" target="#b33">[34]</ref> in the ResNet bottle block, which converts the multi-path
bibr" target="#b54">[55]</ref> as shown in Table <ref type="table">1</ref>. Our single Cascade-RCNN <ref type="bibr" target="#b2">[3]</ref> model using a ResNeSt-101 backbone achieves 48.3% box mAP an
p><p>Multi-path and Feature-map Attention. Multi-path representation has shown success in GoogleNet <ref type="bibr" target="#b51">[52]</ref>, in which each network block consists of different convolu d training crop size of 224, while the Inception-Net family<ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref> uses a training c
vie reviews (MR) <ref type="bibr" target="#b36">(Pang &amp; Lee, 2005)</ref>.</p><p>Comparison with <ref type="bibr" target="#b23">Kalantidis et al. (2020)</ref>: <ref type="bibr" target="#b23">Kalant , 2005)</ref>.</p><p>Comparison with <ref type="bibr" target="#b23">Kalantidis et al. (2020)</ref>: <ref type="bibr" target="#b23">Kalantidis et al. (2020)</ref> also consider ways to sample negatives
ect detection and segmentation tasks <ref type="bibr" target="#b30">(Misra &amp; Maaten, 2020;</ref><ref type="bibr" target="#b19">He et al., 2020)</ref>.</p><p>Contrastive learning relies on two key </ref><ref type="bibr" target="#b4">Chen et al., 2020a;</ref><ref type="bibr" target="#b12">c;</ref><ref type="bibr" target="#b19">He et al., 2020;</ref><ref type="bibr" target="#b6">Chen et al., 2020 very large, we also run experiments using MoCo-v2 with standard negative memory bank size N = 65536 <ref type="bibr" target="#b19">(He et al., 2020;</ref><ref type="bibr" target="#b6">Chen et al., 202
"bibr" target="#b6">Chen et al., 2020c;</ref><ref type="bibr" target="#b34">Oord et al., 2018;</ref><ref type="bibr" target="#b20">HÃ©naff et al., 2020)</ref>. In this paper we ask: is there a better w
ntrastive frameworks.</p><p>Fig. <ref type="figure">3</ref> shows the results of fine-tuning an SVM <ref type="bibr" target="#b3">(Boser et al., 1992;</ref><ref type="bibr" target="#b10">Cortes &amp;
multi-view and contrastive learning <ref type="bibr" target="#b2">(Blum &amp; Mitchell, 1998;</ref><ref type="bibr" target="#b54">Xu et al., 2013;</ref><ref type="bibr" target="#b1">Bachman et al., 2 a node u are collected together to obtain a single final summary embedding for u. As recommended by <ref type="bibr" target="#b54">Xu et al. (2019)</ref> we use concatenation,</p><formula xml:id="form implicity we trained all models using the same set of hyperparameters: we used the GIN architecture <ref type="bibr" target="#b54">(Xu et al., 2019)</ref> with K = 3 layers and embedding dimension d =
s (before/after) as positive samples. Embeddings are trained using the unlabeled BookCorpus dataset <ref type="bibr" target="#b26">(Kiros et al., 2015)</ref>, and evaluated following the protocol of < t the default values and change only the s2v-model.py script. Since the official BookCorpus dataset <ref type="bibr" target="#b26">Kiros et al. (2015)</ref> is not available, we use an unofficial vers



ical success, contrastive learning methods <ref type="bibr" target="#b7">(Chopra et al., 2005;</ref><ref type="bibr" target="#b15">Hadsell et al., 2006)</ref> have become one of the most popular self-
ical success, contrastive learning methods <ref type="bibr" target="#b7">(Chopra et al., 2005;</ref><ref type="bibr" target="#b15">Hadsell et al., 2006)</ref> have become one of the most popular self-
div xmlns="http://www.tei-c.org/ns/1.0"><p>We generalize deep self-attention distillation in MINILM <ref type="bibr" target="#b40">(Wang et al., 2020)</ref> by only using self-attention relation disti den size the same, layer-wisely transferring hidden states and self-attention distributions. MINILM <ref type="bibr" target="#b40">(Wang et al., 2020)</ref> proposes deep self-attention distillation, teacher.</p><p>In this work, we generalize and simplify deep self-attention distillation of MINILM <ref type="bibr" target="#b40">(Wang et al., 2020)</ref> by using self-attention relation distillati the student has the same number of layers as its teacher to perform layer-wise distillation. MINILM <ref type="bibr" target="#b40">(Wang et al., 2020)</ref> transfers selfattention knowledge of teache "bibr" target="#b14">Jiao et al., 2019;</ref><ref type="bibr" target="#b34">Sun et al., 2019b;</ref><ref type="bibr" target="#b40">Wang et al., 2020)</ref>. The student models are distilled from large "bibr" target="#b14">Jiao et al., 2019;</ref><ref type="bibr" target="#b33">Sun et al., 2019a;</ref><ref type="bibr" target="#b40">Wang et al., 2020)</ref>  MobileBERT compresses a specially designed esults of Distil-BERT, TinyBERT 2 , BERT SMALL , Truncated BERT BASE and 6Ã768 MINILM are taken from<ref type="bibr" target="#b40">Wang et al. (2020)</ref>. BERT SMALL</figDesc><table><row><cell>Model
ces and latency. Knowledge distillation (KD; <ref type="bibr" target="#b11">Hinton et al. 2015</ref><ref type="bibr" target="#b28">, Romero et al. 2015)</ref> has been widely employed to compress pret oft target probabilities to train student models. More fine-grained knowledge such as hidden states <ref type="bibr" target="#b28">(Romero et al., 2015)</ref> and attention distributions <ref type="bi
" target="#b23">(Radford et al., 2018;</ref><ref type="bibr" target="#b7">Devlin et al., 2018;</ref><ref type="bibr" target="#b9">Dong et al., 2019;</ref><ref type="bibr" target="#b9">Yang et al., 201 "bibr" target="#b7">Devlin et al., 2018;</ref><ref type="bibr" target="#b9">Dong et al., 2019;</ref><ref type="bibr" target="#b9">Yang et al., 2019;</ref><ref type="bibr" target="#b15">Joshi et al., 2 to pretrain a deep bidirectional Transformer using masked language modeling (MLM) objective. UNILM <ref type="bibr" target="#b9">(Dong et al., 2019)</ref> is jointly pretrained on three types languag

stic compression of pretrained Transformers <ref type="bibr" target="#b30">(Sanh et al., 2019;</ref><ref type="bibr" target="#b36">Tsai et al., 2019;</ref><ref type="bibr" target="#b14">Jiao et al., 2
al., 2015)</ref> and attention distributions <ref type="bibr">(Zagoruyko and Komodakis, 2017;</ref><ref type="bibr" target="#b13">Hu et al., 2018)</ref> are introduced to improve the student model.</
/www.tei-c.org/ns/1.0"><head n="2.1">Backbone Network: Transformer</head><p>Multi-layer Transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> has been widely adopted in pretrained mo
" target="#b23">(Radford et al., 2018;</ref><ref type="bibr" target="#b7">Devlin et al., 2018;</ref><ref type="bibr" target="#b9">Dong et al., 2019;</ref><ref type="bibr" target="#b9">Yang et al., 201 "bibr" target="#b7">Devlin et al., 2018;</ref><ref type="bibr" target="#b9">Dong et al., 2019;</ref><ref type="bibr" target="#b9">Yang et al., 2019;</ref><ref type="bibr" target="#b15">Joshi et al., 2 to pretrain a deep bidirectional Transformer using masked language modeling (MLM) objective. UNILM <ref type="bibr" target="#b9">(Dong et al., 2019)</ref> is jointly pretrained on three types languag
ces and latency. Knowledge distillation (KD; <ref type="bibr" target="#b11">Hinton et al. 2015</ref><ref type="bibr" target="#b28">, Romero et al. 2015)</ref> has been widely employed to compress pret oft target probabilities to train student models. More fine-grained knowledge such as hidden states <ref type="bibr" target="#b28">(Romero et al., 2015)</ref> and attention distributions <ref type="bi
"#b0">Aguilar et al., 2019;</ref><ref type="bibr" target="#b22">Mukherjee and Awadallah, 2020;</ref><ref type="bibr" target="#b20">Xu et al., 2020;</ref><ref type="bibr" target="#b12">Hou et al., 2020 pe="bibr" target="#b20">Xu et al., 2020;</ref><ref type="bibr" target="#b12">Hou et al., 2020;</ref><ref type="bibr" target="#b20">Li et al., 2020)</ref>, which uses a finetuned teacher model to guide
ces and latency. Knowledge distillation (KD; <ref type="bibr" target="#b11">Hinton et al. 2015</ref><ref type="bibr" target="#b28">, Romero et al. 2015)</ref> has been widely employed to compress pret oft target probabilities to train student models. More fine-grained knowledge such as hidden states <ref type="bibr" target="#b28">(Romero et al., 2015)</ref> and attention distributions <ref type="bi
ral architectures, such as DRMM <ref type="bibr" target="#b8">(Guo et al., 2016)</ref> and Co-PACRR <ref type="bibr" target="#b13">(Hui et al., 2018)</ref>, adopt an interaction-based design. They ope
ectures, e.g., Siamese networks <ref type="bibr" target="#b9">(He et al., 2016)</ref> and attention <ref type="bibr" target="#b26">(Seo et al., 2017;</ref><ref type="bibr" target="#b31">Tay et al., 20 her than specific term matches. Context-aware representation learning, such as co-attention methods <ref type="bibr" target="#b26">(Seo et al., 2017)</ref>, has been proved effective in many benchmark ng elements in the missing dimen-sions. Softmax col is the column-wise softmax operator. Similar to <ref type="bibr" target="#b26">Seo et al. (2017)</ref>, we perform co-attention from two directions: ; (2) interaction and attention mecha-nisms <ref type="bibr" target="#b31">(Tay et al., 2019b;</ref><ref type="bibr" target="#b26">Seo et al., 2017;</ref><ref type="bibr" target="#b21">Parikh et al.,
r" target="#b9">(He and Lin, 2016;</ref><ref type="bibr" target="#b29">Sutskever et al., 2014;</ref><ref type="bibr" target="#b39">Yin et al., 2016;</ref><ref type="bibr" target="#b24">Rao et al., 201
n shown to be critical for relevance matching <ref type="bibr" target="#b5">(Dai et al., 2018;</ref><ref type="bibr" target="#b25">Rao et al., 2019)</ref>. On the other hand, the contextual encoder en ">(Lin and Efron, 2013;</ref><ref type="bibr" target="#b18">Lin et al., 2014)</ref>, as prepared by <ref type="bibr" target="#b25">Rao et al. (2019)</ref>, where each dataset contains around 50 querie ults on each dataset.</p><p>For the tweet search task, we mostly follow the experimental setting in <ref type="bibr" target="#b25">Rao et al. (2019)</ref>. Baselines include the classic query likeliho ot" target="#foot_1">2</ref> For L2R, we used LambdaMART (Burges, 2010) on the same feature sets as <ref type="bibr" target="#b25">Rao et al. (2019)</ref> In our experiments, we use trainable 300d wor apture matching signals at different strength levels. Sharing similarities with our architecture is <ref type="bibr" target="#b25">Rao et al. (2019)</ref>, who developed a multi-perspective relevance
n shown to be critical for relevance matching <ref type="bibr" target="#b5">(Dai et al., 2018;</ref><ref type="bibr" target="#b25">Rao et al., 2019)</ref>. On the other hand, the contextual encoder en ">(Lin and Efron, 2013;</ref><ref type="bibr" target="#b18">Lin et al., 2014)</ref>, as prepared by <ref type="bibr" target="#b25">Rao et al. (2019)</ref>, where each dataset contains around 50 querie ults on each dataset.</p><p>For the tweet search task, we mostly follow the experimental setting in <ref type="bibr" target="#b25">Rao et al. (2019)</ref>. Baselines include the classic query likeliho ot" target="#foot_1">2</ref> For L2R, we used LambdaMART (Burges, 2010) on the same feature sets as <ref type="bibr" target="#b25">Rao et al. (2019)</ref> In our experiments, we use trainable 300d wor apture matching signals at different strength levels. Sharing similarities with our architecture is <ref type="bibr" target="#b25">Rao et al. (2019)</ref>, who developed a multi-perspective relevance
models mainly focus on representation-based modeling between the query and documents, such as DSSM <ref type="bibr" target="#b11">(Huang et al., 2013)</ref>, C-DSSM <ref type="bibr">(Shen et al., 201

ibr" target="#b8">(Guo et al., 2016)</ref>, <ref type="bibr">DUET (Mitra et al., 2017)</ref>, K-NRM <ref type="bibr" target="#b36">(Xiong et al., 2017b)</ref>, and PACRR <ref type="bibr" target="#b12" m gating network aggregates weighted matching signals from different query terms. Inspired by DRMM, <ref type="bibr" target="#b36">Xiong et al. (2017b)</ref> proposed K-NRM, which introduced a differe
(Conneau et al., 2017)</ref>, ESIM <ref type="bibr" target="#b3">(Chen et al., 2017b)</ref>, DecAtt <ref type="bibr" target="#b21">(Parikh et al., 2016)</ref>, and PWIM <ref type="bibr" target="#b9">( "bibr" target="#b31">(Tay et al., 2019b;</ref><ref type="bibr" target="#b26">Seo et al., 2017;</ref><ref type="bibr" target="#b21">Parikh et al., 2016;</ref><ref type="bibr" target="#b4">Conneau et al
estion answering or reading comprehension <ref type="bibr" target="#b35">(Xiong et al., 2017a;</ref><ref type="bibr" target="#b30">Tay et al., 2019a)</ref>, SM can help identify the correct answer spa
at existing approaches for textual similarity modeling in NLP can produce poor results for IR tasks <ref type="bibr" target="#b8">(Guo et al., 2016)</ref>, and vice versa <ref type="bibr" target="#b10 relevance matching is fundamentally a matching task, most recent neural architectures, such as DRMM <ref type="bibr" target="#b8">(Guo et al., 2016)</ref> and Co-PACRR <ref type="bibr" target="#b13">( ntext.</p><p>It's worth noting that term importance modeling can be important for some search tasks <ref type="bibr" target="#b8">(Guo et al., 2016)</ref>; therefore, we inject external weights as pri eel et al., 2004)</ref>, learning to rank (L2R), as well as a number of neural ranking models: DRMM <ref type="bibr" target="#b8">(Guo et al., 2016)</ref>, <ref type="bibr">DUET (Mitra et al., 2017)</ uery and the document, often with countbased techniques to address data sparsity. For example, DRMM <ref type="bibr" target="#b8">(Guo et al., 2016)</ref> introduced a pyramid pooling technique to con /ref>, and vice versa <ref type="bibr" target="#b10">(Htut et al., 2018)</ref>.</p><p>Specifically, <ref type="bibr" target="#b8">Guo et al. (2016)</ref> point out three distinguishing characteristics
uery and context representations. Our semantic matching method behaves similarly to the transformer <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref>, which also uses attention (specifically
n the other two on TrecQA, but is comparable on all other datasets. This finding is consistent with <ref type="bibr" target="#b23">Rao et al. (2017a)</ref>, which shows that keyword matching signals a
(Conneau et al., 2017)</ref>, ESIM <ref type="bibr" target="#b3">(Chen et al., 2017b)</ref>, DecAtt <ref type="bibr" target="#b21">(Parikh et al., 2016)</ref>, and PWIM <ref type="bibr" target="#b9">( "bibr" target="#b31">(Tay et al., 2019b;</ref><ref type="bibr" target="#b26">Seo et al., 2017;</ref><ref type="bibr" target="#b21">Parikh et al., 2016;</ref><ref type="bibr" target="#b4">Conneau et al
the following baselines: InferSent <ref type="bibr" target="#b4">(Conneau et al., 2017)</ref>, ESIM <ref type="bibr" target="#b3">(Chen et al., 2017b)</ref>, DecAtt <ref type="bibr" target="#b21">(Par >Gong et al., 2018)</ref> to emphasize salient word pair interactions;</p><p>(3) structure modeling <ref type="bibr" target="#b3">(Chen et al., 2017b)</ref>.</p></div> <div xmlns="http://www.tei-c.org
ce poor results for IR tasks <ref type="bibr" target="#b8">(Guo et al., 2016)</ref>, and vice versa <ref type="bibr" target="#b10">(Htut et al., 2018)</ref>.</p><p>Specifically, <ref type="bibr" targe
ethod (RM3) outperforms most of the neural ranking models except for BERT, which is consistent with <ref type="bibr" target="#b37">Yang et al. (2019a)</ref>. We suggest two reasons: (1) tweets are muc
n many NLP tasks, such as question answering <ref type="bibr" target="#b22">(Rao et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2017a)</ref>, paraphrase detection <ref type="bibr" targe shown from adding exact match signals into representation learning, for example, the Dr.QA model of <ref type="bibr" target="#b2">Chen et al. (2017a)</ref> concatenates exact match scores to word embe
ao et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2017a)</ref>, paraphrase detection <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, and textual semantic similarity modeling <
ype="bibr" target="#b25">Rao et al. (2019)</ref> In our experiments, we use trainable 300d word2vec <ref type="bibr" target="#b19">(Mikolov et al., 2013)</ref> embeddings with the SGD optimizer. For o
et="#b11">(Huang et al., 2013)</ref>, C-DSSM <ref type="bibr">(Shen et al., 2014)</ref>, and SM-CNN <ref type="bibr" target="#b27">(Severyn and Moschitti, 2015)</ref>. These meth-Label SM Score RM Sco
ao et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2017a)</ref>, paraphrase detection <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, and textual semantic similarity modeling <
eration for paraphrasing <ref type="bibr" target="#b15">(Liu et al., 2020)</ref> and summa-rization <ref type="bibr" target="#b30">(Schumann et al., 2020)</ref>.</p></div> <div xmlns="http://www.tei-c
urrent work further shows the success of search-based unsupervised text generation for paraphrasing <ref type="bibr" target="#b15">(Liu et al., 2020)</ref> and summa-rization <ref type="bibr" target="
fer. However, their model is hard to interpret and control, like other neural network-based models. <ref type="bibr" target="#b22">Narayan and Gardent (2016)</ref> attempted to address both issues usi model seeks the best simplified candidate sentence according to the scoring function. Compared with <ref type="bibr" target="#b22">Narayan and Gardent (2016)</ref>, the order of our simplification ope ot perform syntactic simplification since syntax typically does not change in style-transfer tasks. <ref type="bibr" target="#b22">Narayan and Gardent (2016)</ref> built a pipeline-based unsupervised
)</ref> performed probabilistic sentence splitting and deletion, followed by MT-based paraphrasing. <ref type="bibr" target="#b23">Nisioi et al. (2017)</ref> employed neural machine translation (NMT) neural machine translation (NMT) systems: EncDecA, which is a vanilla Seq2Seq model with attention <ref type="bibr" target="#b23">(Nisioi et al., 2017)</ref>; Dress and Dress-Ls, which are based on d
ed that simplification benefits from multi-task learning with paraphrase and entailment generation. <ref type="bibr" target="#b17">Martin et al. (2019)</ref> enhanced the transformer architecture with target="#b12">(Kriz et al., 2019)</ref>; and Access, which is based on the transformer architecture <ref type="bibr" target="#b17">(Martin et al., 2019)</ref>. Finally, we compare with a supervised ed
ed way by either phrase-based machine translation <ref type="bibr">(PBMT, Wubben et al., 2012;</ref><ref type="bibr" target="#b21">Narayan and Gardent, 2014;</ref><ref type="bibr" target="#b38">Xu et ., 2010)</ref> and dependency trees <ref type="bibr" target="#b1">(Bingel and SÃ¸gaard, 2016)</ref>. <ref type="bibr" target="#b21">Narayan and Gardent (2014)</ref> performed probabilistic sentence spl br" target="#b38">(Xu et al., 2016)</ref>, which uses an external paraphrasing database; and Hybrid <ref type="bibr" target="#b21">(Narayan and Gardent, 2014)</ref>, which uses a combination of PBMT a
tions are reported in the columns "Add," "Delete," and "Keep."</p><p>We also compute the BLEU score <ref type="bibr" target="#b24">(Papineni et al., 2002)</ref> to measure the closeness between a cand
le, constituency trees <ref type="bibr" target="#b42">(Zhu et al., 2010)</ref> and dependency trees <ref type="bibr" target="#b1">(Bingel and SÃ¸gaard, 2016)</ref>. <ref type="bibr" target="#b21">Naray
p>First, we replace an LM's estimated sentence probability with the syntactic log-odds ratio (SLOR, <ref type="bibr" target="#b25">Pauls and Klein, 2012)</ref>, to better measure fluency and human acc
tions are reported in the columns "Add," "Delete," and "Keep."</p><p>We also compute the BLEU score <ref type="bibr" target="#b24">(Papineni et al., 2002)</ref> to measure the closeness between a cand
ill require large volumes of aligned data to learn these operations. To deal with the second issue, <ref type="bibr" target="#b33">Surya et al. (2019)</ref> recently proposed an unsupervised neural te h word and phrase levels in an unsupervised manner.</p><p>For unsupervised sentence simplification, <ref type="bibr" target="#b33">Surya et al. (2019)</ref> adopted style-transfer techniques, using ad g different automatic evaluation metrics.</p><p>For unsupervised competing methods, we compare with <ref type="bibr" target="#b33">Surya et al. (2019)</ref>, which is inspired by unsupervised neural m
ral language generation tasks, such as style transfer, paraphrasing, and sentence error correction. <ref type="bibr" target="#b14">Li et al. (2018)</ref> proposed edit-based style transfer without par
ness between a candidate and a reference. <ref type="bibr" target="#b38">Xu et al. (2016)</ref> and <ref type="bibr" target="#b32">Sulem et al. (2018)</ref> show that BLEU correlates with human judgem
ill require large volumes of aligned data to learn these operations. To deal with the second issue, <ref type="bibr" target="#b33">Surya et al. (2019)</ref> recently proposed an unsupervised neural te h word and phrase levels in an unsupervised manner.</p><p>For unsupervised sentence simplification, <ref type="bibr" target="#b33">Surya et al. (2019)</ref> adopted style-transfer techniques, using ad g different automatic evaluation metrics.</p><p>For unsupervised competing methods, we compare with <ref type="bibr" target="#b33">Surya et al. (2019)</ref>, which is inspired by unsupervised neural m
ed way by either phrase-based machine translation <ref type="bibr">(PBMT, Wubben et al., 2012;</ref><ref type="bibr" target="#b21">Narayan and Gardent, 2014;</ref><ref type="bibr" target="#b38">Xu et ., 2010)</ref> and dependency trees <ref type="bibr" target="#b1">(Bingel and SÃ¸gaard, 2016)</ref>. <ref type="bibr" target="#b21">Narayan and Gardent (2014)</ref> performed probabilistic sentence spl br" target="#b38">(Xu et al., 2016)</ref>, which uses an external paraphrasing database; and Hybrid <ref type="bibr" target="#b21">(Narayan and Gardent, 2014)</ref>, which uses a combination of PBMT a
/ref> used reinforcement learning to optimize a reward based on simplicity, fluency, and relevance. <ref type="bibr" target="#b40">Zhao et al. (2018a)</ref> integrated the transformer architecture and on deep reinforcement learning <ref type="bibr" target="#b39">(Zhang and Lapata, 2017)</ref>; DMass <ref type="bibr" target="#b40">(Zhao et al., 2018a)</ref>, which is a transformer-based model with e
p>First, we replace an LM's estimated sentence probability with the syntactic log-odds ratio (SLOR, <ref type="bibr" target="#b25">Pauls and Klein, 2012)</ref>, to better measure fluency and human acc
le, constituency trees <ref type="bibr" target="#b42">(Zhu et al., 2010)</ref> and dependency trees <ref type="bibr" target="#b1">(Bingel and SÃ¸gaard, 2016)</ref>. <ref type="bibr" target="#b21">Naray
ed way by either phrase-based machine translation <ref type="bibr">(PBMT, Wubben et al., 2012;</ref><ref type="bibr" target="#b21">Narayan and Gardent, 2014;</ref><ref type="bibr" target="#b38">Xu et ., 2010)</ref> and dependency trees <ref type="bibr" target="#b1">(Bingel and SÃ¸gaard, 2016)</ref>. <ref type="bibr" target="#b21">Narayan and Gardent (2014)</ref> performed probabilistic sentence spl br" target="#b38">(Xu et al., 2016)</ref>, which uses an external paraphrasing database; and Hybrid <ref type="bibr" target="#b21">(Narayan and Gardent, 2014)</ref>, which uses a combination of PBMT a
r, syntactic information was also considered in the PBMT framework, for example, constituency trees <ref type="bibr" target="#b42">(Zhu et al., 2010)</ref> and dependency trees <ref type="bibr" target
#b25">Pauls and Klein, 2012)</ref>, to better measure fluency and human acceptability. According to <ref type="bibr" target="#b13">Lau et al. (2017)</ref>, SLOR shows the best correlation to human acc
LM, i.e., in addition to words, we use part-of-speech (POS) and dependency tags as inputs to the LM <ref type="bibr" target="#b41">(Zhao et al., 2018b)</ref>. For a word w i , the input to the syntaxa
">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, and typically require pixel-level correspondences of sens better correspondence, MMF <ref type="bibr" target="#b25">[26]</ref> adopts continuous convolution <ref type="bibr" target="#b13">[14]</ref> to build dense LiDAR BEV feature maps and do point-wise fe
nformation, they suffer from sensitivity to data alignment, often involve complicated architectures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bib -based proposal generation might fail in some cases that could only be observed from 3D space. MV3D <ref type="bibr" target="#b10">[11]</ref> and AVOD <ref type="bibr" target="#b11">[12]</ref> project
data alignment, often involve complicated architectures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bib that could only be observed from 3D space. MV3D <ref type="bibr" target="#b10">[11]</ref> and AVOD <ref type="bibr" target="#b11">[12]</ref> project the raw point cloud into bird's eye view (BEV) to
ef> estimate 3D object information by calculating the similarity between 3D objects and CAD models. <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref> explore usin
="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> are hampered by typically lower input data resolution than v data structure, it uses sparse 3D CNNs which reduces the inference time significantly. PointPillars <ref type="bibr" target="#b8">[9]</ref> uses PointNets <ref type="bibr" target="#b6">[7]</ref> in an The 3D detectors we incorporated are: SECOND <ref type="bibr" target="#b5">[6]</ref>, PointPillars <ref type="bibr" target="#b8">[9]</ref>, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and PV-RC is 0.5. Here for 3D detectors, only SECOND <ref type="bibr" target="#b5">[6]</ref> and PointPillars <ref type="bibr" target="#b8">[9]</ref> publish their training configurations for class pedestrian a
mpared to 2D object detection, which has been well-studied <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t
on, LiDAR methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" t e inference time significantly. PointPillars <ref type="bibr" target="#b8">[9]</ref> uses PointNets <ref type="bibr" target="#b6">[7]</ref> in an encoder that represents point clouds organized in vert
been well-studied <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, 3D object detectio
submit the detection results to KITTI server. For experimental studies, we follow the convention in <ref type="bibr" target="#b27">[28]</ref> to split the original training samples into 3712 training
CNN and PointNet-based set abstraction to learn more discriminative features. Besides, Part-A 2 in <ref type="bibr" target="#b22">[23]</ref> explores predicting intra-object part locations (lower lef
parameters needed to specify 3D oriented bounding boxes around targets. In addition, LiDAR methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" t is still relatively poor. Methods vary by how they encode and learn features from raw point cloud. <ref type="bibr" target="#b4">[5]</ref> uses voxels to encode the raw point cloud, and 3D CNNs (Conv d bounding box regression. SECOND <ref type="bibr" target="#b5">[6]</ref> is the upgrade version of <ref type="bibr" target="#b4">[5]</ref>, since raw LiDAR point cloud has very sparse data structure,
xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D Detection Using 2D Images</head><p>Mousavian et al. <ref type="bibr" target="#b14">[15]</ref> leverage the geometric constraints between 2D and 3D bound
parameters needed to specify 3D oriented bounding boxes around targets. In addition, LiDAR methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" t is still relatively poor. Methods vary by how they encode and learn features from raw point cloud. <ref type="bibr" target="#b4">[5]</ref> uses voxels to encode the raw point cloud, and 3D CNNs (Conv d bounding box regression. SECOND <ref type="bibr" target="#b5">[6]</ref> is the upgrade version of <ref type="bibr" target="#b4">[5]</ref>, since raw LiDAR point cloud has very sparse data structure,
sor T. Note that for the first three convolution layers, after each convolution layer applied, ReLU <ref type="bibr" target="#b26">[27]</ref> is used. Since we have saved the indices of these non-empt
target="#b23">[24]</ref>, Pointfusion <ref type="bibr" target="#b12">[13]</ref> and Frustum ConvNet <ref type="bibr" target="#b24">[25]</ref> are the representatives of 2D driven 3D detectors, which e
="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> are hampered by typically lower input data resolution than v data structure, it uses sparse 3D CNNs which reduces the inference time significantly. PointPillars <ref type="bibr" target="#b8">[9]</ref> uses PointNets <ref type="bibr" target="#b6">[7]</ref> in an The 3D detectors we incorporated are: SECOND <ref type="bibr" target="#b5">[6]</ref>, PointPillars <ref type="bibr" target="#b8">[9]</ref>, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and PV-RC is 0.5. Here for 3D detectors, only SECOND <ref type="bibr" target="#b5">[6]</ref> and PointPillars <ref type="bibr" target="#b8">[9]</ref> publish their training configurations for class pedestrian a
eatures. In order to fuse features from different sensor modalities with better correspondence, MMF <ref type="bibr" target="#b25">[26]</ref> adopts continuous convolution <ref type="bibr" target="#b1
="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> are hampered by typically lower input data resolution than v data structure, it uses sparse 3D CNNs which reduces the inference time significantly. PointPillars <ref type="bibr" target="#b8">[9]</ref> uses PointNets <ref type="bibr" target="#b6">[7]</ref> in an The 3D detectors we incorporated are: SECOND <ref type="bibr" target="#b5">[6]</ref>, PointPillars <ref type="bibr" target="#b8">[9]</ref>, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and PV-RC is 0.5. Here for 3D detectors, only SECOND <ref type="bibr" target="#b5">[6]</ref> and PointPillars <ref type="bibr" target="#b8">[9]</ref> publish their training configurations for class pedestrian a
="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> are hampered by typ etection; it enables inference at 62 Hz; Compared with one-stage methods discussed above, PointRCNN <ref type="bibr" target="#b7">[8]</ref>, Fast PointRCNN <ref type="bibr" target="#b19">[20]</ref> an type="bibr" target="#b5">[6]</ref>, PointPillars <ref type="bibr" target="#b8">[9]</ref>, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and PV-RCNN <ref type="bibr" target="#b21">[22]</ref>. While ef> and Cascade R-CNN <ref type="bibr" target="#b30">[31]</ref>, written as CLOCs SecCas, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type="bib
erate 3D proposals in a bottomup manner and then refines these proposals in a second stage. PV-RCNN <ref type="bibr" target="#b21">[22]</ref> leverages the advantages of both 3D voxel CNN and PointNet f type="bibr" target="#b8">[9]</ref>, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and PV-RCNN <ref type="bibr" target="#b21">[22]</ref>. While not the top performers within the KITTI leaderboard as, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type="bibr" target="#b21">[22]</ref> and Cascade R-CNN, as CLOCs PVCas. All the other combinati
been well-studied <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, 3D object detectio
="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> are hampered by typically lower input data resolution than v data structure, it uses sparse 3D CNNs which reduces the inference time significantly. PointPillars <ref type="bibr" target="#b8">[9]</ref> uses PointNets <ref type="bibr" target="#b6">[7]</ref> in an The 3D detectors we incorporated are: SECOND <ref type="bibr" target="#b5">[6]</ref>, PointPillars <ref type="bibr" target="#b8">[9]</ref>, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and PV-RC is 0.5. Here for 3D detectors, only SECOND <ref type="bibr" target="#b5">[6]</ref> and PointPillars <ref type="bibr" target="#b8">[9]</ref> publish their training configurations for class pedestrian a
xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D Detection Using 2D Images</head><p>Mousavian et al. <ref type="bibr" target="#b14">[15]</ref> leverage the geometric constraints between 2D and 3D bound
been well-studied <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, 3D object detectio
eatures. In order to fuse features from different sensor modalities with better correspondence, MMF <ref type="bibr" target="#b25">[26]</ref> adopts continuous convolution <ref type="bibr" target="#b1
ref> leverage the geometric constraints between 2D and 3D bounding boxes to recover 3D information. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> estimate 3D obj
architectures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, and typically etup for self-driving cars. Frustum PointNet <ref type="bibr" target="#b23">[24]</ref>, Pointfusion <ref type="bibr" target="#b12">[13]</ref> and Frustum ConvNet <ref type="bibr" target="#b24">[25]</r
target="#b23">[24]</ref>, Pointfusion <ref type="bibr" target="#b12">[13]</ref> and Frustum ConvNet <ref type="bibr" target="#b24">[25]</ref> are the representatives of 2D driven 3D detectors, which e
in this section since this is the most common sensor setup for self-driving cars. Frustum PointNet <ref type="bibr" target="#b23">[24]</ref>, Pointfusion <ref type="bibr" target="#b12">[13]</ref> and
parameters needed to specify 3D oriented bounding boxes around targets. In addition, LiDAR methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" t is still relatively poor. Methods vary by how they encode and learn features from raw point cloud. <ref type="bibr" target="#b4">[5]</ref> uses voxels to encode the raw point cloud, and 3D CNNs (Conv d bounding box regression. SECOND <ref type="bibr" target="#b5">[6]</ref> is the upgrade version of <ref type="bibr" target="#b4">[5]</ref>, since raw LiDAR point cloud has very sparse data structure,
bounding boxes around targets. In addition, LiDAR methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" t etworks) are applied to learn voxel features for classification and bounding box regression. SECOND <ref type="bibr" target="#b5">[6]</ref> is the upgrade version of <ref type="bibr" target="#b4">[5]< ted examples. Because we take the raw predictions before NMS, k and n are large numbers, for SECOND <ref type="bibr" target="#b5">[6]</ref>, there are 70400 (200 Ã 176 Ã 2) predictions in each frame. scade R-CNN <ref type="bibr" target="#b30">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type="bibr" target="#b5">[6]</ref>, PointPillars <ref type="bibr" target="#b8">[9]</ref>, Point official KITTI test server from three fusion combinations of 2D and 3D detectors, which are SECOND <ref type="bibr" target="#b5">[6]</ref> and Cascade R-CNN <ref type="bibr" target="#b30">[31]</ref>, dation set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type="bibr" target="#b5">[6]</ref> and PointPillars <ref type="bibr" target="#b8">[9]</ref> pub
been well-studied <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, 3D object detectio
erate 3D proposals in a bottomup manner and then refines these proposals in a second stage. PV-RCNN <ref type="bibr" target="#b21">[22]</ref> leverages the advantages of both 3D voxel CNN and PointNet f type="bibr" target="#b8">[9]</ref>, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and PV-RCNN <ref type="bibr" target="#b21">[22]</ref>. While not the top performers within the KITTI leaderboard as, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type="bibr" target="#b21">[22]</ref> and Cascade R-CNN, as CLOCs PVCas. All the other combinati
so related to metric learning works that employ generators <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">87]</ref>. Apart from not requiring labels, our method exploits the m od exploits the memory component, something not present in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">87]</ref>. It has no extra parameters or loss terms that need to be o </ref><ref type="bibr" target="#b36">35]</ref>. Works like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">87]</ref> use generators to synthesize negatives in a supervised scen and exploit its memory component. What is more, and unlike <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">87]</ref>, we do not require a generator, i.e. have no extra paramete izing negatives was explored in metric learning literature <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">87,</ref><ref type="bibr" target="#b36">35]</ref>. Works like <ref ty
earning visual representations in a self-supervised manner <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">30]</ref>. Pushing the embeddings of two transformed versions of the ations learned in an unsupervised way. It is however shown <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">30]</ref> that increasing the memory/batch size leads to diminishing t also use contrastive learning losses. These include MoCo <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">30]</ref>, SimCLR <ref type="bibr" target="#b10">[11,</ref><ref type= contain the whole training set, while the recent Momentum Contrast (or MoCo) approach of He et al. <ref type="bibr" target="#b31">[30]</ref> keeps a queue with features of the last few batches as mem t of self-supervised representation learning. We delve deeper into learning with a momentum encoder <ref type="bibr" target="#b31">[30]</ref> and show evidence that harder negatives are required to fa ions. a)</head><p>We delve deeper into a top-performing contrastive self-supervised learning method <ref type="bibr" target="#b31">[30]</ref> and observe the need for harder negatives; b) We propose h air, which is contrasted with every feature n in the bank of negatives (Q) also called the queue in <ref type="bibr" target="#b31">[30]</ref>. A popular and highly successful loss function for contras "bibr" target="#b65">64,</ref><ref type="bibr" target="#b78">77]</ref>, a queue of the last batches <ref type="bibr" target="#b31">[30]</ref>, or simply be every other image in the current minibatch < computing them as the encoder keeps changing. The Momentum Contrast (or MoCo) approach of He et al. <ref type="bibr" target="#b31">[30]</ref> offers a compromise between the two negative sampling extr and all features in Q are encoded with the key encoder.</p><p>How hard are MoCo negatives? In MoCo <ref type="bibr" target="#b31">[30]</ref> (resp. SimCLR <ref type="bibr" target="#b10">[11]</ref>) t helping a lot towards learning the proxy task.</p><p>On the difficulty of the proxy task. For MoCo <ref type="bibr" target="#b31">[30]</ref>, SimCLR <ref type="bibr" target="#b10">[11]</ref>, InfoMin i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type="bibr" target="#b31">[30]</ref> and MoCo-v2 <ref type="bibr" target="#b12">[13]</ref>. MoC rop testing. For object detection on PASCAL VOC <ref type="bibr" target="#b18">[19]</ref> we follow <ref type="bibr" target="#b31">[30]</ref> and fine-tune a Faster R-CNN <ref type="bibr" target="#b55 pe="foot" target="#foot_2">3</ref> code and report the common AP, AP50 and AP75 metrics. Similar to <ref type="bibr" target="#b31">[30]</ref>, we do not perform hyperparameter tuning for the object de tic segmentation on the COCO dataset <ref type="bibr" target="#b42">[41]</ref>. Following He et al. <ref type="bibr" target="#b31">[30]</ref>, we use Mask R-CNN <ref type="bibr" target="#b29">[29]</re nd on the train2017 set (118k images) and evaluate on val2017. We adopt feature normalization as in <ref type="bibr" target="#b31">[30]</ref> when fine-tuning. MoCHi and MoCo use the same hyper-parame llowing our discussion in Section 3.2, we wanted to verify that hardness of the proxy task for MoCo <ref type="bibr" target="#b31">[30]</ref> is directly correlated to the difficulty of the transforma i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type="bibr" target="#b31">[30]</ref>, MoCo-v2 <ref type="bibr" target="#b12">[13]</ref> and som et="#b53">[52]</ref> study the robustness of contrastive self-supervised learning methods like MoCo <ref type="bibr" target="#b31">[30]</ref> and PIRL <ref type="bibr" target="#b47">[46]</ref> and saw [11]</ref>, e.g. the addition of a target network whose parameter update is lagging similar to MoCo <ref type="bibr" target="#b31">[30]</ref>  Synthesizing for supervised metric learning. Recently, sy rt in parenthesis the difference to MoCo-v2. * denotes reproduced results. â  results are copied from<ref type="bibr" target="#b31">[30]</ref>. We bold (resp. underline) the highest results overall (re rg/ns/1.0" place="foot" n="1" xml:id="foot_0">In this section we study contrastive learning for MoCo<ref type="bibr" target="#b31">[30]</ref> on ImageNet-100, a subset of ImageNet consisting of 100 cl et="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" tar d highly successful loss function for contrastive learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b65">64]</ref> is the following:</ 2 -normalized. In a number of recent successful approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" tar pervised learning papers do not discuss variance ; in fact only papers from highly resourceful labs <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" ta
then predicting or verifying the order of frames or clips <ref type="bibr" target="#b48">[47,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b80">79]</ref>, predicting the "ar
et="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b49">48]</ref>. Instance discrimin
speech transcripts <ref type="bibr" target="#b63">[62,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b45">44]</ref> or surrounding text <ref type="bibr" target="#b23">[24]</re
ab_4">2</ref> we present results for object detection and semantic segmentation on the COCO dataset <ref type="bibr" target="#b42">[41]</ref>. Following He et al. <ref type="bibr" target="#b31">[30]</
then predicting or verifying the order of frames or clips <ref type="bibr" target="#b48">[47,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b80">79]</ref>, predicting the "ar
0">[11]</ref>, or keep large memory banks. Approaches like <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b78">77]</ref> use memories that contain the whole training set, while the age in the dataset <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" target="#b78">77]</ref>, a queue of the last batches <ref type="bibr" target="#b31" e time consuming task of keeping a large memory up-to-date <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b78">77]</ref>. In the latter case, a trade-off exists between the "freshn ype="bibr" target="#b37">36,</ref><ref type="bibr" target="#b49">48]</ref>. Instance discrimination <ref type="bibr" target="#b78">[77]</ref> and CPC <ref type="bibr" target="#b50">[49]</ref> were amo
ods in the table, e.g. PCL <ref type="bibr" target="#b41">[40]</ref>, or the clustering approach of <ref type="bibr" target="#b8">[9]</ref>. Both unfortunately use a different setup for PASCAL VOC and

ab_4">2</ref> we present results for object detection and semantic segmentation on the COCO dataset <ref type="bibr" target="#b42">[41]</ref>. Following He et al. <ref type="bibr" target="#b31">[30]</
br" target="#b39">38,</ref><ref type="bibr" target="#b80">79]</ref>, predicting the "arrow of time" <ref type="bibr" target="#b75">[74]</ref>, pace <ref type="bibr" target="#b73">[72]</ref> or predict
get="#b28">28,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b58">57,</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" tar
-supervised learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b77">76,</ref><ref type="bibr" tar contrastive learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" tar /ref><ref type="bibr" target="#b77">76,</ref><ref type="bibr" target="#b79">78]</ref>. Iscen et al. <ref type="bibr" target="#b34">[33]</ref> mine hard negatives from a large set by focusing on the fe
0">[11]</ref>, or keep large memory banks. Approaches like <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b78">77]</ref> use memories that contain the whole training set, while the age in the dataset <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" target="#b78">77]</ref>, a queue of the last batches <ref type="bibr" target="#b31" e time consuming task of keeping a large memory up-to-date <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b78">77]</ref>. In the latter case, a trade-off exists between the "freshn ype="bibr" target="#b37">36,</ref><ref type="bibr" target="#b49">48]</ref>. Instance discrimination <ref type="bibr" target="#b78">[77]</ref> and CPC <ref type="bibr" target="#b50">[49]</ref> were amo
speech transcripts <ref type="bibr" target="#b63">[62,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b45">44]</ref> or surrounding text <ref type="bibr" target="#b23">[24]</re
0">[11]</ref>, or keep large memory banks. Approaches like <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b78">77]</ref> use memories that contain the whole training set, while the age in the dataset <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" target="#b78">77]</ref>, a queue of the last batches <ref type="bibr" target="#b31" e time consuming task of keeping a large memory up-to-date <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b78">77]</ref>. In the latter case, a trade-off exists between the "freshn ype="bibr" target="#b37">36,</ref><ref type="bibr" target="#b49">48]</ref>. Instance discrimination <ref type="bibr" target="#b78">[77]</ref> and CPC <ref type="bibr" target="#b50">[49]</ref> were amo
time, data mixing techniques operating at either the pixel <ref type="bibr" target="#b71">[70,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b86">85]</ref> or the feature leve pe="bibr" target="#b57">[56,</ref><ref type="bibr" target="#b70">69,</ref><ref type="bibr">71,</ref><ref type="bibr" target="#b84">83]</ref> have been shown to be highly effective data augmentation st
best of both worlds by combining contrastive and clustering losses. Methods like local aggregation <ref type="bibr" target="#b90">[89]</ref>, Prototypical Contrastive learning <ref type="bibr" target
time, data mixing techniques operating at either the pixel <ref type="bibr" target="#b71">[70,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b86">85]</ref> or the feature leve pe="bibr" target="#b57">[56,</ref><ref type="bibr" target="#b70">69,</ref><ref type="bibr">71,</ref><ref type="bibr" target="#b84">83]</ref> have been shown to be highly effective data augmentation st
rget="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b66">65,</ref><ref type="bibr" target="#b74">73]</ref>, heavy data augmentations applied to the same image are cru edding space.</p><p>Measuring the utilization of the embedding space. Very recently, Wang and Isola <ref type="bibr" target="#b74">[73]</ref> presented two losses/metrics for assessing contrastive lea /head><p>The uniformity experiment in Figure <ref type="figure">3c</ref> is based on Wang and Isola <ref type="bibr" target="#b74">[73]</ref>. We follow the same definitions of the losses/metrics as p erstand the underlying mechanism that make it work so well <ref type="bibr" target="#b68">[67,</ref><ref type="bibr" target="#b74">73,</ref><ref type="bibr" target="#b60">59,</ref><ref type="bibr" tar
et="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b49">48]</ref>. Instance discrimin
een proposed and achieved state-of-the-art results in SISR <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" tar have been proposed for a better performance. Lim et al. proposed a very deep and wide network EDSR <ref type="bibr" target="#b17">[18]</ref> by stacking modified residual blocks in which the batch no the performance. Fig. <ref type="figure">3</ref>(Left) depicts a basic residual module used in EDSR <ref type="bibr" target="#b17">[18]</ref> and ESRGAN <ref type="bibr" target="#b30">[31]</ref>. The ion, we investigate the combination of our RFA framework with the basic residual block used in EDSR <ref type="bibr" target="#b17">[18]</ref>. Different from the original residual block used in image N <ref type="bibr" target="#b14">[15]</ref>, MemNet <ref type="bibr" target="#b24">[25]</ref>, EDSR <ref type="bibr" target="#b17">[18]</ref>, SRMD <ref type="bibr" target="#b35">[36]</ref>, NLRN <ref n the top row, which can ease the training difficulty to some extent (e.g. residual scaling in EDSR <ref type="bibr" target="#b17">[18]</ref>).</p><p>(2) Feature maps after the attention mechanism ten rchitectures. Here we introduce one of the basic architecture used by some state-of-the-art methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" ta
in recent computer vision tasks, such as image captioning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2]</ref>, image and video classification <ref type="bibr" target="#b7"
in recent computer vision tasks, such as image captioning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2]</ref>, image and video classification <ref type="bibr" target="#b7"
me. To address this problem, Shi et al. proposed an efficient subpixel convolutional layer in ESPCN <ref type="bibr" target="#b22">[23]</ref>, where LR feature maps are upscaled into HR output at the key module of the reconstruction part is the upscale module, where appropriate number of sub-pixel <ref type="bibr" target="#b22">[23]</ref> convolutions are applied.</p><p>The SR network will be opt
arget="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar <ref type="bibr" target="#b32">[33]</ref>, B100 <ref type="bibr" target="#b19">[20]</ref>, Urban100 <ref type="bibr" target="#b11">[12]</ref>, and Manga109 <ref type="bibr" target="#b20">[21]</ref>. B
the most informative parts of an input signal <ref type="bibr" target="#b7">[8]</ref>. Wang et al. <ref type="bibr" target="#b28">[29]</ref> proposed a powerful trunk-and-mask attention mechanism ins
me. To address this problem, Shi et al. proposed an efficient subpixel convolutional layer in ESPCN <ref type="bibr" target="#b22">[23]</ref>, where LR feature maps are upscaled into HR output at the key module of the reconstruction part is the upscale module, where appropriate number of sub-pixel <ref type="bibr" target="#b22">[23]</ref> convolutions are applied.</p><p>The SR network will be opt
arget="#b33">[34]</ref>, and recent learning based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar
f> lacks of a large receptive field which is essential for image SR and the Non-Local mechanisms in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3]</ref> consume a lot of com y to implement the spatial attention block is to use the Non-Local block. Actually, there are works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3]</ref> that have attempted DSR <ref type="bibr" target="#b17">[18]</ref>, SRMD <ref type="bibr" target="#b35">[36]</ref>, NLRN <ref type="bibr" target="#b18">[19]</ref>, DBPN <ref type="bibr" target="#b5">[6]</ref>, RDN <ref ty N <ref type="bibr" target="#b14">[15]</ref>, MemNet <ref type="bibr" target="#b24">[25]</ref>, NLRN <ref type="bibr" target="#b18">[19]</ref>, SRMD <ref type="bibr" target="#b35">[36]</ref>, DBPN <ref
ring works, many CNN-based methods have been proposed and achieved state-of-the-art results in SISR <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta CNN <ref type="bibr" target="#b4">[5]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, MemNet <ref type="bibr" target="#b24">[25]</ref>, EDSR <r CNN <ref type="bibr" target="#b4">[5]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, MemNet <ref type="bibr" target="#b24">[25]</ref>, NLRN <r
sting, we use five standard benchmark datasets: Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b32">[33]</ref>, B100 <ref type="bibr" target="#b19">[20]</ref>, Urban100
l-established and recent spatial L2 prefetchers (prefetchers that prefetch within a spatial region) <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bib a prefetching (VLDP) <ref type="bibr" target="#b44">[45]</ref> and signature path prefetching (SPP) <ref type="bibr" target="#b32">[33]</ref> are well known delta prefetchers. VLDP stores the history ing proposals <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b32">[33]</ref> have also been coded and evaluated with ChampSim, helping improve performance for server workloads like CloudSuite <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bib
(throttlers) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bib
017 <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and CloudSuite <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref> (four-core mixes #b16">[17]</ref> (four-core mixes spread across six phases <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>) benchmarks. We also use a set of Convolutional Neural Netwo
efetchers such as Bestoffset Prefetcher (BOP) <ref type="bibr" target="#b37">[38]</ref> and Sandbox <ref type="bibr" target="#b41">[42]</ref> prefetcher explore multiple offsets. An offset of k means

ion has been used extensively at the L2 and the LLC <ref type="bibr" target="#b13">[14]</ref> [56], <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bib ">[29]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b59">[60]</ref>. IPCP is resilient to the underlying replacement policies

hese spatial prefetchers, there are temporal prefetchers <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bib target="#b22">[23]</ref>.</p><p>Temporal Prefetchers: Temporal prefetchers like temporal streaming <ref type="bibr" target="#b54">[55]</ref>, Irregular Stream Buffer (ISB) <ref type="bibr" target="#b
">[20]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, <ref type="bib
rks. We also use a set of Convolutional Neural Networks (CNNs) and a Recurrent Neural Network (RNN) <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bib
ses. Hardware prefetchers such as next-line (NL) and stride based on instruction pointer (IPstride) <ref type="bibr" target="#b17">[18]</ref> are some of the simple, efficient, and light-weight data p
similar from every other image in the dataset <ref type="bibr" target="#b50">(Wu et al., 2018;</ref><ref type="bibr" target="#b1">Chen et al., 2020)</ref>. We propose to train feature representations " target="#b26">Misra &amp; Maaten, 2020;</ref><ref type="bibr" target="#b15">He et al., 2020;</ref><ref type="bibr" target="#b1">Chen et al., 2020)</ref> which aims to learn representations by consid main. We use a state-of-the-art self-supervised loss function based on contrastive learning: SimCLR <ref type="bibr" target="#b1">(Chen et al., 2020)</ref>. The SimCLR loss encourages two augmentation seline, SimCLR that uses the novel domain unlabeled data D u to train a representation using SimCLR <ref type="bibr" target="#b1">(Chen et al., 2020)</ref>, and then uses the resulting representation
br" target="#b54">Yu et al., 2020;</ref><ref type="bibr" target="#b33">RodrÃ­guez et al., 2020;</ref><ref type="bibr" target="#b47">Wang et al., 2020)</ref> and transductive few-shot learning (T-FSL) < bibr" target="#b5">Dhillon et al., 2020;</ref><ref type="bibr" target="#b18">Hou et al., 2019;</ref><ref type="bibr" target="#b47">Wang et al., 2020;</ref><ref type="bibr" target="#b33">RodrÃ­guez et a
ving jigsaw puzzles <ref type="bibr" target="#b29">(Noroozi &amp; Favaro, 2016)</ref>, colorization <ref type="bibr" target="#b56">(Zhang et al., 2016)</ref> or predicting rotation <ref type="bibr">(G
e="bibr" target="#b32">(Ren et al., 2018;</ref><ref type="bibr" target="#b22">Li et al., 2019;</ref><ref type="bibr" target="#b54">Yu et al., 2020;</ref><ref type="bibr" target="#b33">RodrÃ­guez et al.




> and transductive few-shot learning (T-FSL) <ref type="bibr" target="#b23">(Liu et al., 2019;</ref><ref type="bibr" target="#b5">Dhillon et al., 2020;</ref><ref type="bibr" target="#b18">Hou et al.,

"bibr">Ravi &amp; Larochelle, 2017;</ref><ref type="bibr" target="#b28">Nichol &amp; Schulman;</ref><ref type="bibr" target="#b34">Rusu et al., 2019;</ref><ref type="bibr" target="#b37">Sun et al., 20
large-scale ST dataset, multitask learning <ref type="bibr" target="#b32">(Weiss et al. 2017;</ref><ref type="bibr" target="#b6">BÃ©rard et al. 2018</ref>) and pretraining techniques <ref type="bibr" ficantly increases the learning difficulty.</p><p>â¢ Non-pre-trained Attention Module: Previous work <ref type="bibr" target="#b6">(BÃ©rard et al. 2018)</ref> trains attention modules for ASR, MT and ST
l. 2018)</ref>. To tackle this problem, existing approaches can be categorized into cascaded method <ref type="bibr" target="#b25">(Ney 1999;</ref><ref type="bibr" target="#b21">Ma et al. 2019)</ref>,
t al. 2017;</ref><ref type="bibr" target="#b6">BÃ©rard et al. 2018</ref>) and pretraining techniques <ref type="bibr" target="#b3">(Bansal et al. 2019)</ref> have been applied to end-to-end ST model to

outputs of an automatic speech recognition (ASR) system into target language, and end-to-end method <ref type="bibr" target="#b8">(Duong et al. 2016;</ref><ref type="bibr" target="#b32">Weiss et al. 2
or example in emergency calls, where agents have to respond emergent requests in a foreign language <ref type="bibr" target="#b24">(Munro 2010)</ref>; or in online courses, where audiences and speaker



t al. 2017;</ref><ref type="bibr" target="#b6">BÃ©rard et al. 2018</ref>) and pretraining techniques <ref type="bibr" target="#b3">(Bansal et al. 2019)</ref> have been applied to end-to-end ST model to

obabilities. But complex and tricky training methods make it hard to implement. Triggered attention <ref type="bibr" target="#b8">[9]</ref> utilizes the spikes produced by connectionist temporal class
DUCTION</head><p>Attention-based sequence-to-sequence models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target= pe="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, especially transformer model <ref type="bibr" target="#b1">[2]</ref>, have shown great success in various tasks, e.g. neural mach > <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model</head><p>Similar to the transformer <ref type="bibr" target="#b1">[2]</ref>, a Sync-Transformer consists of an encoder and a decoder, as e dimensions from 40 to 256), time-axis down-sampling and adding sine-cosine positional information <ref type="bibr" target="#b1">[2]</ref>. Let x 1:T be the input feature sequence, the processed sequ ore, we adopt an Adam optimizer with warmup steps 25000 and the learning rate scheduler reported in <ref type="bibr" target="#b1">[2]</ref>.</p><p>During decoding, we use a beam search with a width of at success in various tasks, e.g. neural machine translation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, image captioning <ref type="bibr" target="#b2">[3]</ref> and >1</ref>(a). Both encoder and decoder are composed of multi-head attentions and feed-forward layers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target
ible alignment paths and calculate the negative log loss function as the same as the RNN-Transducer <ref type="bibr" target="#b13">[14]</ref> and SA-T <ref type="bibr" target="#b9">[10]</ref>. We eval
DUCTION</head><p>Attention-based sequence-to-sequence models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target= pe="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, especially transformer model <ref type="bibr" target="#b1">[2]</ref>, have shown great success in various tasks, e.g. neural mach > <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model</head><p>Similar to the transformer <ref type="bibr" target="#b1">[2]</ref>, a Sync-Transformer consists of an encoder and a decoder, as e dimensions from 40 to 256), time-axis down-sampling and adding sine-cosine positional information <ref type="bibr" target="#b1">[2]</ref>. Let x 1:T be the input feature sequence, the processed sequ ore, we adopt an Adam optimizer with warmup steps 25000 and the learning rate scheduler reported in <ref type="bibr" target="#b1">[2]</ref>.</p><p>During decoding, we use a beam search with a width of at success in various tasks, e.g. neural machine translation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, image captioning <ref type="bibr" target="#b2">[3]</ref> and >1</ref>(a). Both encoder and decoder are composed of multi-head attentions and feed-forward layers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target
rgence, we replace the ReLU activation function in the feed-forward network with gated linear units <ref type="bibr" target="#b18">[19]</ref>. We empirically set the left context of every node in the
r" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, especially transformer model <ref type="bibr" target="#b1">[ and speech recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>For conventional attention-based sequence-to-sequence h can perform encoding and decoding at the same time. The Sync-Transformer combines the transformer <ref type="bibr" target="#b5">[6]</ref> and self-attention transducer (SA-T) <ref type="bibr" target re composed of multi-head attentions and feed-forward layers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>As shown in Fig
Neural Transducer <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, the decoder generates the output sequence chunk by chunk.
DUCTION</head><p>Attention-based sequence-to-sequence models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target= pe="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, especially transformer model <ref type="bibr" target="#b1">[2]</ref>, have shown great success in various tasks, e.g. neural mach > <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model</head><p>Similar to the transformer <ref type="bibr" target="#b1">[2]</ref>, a Sync-Transformer consists of an encoder and a decoder, as e dimensions from 40 to 256), time-axis down-sampling and adding sine-cosine positional information <ref type="bibr" target="#b1">[2]</ref>. Let x 1:T be the input feature sequence, the processed sequ ore, we adopt an Adam optimizer with warmup steps 25000 and the learning rate scheduler reported in <ref type="bibr" target="#b1">[2]</ref>.</p><p>During decoding, we use a beam search with a width of at success in various tasks, e.g. neural machine translation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, image captioning <ref type="bibr" target="#b2">[3]</ref> and >1</ref>(a). Both encoder and decoder are composed of multi-head attentions and feed-forward layers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target
nce-to-sequence models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target= ion <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, image captioning <ref type="bibr" target="#b2">[3]</ref> and speech recognition <ref type="bibr" target="#b3">[4,</re
dict symbols immediately. Similar to the Neural Transducer <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, the decoder genera
r" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, especially transformer ef type="bibr" target="#b2">[3]</ref> and speech recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>For conventional
ary in PyTorch for performing micro-batch pipeline parallelism with checkpointing proposed by GPipe <ref type="bibr" target="#b10">[11]</ref>. In particular, we develop a set of design components to e e performance. For example, AmoebaNet-B <ref type="bibr" target="#b22">[23]</ref> scaled with GPipe <ref type="bibr" target="#b10">[11]</ref> has 557 million parameters and has achieved top-1 accuracy 28]</ref> language model which has 1.5 billion parameters (see Figure <ref type="figure">1</ref> of <ref type="bibr" target="#b10">[11]</ref> for the effect of model scaling). However, training such a training by combining model parallelism with data pipelining, either in synchronous way as in GPipe <ref type="bibr" target="#b10">[11]</ref> or in asynchronous way as in <ref type="bibr" target="#b11 must be completed before executing B iâ1,j .</p><p>In addition to the micro-batch pipelining, GPipe <ref type="bibr" target="#b10">[11]</ref> further reduces the memory requirement by utilizing gradie e the efficiency of torchgpipe, we report performance benchmarks similar to that conducted by GPipe <ref type="bibr" target="#b10">[11]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head ef type="foot" target="#foot_2">5</ref> to make a fair comparison of loss due to checkpointing with <ref type="bibr" target="#b10">[11]</ref>. The model we used is our implementation of a sequential v hmark on AmoebaNet-D <ref type="bibr" target="#b17">(18,</ref><ref type="bibr">256)</ref>.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, Cloud TPUv3s were used while we used NVIDIA Tesla P40 GPU -touse library in PyTorch for micro-batch pipeline parallelism with checkpointing proposed by GPipe <ref type="bibr" target="#b10">[11]</ref>. This library is designed and implemented in PyTorch's def arget="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>, and recent lines of
ines of research questions how to find an optimal strategy <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar
ization strategies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar
bibr" target="#b6">7]</ref>, and recent lines of research questions how to find an optimal strategy <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta
rget="#b0">1]</ref>, designing more efficient architectures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref>, architecture search under resource constraints <ref type="
ines of research questions how to find an optimal strategy <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar
ization strategies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar
the speed and memory benchmarks on AmoebaNet-D <ref type="bibr" target="#b22">[23]</ref> and U-Net <ref type="bibr" target="#b23">[24]</ref> when trained with the library.</p><p>The rest of the paper <p>To evaluate the effectiveness of torchgpipe for models with long skip connections, we used U-Net <ref type="bibr" target="#b23">[24]</ref> for 2dimensional segmentation. The version of U-Net we use respectively). Our implementation of U-Net is rather symmetric than the original model proposed in <ref type="bibr" target="#b23">[24]</ref> for effective balancing.</p><p>We conducted an experiment
ssible to compute gradient even when a single data point is fed into the network. Model parallelism <ref type="bibr" target="#b4">[5]</ref> is a method for training such a massive model, which partiti
ization strategies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar
ines of research questions how to find an optimal strategy <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar
ressing these concerns, approaches utilizing a dynamic vocabulary of slot values have been proposed <ref type="bibr" target="#b13">(Rastogi, Gupta, and Hakkani-Tur 2018;</ref><ref type="bibr" target=" representations for potentially unseen inputs from new services. Recent pretrained models like ELMo <ref type="bibr" target="#b13">(Peters et al. 2018)</ref> and BERT <ref type="bibr" target="#b3">(De
line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ <ref type="bibr" target="#b1">(Budzianowski et al. 2018)</ref>, M2M <ref type="bibr" target="#b15">( Asri et al. 2017)</ref>, M2M <ref type="bibr" target="#b15">(Shah et al. 2018</ref>) and Multi-WOZ <ref type="bibr" target="#b1">(Budzianowski et al. 2018</ref>). These datasets have utilized a varie
and Williams 2014a;</ref><ref type="bibr" target="#b8">Henderson, Thomson, and Williams 2014b;</ref><ref type="bibr" target="#b10">Kim et al. 2017</ref>) contributed to the creation of dialogue datase
tion of dialogue datasets with increasing complexity. Other notable related datasets include WOZ2.0 <ref type="bibr" target="#b17">(Wen et al. 2017</ref><ref type="bibr">), FRAMES (El Asri et al. 2017 er all possible slot-values <ref type="bibr" target="#b9">(Henderson, Thomson, and Young 2014;</ref><ref type="bibr" target="#b17">Wen et al. 2017)</ref> or individually score all slot-value combinati
pe="bibr" target="#b16">Shah et al. 2019)</ref>, domain adaptation and transfer learning techniques <ref type="bibr" target="#b14">(Rastogi, Hakkani-TÃ¼r, and Heck 2017)</ref>. Deep-learning based appr
on small-scale datasets estimate the dialogue state as a distribution over all possible slot-values <ref type="bibr" target="#b9">(Henderson, Thomson, and Young 2014;</ref><ref type="bibr" target="#b1
ta, and Hakkani-Tur 2018;</ref><ref type="bibr" target="#b6">Goel, Paul, and Hakkani-TÃ¼r 2019;</ref><ref type="bibr" target="#b19">Wu et al. 2019</ref>).</p></div> <div xmlns="http://www.tei-c.org/ns/
on small-scale datasets estimate the dialogue state as a distribution over all possible slot-values <ref type="bibr" target="#b9">(Henderson, Thomson, and Young 2014;</ref><ref type="bibr" target="#b1
<p>As virtual assistants incorporate diverse domains, recent work has focused on zero-shot modeling <ref type="bibr" target="#b0">(Bapna et al. 2017;</ref><ref type="bibr" target="#b20">Xia et al. 201
that arise with scaling virtual assistants in production. These assistants need to support a large <ref type="bibr" target="#b11">(Kim et al. 2018)</ref>, constantly increasing number of services ove
ianowski et al. 2018)</ref>, M2M <ref type="bibr" target="#b15">(Shah et al. 2018</ref>) and FRAMES <ref type="bibr" target="#b4">(El Asri et al. 2017)</ref>.</p><p>However, existing datasets for mult
l architecture for multi-domain dialogue state tracking. By using large pretrained models like BERT <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref>, our model can generalize to unseen service cent pretrained models like ELMo <ref type="bibr" target="#b13">(Peters et al. 2018)</ref> and BERT <ref type="bibr" target="#b3">(Devlin et al. 2019</ref>) can help, since they are trained on very la
ueled by the development of new datasets. Initial datasets were limited to one domain, such as ATIS <ref type="bibr" target="#b7">(Hemphill, Godfrey, and Doddington 1990)</ref> for spoken language und
tion of dialogue datasets with increasing complexity. Other notable related datasets include WOZ2.0 <ref type="bibr" target="#b17">(Wen et al. 2017</ref><ref type="bibr">), FRAMES (El Asri et al. 2017 er all possible slot-values <ref type="bibr" target="#b9">(Henderson, Thomson, and Young 2014;</ref><ref type="bibr" target="#b17">Wen et al. 2017)</ref> or individually score all slot-value combinati
ueled by the development of new datasets. Initial datasets were limited to one domain, such as ATIS <ref type="bibr" target="#b7">(Hemphill, Godfrey, and Doddington 1990)</ref> for spoken language und
tion of dialogue datasets with increasing complexity. Other notable related datasets include WOZ2.0 <ref type="bibr" target="#b17">(Wen et al. 2017</ref><ref type="bibr">), FRAMES (El Asri et al. 2017 er all possible slot-values <ref type="bibr" target="#b9">(Henderson, Thomson, and Young 2014;</ref><ref type="bibr" target="#b17">Wen et al. 2017)</ref> or individually score all slot-value combinati
that arise with scaling virtual assistants in production. These assistants need to support a large <ref type="bibr" target="#b11">(Kim et al. 2018)</ref>, constantly increasing number of services ove
on small-scale datasets estimate the dialogue state as a distribution over all possible slot-values <ref type="bibr" target="#b9">(Henderson, Thomson, and Young 2014;</ref><ref type="bibr" target="#b1
ecent work has focused on zero-shot modeling <ref type="bibr" target="#b0">(Bapna et al. 2017;</ref><ref type="bibr" target="#b20">Xia et al. 2018;</ref><ref type="bibr" target="#b16">Shah et al. 2019
<p>As virtual assistants incorporate diverse domains, recent work has focused on zero-shot modeling <ref type="bibr" target="#b0">(Bapna et al. 2017;</ref><ref type="bibr" target="#b20">Xia et al. 201
tion of dialogue datasets with increasing complexity. Other notable related datasets include WOZ2.0 <ref type="bibr" target="#b17">(Wen et al. 2017</ref><ref type="bibr">), FRAMES (El Asri et al. 2017 er all possible slot-values <ref type="bibr" target="#b9">(Henderson, Thomson, and Young 2014;</ref><ref type="bibr" target="#b17">Wen et al. 2017)</ref> or individually score all slot-value combinati
vidually score all slot-value combinations <ref type="bibr" target="#b12">(MrkÅ¡iÄ et al. 2017;</ref><ref type="bibr" target="#b21">Zhong, Xiong, and Socher 2018)</ref>. Such approaches are not practic
ile feature maps generated by the decoder contain high-level and coarse-gained semantic information <ref type="bibr" target="#b14">[15]</ref>. And skip connections, which combine the low-level and hig ive method to boost the semantic extraction ability of encoder-decoder frameworks.</p><p>In U-Net++ <ref type="bibr" target="#b14">[15]</ref>, plain skip connections are substituted by nested and dens ="bibr" target="#b20">[21]</ref>, FGC <ref type="bibr" target="#b21">[22]</ref>, MSFCN, and U-Net++ <ref type="bibr" target="#b14">[15]</ref>. The major C. Duan is with the State Key Laboratory of Inf ="bibr" target="#b20">[21]</ref>, FGC <ref type="bibr" target="#b21">[22]</ref>, MSFCN, and U-Net++ <ref type="bibr" target="#b14">[15]</ref>. Excluding SegNet, DeepLab V3 and DeepLab V3+, the remaini
o make utmost of the multi-scale features, the full-scale skip connections are designed in U-Net 3+ <ref type="bibr" target="#b15">[16]</ref>. However, the design philosophy of full-scale skip connect
ts of three convolutions, which just cause finite increasing in additional computational complexity <ref type="bibr" target="#b16">[17]</ref>. The effectiveness of ACB has been verified in the fields [17]</ref>. The effectiveness of ACB has been verified in the fields including image classification <ref type="bibr" target="#b16">[17]</ref>, image denoising <ref type="bibr" target="#b17">[18]</ref> should be robust to rotation and renders consistent results in different rotations. As reported in <ref type="bibr" target="#b16">[17]</ref>, different asymmetric convolutions are robust with differe network.</p><p>Based on above-mentioned insight, we modify the asymmetric convolutions proposed in <ref type="bibr" target="#b16">[17]</ref> and design an asymmetric convolution block (ACB) to captur
<ref type="bibr" target="#b5">[6]</ref>, to advanced methods including support vector machine (SVM) <ref type="bibr" target="#b6">[7]</ref> and random forest (RF) <ref type="bibr" target="#b7">[8]</re
images, i.e., the assignment of assigning the precise category to every pixel contained in an image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, plays a critical role
" target="#b9">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks such as SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, and Deep verify the effectiveness of MACU-Net, we compare the performance of proposed algorithm with SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab rg/ns/1.0"><head>B. Experimental Setting</head><p>To evaluate the effectiveness of MACU-Net, SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab
7]</ref>, image denoising <ref type="bibr" target="#b17">[18]</ref>, and medical image segmentation <ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, only the co get="#b17">[18]</ref>, and medical image segmentation <ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, only the convolutional layers of encoder are replaced by
cluding support vector machine (SVM) <ref type="bibr" target="#b6">[7]</ref> and random forest (RF) <ref type="bibr" target="#b7">[8]</ref>. However, the high dependency on hand-crafted visual feature
ype="bibr" target="#b19">[20]</ref>, Attention U-Net <ref type="bibr" target="#b20">[21]</ref>, FGC <ref type="bibr" target="#b21">[22]</ref>, MSFCN, and U-Net++ <ref type="bibr" target="#b14">[15]</r ype="bibr" target="#b19">[20]</ref>, Attention U-Net <ref type="bibr" target="#b20">[21]</ref>, FGC <ref type="bibr" target="#b21">[22]</ref>, MSFCN, and U-Net++ <ref type="bibr" target="#b14">[15]</r
cluding support vector machine (SVM) <ref type="bibr" target="#b6">[7]</ref> and random forest (RF) <ref type="bibr" target="#b7">[8]</ref>. However, the high dependency on hand-crafted visual feature
as well as realign channel-wise features. Motivated by Convolutional Block Attention Module (CBAM) <ref type="bibr" target="#b22">[23]</ref>, we design the channel attention block (CAB) to reweightin
" target="#b9">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks such as SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, and Deep verify the effectiveness of MACU-Net, we compare the performance of proposed algorithm with SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab rg/ns/1.0"><head>B. Experimental Setting</head><p>To evaluate the effectiveness of MACU-Net, SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab
f application scenarios such as land resource management, yield estimation, and economic assessment <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" targe
7]</ref>, image denoising <ref type="bibr" target="#b17">[18]</ref>, and medical image segmentation <ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, only the co get="#b17">[18]</ref>, and medical image segmentation <ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, only the convolutional layers of encoder are replaced by
cluding support vector machine (SVM) <ref type="bibr" target="#b6">[7]</ref> and random forest (RF) <ref type="bibr" target="#b7">[8]</ref>. However, the high dependency on hand-crafted visual feature
f application scenarios such as land resource management, yield estimation, and economic assessment <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" targe
" target="#b9">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks such as SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, and Deep verify the effectiveness of MACU-Net, we compare the performance of proposed algorithm with SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab rg/ns/1.0"><head>B. Experimental Setting</head><p>To evaluate the effectiveness of MACU-Net, SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab
asets</head><p>The effectiveness of MACU-Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and Gaofen Image D
" target="#b9">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks such as SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, and Deep verify the effectiveness of MACU-Net, we compare the performance of proposed algorithm with SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab rg/ns/1.0"><head>B. Experimental Setting</head><p>To evaluate the effectiveness of MACU-Net, SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab
Net <ref type="bibr" target="#b11">[12]</ref>, and DeepLab <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> have become the frequently-used schemes. Generally, feature ="bibr" target="#b11">[12]</ref>, DeepLab V3 <ref type="bibr" target="#b12">[13]</ref>, DeepLab V3+ <ref type="bibr" target="#b13">[14]</ref>, FC-DenseNet57 <ref type="bibr" target="#b19">[20]</ref>, ="bibr" target="#b11">[12]</ref>, DeepLab V3 <ref type="bibr" target="#b12">[13]</ref>, DeepLab V3+ <ref type="bibr" target="#b13">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type="bibr" target="#b19">[
as well as realign channel-wise features. Motivated by Convolutional Block Attention Module (CBAM) <ref type="bibr" target="#b22">[23]</ref>, we design the channel attention block (CAB) to reweightin
erarchical features from images, and have dramatically influenced the field of computer vision (CV) <ref type="bibr" target="#b9">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks
f application scenarios such as land resource management, yield estimation, and economic assessment <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" targe
as well as realign channel-wise features. Motivated by Convolutional Block Attention Module (CBAM) <ref type="bibr" target="#b22">[23]</ref>, we design the channel attention block (CAB) to reweightin
ng the precise category to every pixel contained in an image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, plays a critical role in wide range of application scenarios
ng the precise category to every pixel contained in an image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, plays a critical role in wide range of application scenarios
ype="bibr" target="#b19">[20]</ref>, Attention U-Net <ref type="bibr" target="#b20">[21]</ref>, FGC <ref type="bibr" target="#b21">[22]</ref>, MSFCN, and U-Net++ <ref type="bibr" target="#b14">[15]</r ype="bibr" target="#b19">[20]</ref>, Attention U-Net <ref type="bibr" target="#b20">[21]</ref>, FGC <ref type="bibr" target="#b21">[22]</ref>, MSFCN, and U-Net++ <ref type="bibr" target="#b14">[15]</r
ion, the encoder-decoder frameworks such as SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, and DeepLab <ref type="bibr" target="#b12">[13,</ref><ref the performance of proposed algorithm with SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab V3 <ref type="bibr" target="#b12">[13]</ref>, Dee >To evaluate the effectiveness of MACU-Net, SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab V3 <ref type="bibr" target="#b12">[13]</ref>, Dee
cluding support vector machine (SVM) <ref type="bibr" target="#b6">[7]</ref> and random forest (RF) <ref type="bibr" target="#b7">[8]</ref>. However, the high dependency on hand-crafted visual feature
<ref type="bibr" target="#b5">[6]</ref>, to advanced methods including support vector machine (SVM) <ref type="bibr" target="#b6">[7]</ref> and random forest (RF) <ref type="bibr" target="#b7">[8]</re
as well as realign channel-wise features. Motivated by Convolutional Block Attention Module (CBAM) <ref type="bibr" target="#b22">[23]</ref>, we design the channel attention block (CAB) to reweightin
ype="bibr" target="#b19">[20]</ref>, Attention U-Net <ref type="bibr" target="#b20">[21]</ref>, FGC <ref type="bibr" target="#b21">[22]</ref>, MSFCN, and U-Net++ <ref type="bibr" target="#b14">[15]</r ype="bibr" target="#b19">[20]</ref>, Attention U-Net <ref type="bibr" target="#b20">[21]</ref>, FGC <ref type="bibr" target="#b21">[22]</ref>, MSFCN, and U-Net++ <ref type="bibr" target="#b14">[15]</r
ng the precise category to every pixel contained in an image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, plays a critical role in wide range of application scenarios
olves these problems using the following techniques:</p><p>â¢ Reversible layers, first introduced in <ref type="bibr" target="#b7">Gomez et al. (2017)</ref>, enable storing only a single copy of activa term: the b â¢ n h â¢ l â¢ d k ,</formula><p>RevNets. Reversible residual networks were introduced by <ref type="bibr" target="#b7">Gomez et al. (2017)</ref> where it was shown that they can replace Res mer model's self-attention mechanism <ref type="bibr" target="#b21">(Sukhbaatar et al., 2019a;</ref><ref type="bibr" target="#b7">b)</ref> have also recently been explored.</p><p>In particular, levera
ng other modalities, like music <ref type="bibr" target="#b9">(Huang et al., 2018)</ref> and images <ref type="bibr" target="#b13">(Parmar et al., 2018)</ref>, even longer sequences are commonplace. T erse data such as music scores <ref type="bibr" target="#b9">(Huang et al., 2018)</ref>, and images <ref type="bibr" target="#b13">(Parmar et al., 2018;</ref><ref type="bibr" target="#b16">Ramachandra



ng other modalities, like music <ref type="bibr" target="#b9">(Huang et al., 2018)</ref> and images <ref type="bibr" target="#b13">(Parmar et al., 2018)</ref>, even longer sequences are commonplace. T erse data such as music scores <ref type="bibr" target="#b9">(Huang et al., 2018)</ref>, and images <ref type="bibr" target="#b13">(Parmar et al., 2018;</ref><ref type="bibr" target="#b16">Ramachandra
"bibr" target="#b17">Santoro et al. (2016)</ref> at the cost of memory size and later alleviated by <ref type="bibr" target="#b15">Rae et al. (2016)</ref>. The last paper considered memory lookups wit
ng other modalities, like music <ref type="bibr" target="#b9">(Huang et al., 2018)</ref> and images <ref type="bibr" target="#b13">(Parmar et al., 2018)</ref>, even longer sequences are commonplace. T erse data such as music scores <ref type="bibr" target="#b9">(Huang et al., 2018)</ref>, and images <ref type="bibr" target="#b13">(Parmar et al., 2018;</ref><ref type="bibr" target="#b16">Ramachandra
"bibr" target="#b17">Santoro et al. (2016)</ref> at the cost of memory size and later alleviated by <ref type="bibr" target="#b15">Rae et al. (2016)</ref>. The last paper considered memory lookups wit
ng other modalities, like music <ref type="bibr" target="#b9">(Huang et al., 2018)</ref> and images <ref type="bibr" target="#b13">(Parmar et al., 2018)</ref>, even longer sequences are commonplace. T erse data such as music scores <ref type="bibr" target="#b9">(Huang et al., 2018)</ref>, and images <ref type="bibr" target="#b13">(Parmar et al., 2018;</ref><ref type="bibr" target="#b16">Ramachandra

ial for training large and complex deep learning architectures. RNN-T models are difficult to train <ref type="bibr" target="#b10">[11]</ref> and also require significantly large amount of data to joi oral classification (CTC) model <ref type="bibr" target="#b1">[2]</ref> or cross entropy (CE) model <ref type="bibr" target="#b10">[11]</ref>, and the prediction network with LSTM language model (LM) on.</p><p>model for encoder and prediction network in the context of TL the RNN-T model. Authors in <ref type="bibr" target="#b10">[11]</ref> have shown that CE initialized RNN-T models perform better in blocks. gets (necessary for CE training), is obtained from word level alignments as discussed in <ref type="bibr" target="#b10">[11]</ref>. From the word alignments, the start frame, end frame and ned by using byte pair encoding <ref type="bibr" target="#b25">[26]</ref> algorithm as described in <ref type="bibr" target="#b10">[11]</ref>.</p><p>We also report the word error rate (WER) on hybrid
get="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar uage to bootstrap the low-resource AM; multi-task training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and ensemble learning <ref type="bibr" target="#b18">[19,</
get="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar a well trained AM from high-resource language to bootstrap the low-resource AM; multi-task training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and ensemble learn
the E2E framework <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Authors in <ref type="bibr" target="#b20">[21]</ref> propo ased multi-lingual E2E model, along with methods to incorporate language information is proposed in <ref type="bibr" target="#b22">[23]</ref>. Although multi-lingual methods are attractive to address
get="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Successful strateg type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and ensemble learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> that aim to utiliz
get="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar uage to bootstrap the low-resource AM; multi-task training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and ensemble learning <ref type="bibr" target="#b18">[19,</
the E2E framework <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Authors in <ref type="bibr" target="#b20">[21]</ref> propo ased multi-lingual E2E model, along with methods to incorporate language information is proposed in <ref type="bibr" target="#b22">[23]</ref>. Although multi-lingual methods are attractive to address
get="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar a well trained AM from high-resource language to bootstrap the low-resource AM; multi-task training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and ensemble learn
single neural network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target=
the E2E framework <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Authors in <ref type="bibr" target="#b20">[21]</ref> propo ased multi-lingual E2E model, along with methods to incorporate language information is proposed in <ref type="bibr" target="#b22">[23]</ref>. Although multi-lingual methods are attractive to address
single neural network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target=
ure the representation quality. All of these works sample negative examples from p(x). Arora et al. <ref type="bibr" target="#b0">[1]</ref> theoretically analyze the effect of contrastive representati erparameter. Without loss of generality, we set t = 1 for all theoretical results.</p><p>Similar to <ref type="bibr" target="#b0">[1]</ref>, we assume an underlying set of discrete latent classes C th f ) = inf WâR KÃd L Softmax (T , W f ).<label>(10)</label></formula><p>In line with the approach of <ref type="bibr" target="#b0">[1]</ref> we analyze the supervised loss of a mean classifier <ref typ commonly the case. The dependence on on N and T in Theorem 5 is roughly equivalent to the result in <ref type="bibr" target="#b0">[1]</ref>, but the two bounds are not directly comparable since the pr .</formula><p>In order to derive our bound we will exploit a concentration of measure result due to <ref type="bibr" target="#b0">[1]</ref>. They consider an objective of the form</p><formula xml:id="
entation learning algorithms that use a contrastive loss have outperformed even supervised learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ rvation that a larger number of negative/positive examples in the objective leads to better results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. The last two terms </ref>, or different views of the same scene <ref type="bibr" target="#b32">[33]</ref>. Chen et al. <ref type="bibr" target="#b1">[2]</ref> extensively study verious data augmentation methods. For lan br" target="#b22">[23]</ref> and STL10 <ref type="bibr" target="#b5">[6]</ref>, we implement SimCLR <ref type="bibr" target="#b1">[2]</ref> with ResNet-50 <ref type="bibr" target="#b14">[15]</ref> as ef type="bibr" target="#b18">[19]</ref> with learning rate 0.001 and weight decay 1e â 6. Following <ref type="bibr" target="#b1">[2]</ref>, we set the temperature t = 0.5 and the dimension of the lat /ns/1.0"><head>B Experiment Details</head><p>Cifar10 and STL10 We adopt PyTorch to implement SimCLR <ref type="bibr" target="#b1">[2]</ref> with Resnet-50 <ref type="bibr" target="#b14">[15]</ref> as
target="#b5">[6]</ref>, we implement SimCLR <ref type="bibr" target="#b1">[2]</ref> with ResNet-50 <ref type="bibr" target="#b14">[15]</ref> as the encoder architecture and use the Adam optimizer <re d STL10 We adopt PyTorch to implement SimCLR <ref type="bibr" target="#b1">[2]</ref> with Resnet-50 <ref type="bibr" target="#b14">[15]</ref> as the encoder architecture and use the Adam optimizer <re
a human labeler. For instance, in computer vision, representations can be learned from colorization <ref type="bibr" target="#b37">[38]</ref>, predicting transformations <ref type="bibr" target="#b8">
rget="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. The key idea of contrastive learning is to contrast semant b1">[2]</ref> extensively study verious data augmentation methods. For language, Logeswaran and Lee <ref type="bibr" target="#b23">[24]</ref> treat the context sentences as positive samples to efficie r" target="#b7">[8]</ref>. Our experimental settings follow those for quick-thought (QT) vectors in <ref type="bibr" target="#b23">[24]</ref>.</p><p>In contrast to vision tasks, positive pairs here ar s/1.0"><head>Sentence Embedding</head><p>We adopt the official code 3 of quick-thought (QT) vectors <ref type="bibr" target="#b23">[24]</ref>. To implement the debiased objective, we only modify the "
supervised learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. The key idea of co en improved by maintaining a dictionary of negative examples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. Concurrently, Wang and Isola <ref type="bibr" target="#b34 e/positive examples in the objective leads to better results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. The last two terms in the bound grow slowly with N, but th s the minimum of two real numbers a and b.</p><p>Recent works often use large N, e.g., N = 65536 in <ref type="bibr" target="#b15">[16]</ref>, making the last term negligible. While, in general, minim
e="bibr" target="#b20">[21]</ref> and examine six classification tasks: movie review sentiment (MR) <ref type="bibr" target="#b28">[29]</ref>, product reviews (CR) <ref type="bibr" target="#b16">[17]<
tasks: movie review sentiment (MR) <ref type="bibr" target="#b28">[29]</ref>, product reviews (CR) <ref type="bibr" target="#b16">[17]</ref>, subjectivity classification (SUBJ) <ref type="bibr" targe
outlier detection <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Our approach is related to unbiased PU learning, where th
supervised learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. The key idea of co en improved by maintaining a dictionary of negative examples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. Concurrently, Wang and Isola <ref type="bibr" target="#b34 e/positive examples in the objective leads to better results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. The last two terms in the bound grow slowly with N, but th s the minimum of two real numbers a and b.</p><p>Recent works often use large N, e.g., N = 65536 in <ref type="bibr" target="#b15">[16]</ref>, making the last term negligible. While, in general, minim
ghted appropriately <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. While these works focus on zero-one losses, we here addres
tasks: movie review sentiment (MR) <ref type="bibr" target="#b28">[29]</ref>, product reviews (CR) <ref type="bibr" target="#b16">[17]</ref>, subjectivity classification (SUBJ) <ref type="bibr" targe
the finite N. The class prior Ï + can be estimated from data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> or treated as a hyperparameter. Theorem 3 bounds the error
oach of <ref type="bibr" target="#b0">[1]</ref> we analyze the supervised loss of a mean classifier <ref type="bibr" target="#b29">[30]</ref>, where for each class c, the rows of W are set to the mean
="bibr" target="#b37">[38]</ref>, predicting transformations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>, or generative modeling <ref type="bibr" target="#b2">[3,</
type="bibr" target="#b25">26]</ref>, or generative modeling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>. Remarkable success
><p>where, for simplicity, we set Q to the finite N. The class prior Ï + can be estimated from data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> or treated as a hype
rget="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. The key idea of contrastive learning is to contrast semant b1">[2]</ref> extensively study verious data augmentation methods. For language, Logeswaran and Lee <ref type="bibr" target="#b23">[24]</ref> treat the context sentences as positive samples to efficie r" target="#b7">[8]</ref>. Our experimental settings follow those for quick-thought (QT) vectors in <ref type="bibr" target="#b23">[24]</ref>.</p><p>In contrast to vision tasks, positive pairs here ar s/1.0"><head>Sentence Embedding</head><p>We adopt the official code 3 of quick-thought (QT) vectors <ref type="bibr" target="#b23">[24]</ref>. To implement the debiased objective, we only modify the "
ghted appropriately <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. While these works focus on zero-one losses, we here addres
lications of PU learning are retrieval or outlier detection <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Our approach is used as negative examples, but down-weighted appropriately <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. While these works
a human labeler. For instance, in computer vision, representations can be learned from colorization <ref type="bibr" target="#b37">[38]</ref>, predicting transformations <ref type="bibr" target="#b8">
the language domain <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Recently, self-supervised representation learning al
al networks has concentrated on node classification task <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bib ake a specific node (e.g., a person in social networks) misclassified. In this scenario, Dai et al. <ref type="bibr" target="#b12">[13]</ref> study the adversarial attack on graph structure data and p olutional Network (SGC) <ref type="bibr" target="#b16">[17]</ref> and gradient-based attack methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, we propose a n cient way to generate destructive adversarial examples. Focusing on the targeted attack, Dai et al. <ref type="bibr" target="#b12">[13]</ref> propose GradArgmax, which extracts gradients of the surrog Network (GCN)</head><p>Since a number of existing works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bib f-the-art targeted attack methods: Nettack <ref type="bibr" target="#b11">[12]</ref> and GradArgmax <ref type="bibr" target="#b12">[13]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head ly nodes belonging to the same class/different classes will be disconnected/connected. â¢ GradArgmax <ref type="bibr" target="#b12">[13]</ref>. Since the attack budget â is defined as the degrees of ta
matrix A (sub) is computed for each edge e = (u, v) â E s .</p><p>However, as stated by Guo et al. <ref type="bibr" target="#b32">[33]</ref>, most of the modern neural networks are poorly calibrated,
ks: GCN <ref type="bibr" target="#b7">[8]</ref>, SGC <ref type="bibr" target="#b16">[17]</ref>, GAT <ref type="bibr" target="#b40">[41]</ref>, GraphSAGE <ref type="bibr" target="#b39">[40]</ref>, Clus h achieves competitive results and even significantly improves the training efficiency.</p><p>â¢ GAT <ref type="bibr" target="#b40">[41]</ref>. GAT enhances GCN by leveraging a masked self-attention me
uitous in nature and society and there are a great deal of research interest in studying graph data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" t
udying graph data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" t
ur well-known datasets: Citeseer, Cora, Pubmed <ref type="bibr" target="#b38">[39]</ref> and Reddit <ref type="bibr" target="#b39">[40]</ref>. The first three are commonly citation networks on node cl <ref type="bibr" target="#b16">[17]</ref>, GAT <ref type="bibr" target="#b40">[41]</ref>, GraphSAGE <ref type="bibr" target="#b39">[40]</ref>, ClusterGCN <ref type="bibr" target="#b41">[42]</ref>. For -attention mechanism to specifying different weights to different neighbor nodes.</p><p>â¢ GraphSAGE <ref type="bibr" target="#b39">[40]</ref>. GraphSAGE is a general inductive framework, which uniform
aw distribution <ref type="bibr" target="#b36">[37]</ref>, i.e., <ref type="bibr">3.</ref> Refer to <ref type="bibr" target="#b35">[36]</ref>, the time complexity of GCN is O(|E|d 2 ), and the computa
="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Undoubtedly, graph 8">[9]</ref>. Undoubtedly, graph plays a crucial role in many high impact applications in the world <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" rget nodes). Besides, due to the lack of time and space efficiency, Graph Convolution Network (GCN) <ref type="bibr" target="#b7">[8]</ref> is unfit to serve as a surrogate model despite being used fr ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b24">[25]</ref> use vanilla GCN <ref type="bibr" target="#b7">[8]</ref> as a surrogate model to conduct attacks, thus we first intro n on SGC <ref type="bibr" target="#b16">[17]</ref> -a simplified variant of GCN. Refer to this work <ref type="bibr" target="#b7">[8]</ref>, GCN is recursively defined as</p><formula xml:id="formula_0 ransferability of our method, we conduct attack on several commonly used graph neural networks: GCN <ref type="bibr" target="#b7">[8]</ref>, SGC <ref type="bibr" target="#b16">[17]</ref>, GAT <ref typ >. For each target model, our method is compared with other adversarial attack methods.</p><p>â¢ GCN <ref type="bibr" target="#b7">[8]</ref>. GCN is one of the most representative graph neural networks
"bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> and degree distributions <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Unlike previou
#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The obtained r
pe="bibr" target="#b40">[41]</ref>, GraphSAGE <ref type="bibr" target="#b39">[40]</ref>, ClusterGCN <ref type="bibr" target="#b41">[42]</ref>. For each target model, our method is compared with other ors with a fixed size, instead of using a full-neighborhood set during training.</p><p>â¢ ClusterGCN <ref type="bibr" target="#b41">[42]</ref>. This is the state-of-the-art mini-batch GCN framework. It
uage understanding tasks for the existing VLP models is VQA. The SoTA result for VQA is from UNITER <ref type="bibr" target="#b6">[6]</ref> large model. Table <ref type="table" target="#tab_3">6</ref> Oscar B is the best among the models with equivalent size, even slightly better (0.04%) than UNITER <ref type="bibr" target="#b6">[6]</ref> large. And the Oscar L improves the SoTA overall accuracy wi other major task for the existing VLP models is NLVR2. Similarly, the SoTA model on NLVR2 is UNITER <ref type="bibr" target="#b6">[6]</ref> large. As reported in Table <ref type="table" target="#tab_4 ="bibr" target="#b19">19,</ref><ref type="bibr" target="#b10">10]</ref> employ BERT-like objectives <ref type="bibr" target="#b6">[6]</ref> to learn crossmodal representations from a concatenated-sequ
Oscar for improvement in the future. (ii) 12-in-1 is a recently proposed multi-task learning model <ref type="bibr" target="#b22">[22]</ref> for V+L, implemented on BERT base. We see that Oscar B out
ed embedding space to align the inter-modal corres ages and text. Early attempts from Socher et al. <ref type="bibr" target="#b33">[33]</ref> pr regions into a common space using kernelized canonical CAMP <ref type="bibr" target="#b42">[42]</ref>, SCAN <ref type="bibr" target="#b18">[18]</ref>, SCG <ref type="bibr" target="#b33">[33]</ref>, PFAN <ref type="bibr" target="#b41">[41]</ref>, Unicoder-
d image retrieval <ref type="bibr" target="#b30">[30]</ref>. In particular, the seminal work DeViSE <ref type="bibr" target="#b8">[8]</ref> proposes to identify visual objects using semantic informati
ethods on all seven tasks, and achieves new SoTA on six of them. On GQA, neural state machine (NSM) <ref type="bibr" target="#b12">[12]</ref> relies on a strong structural prior, which can also be inc ype="bibr" target="#b4">[4]</ref> 60.83 12-in-1 <ref type="bibr" target="#b24">[24]</ref> 60.65 NSM <ref type="bibr" target="#b12">[12]</ref> 63.  Image Retrieval R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@1 MMN <ref type="bibr" target="#b4">[4]</ref>, 12-in-1 <ref type="bibr" target="#b24">[24]</ref>, NSM <ref type="bibr" target="#b12">[12]</ref>.</p><p>NLVR2 For the Oscar B model, we fine-tune for 20 ep
L) tasks, such as visual question answering, image-text retrieval, image captioning. Recent studies <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" ta ations that are appropriately contextualized in both modalities. For example, early efforts such as <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b39">39]</ref> propose a two-stre e existing methods <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" targe
ervation that the salient objects in an image can be accurately detected by modern object detectors <ref type="bibr" target="#b29">[29]</ref>, and that these objects are often mentioned in the paired ly extracted from over-sampled regions <ref type="bibr">[2]</ref> via Faster R-CNN object detectors <ref type="bibr" target="#b29">[29]</ref>, which inevitably results in overlaps among image regions s follows. Given an image with K regions of objects (normally over-sampled and noisy), Faster R-CNN <ref type="bibr" target="#b29">[29]</ref> is used to extract the visual semantics of each region as introduce the bottom-up mechanism to represent an image as a set of visual regions via Faster R-CNN <ref type="bibr" target="#b29">[29]</ref>, each with an associated feature vector. It enables attent
L) tasks, such as visual question answering, image-text retrieval, image captioning. Recent studies <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" ta ations that are appropriately contextualized in both modalities. For example, early efforts such as <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b39">39]</ref> propose a two-stre e existing methods <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" targe
ed embedding space to align the inter-modal corres ages and text. Early attempts from Socher et al. <ref type="bibr" target="#b33">[33]</ref> pr regions into a common space using kernelized canonical CAMP <ref type="bibr" target="#b42">[42]</ref>, SCAN <ref type="bibr" target="#b18">[18]</ref>, SCG <ref type="bibr" target="#b33">[33]</ref>, PFAN <ref type="bibr" target="#b41">[41]</ref>, Unicoder-
usands of novel labels that have never been seen by the vision model. The idea has been extended in <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" ta

ww.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of Grounded video description (GVD) <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> aims to generate more grounded and accurate existing works either encode region proposals independently or using selfattention-based mechanisms <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>. Therefore, it either fails to consider impl a]</ref>, many works model the video in both global video features and regional object features. In <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>, they encode the objects with transformer <r /head><p>We model the video's global level feature by a Bi-directional LSTM network like most works <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> given by: h = BiLST M (v) = {h 1 , h 2 , ... ose a novel visual representation method from the perspective of regions. First of all, inspired by <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>, we enhance the proposal features by adding <div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Enhancement</head><p>In this part, we follow <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>'s work, which fusing the spatial-temporal an feature aggregation on the enhanced feature R.</p><p>We adopt the same classification loss just as <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> do denoted as L cls .</p></div> <div xmlns=" > t , h f rame + h attention ). h t is used to generate descriptions. We adopt the same MLE loss as <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> which denoted by L sent .</p><p>Finally, the ad n="4.1">Dataset</head><p>We conduct our experiments on the Grounded ActivityNet-Entities Dataset <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> for evaluation. It contains 15k video with 1 ph2Seq method. Data processing. For a fair comparison, the data processing procedure is the same to <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>. For each video segment in the dataset, we u 2018]</ref>, BiM-STM+TempoAtnn <ref type="bibr" target="#b7">[Zhou et al., 2018]</ref> and ZhouGVD <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> on Grounded ActivityNet Captions Dataset to emove the hierarchical attention and replace it with the coarsegrain proposal attention proposed by <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>.</p><p>Table <ref type="table" target="#tab_
based methods which model the regions with abundant semantic relations are introduced to this area. <ref type="bibr" target="#b6">[Yao et al., 2018]</ref>  </p></div> <div xmlns="http://www.tei-c.org/
based methods which model the regions with abundant semantic relations are introduced to this area. <ref type="bibr" target="#b6">[Yao et al., 2018]</ref>  </p></div> <div xmlns="http://www.tei-c.org/
based methods which model the regions with abundant semantic relations are introduced to this area. <ref type="bibr" target="#b6">[Yao et al., 2018]</ref>  </p></div> <div xmlns="http://www.tei-c.org/
es that separated in frames.</p><p>Borrowing the ideas of spatial-attention in image caption domain <ref type="bibr" target="#b0">[Anderson et al., 2018;</ref><ref type="bibr" target="#b4">Liu et al.,
es that separated in frames.</p><p>Borrowing the ideas of spatial-attention in image caption domain <ref type="bibr" target="#b0">[Anderson et al., 2018;</ref><ref type="bibr" target="#b4">Liu et al.,
to generate the description of a video using the attention-based encoder-decoder like architectures <ref type="bibr" target="#b5">[Venugopalan et al., 2015;</ref><ref type="bibr" target="#b5">Xu et al ed encoder-decoder like architectures <ref type="bibr" target="#b5">[Venugopalan et al., 2015;</ref><ref type="bibr" target="#b5">Xu et al., 2018b;</ref><ref type="bibr" target="#b4">Liu et al., 2016] which learns a mapping between graph inputs to sequence outputs through attention-based mechanisms <ref type="bibr" target="#b5">[Xu et al., 2018a;</ref><ref type="bibr" target="#b2">Chen et al., 202
es that separated in frames.</p><p>Borrowing the ideas of spatial-attention in image caption domain <ref type="bibr" target="#b0">[Anderson et al., 2018;</ref><ref type="bibr" target="#b4">Liu et al.,
ce outputs through attention-based mechanisms <ref type="bibr" target="#b5">[Xu et al., 2018a;</ref><ref type="bibr" target="#b2">Chen et al., 2020;</ref><ref type="bibr" target="#b2">Gao et al., 2019 e="bibr" target="#b5">[Xu et al., 2018a;</ref><ref type="bibr" target="#b2">Chen et al., 2020;</ref><ref type="bibr" target="#b2">Gao et al., 2019]</ref>. However, since there is no explicit graph str ethod: Relation Graph. Since the region features are extracted by a pre-trained model trained on VG <ref type="bibr" target="#b2">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation o detect 100 region proposals and extract the feature. The detector is pre-trained on Visual Genome <ref type="bibr" target="#b2">[Krishna et al., 2017]</ref>. Finally, for the video feature, the temp to aggregate the features of the nodes modeled by topology A.</p><p>Inspired by resnet architecture <ref type="bibr" target="#b2">[He et al., 2016]</ref>, we propose the basic module of our architectu
based methods which model the regions with abundant semantic relations are introduced to this area. <ref type="bibr" target="#b6">[Yao et al., 2018]</ref>  </p></div> <div xmlns="http://www.tei-c.org/
based methods which model the regions with abundant semantic relations are introduced to this area. <ref type="bibr" target="#b6">[Yao et al., 2018]</ref>  </p></div> <div xmlns="http://www.tei-c.org/
i to see how well the model performs on that task. The goal of Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b9">[9]</ref> is to obtain a parameter initialization Î¸ * that can adapt t nowledge across meta-training tasks and is the optimal parameter to adapt to unseen tasks quickly.  <ref type="bibr" target="#b9">[9]</ref> switches ProtoNet to MAML as the meta-learner. All experimen
e accumulated gradually across tasks that captures how task structure changes across target domains <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" targ
fields to extract useful features from graphs. Ego-CNN uses ego graphs to find critical structures <ref type="bibr" target="#b36">[35]</ref>. SEAL <ref type="bibr" target="#b50">[49]</ref> develops a
milarity enables G-META to form the much-needed inductive bias by using a metric-learning algorithm <ref type="bibr" target="#b30">[29]</ref>. Moreover, local subgraphs also allow for effective featur pe="bibr" target="#b52">[51,</ref><ref type="bibr" target="#b18">18]</ref>. Metric-learning methods <ref type="bibr" target="#b30">[29]</ref> learn a task-specific metric to classify query set data us ata using the closest point from the support set. It has been proved as an effective inductive bias <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b35">34]</ref>. Equipped with sub
cross target domains <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b27">26]</ref>. A powerful GNN trained to meta-learn can quickly learn nev


milarity enables G-META to form the much-needed inductive bias by using a metric-learning algorithm <ref type="bibr" target="#b30">[29]</ref>. Moreover, local subgraphs also allow for effective featur pe="bibr" target="#b52">[51,</ref><ref type="bibr" target="#b18">18]</ref>. Metric-learning methods <ref type="bibr" target="#b30">[29]</ref> learn a task-specific metric to classify query set data us ata using the closest point from the support set. It has been proved as an effective inductive bias <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b35">34]</ref>. Equipped with sub


target="#b2">2,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b44">43]</ref>. While these methods provide a promising approach to meta-l
by all the modalities.</p><p>Inspired by such an assumption and the fact that the Total Correlation <ref type="bibr" target="#b27">[29]</ref> can measure the amount of information shared by M (M â¥ 2) sifiers of each modality.</p><p>Total Correlation/Mutual information maximization Total Correlation <ref type="bibr" target="#b27">[29]</ref>, as an extension of Mutual Information, measures the amoun
et="#b4">[6]</ref>; and (ii) learning joint representation across modalities in an unsupervised way <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b26">28]</ref>. These methods suf els in each modality, may not be consistent with the real settings.</p><p>The second branch of work <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" ta
News-M10. They contain 500, 500, 1000 data points with 2, 5, 10 categories respectively. Following <ref type="bibr" target="#b31">[33]</ref>, we use 60% for training, 20% for validation and 20% for t es (the percentage of labeled data points in each modality): {10%, 30%} for each dataset. We follow <ref type="bibr" target="#b31">[33]</ref> for classifiers. Adam with default parameters and learning f type="bibr" target="#b22">[24]</ref> uses adversarial training for semi-supervised learning; PVCC <ref type="bibr" target="#b31">[33]</ref> that considers the consistency of data points under differ
hat maps all the data points to the same representation. A common belief in multi-modality learning <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" targ ared by these M modalities, which is commonly assumed in the literature of semi-supervised learning <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" targ
ning algorithms and have shown superior performance on various tasks. Belghazi et allet@tokeeonedot <ref type="bibr" target="#b3">[4]</ref> presents a mutual information neural estimator, which are ut = E p [M ] (PTC).</formula><p>The Lemma 1 is commonly utilized for estimation of Mutual information <ref type="bibr" target="#b3">[4]</ref> or optimization as variational lower bound in the machine le
applies the co-training algorithm proposed by Blum et. al <ref type="bibr" target="#b4">[6]</ref>. <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" targ
ssumptions to ensure that the ground-truth label can be identified. According to Proposition 2.1 in <ref type="bibr" target="#b0">[1]</ref>, the label Y can be viewed as the generating factor of data ariables z 1 , ..., z M such that</p><p>Proof. This proof is a generalization of the Proposition in <ref type="bibr" target="#b0">[1]</ref>. For z 1 , ..., z M i.i.d â¼ U nif orm(0, 1), then from <ref n in <ref type="bibr" target="#b0">[1]</ref>. For z 1 , ..., z M i.i.d â¼ U nif orm(0, 1), then from <ref type="bibr" target="#b0">[1]</ref> we have x m y = F â1 y,m (z m ) where F y,m (t) = P(x m â¤ t
nt representation across modalities in an unsupervised way <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b26">28]</ref>. These methods suffer from either too strong assumptions or nt with the real settings.</p><p>The second branch of work <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" targe
<p>Dataset We evaluate our methods on two multi-modal emotion recognition datasets: IEMOCAP dataset <ref type="bibr" target="#b5">[7]</ref> and MOSI dataset <ref type="bibr" target="#b32">[34]</ref>.
News-M10. They contain 500, 500, 1000 data points with 2, 5, 10 categories respectively. Following <ref type="bibr" target="#b31">[33]</ref>, we use 60% for training, 20% for validation and 20% for t es (the percentage of labeled data points in each modality): {10%, 30%} for each dataset. We follow <ref type="bibr" target="#b31">[33]</ref> for classifiers. Adam with default parameters and learning f type="bibr" target="#b22">[24]</ref> uses adversarial training for semi-supervised learning; PVCC <ref type="bibr" target="#b31">[33]</ref> that considers the consistency of data points under differ
rget="#b20">[22,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20]</ref> is that conditioning on ground truth label Y , these modali rget="#b20">[22,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20]</ref>. However, the Y may not be the unique information intersect
2017;</ref><ref type="bibr" target="#b34">VeliÄkoviÄ et al., 2018)</ref> or variants of Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> that apply self-attention on all nodes t ype="bibr" target="#b7">Cai and Lam, 2020)</ref> base their encoder on the Transformer architecture <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> and thus, in each layer, compute self-at raformer follows the general multi-layer encoderdecoder pattern known from the original Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>. In the following, we first describe our computations for one head. The output of multiple heads is combined as in the original Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>.</p><p>Text self-attention. <ref type="b ead n="3.4">Graformer decoder</head><p>Our decoder follows closely the standard Transformer decoder <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>, except for the modifications suggested of the ith node's label.</p><p>To compute the node representation H (L) in the Lth layer, we follow <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>, i.e., we first normalize the input from
"4.3">Hyperparameters and training details</head><p>We train Graformer with the Adafactor optimizer <ref type="bibr" target="#b31">(Shazeer and Stern, 2018)</ref> for 40 epochs on AGENDA and 200 epoch
incorporate information about the graph topology with some variant of relative position embeddings <ref type="bibr" target="#b30">(Shaw et al., 2018)</ref>. They, however, assume that there is always ere p differentiates between different relative positions in the original entity string, similar to <ref type="bibr" target="#b30">(Shaw et al., 2018)</ref>. See Fig. <ref type="figure" target="#fig_1 Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>.</p><p>Text self-attention. <ref type="bibr" target="#b30">Shaw et al. (2018)</ref> introduced position-aware self-attention in > <figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>6)<ref type="bibr" target="#b30">Shaw et al. (2018)</ref> share their position embeddings across atten
racters during tokenization.</p><p>For both datasets, we train a BPE vocabulary using sentencepiece <ref type="bibr" target="#b17">(Kudo and Richardson, 2018)</ref> on the train set, i.e., a concatena
.g., in biomedicine <ref type="bibr" target="#b35">(Wishart et al., 2018)</ref> and computer vision <ref type="bibr" target="#b16">(Krishna et al., 2017)</ref>. Generating a natural language descripti

of Koncel-Kedziorski et al. ( <ref type="formula">2019</ref>) and length-based curriculum learning <ref type="bibr" target="#b24">(Platanios et al., 2019)</ref>: We sort the train set by target text
ter we apply Optuna with default parameters <ref type="bibr" target="#b0">(Akiba et al., 2019;</ref><ref type="bibr" target="#b5">Bergstra et al., 2011)</ref> and median pruning, i.e., after each epoc
incorporate information about the graph topology with some variant of relative position embeddings <ref type="bibr" target="#b30">(Shaw et al., 2018)</ref>. They, however, assume that there is always ere p differentiates between different relative positions in the original entity string, similar to <ref type="bibr" target="#b30">(Shaw et al., 2018)</ref>. See Fig. <ref type="figure" target="#fig_1 Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>.</p><p>Text self-attention. <ref type="bibr" target="#b30">Shaw et al. (2018)</ref> introduced position-aware self-attention in > <figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>6)<ref type="bibr" target="#b30">Shaw et al. (2018)</ref> share their position embeddings across atten
ter we apply Optuna with default parameters <ref type="bibr" target="#b0">(Akiba et al., 2019;</ref><ref type="bibr" target="#b5">Bergstra et al., 2011)</ref> and median pruning, i.e., after each epoc
018)</ref>, datato-document generation <ref type="bibr" target="#b21">(Moryossef et al., 2019;</ref><ref type="bibr" target="#b15">Koncel-Kedziorski et al., 2019)</ref> and interpretability of KGs in mer.</p><p>Following previous work, we evaluate Graformer on two benchmarks: (i) the AGENDA dataset <ref type="bibr" target="#b15">(Koncel-Kedziorski et al., 2019)</ref>, i.e., the generation of scien ead><p>We evaluate our new architecture on two popular benchmarks for KG-to-text generation, AGENDA <ref type="bibr" target="#b15">(Koncel-Kedziorski et al., 2019)</ref> and WebNLG <ref type="bibr" ta ecoder architectures <ref type="bibr" target="#b20">(Marcheggiani and Perez-Beltrachini, 2018;</ref><ref type="bibr" target="#b15">Koncel-Kedziorski et al., 2019;</ref><ref type="bibr" target="#b27">R rms previous Transformerbased models that only consider first-order neighborhoods per encoder layer <ref type="bibr" target="#b15">(Koncel-Kedziorski et al., 2019;</ref><ref type="bibr" target="#b2">A
Sutton, 2017)</ref>, a state-of-the-art topic model that implements black-box variational inference <ref type="bibr" target="#b20">(Ranganath et al., 2014)</ref>, to include BERT representations. Our
hat incorporate several types of information <ref type="bibr" target="#b27">(Xun et al., 2017;</ref><ref type="bibr" target="#b4">Das et al., 2015;</ref><ref type="bibr" target="#b16">Nguyen et al., 2 2013;</ref><ref type="bibr" target="#b28">Yang et al., 2015)</ref>, or pre-trained word embeddings <ref type="bibr" target="#b4">(Das et al., 2015;</ref><ref type="bibr" target="#b6">Dieng et al., 20
i-c.org/ns/1.0"><head n="4">Related Work</head><p>Several topic models are based on neural networks <ref type="bibr" target="#b10">(Larochelle and Lauly, 2012;</ref><ref type="bibr" target="#b23">Sala
via intrusion tests <ref type="bibr" target="#b2">(Chang et al., 2009)</ref> to approximated scores <ref type="bibr" target="#b11">(Lau et al., 2014;</ref><ref type="bibr" target="#b21">RÃ¶der et al.,
hat incorporate several types of information <ref type="bibr" target="#b27">(Xun et al., 2017;</ref><ref type="bibr" target="#b4">Das et al., 2015;</ref><ref type="bibr" target="#b16">Nguyen et al., 2 2013;</ref><ref type="bibr" target="#b28">Yang et al., 2015)</ref>, or pre-trained word embeddings <ref type="bibr" target="#b4">(Das et al., 2015;</ref><ref type="bibr" target="#b6">Dieng et al., 20
ional Document Model (NVDM) <ref type="bibr" target="#b13">(Miao et al., 2016)</ref>; and (iii) LDA <ref type="bibr" target="#b0">(Blei et al., 2003)</ref>.</p><p>Configurations We train all models wi
g by different researchers, though<ref type="bibr" target="#b24">(Srivastava and Sutton, 2017;</ref><ref type="bibr" target="#b25">Wang et al., 2020)</ref>. Our first results with this model were also
"bibr" target="#b4">Das et al., 2015;</ref><ref type="bibr" target="#b16">Nguyen et al., 2015;</ref><ref type="bibr" target="#b18">Petterson et al., 2010)</ref>, use word relationships derived from ex
ef>.</p><p>Topic models have inspired many extensions that incorporate several types of information <ref type="bibr" target="#b27">(Xun et al., 2017;</ref><ref type="bibr" target="#b4">Das et al., 201
e="bibr" target="#b27">(Xun et al., 2017;</ref><ref type="bibr" target="#b4">Das et al., 2015;</ref><ref type="bibr" target="#b16">Nguyen et al., 2015;</ref><ref type="bibr" target="#b18">Petterson et ="bibr" target="#b4">(Das et al., 2015;</ref><ref type="bibr" target="#b6">Dieng et al., 2019;</ref><ref type="bibr" target="#b16">Nguyen et al., 2015)</ref>. Even for neural topic models, there exist
="#b18">Petterson et al., 2010)</ref>, use word relationships derived from external knowledge bases <ref type="bibr" target="#b3">(Chen et al., 2013;</ref><ref type="bibr" target="#b28">Yang et al., 2
t, thus it's effective to learn information from their interactions, such as low-order interactions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> or high-order interact
ased on dimension recalibration and self-attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> to learn the relations among latent fields. Since each latent
8 0.1372 DCN <ref type="bibr" target="#b9">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type="bibr" target="#b6">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=
ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> to learn the relations
360 xDeepFM <ref type="bibr" target="#b4">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type="bibr" target="#b5">[6]</ref> 0.8093 0.4422 0.7878 0.3752 0.6709 0.1361 FED (Ours) 0.8113
ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> to learn the relations
ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> to learn the relations
tion from their interactions, such as low-order interactions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> or high-order interactions <ref type="bibr" target="#b2">[3,<
ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> to learn the relations
0.1457 DeepFM <ref type="bibr" target="#b2">[3]</ref> 0.8091 0.4423 0.7878 0.3753 0.6708 0.1372 DCN <ref type="bibr" target="#b9">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type="bi dy clear compared with recent works such as xDeepFM <ref type="bibr" target="#b4">[5]</ref> and DCN <ref type="bibr" target="#b9">[10]</ref>.</p><p>Since DRM learns the relations of dimensions in embe
ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> to learn the relations
w method, hence we fail to answer the question.</p><p>Pretrained language models, pioneered by BERT <ref type="bibr" target="#b11">[12]</ref>, have emerged as silver bullets for many NLP tasks, such a ficial obstacle for long texts is that the pretrained max position embedding is usually 512 in BERT <ref type="bibr" target="#b11">[12]</ref>. However, even if the embeddings for larger positions are
</p><p>The maximum length limit in BERT naturally reminds us the limited capacity of Working Memory <ref type="bibr" target="#b1">[2]</ref>, a human cognitive system storing information for logical re ttentional system capable of selecting and operating control processes and strategies", as Baddeley <ref type="bibr" target="#b1">[2]</ref> pointed out in his 1992 classic. Later research detailed tha
rking memory decay over time <ref type="bibr" target="#b4">[5]</ref>, unless are kept via rehearsal <ref type="bibr" target="#b2">[3]</ref>, i.e. paying attention to and refreshing the information in y decay to 1/10 of the max learning rates. The common hyperparameters are batch size = 32, strides= <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, t up = 0.2 and t down
nformation is constantly updated with relevant items from long-term memory by retrieval competition <ref type="bibr" target="#b51">[52]</ref>, collecting sufficient information for reasoning in the wo
We finetune RoBERTa for 6 epochs in CogLTX.  <ref type="bibr" target="#b15">[16]</ref> 79.4 MS-CNN <ref type="bibr" target="#b31">[32]</ref> 86.1 Text GCN <ref type="bibr" target="#b53">[54]</ref> 86
epeating the procedure with new z + . The importance of iterative retrieval is highlighted by CogQA <ref type="bibr" target="#b12">[13]</ref>, as the answer sentence fails to be directly retrieved by re information from new blocks in z + , which is neglected by previous multi-step reasoning methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" targ s. Previous methods usually leverage the graph structure between key entities across the paragraphs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref>. However, if we ca
les attentions between faraway sentences. Similar ideas were investigated on document-level in DrQA <ref type="bibr" target="#b5">[6]</ref> and ORQA <ref type="bibr" target="#b22">[23]</ref>, and ther
et="#b11">[12]</ref>, have emerged as silver bullets for many NLP tasks, such as question answering <ref type="bibr" target="#b37">[38]</ref> and text classification <ref type="bibr" target="#b21">[22 BERT (usually 512 tokens). This situation may be rare for normalized benchmarks, for example SQuAD <ref type="bibr" target="#b37">[38]</ref> and GLUE <ref type="bibr" target="#b46">[47]</ref>, but ve
et="#b11">[12]</ref>, have emerged as silver bullets for many NLP tasks, such as question answering <ref type="bibr" target="#b37">[38]</ref> and text classification <ref type="bibr" target="#b21">[22 BERT (usually 512 tokens). This situation may be rare for normalized benchmarks, for example SQuAD <ref type="bibr" target="#b37">[38]</ref> and GLUE <ref type="bibr" target="#b46">[47]</ref>, but ve
We finetune RoBERTa for 6 epochs in CogLTX.  <ref type="bibr" target="#b15">[16]</ref> 79.4 MS-CNN <ref type="bibr" target="#b31">[32]</ref> 86.1 Text GCN <ref type="bibr" target="#b53">[54]</ref> 86
or p(z|y, x; Î¸) in E-step, while variational bayes methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref> use an estimationfriendly q(z|y, x). However, in CogLTX z h
graph structure between key entities across the paragraphs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref>. However, if we can handle long texts with CogLTX, the prob
et="#tab_3">1</ref> show that CogLTX-base outperforms well-established QA models, for example BiDAF <ref type="bibr" target="#b40">[41]</ref> (+17.8% F 1 ), previous SOTA DECAPROP <ref type="bibr" tar

epeating the procedure with new z + . The importance of iterative retrieval is highlighted by CogQA <ref type="bibr" target="#b12">[13]</ref>, as the answer sentence fails to be directly retrieved by re information from new blocks in z + , which is neglected by previous multi-step reasoning methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" targ s. Previous methods usually leverage the graph structure between key entities across the paragraphs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref>. However, if we ca
tasks, such as question answering <ref type="bibr" target="#b37">[38]</ref> and text classification <ref type="bibr" target="#b21">[22]</ref>. Researchers and engineers breezily build state-of-the-art e="bibr" target="#b43">[44]</ref>, HotpotQA <ref type="bibr" target="#b52">[53]</ref>, 20NewsGroups <ref type="bibr" target="#b21">[22]</ref> and Alibaba, with constant memory consumption regardless o al to analyze the topic, sentiment, intent, etc. We conduct experiments on the classic 20NewsGroups <ref type="bibr" target="#b21">[22]</ref>, which contains 18,846 documents from 20 classes. We finet
re">1</ref>). Since the problem roots in the high O(L 2 ) time and space complexity in transformers <ref type="bibr" target="#b45">[46]</ref> (L is the length of the text), another line of research at
are for normalized benchmarks, for example SQuAD <ref type="bibr" target="#b37">[38]</ref> and GLUE <ref type="bibr" target="#b46">[47]</ref>, but very common for more complex tasks <ref type="bibr" t
a human cognitive system storing information for logical reasoning and decision-making. Experiments <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" targ
te enough without interaction and comparison between blocks, similar to the motivation of reranking <ref type="bibr" target="#b6">[7]</ref>.</p><p>MemRecall in nature enables multi-step reasoning by r .  In the first epoch, the judge is nearly untrained and selects some blocks at random. Among them, <ref type="bibr" target="#b6">(7)</ref> contributes most to the correct classification, thus is mark ype="formula" target="#formula_0">1</ref>) is marked as "relevant" at once. Then in the next epoch, <ref type="bibr" target="#b6">(7)</ref> becomes not essential for classification and is marked as "i an).In the first epoch, the judge is nearly untrained and selects some blocks at random. Among them,<ref type="bibr" target="#b6">(7)</ref> contributes most to the correct classification, thus is mark (1) with strong evidence "prayers" and (1) is marked as "relevant" at once. Then in the next epoch,<ref type="bibr" target="#b6">(7)</ref> becomes not essential for classification and is marked as "i
rks to extract important sentences in unsupervised ways, e.g. based on the metadata about structure <ref type="bibr" target="#b23">[24]</ref>. Experiments on 4 different large datasets show its compet
e="foot" n="3" xml:id="foot_2">We use the original version instead of the simplified version in MRQA<ref type="bibr" target="#b14">[15]</ref>, which removed long texts.</note> 			<note xmlns="http://w
s) and directly harness the resultant example to fool the remote target model (i.e., victim models) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Among these t " target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, and the other one is transfer-based <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta or fair comparisons, we adopt default parameters as recommended in benchmark approaches and Foolbox <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b27">28]</ref>. The random noise of the source model <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7]</ref>. Specifically, althou ork is the regularization-based approach: transferable adversarial perturbation (TAP) introduced by <ref type="bibr" target="#b40">[41]</ref>. TAP injects two regularization terms into the vanilla tra 19">[20,</ref><ref type="bibr" target="#b3">4]</ref>. We follow the protocol of the baseline method <ref type="bibr" target="#b40">[41]</ref> to curate experimental datasets and target models for fair cannot strictly meet the l â budget, we employ the modified l â version of C&amp;W as introduced by <ref type="bibr" target="#b40">[41]</ref>, which can explicitly satisfy the l â norm constraint. Sim b40">[41]</ref>, which can explicitly satisfy the l â norm constraint. Similar to our strategy, TAP <ref type="bibr" target="#b40">[41]</ref> boosts adversarial transferability through two regularizat e random noise is sampled from a clipped normal distribution with mean 0 and variance 1.  Following <ref type="bibr" target="#b40">[41]</ref>, we fix the perturbation budget Ç« to 16 for all methods. W ext attack models defended by adversarial training. For fair comparisons with the baseline approach <ref type="bibr" target="#b40">[41]</ref>, we stick to employing undefended models as local source m d the other one is the regularization-based transferable adversarial perturbation (TAP) proposed by <ref type="bibr" target="#b40">[41]</ref>. With the integrated attacks, we conduct experiments simil
arget="#b38">39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Query-based black-box attacks can settle the suscept >32]</ref> or images <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref>. More related to our work is the regularization-based appro
synthesized by such a scheme are prone to overfit to the exclusive blind spots of the source model <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" targ models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32]</ref> or images <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" targ arget="#b9">10]</ref>, and the other one is transfer-based <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" targe
ibr" target="#b4">[5]</ref>, which also admits the employment of sophisticated optimizers like Adam <ref type="bibr" target="#b16">[17]</ref> during the search for deceptive noises. Jacobian-based Sal
ameters as recommended in benchmark approaches and Foolbox <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b27">28]</ref>. The random noise is sampled from a clipped normal distribu
s of a DNN image classifier are often so generic that they can adapt to different domains and tasks <ref type="bibr" target="#b30">[31]</ref>. Inspired by the fact, we expect that lots of feature desc
"#b16">[17]</ref> during the search for deceptive noises. Jacobian-based Saliency Map Attack (JSMA) <ref type="bibr" target="#b24">[25]</ref> is tailored for seeking the adversarial noise with the min ref type="bibr" target="#b17">[18]</ref>, C&amp;W <ref type="bibr" target="#b4">[5]</ref>, and JSMA <ref type="bibr" target="#b24">[25]</ref>, to showcase the effectiveness of our algorithm in allevia
b2">[3]</ref>. Failed attempts include pre-processing the input images to diminish malicious noises <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1]</ref>, defensive distillat
malicious instances <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5]</ref>, while black-box attacks can be further divided into two cate rget="#b35">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5]</ref>. Different from the process of model training, they feature a Wagner attacks (C&amp;W) devise a novel attack object to absorb the perturbation budget constraint <ref type="bibr" target="#b4">[5]</ref>, which also admits the employment of sophisticated optimizer GSM <ref type="bibr" target="#b8">[9]</ref>, BIM <ref type="bibr" target="#b17">[18]</ref>, C&amp;W <ref type="bibr" target="#b4">[5]</ref>, and JSMA <ref type="bibr" target="#b24">[25]</ref>, to show
-of-the-art adversarially trained models as remote targets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19]</ref>, since adversarial training is arguably the most promising
malicious instances <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5]</ref>, while black-box attacks can be further divided into two cate rget="#b35">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5]</ref>. Different from the process of model training, they feature a Wagner attacks (C&amp;W) devise a novel attack object to absorb the perturbation budget constraint <ref type="bibr" target="#b4">[5]</ref>, which also admits the employment of sophisticated optimizer GSM <ref type="bibr" target="#b8">[9]</ref>, BIM <ref type="bibr" target="#b17">[18]</ref>, C&amp;W <ref type="bibr" target="#b4">[5]</ref>, and JSMA <ref type="bibr" target="#b24">[25]</ref>, to show
. One is query-based <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, and the other one is transfer-based <ref type="bibr" target e susceptible direction of the victim model as per the response of the target model to given inputs <ref type="bibr" target="#b9">[10]</ref>. Alternatively, attackers can approximate the loss gradient
k-box attacks can be further divided into two categories according to the mechanism attackers adopt <ref type="bibr" target="#b7">[8]</ref>. One is query-based, and the other one is transfer-based. Qu sorts of black-box attacks, the transfer-based one has attracted ever-increasing attention recently <ref type="bibr" target="#b7">[8]</ref>. In general, only costly query access to deployed models is applicability than the transfer-based counterparts due to the prohibitive query cost they may incur <ref type="bibr" target="#b7">[8]</ref>.</p><p>Thanks to the observed cross-model transferability of require excessive queries before a successful trial and thus have limited applicability in practice <ref type="bibr" target="#b7">[8]</ref>.</p><p>Transfer-based black-box attacks are motivated by the introduced by our strategy. One is the ensemble-based translationinvariant attack (TI) developed by <ref type="bibr" target="#b7">[8]</ref>, and the other one is the regularization-based transferable mple to fool the remote target model (i.e., victim models) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Among these two sorts of black-box attacks, the transf o overfit to the exclusive blind spots of the source model <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" targe is transfer-based <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" targe 1,</ref><ref type="bibr" target="#b31">32]</ref> or images <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref>. More related to our
bibr" target="#b12">13]</ref>, Inception V3 <ref type="bibr" target="#b34">[35]</ref>, Inception V4 <ref type="bibr" target="#b33">[34]</ref>, and Inception-ResNet V2 <ref type="bibr" target="#b33">[3 ="#b34">[35]</ref>, Inception V4 <ref type="bibr" target="#b33">[34]</ref>, and Inception-ResNet V2 <ref type="bibr" target="#b33">[34]</ref>  We also attack the corresponding ensemble model (referred
ular practice is to freely employ any white-box attack strategy as transfer-based black-box attacks <ref type="bibr" target="#b20">[21]</ref>. Unfortunately, the malicious images synthesized by such a rget="#b40">[41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Query-based -based mechanisms of-ten require the deduced distortion to remain harmful for an ensemble of models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32]</ref> or images <ref typ
arget="#b34">[35]</ref>) for a cat prediction. The visualization is generated with the technique of <ref type="bibr" target="#b29">[30]</ref> as detailed in Section 4.2. Redder regions possess higher tention weights, we propose to visualize the attention maps of various models with the technique of <ref type="bibr" target="#b29">[30]</ref>. Such visualization aims to explore what the model attenti
eatmaps of three representative models (VGG 16 <ref type="bibr" target="#b32">[33]</ref>, ResNet V2 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, and Inception V3 d models, we employ numerous top-performance models with diverse architectures, including ResNet V2 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, Inception V3 <ref
ge classifier may face, there are generally two kinds of threat models considered in the literature <ref type="bibr" target="#b19">[20]</ref>. One is white-box settings, where attackers have full acce that they are tailored for, there exist two sorts of attacks: white-box attacks and black-box ones <ref type="bibr" target="#b19">[20]</ref>. White-box attacks can exploit the exact gradient informat r technique is the ImageNet-compatible dataset released by the NeurIPS 2017 adversarial competition <ref type="bibr" target="#b19">[20]</ref>. This test set contains 1000 images that are not included s to train the model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref>. Moreover, exploiting the malicious examples tailored for d 29]</ref>, which is the most broadly recognized benchmark task for transfer-based black-box attacks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref>. We follow the proto
ibr" target="#b4">[5]</ref>, which also admits the employment of sophisticated optimizers like Adam <ref type="bibr" target="#b16">[17]</ref> during the search for deceptive noises. Jacobian-based Sal
ient descent (PGD) extends BIM with random start to diversify the synthesized adversarial instances <ref type="bibr" target="#b21">[22]</ref>. Carlini and Wagner attacks (C&amp;W) devise a novel attac >19]</ref>, since adversarial training is arguably the most promising and effective defense to date <ref type="bibr" target="#b21">[22]</ref>. These adversarially trained models include adversarially e clean training data with such instances to train the model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref>. Moreover, exploiti
t="#b0">1]</ref>, defensive distillation to mask gradients <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b5">6]</ref>, and feature squeezing to detect adversarial samples <ref typ
er-based. Query-based black-box attacks usually require excessive queries before a successful trial <ref type="bibr" target="#b15">[16]</ref>. On the contrary, without the feedback information from th ="#b6">7]</ref>. Specifically, although the crafted adversarial samples can attack Inception V3 VGG <ref type="bibr" target="#b15">16</ref> ResNet V2</p><p>Figure <ref type="figure">1</ref>: The atten

more sophisticated operator Ï , such as borrowing the structure of descriptors in manifold geometry <ref type="bibr" target="#b9">(Kokkinos et al., 2012;</ref><ref type="bibr" target="#b13">Monti et a
more likely to be proximal, and vice versa), but may be inappropriate to the disassortative graphs <ref type="bibr" target="#b17">(Newman, 2002)</ref> where node homophily does not hold. For example,
s the gap between continuous space and graph <ref type="bibr" target="#b6">(Hoff et al., 2002;</ref><ref type="bibr" target="#b14">Muscoloni et al., 2017)</ref>. Network geometry aims to understand ne

ref type="bibr" target="#b17">(Newman, 2002)</ref> where node homophily does not hold. For example, <ref type="bibr" target="#b21">Ribeiro et al. (2017)</ref> shows disassortative graphs where nodes o , Poincare embedding <ref type="bibr" target="#b18">(Nickel &amp; Kiela, 2017)</ref>, and struc2vec <ref type="bibr" target="#b21">(Ribeiro et al., 2017)</ref>, which result in three Geom-GCN variants
om brain networks to online social network <ref type="bibr" target="#b5">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b29">Wang et al., 2019)</ref>. In a layer of MPNNs, each node sends its fe
s the gap between continuous space and graph <ref type="bibr" target="#b6">(Hoff et al., 2002;</ref><ref type="bibr" target="#b14">Muscoloni et al., 2017)</ref>. Network geometry aims to understand ne
etworks (MPNNs), such as GNN <ref type="bibr" target="#b23">(Scarselli et al., 2008)</ref>, ChebNet <ref type="bibr" target="#b3">(Defferrard et al., 2016)</ref>, GG-NN <ref type="bibr" target="#b11">
icular topology patterns (e.g., hierarchy) are preserved. We employ three embedding methods, Isomap <ref type="bibr" target="#b26">(Tenenbaum et al., 2000)</ref>, Poincare embedding <ref type="bibr" t
icular topology patterns (e.g., hierarchy) are preserved. We employ three embedding methods, Isomap <ref type="bibr" target="#b26">(Tenenbaum et al., 2000)</ref>, Poincare embedding <ref type="bibr" t

ion probabilistic modeling (DDPM) <ref type="bibr" target="#b39">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b17">Ho et al., 2020)</ref> trains a sequence of probabilistic models to r at generation of images <ref type="bibr">(Song &amp; Ermon, 2019;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b17">Ho et al., 2020)</ref>, audio <ref type="bibr" target="#b6">(Chen et s of SMLD and DDPM can be unified into our framework as discretizations of different SDEs. Although <ref type="bibr" target="#b17">Ho et al. (2020)</ref> has reported higher sample quality than <ref t Eq. ( <ref type="formula" target="#formula_3">3</ref>) described here is equivalent to L simple in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, but we re-write it in a slightly different fo rget="#b16">(Ho et al., 2019)</ref> or discrete data). Main results: (i) For the same DDPM model in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, we obtain better bits/dim compared to the upp e="bibr" target="#b17">(Ho et al., 2020)</ref>. With PC samplers and the same model architecture in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, the gaps can be reduced, but score-based mode data pxq. In our experiments, we let Î²min " 0.1 and Î²max " 20, which correspond to the settings in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>. The perturbation kernel is given by</p><formu arget="#fig_4">4</ref>, we use a DDPM model trained on 256 Ë256 CelebA-HQ with the same settings in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>. We use the RK45 ODE solver <ref type="bibr" t on and temperature scaling. The model tested here is a DDPM model trained with the same settings in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns tially a different discretization to the same reverse-time SDE. This unifies the sampling method in <ref type="bibr" target="#b17">Ho et al. (2020)</ref> as a numerical solver to the reverse-time VP S Â´Ï2 iÂ´1 qIq, i " 1, 2, Â¨Â¨Â¨, N.</formula><p>Here we assume Ï 0 " 0 to simplify notations. Following <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, we can compute</p><formula xml:id="formula_55 " x i `pÏ 2 i Â´Ï2 iÂ´1 qs Î¸ px i , iq,</formula><p>where s Î¸ px i , iq is to estimate z{Ï i . As in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, we let Ï i "</p><formula xml:id="formula_58"> DDITIONAL DETAILS ON PREDICTOR-CORRECTOR SAMPLERS</head><p>Training We use the same architecture in <ref type="bibr" target="#b17">Ho et al. (2020)</ref> for our score-based models. For the VE SDE, we noise scales at test time. The specific architecture of the noise-conditional score-based model in <ref type="bibr" target="#b17">Ho et al. (2020)</ref> uses sinusoidal positional embeddings for cond amples with TF-GAN. For sampling, we use the PC sampler discretized at 1000 noise scales. We follow <ref type="bibr" target="#b17">Ho et al. (2020)</ref> for optimization, including the learning rate, mula" target="#formula_0">1</ref>) and use a batch size of 128. Our architecture is mostly based on <ref type="bibr" target="#b17">Ho et al. (2020)</ref>. We additionally search over the following com pe="bibr" target="#b20">Karras et al. (2018)</ref>; <ref type="bibr">Song &amp; Ermon (2019)</ref>; <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, the FID value here is the lowest over the cou odel upon conditioning on continuous time variables, we change positional embeddings, the layers in <ref type="bibr" target="#b17">Ho et al. (2020)</ref> for conditioning on discrete time steps, to ra al., 2018)</ref> 3.40 -Flow++ <ref type="bibr" target="#b16">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> Ä 3.70 13.51 DDPM (Lsimple) <ref type="bibr" > 3.29 -DDPM (L) <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> Ä 3.70 13.51 DDPM (Lsimple) <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> Ä 3.75 3.17   <ref type="bibr">(Song &amp; Er )</ref> 25.32 8.87 Ë.12 NCSNv2 <ref type="bibr">(Song &amp; Ermon, 2020)</ref> 10.87 8.40 Ë.07 DDPM <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> 3.17 9.46 Ë.11 Exact likelihood computation L s on CIFAR-10 is 10.23 <ref type="bibr">(Song &amp; Ermon, 2020)</ref>, whereas for DDPM it is 3.17 <ref type="bibr" target="#b17">(Ho et al., 2020)</ref>. With PC samplers and the same model architec "#formula_52">41</ref>)) reverse diffusion samplers.</p><p>Note that the ancestral sampling of DDPM <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> (Eq. ( <ref type="formula" target="#formula_4
et="#fig_4">4</ref>). RealNVP <ref type="bibr" target="#b9">(Dinh et al., 2016)</ref> 3.49 -iResNet <ref type="bibr" target="#b2">(Behrmann et al., 2019)</ref> 3.45 -Glow <ref type="bibr" target="#b24



nce of decreasing noise scales during generation. Denoising diffusion probabilistic modeling (DDPM) <ref type="bibr" target="#b39">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b17">Ho


)</ref> 3.45 -Glow <ref type="bibr" target="#b24">(Kingma &amp; Dhariwal, 2018)</ref> 3.35 -MintNet <ref type="bibr" target="#b43">(Song et al., 2019b)</ref> 3.32 -Residual Flow <ref type="bibr">(Chen ls such as neural ODEs and normalizing flows <ref type="bibr" target="#b9">(Dinh et al., 2016;</ref><ref type="bibr" target="#b43">Song et al., 2019b)</ref>, we can manipulate this latent representati

generative models, and related techniques <ref type="bibr" target="#b3">(Bordes et al., 2017;</ref><ref type="bibr" target="#b13">Goyal et al., 2017)</ref>, have proven effective at generation of ima

20b)</ref>.</p><p>â¢ Replacing the original residual blocks in DDPM with residual blocks from BigGAN <ref type="bibr" target="#b4">(Brock et al., 2018)</ref>. â¢ Increasing the number of residual blocks
orks, including ProgressiveGAN <ref type="bibr" target="#b20">(Karras et al., 2018)</ref>, StyleGAN <ref type="bibr" target="#b21">(Karras et al., 2019)</ref> and <ref type="bibr">StyleGAN-2 (Karras e models like Progressive-GAN <ref type="bibr" target="#b20">(Karras et al., 2018)</ref> and StyleGAN <ref type="bibr" target="#b21">(Karras et al., 2019)</ref>. However, we found it harmful at an early
et="#b48">(Zhang, 2019)</ref>. We follow the same implementation and hyper-parameters in StyleGAN-2 <ref type="bibr" target="#b23">(Karras et al., 2020b)</ref>. â¢ Rescaling all skip connections by 1 {
nce of decreasing noise scales during generation. Denoising diffusion probabilistic modeling (DDPM) <ref type="bibr" target="#b39">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b17">Ho
generative models, and related techniques <ref type="bibr" target="#b3">(Bordes et al., 2017;</ref><ref type="bibr" target="#b13">Goyal et al., 2017)</ref>, have proven effective at generation of ima
t with the Skilling-Hutchinson trace estimator <ref type="bibr" target="#b38">(Skilling, 1989;</ref><ref type="bibr" target="#b18">Hutchinson, 1990)</ref>.</p><p>In particular, we have</p><formula xml
be open sourced.   <ref type="bibr" target="#b27">Krizhevsky et al., 2009)</ref> and 64 Ë64 CelebA <ref type="bibr" target="#b28">(Liu et al., 2015)</ref>, which is pre-processed following <ref type=
training a score-based model on samples with score matching <ref type="bibr">(HyvÃ¤rinen, 2005;</ref><ref type="bibr" target="#b42">Song et al., 2019a</ref> </p></div> <div xmlns="http://www.tei-c.org/ ) uses denoising score matching, but other score matching objectives, such as sliced score matching <ref type="bibr" target="#b42">(Song et al., 2019a)</ref> and finite-difference score matching <ref not require computing â xptq log p 0t pxptq | xp0qq. For example, when using sliced score matching <ref type="bibr" target="#b42">(Song et al., 2019a)</ref>, our training objective Eq. ( <ref type="f
oaches improve results and enable more efficient sampling, they remain slower at sampling than GANs <ref type="bibr" target="#b12">(Goodfellow et al., 2014)</ref> on the same datasets. Identifying way
mising application area for deep learning <ref type="bibr" target="#b11">(Graves et al., 2020;</ref><ref type="bibr" target="#b16">Ingraham et al., 2019;</ref><ref type="bibr" target="#b24">Pereira et aphQA <ref type="bibr" target="#b1">(Baldassarre et al., 2020)</ref> on MQA, Structured Transformer <ref type="bibr" target="#b16">(Ingraham et al., 2019)</ref> on CPD, and ProteinSolver <ref type="bi bstantial improvement both in terms of perplexity and sequence recovery over Structured Transformer <ref type="bibr" target="#b16">(Ingraham et al., 2019)</ref>, a GNN method which was trained using t ve model over the space of protein sequences conditioned on the given backbone structure. Following <ref type="bibr" target="#b16">Ingraham et al. (2019)</ref>, we frame this as an autoregressive task the same training and validation sets (Table <ref type="table" target="#tab_3">3</ref>). Following <ref type="bibr" target="#b16">Ingraham et al. (2019)</ref>, we report evaluation on short (100 or f >Protein design As described in the main text, we use the CATH 4.2 dataset and splits as curated by <ref type="bibr" target="#b16">Ingraham et al. (2019)</ref> and the TS50 test set as curated by <ref et should bear minimal similarity to the training structures. We use the CATH 4.2 dataset curated by<ref type="bibr" target="#b16">Ingraham et al. (2019)</ref> in which all available structures with 4
d as a connectivity pattern rather than a rigid shape. Recent state-of-the-art GNNs include GraphQA <ref type="bibr" target="#b1">(Baldassarre et al., 2020)</ref> on MQA, Structured Transformer <ref t </ref> and Ornate <ref type="bibr" target="#b23">(PagÃ¨s et al., 2019)</ref>, the GNN method GraphQA <ref type="bibr" target="#b1">(Baldassarre et al., 2020)</ref>, and three methods that use sequentia T1009, T1011, and T1016. This information is obtained from the CASP download center as described by <ref type="bibr" target="#b1">Baldassarre et al. (2020)</ref>. The exact numbers of targets and stru comparison for CASP 13. All predictions were obtained from the CASP download center as described by <ref type="bibr" target="#b1">Baldassarre et al. (2020)</ref>.</p></div> <div xmlns="http://www.tei-
="bibr" target="#b6">(Cohen &amp; Shashua, 2017)</ref> and attention in natural language processing <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. What are the relevant considerations in unctions. <ref type="foot" target="#foot_3">4</ref>â¢ A sinusoidal encoding of i â j as described in <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>, representing distance along the backbone

target. GDT-TS is a scalar measure of how similar two protein backbones are after global alignment <ref type="bibr" target="#b34">(Zemla et al., 2001)</ref>.</p><p>In addition to accurately predictin
0"><head n="3.3">NETWORK ARCHITECTURE</head><p>Our architecture (GVP-GNN) leverages message passing <ref type="bibr" target="#b10">(Gilmer et al., 2017)</ref> in which messages from neighboring nodes

ein from a large pool of candidate structures and is a crucial step in protein structure prediction <ref type="bibr" target="#b5">(Cheng et al., 2019)</ref>. Computational protein design (CPD) is the evaluated every two years in the community-wide Critical Assessment of Structure Prediction (CASP) <ref type="bibr" target="#b5">(Cheng et al., 2019)</ref>. For a number of recently solved but unrele sequence profiles. SASHAN learns a linear model over secondary structure and contact-based features<ref type="bibr" target="#b5">(Cheng et al., 2019)</ref>. FaeNNz 7<ref type="bibr" target="#b28">(St are available. Seven of these methods were highlighted as bestperforming by the CASP organizers in <ref type="bibr" target="#b5">Cheng et al. (2019)</ref> and are shown along with GVP-GNN Finally, be and SBROD on CASP 11-12. All of the methods highlighted as top-performing by the CASP organizers in <ref type="bibr" target="#b5">Cheng et al. (2019)</ref> are in our comparison for CASP 13. All predi
dinates <ref type="bibr" target="#b17">(Karasikov et al., 2019)</ref>, physicsinspired energy terms <ref type="bibr" target="#b21">(O'Connell et al., 2018;</ref><ref type="bibr" target="#b30">Uziela e uresis typically used to benchmark CPD models <ref type="bibr" target="#b20">(Li et al., 2014;</ref><ref type="bibr" target="#b21">O'Connell et al., 2018;</ref><ref type="bibr" target="#b32">Wang et a g et al., 2019)</ref> 40.7 SBROF <ref type="bibr" target="#b4">(Chen et al., 2019)</ref> 39.2 SPIN2 <ref type="bibr" target="#b21">(O'Connell et al., 2018)</ref> 33.6 Wang's model <ref type="bibr" tar
ayer perceptron or statistical potential on top of such structural features. Finally, MULTICOM-NOVEL<ref type="bibr" target="#b14">(Hou et al., 2019)</ref> and ProQ4<ref type="bibr" target="#b15">(Hur
d as a connectivity pattern rather than a rigid shape. Recent state-of-the-art GNNs include GraphQA <ref type="bibr" target="#b1">(Baldassarre et al., 2020)</ref> on MQA, Structured Transformer <ref t </ref> and Ornate <ref type="bibr" target="#b23">(PagÃ¨s et al., 2019)</ref>, the GNN method GraphQA <ref type="bibr" target="#b1">(Baldassarre et al., 2020)</ref>, and three methods that use sequentia T1009, T1011, and T1016. This information is obtained from the CASP download center as described by <ref type="bibr" target="#b1">Baldassarre et al. (2020)</ref>. The exact numbers of targets and stru comparison for CASP 13. All predictions were obtained from the CASP download center as described by <ref type="bibr" target="#b1">Baldassarre et al. (2020)</ref>.</p></div> <div xmlns="http://www.tei-
ll a challenge for training diversification models.</p><p>To tackle this problem, inspired by IRGAN <ref type="bibr" target="#b18">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref ty e Carlo search. It is also used in the traditional information retrieval area, Wang proposed IR-GAN <ref type="bibr" target="#b18">[19]</ref> which consists of two information retrieval models in it. ð, ð)) 1 + exp(ð ð (ð |ð, ð)) .<label>(7)</label></formula><p>Please note that different from IRGAN <ref type="bibr" target="#b18">[19]</ref>, DVGAN-doc has an additional component ð to represent the , it is difficult to calculate the generator gradient due to its discrete nature. Inspired by IRGAN <ref type="bibr" target="#b18">[19]</ref>, we generate negative document set ð· â² by selecting the do
hes have been proposed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar ype="bibr" target="#b23">[24]</ref>. The explicit approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar to solve this problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar tly leverage subtopics to determine the diversity of results <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" tar target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref> and supervised approaches such as DSSA <ref type="bibr" target="#b11">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref xplicit method calculating the distribution by counting the relevant document of the subtopic. DSSA <ref type="bibr" target="#b11">[12]</ref> introduces the machine learning method into explicit appro ent ð¶ to fool the discriminator. So it also needs a score function. In our method we adapt the DSSA <ref type="bibr" target="#b11">[12]</ref> score function for the generator. We introduce the score f <p>In the training process, we first train R-LTR <ref type="bibr" target="#b25">[26]</ref> and DSSA <ref type="bibr" target="#b11">[12]</ref> respectively using MLE loss in both ways. It is because ou br" target="#b5">[6]</ref>, R-LTR-NTN, PAMM-NTN <ref type="bibr" target="#b23">[24]</ref>, and DSSA <ref type="bibr" target="#b11">[12]</ref> as supervised baseline methods. Top 20 results of Lemur ar b7">[8]</ref> as the RNN cell for comparison. In our experiments, we conduct the list-pairwise loss <ref type="bibr" target="#b11">[12]</ref> to train DSSA method. The feature vector 1 http://playbigd DSSA <ref type="bibr" target="#b11">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta
is large enough but the quality of it depends on some hyper-parameters such as the range of ð¼-nDCG <ref type="bibr" target="#b3">[4]</ref> of negative ranking samples, which may cause the model hard that ð is ranked. So the sampler also needs to re-rank ð by the diversification metric like ð¼-nDCG <ref type="bibr" target="#b3">[4]</ref>. In practice, half of the selected document ranking ð is sam <ref type="bibr">[2-4, 20, 21]</ref>, we use ERR-IA <ref type="bibr" target="#b1">[2]</ref>, ð¼-NDCG <ref type="bibr" target="#b3">[4]</ref>, and NRBP <ref type="bibr" target="#b2">[3]</ref> as our div train. We use 5-fold cross validation to tune the parameters in all experiments based on ð¼-nDCG@20 <ref type="bibr" target="#b3">[4]</ref>. A brief introduction to these baselines is as follows.</p><
criminator to the generator, GAN has just been introduced into a discrete area. For example, SeqGAN <ref type="bibr" target="#b12">[13]</ref> introduces the GAN to the text sequence generation area co
IRGAN <ref type="bibr" target="#b18">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref type="bibr" target="#b9">[10]</ref> into search result diversification. Generator generates neg g/ns/1.0"><head n="2.2">Generative Adversarial Network</head><p>Generative Adversarial Network(GAN) <ref type="bibr" target="#b9">[10]</ref> is initially used in the area of computer vision to generat
b17">18]</ref> and PM2 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref> and supervised approaches such as DSSA <ref type="bibr" targe en at subtopic level. We use google query suggestions as subtopics, which are released by Hu et al. <ref type="bibr" target="#b8">[9]</ref> on their website 1 . we only use the first level subtopics a PM2 <ref type="bibr" target="#b5">[6]</ref>, TPM2 <ref type="bibr" target="#b4">[5]</ref>, and HPM2 <ref type="bibr" target="#b8">[9]</ref> as our unsupervised baseline methods. We use ListMLE <ref ty
â¢ , ð ð¡ â1 in convenience of representation. In spite of the kinds of RNN(in our model, we use LSTM <ref type="bibr" target="#b7">[8]</ref>), we use ð» to denote the RNN cell and â ð¡ to denote the hidd ensor slices is tuned from 1 to 10.</p><p>DSSA. DSSA is the supervised explicit method. We use LSTM <ref type="bibr" target="#b7">[8]</ref> as the RNN cell for comparison. In our experiments, we condu
â¢ , ð ð¡ â1 in convenience of representation. In spite of the kinds of RNN(in our model, we use LSTM <ref type="bibr" target="#b7">[8]</ref>), we use ð» to denote the RNN cell and â ð¡ to denote the hidd ensor slices is tuned from 1 to 10.</p><p>DSSA. DSSA is the supervised explicit method. We use LSTM <ref type="bibr" target="#b7">[8]</ref> as the RNN cell for comparison. In our experiments, we condu
e negative training samples with higher quality. In the personalized search area, Lu proposed PSGAN <ref type="bibr" target="#b14">[15]</ref> inspired by IRGAN. Our framework is inspired by the former
core function of ð (particular ð), i.e., D ð (ð |ð, ð¶) and G ð (ð |ð, ð¶), using Plackett-Luce model <ref type="bibr" target="#b6">[7]</ref>. Specifically, we have:</p><formula xml:id="formula_3">DVGAN
core function of ð (particular ð), i.e., D ð (ð |ð, ð¶) and G ð (ð |ð, ð¶), using Plackett-Luce model <ref type="bibr" target="#b6">[7]</ref>. Specifically, we have:</p><formula xml:id="formula_3">DVGAN
thod into explicit approaches. It calculates the distribution using the RNN and attention mechanism <ref type="bibr" target="#b15">[16]</ref>. In our framework, we mainly use the score function of the . As DSSA, we will introduce the distribution of subtopic ð´(ð |ð). DSSA uses both RNN and attention <ref type="bibr" target="#b15">[16]</ref> mechanism to calculate it. Noticed that the selected docum
representation of document. It can be constructed in different ways, In this paper, we use doc2vec <ref type="bibr" target="#b13">[14]</ref> to get document embeddings.</p><p>ð¥ ð,ð and ð¥ ð,ð : Releva
criminator to the generator, GAN has just been introduced into a discrete area. For example, SeqGAN <ref type="bibr" target="#b12">[13]</ref> introduces the GAN to the text sequence generation area co
/ref>) using Plackett-Luce model and the ð¸ is the diversification metrics such as ð¼-NDCG and ERR-IA <ref type="bibr" target="#b1">[2]</ref>. The form of â is inspired by PAMM <ref type="bibr" target=" trics</head><p>Among all the evaluation metrics <ref type="bibr">[2-4, 20, 21]</ref>, we use ERR-IA <ref type="bibr" target="#b1">[2]</ref>, ð¼-NDCG <ref type="bibr" target="#b3">[4]</ref>, and NRBP <r gs but makes it hard for generator to imitate the real distribution of data because it is too ideal <ref type="bibr" target="#b1">(2)</ref>. random sampling makes it easy to imitate for generator but
A <ref type="bibr" target="#b1">[2]</ref>, ð¼-NDCG <ref type="bibr" target="#b3">[4]</ref>, and NRBP <ref type="bibr" target="#b2">[3]</ref> as our diversity evaluation metrics. They measure the docume
is large enough but the quality of it depends on some hyper-parameters such as the range of ð¼-nDCG <ref type="bibr" target="#b3">[4]</ref> of negative ranking samples, which may cause the model hard that ð is ranked. So the sampler also needs to re-rank ð by the diversification metric like ð¼-nDCG <ref type="bibr" target="#b3">[4]</ref>. In practice, half of the selected document ranking ð is sam <ref type="bibr">[2-4, 20, 21]</ref>, we use ERR-IA <ref type="bibr" target="#b1">[2]</ref>, ð¼-NDCG <ref type="bibr" target="#b3">[4]</ref>, and NRBP <ref type="bibr" target="#b2">[3]</ref> as our div train. We use 5-fold cross validation to tune the parameters in all experiments based on ð¼-nDCG@20 <ref type="bibr" target="#b3">[4]</ref>. A brief introduction to these baselines is as follows.</p><

criminator to the generator, GAN has just been introduced into a discrete area. For example, SeqGAN <ref type="bibr" target="#b12">[13]</ref> introduces the GAN to the text sequence generation area co
arget="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" ta rget="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref> stress the relevanc arget="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" ta lso be categorized into heuristic approaches such as xQuAD <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and PM2 <ref type="bibr" target="#b4">[5,</ref><ref type="b e heuristic approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref> by learning an optimized ranking function. However, the lar rget="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Most explicit approaches focus on the subtopic coverage of relevance to the query ð and the ð sub function reflects the ð's relevance to the subtopics. xQuAD <ref type="bibr" target="#b17">[18]</ref> is one of the representative methods of unsupervised expli fication methods. We use Lemur as our non-diversified baseline method. We use xQuAD, TxQuAD, HxQuAD <ref type="bibr" target="#b17">[18]</ref>, PM2 <ref type="bibr" target="#b5">[6]</ref>, TPM2 <ref ty
representation of document. It can be constructed in different ways, In this paper, we use doc2vec <ref type="bibr" target="#b13">[14]</ref> to get document embeddings.</p><p>ð¥ ð,ð and ð¥ ð,ð : Releva
arget="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" ta rget="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref> stress the relevanc arget="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" ta lso be categorized into heuristic approaches such as xQuAD <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and PM2 <ref type="bibr" target="#b4">[5,</ref><ref type="b e heuristic approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref> by learning an optimized ranking function. However, the lar rget="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Most explicit approaches focus on the subtopic coverage of relevance to the query ð and the ð sub function reflects the ð's relevance to the subtopics. xQuAD <ref type="bibr" target="#b17">[18]</ref> is one of the representative methods of unsupervised expli fication methods. We use Lemur as our non-diversified baseline method. We use xQuAD, TxQuAD, HxQuAD <ref type="bibr" target="#b17">[18]</ref>, PM2 <ref type="bibr" target="#b5">[6]</ref>, TPM2 <ref ty
of computing paths and cycles with larger networks, IUAD adopts Weisfeiler-Lehman sub-graph kernel <ref type="bibr" target="#b38">[39]</ref> to evaluate the similarity between two vertices. WL-kernel a j , v a j ) .<label>(4)</label></formula><p>Due to page limitation, more details please refer to <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p><p>For the
ning. To date, existing solutions can be roughly classified into two categories: supervised methods <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b19">[20]</ref> and unsupervise Methods</head><p>Some existing works train classifiers to address the author disambiguation problem <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b19">[20]</ref>, <ref type="bib oauthor names, paper titles, and journal titles, etc., to train classifiers to disambiguate authors <ref type="bibr" target="#b15">[16]</ref>. Treeratpituk et al. extract a set of features, including

vised methods <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Among them, em ">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref>, <ref type="bib ">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref>, <ref type="bib e="bibr">Shin et al. tackle</ref> this problem by splitting vertices in the graph of co-authorships <ref type="bibr" target="#b27">[28]</ref>. Liu et al. introduce a coarse-to-fine multiple clustering s, or kernels <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> are used to measure the similarity of vertices. Due to the
<ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, entity resolution <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>, object identificat "bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b19">[20]</ref> and unsupervised methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, entity resolution <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>, object identificat ches construct the ego-networks, they treat all authors shared the same name as an identical author <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr"
ning. To date, existing solutions can be roughly classified into two categories: supervised methods <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b19">[20]</ref> and unsupervise Methods</head><p>Some existing works train classifiers to address the author disambiguation problem <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b19">[20]</ref>, <ref type="bib oauthor names, paper titles, and journal titles, etc., to train classifiers to disambiguate authors <ref type="bibr" target="#b15">[16]</ref>. Treeratpituk et al. extract a set of features, including
ts, journals and titles, etc., to address the author disambiguation problem in scientific databases <ref type="bibr" target="#b16">[17]</ref>.</p><p>For supervised approaches, they need a lot of label
<ref type="bibr" target="#b2">[3]</ref>, entity resolution <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>, object identification <ref type="bibr" target="#b7">[8]</re <ref type="bibr" target="#b2">[3]</ref>, entity resolution <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>, object identification <ref type="bibr" target="#b7">[8]</re
vised methods <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bib 3">[4]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bib

<ref type="bibr" target="#b2">[3]</ref>, entity resolution <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>, object identification <ref type="bibr" target="#b7">[8]</re <ref type="bibr" target="#b2">[3]</ref>, entity resolution <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>, object identification <ref type="bibr" target="#b7">[8]</re
arget="#b18">19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b42">43]</ref> and have connections to the large literature on metric lear otivated by noise contrastive estimation and N-pair losses <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>, wherein the ability to discriminate between signal and noi <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref> or N-pair losses <ref type="bibr" target="#b42">[43]</ref>. Typically, the loss is applied at the last layer of a dee
type="bibr" target="#b42">43]</ref> and have connections to the large literature on metric learning <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>As the name s function encourages learning from hard positives and hard negatives. We also show that triplet loss <ref type="bibr" target="#b47">[48]</ref> is a special case of our loss when only a single positive ss can thus be seen to be efficient in its training. Other contrastive losses, such as triplet loss <ref type="bibr" target="#b47">[48]</ref>, often use the computationally expensive technique of hard .">Connections to Triplet Loss</head><p>Contrastive learning is closely related to the triplet loss <ref type="bibr" target="#b47">[48]</ref>, which is one of the widely-used alternatives to cross-ent contrastive learning are metric learning and triplet losses <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b39">40]</ref>. These losses have
or computation of the contrastive loss. For our projection network, we use a multi-layer perceptron <ref type="bibr" target="#b17">[18]</ref> with a single hidden layer of size 2048 and output vector
hich may not be mapped correctly, resulting in a worse representation. ferent ways. Label smoothing <ref type="bibr" target="#b44">[45]</ref> makes a fuzzy distinction between correct and incorrect la practice have been approaches that change the reference label distribution, such as label smoothing <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b32">33]</ref>, data augmentation
<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>, and poor margins <ref type="bibr" target="#b3">[4]</ref>. Alternative losses have been proposed; however, the more po
nd a large body of literature is devoted to finding efficient ways to perform hyperparameter tuning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
With LARS we use a cosine learning rate decay. On the other hand we find that the RMSProp optimizer <ref type="bibr" target="#b46">[47]</ref> works best for training the linear classifier. For RMSProp
ave connections to the large literature on metric learning <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>As the name suggests, contrastive losses consist of tw
hich may not be mapped correctly, resulting in a worse representation. ferent ways. Label smoothing <ref type="bibr" target="#b44">[45]</ref> makes a fuzzy distinction between correct and incorrect la practice have been approaches that change the reference label distribution, such as label smoothing <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b32">33]</ref>, data augmentation
nd a large body of literature is devoted to finding efficient ways to perform hyperparameter tuning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
proposed alternatives do not seem to have worked better for large-scale datasets, such as ImageNet <ref type="bibr" target="#b10">[11]</ref>, as evidenced by the continued use of cross-entropy to ach #b21">[22]</ref>. After training the embedding network with supervised contrastive loss on ImageNet <ref type="bibr" target="#b10">[11]</ref>, we replace the projection head of the network with a a ne
formulation makes our learning gradient efficient (see Section 3.2.3 for more details). The work in <ref type="bibr" target="#b14">[15]</ref> also uses a similar loss formulation to ours; however it i
n data or natural corruptions. This has been shown not only with adversarially constructed examples <ref type="bibr" target="#b15">[16]</ref>, but also with naturally occurring variations such as nois
edicting masked out tokens in a sentence or paragraph e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b30">31]</ref>. Downstream fine-tu
itectures.</p><p>In the image domain, predictive approaches have also been used to learn embeddings <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" ta
arget="#b58">[59,</ref><ref type="bibr" target="#b43">44]</ref> and the possibility of poor margins <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>, leading to reduce type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b43">44]</ref>, adversarial examples <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>, and poor margins
s, the losses are inspired by noise contrastive estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref> or N-pair losses <ref type="bibr" target="#b42">[43]</ref>.
hich may not be mapped correctly, resulting in a worse representation. ferent ways. Label smoothing <ref type="bibr" target="#b44">[45]</ref> makes a fuzzy distinction between correct and incorrect la practice have been approaches that change the reference label distribution, such as label smoothing <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b32">33]</ref>, data augmentation
With LARS we use a cosine learning rate decay. On the other hand we find that the RMSProp optimizer <ref type="bibr" target="#b46">[47]</ref> works best for training the linear classifier. For RMSProp
ave connections to the large literature on metric learning <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>As the name suggests, contrastive losses consist of tw


earning has been closing the gap with, and in some cases even surpassing its supervised counterpart <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" targ air), creating contradicting objectives.</p><p>While recent efforts focus on improved architectures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe ef type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref> and data augmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43]</ref>, relatively little w get="#b41">42,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In contrastive learning, the embedding space is govern different images, regardless of their semantic information <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>. Figure <ref type="fi s this problem by maintaining a momentum encoder and a limited queue of previous samples. SimCLR v1 <ref type="bibr" target="#b8">[9]</ref> eschews a momentum encoder in favor of a large batch size, a "bibr" target="#b33">[34]</ref> 63.6 -PCL <ref type="bibr" target="#b31">[32]</ref> 65.9 -SimCLR v1 <ref type="bibr" target="#b8">[9]</ref> 69.3 89.0 MoCo v2 <ref type="bibr" target="#b10">[11]</ref>
core and do not contrast against negative samples, a defining element of contrastive learning. SwAV <ref type="bibr" target="#b6">[7]</ref> is an online clustering-based method that employs swapping p div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">With Multi-crop</head><p>Caron et al. <ref type="bibr" target="#b6">[7]</ref> propose a multi-crop data augmentation strategy that increas > 73.0 91.  Method AP50 Supervised 81.3 MoCo v2 <ref type="bibr" target="#b10">[11]</ref> 82.5 SwAV <ref type="bibr" target="#b6">[7]</ref> 82.6 FNC (ours) 82.8 crop <ref type="bibr" target="#b6">[7]< arget="#b10">[11]</ref> 82.5 SwAV <ref type="bibr" target="#b6">[7]</ref> 82.6 FNC (ours) 82.8 crop <ref type="bibr" target="#b6">[7]</ref> with a support size of eight, which is shared with false neg v2 and a 1.4% boost from the previous best. Among all approaches, our method is second only to SwAV <ref type="bibr" target="#b6">[7]</ref>, a clustering-based method, and is even better than BYOL <re
ives. Most existing methods focus on mining hard negatives <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b24">25]</ref>, or most recently, increasing positive samples to counter-b hard negatives (i.e., those that are close to the anchor) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b24">25]</ref>, and using more positive samples to counter-balance the eff
ven essential to improving performance on downstream tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" tar
sentation learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" tar ef>, or organizing shuffled patches to recover the original image much like solving a jigsaw puzzle <ref type="bibr" target="#b35">[36]</ref>. Other proxy tasks include recovering an image from a corr
arget="#b37">38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b29">30]</ref>. In fact, selfsupervised visual representation learning has f> or colorization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b29">30]</ref>. While these approaches have been effective, the proxy task
et="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" tar r" target="#b35">[36]</ref>. Other proxy tasks include recovering an image from a corrupted version <ref type="bibr" target="#b44">[45]</ref>, predicting part of the image from context <ref type="bibr
ref><ref type="bibr" target="#b22">23]</ref>, Deep InfoMax <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2]</ref>, and contrastive multiview coding <ref type="bibr" target="#b
core and do not contrast against negative samples, a defining element of contrastive learning. SwAV <ref type="bibr" target="#b6">[7]</ref> is an online clustering-based method that employs swapping p div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">With Multi-crop</head><p>Caron et al. <ref type="bibr" target="#b6">[7]</ref> propose a multi-crop data augmentation strategy that increas > 73.0 91.  Method AP50 Supervised 81.3 MoCo v2 <ref type="bibr" target="#b10">[11]</ref> 82.5 SwAV <ref type="bibr" target="#b6">[7]</ref> 82.6 FNC (ours) 82.8 crop <ref type="bibr" target="#b6">[7]< arget="#b10">[11]</ref> 82.5 SwAV <ref type="bibr" target="#b6">[7]</ref> 82.6 FNC (ours) 82.8 crop <ref type="bibr" target="#b6">[7]</ref> with a support size of eight, which is shared with false neg v2 and a 1.4% boost from the previous best. Among all approaches, our method is second only to SwAV <ref type="bibr" target="#b6">[7]</ref>, a clustering-based method, and is even better than BYOL <re
a recent surge in self-supervised representation learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" tar ngle of a rotated image <ref type="bibr" target="#b18">[19]</ref>, the relative location of patches <ref type="bibr" target="#b14">[15]</ref>, or organizing shuffled patches to recover the original im
ef type="bibr" target="#b21">22]</ref> and data augmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43]</ref>, relatively little work considers the effects of negative s e use a 65k memory buffer for the momentum encoder, with a momentum of 0.999. Following Tian et al. <ref type="bibr" target="#b42">[43]</ref>, we use random crops, color distortion, Gaussian blur, and target="#b10">[11]</ref> 71.1 -SimCLR v2 <ref type="bibr" target="#b9">[10]</ref> 71.7 90.4 InfoMin <ref type="bibr" target="#b42">[43]</ref> 73.0 91.  Method AP50 Supervised 81.3 MoCo v2 <ref type="b
equential recommenders <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29]</ref>. It has been demonstrated that contextual information is im ich is able to achieve the same effect as previous methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>. Besides, the pre-trained data representations can be also them by the interaction timestamps ascendingly. Following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, we only keep the 5-core datasets, and filter unpopular ite te the performance, which are widely used in related works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>. Since HR@1 is equal to NDCG@1, we report results on HR@{1, ation Machines to incorporate arbitrary real-valued features to the sequential recommendation. FDSA <ref type="bibr" target="#b28">[29]</ref> employed a feature-level self-attention block to leverage ttribute-aware sequential models such as TransFM <ref type="bibr" target="#b15">[16]</ref> and FDSA <ref type="bibr" target="#b28">[29]</ref> leverage the contextual features to improve the sequential and attribute as the input to the model. ( <ref type="formula" target="#formula_16">11</ref>) FDSA <ref type="bibr" target="#b28">[29]</ref> constructs a feature sequence and uses a featurelevel self
tc. There are also studies that leverage other architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> for sequential reco xt from both directions for sequence representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n owing previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>, we apply the leave-oneout strategy for evaluation. Concret model, which uses the multi-head attention mechanism to recommend the next item.</p><p>(7) BERT4Rec <ref type="bibr" target="#b22">[23]</ref> uses a Cloze objective loss for sequential recommendation
objective alone.</p><p>Inspired by recent progress with MIM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>, we take a different perspective to develop neural sequenti
on way is easy to suffer from issues such as data sparsity <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>a ased approaches such as Factorization Machine <ref type="bibr" target="#b19">[20]</ref> and AutoInt <ref type="bibr" target="#b21">[22]</ref> mainly learn data representations through the interaction characterizes the pairwise interactions between variables using factorized model.</p><p>(3) AutoInt <ref type="bibr" target="#b21">[22]</ref> utilizes the multi-head self-attentive neural network to l
16">[17]</ref>, copy mechanism <ref type="bibr" target="#b17">[18]</ref> and reinforcement learning <ref type="bibr" target="#b26">[27]</ref>, etc. There are also studies that leverage other architect
on way is easy to suffer from issues such as data sparsity <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>a ased approaches such as Factorization Machine <ref type="bibr" target="#b19">[20]</ref> and AutoInt <ref type="bibr" target="#b21">[22]</ref> mainly learn data representations through the interaction characterizes the pairwise interactions between variables using factorized model.</p><p>(3) AutoInt <ref type="bibr" target="#b21">[22]</ref> utilizes the multi-head self-attentive neural network to l
t self-supervised learning with MIM has been applied to improve the sequential recommendation task; <ref type="bibr" target="#b1">(2)</ref> We propose four self-supervised optimization objectives to m /ref><ref type="bibr" target="#b24">25]</ref>, which is based on Noise Contrastive Estimation (NCE) <ref type="bibr" target="#b1">[2]</ref>. InfoNCE is defined as:</p><formula xml:id="formula_2">E p(X
arget="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Typically, sequential recommendation methods <ref ty target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> capture useful sequential patterns from users' historical b it for high-order MCs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref>. With the development of the neural networks, Hidasi et al. other architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> for sequential recommendation. However, these approaches ne " target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref> mainly emphasize the effect of sequential characteristics u that existing methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref> seldom directly model the correlation between the sequentia eural networks (RNNs) <ref type="bibr" target="#b2">[3]</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b23">[24]</ref>, and self-attention mechanisms <ref type="bibr" target="#b dation. We represent the items using embedding vectors rather than one-hot vectors.</p><p>(5) Caser <ref type="bibr" target="#b23">[24]</ref> is a CNN-based method capturing high-order Markov Chains b
ial recommendation has been widely studied in the literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" targe 4]</ref>.</p><p>Typically, sequential recommendation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" targe s of works follow this line and extend it for high-order MCs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref>. With the development ad><p>Existing studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref> mainly emphasize the d is a parameter matrix to learn. Note that existing methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref> seldom directly model nal neural networks (CNNs) <ref type="bibr" target="#b23">[24]</ref>, and self-attention mechanisms <ref type="bibr" target="#b7">[8]</ref> have been proposed to learn good representations of user pre commendation with MIM, which is called S 3 -Rec. Based on a self-attentive recommender architecture <ref type="bibr" target="#b7">[8]</ref>, we propose to first pre-train the sequential recommender wi introduce the base model of our proposed approach that is developed on the Transformer architecture <ref type="bibr" target="#b7">[8]</ref>. Then, we will describe how we utilize the correlation signa tions.</p><p>Sequential models such as GRU4Rec <ref type="bibr" target="#b20">[21]</ref> and SASRec <ref type="bibr" target="#b7">[8]</ref> mainly focus on modeling the sequential dependencies between ng horizontal and vertical convolutional operations for sequential recommendation.</p><p>(6) SASRec <ref type="bibr" target="#b7">[8]</ref> is a self-attention based sequential recommendation model, w heads as 2. The dimension of the embedding is 64, and the maximum sequence length is 50 (following <ref type="bibr" target="#b7">[8]</ref>). Note that our training phase contains two stages (i.e., pr type="bibr" target="#b26">[27]</ref>, etc. There are also studies that leverage other architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" targ tem prediction loss L M I P in Eq. 12 has a similar effect to capture sequential dependencies as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref> except that it can a qual to NDCG@1, we report results on HR@{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" targ ems as candidates for testing. Following the common strategy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, we pair the ground-truth item with 99 randomly sampled negat
ive loss for sequential recommendation by the bidirectional self-attention mechanism.</p><p>(8) HGN <ref type="bibr" target="#b12">[13]</ref> is recently proposed and adopts hierarchical gating networ
on way is easy to suffer from issues such as data sparsity <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>a ased approaches such as Factorization Machine <ref type="bibr" target="#b19">[20]</ref> and AutoInt <ref type="bibr" target="#b21">[22]</ref> mainly learn data representations through the interaction characterizes the pairwise interactions between variables using factorized model.</p><p>(3) AutoInt <ref type="bibr" target="#b21">[22]</ref> utilizes the multi-head self-attentive neural network to l
died in the literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar Early works on sequential recommendation are based on the Markov Chain assumption. MC-based methods <ref type="bibr" target="#b19">[20]</ref> estimated an item-item transition probability matrix and u ork. We make a brief discussion below.</p><p>Feature-based approaches such as Factorization Machine <ref type="bibr" target="#b19">[20]</ref> and AutoInt <ref type="bibr" target="#b21">[22]</ref> main ethod that ranks items according to popularity measured by the number of interactions.</p><p>(2) FM <ref type="bibr" target="#b19">[20]</ref> characterizes the pairwise interactions between variables
on way is easy to suffer from issues such as data sparsity <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>a ased approaches such as Factorization Machine <ref type="bibr" target="#b19">[20]</ref> and AutoInt <ref type="bibr" target="#b21">[22]</ref> mainly learn data representations through the interaction characterizes the pairwise interactions between variables using factorized model.</p><p>(3) AutoInt <ref type="bibr" target="#b21">[22]</ref> utilizes the multi-head self-attentive neural network to l
(such as item attributes) to neural sequential recommenders <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29]</ref>. It has been demonstr el by introducing pair-wise loss functions <ref type="bibr" target="#b3">[4]</ref>, memory networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, hierarchical structur
not been well captured in data representations. As shown in increasing evidence from various fields <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target idea of self-supervised learning for improving sequential recommendation. Self-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref> is a newly emerging tp://www.tei-c.org/ns/1.0"><head n="2.2">Self-supervised Learning</head><p>Self-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target guide the visual feature learning <ref type="bibr" target="#b4">[5]</ref>. As for language modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>, it is a popular sel . It is beneficial to incorporate context from both directions for sequence representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div> <div xmln or sequences can improve the performance of downstream tasks such as machine reading comprehension <ref type="bibr" target="#b0">[1]</ref> and natural language understanding <ref type="bibr" target=" upervised learning objectives with the recently proposed pretraining framework in language modeling <ref type="bibr" target="#b0">[1]</ref>.</p><p>The overview of S 3 -Rec is presented in Fig. <ref ty multi-head self-attention function to remove all connections between Q i and K i . Inspired by BERT <ref type="bibr" target="#b0">[1]</ref>, at the pre-training stage, we remove the mask mechanism to deed observed by the model in the training process. Inspired by the masked language model like BERT <ref type="bibr" target="#b0">[1]</ref>, we propose to model the bidirectional information in item s
t self-supervised learning with MIM has been applied to improve the sequential recommendation task; <ref type="bibr" target="#b1">(2)</ref> We propose four self-supervised optimization objectives to m /ref><ref type="bibr" target="#b24">25]</ref>, which is based on Noise Contrastive Estimation (NCE) <ref type="bibr" target="#b1">[2]</ref>. InfoNCE is defined as:</p><formula xml:id="formula_2">E p(X
t="#b6">7]</ref>, hierarchical structures <ref type="bibr" target="#b16">[17]</ref>, copy mechanism <ref type="bibr" target="#b17">[18]</ref> and reinforcement learning <ref type="bibr" target="#b26">
target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Typically, s recommendation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> capture useful sequ It has been found that such an optimization way is easy to suffer from issues such as data sparsity <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div> <div xm the interaction records by users and sort them by the interaction timestamps ascendingly. Following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, we only keep the and Mean Reciprocal Rank (MRR) to evaluate the performance, which are widely used in related works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>. Since HR@1 is equ enhance data representations instead of making predictions.</p><p>Sequential models such as GRU4Rec <ref type="bibr" target="#b20">[21]</ref> and SASRec <ref type="bibr" target="#b7">[8]</ref> mainly as a similar effect to capture sequential dependencies as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref> except that it can also utilize bidirectional sequential in
arget="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Typically, sequential recommendation methods <ref ty target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> capture useful sequential patterns from users' historical b it for high-order MCs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref>. With the development of the neural networks, Hidasi et al. other architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> for sequential recommendation. However, these approaches ne " target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref> mainly emphasize the effect of sequential characteristics u that existing methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref> seldom directly model the correlation between the sequentia eural networks (RNNs) <ref type="bibr" target="#b2">[3]</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b23">[24]</ref>, and self-attention mechanisms <ref type="bibr" target="#b dation. We represent the items using embedding vectors rather than one-hot vectors.</p><p>(5) Caser <ref type="bibr" target="#b23">[24]</ref> is a CNN-based method capturing high-order Markov Chains b
not been well captured in data representations. As shown in increasing evidence from various fields <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target idea of self-supervised learning for improving sequential recommendation. Self-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref> is a newly emerging tp://www.tei-c.org/ns/1.0"><head n="2.2">Self-supervised Learning</head><p>Self-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target guide the visual feature learning <ref type="bibr" target="#b4">[5]</ref>. As for language modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>, it is a popular sel . It is beneficial to incorporate context from both directions for sequence representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div> <div xmln or sequences can improve the performance of downstream tasks such as machine reading comprehension <ref type="bibr" target="#b0">[1]</ref> and natural language understanding <ref type="bibr" target=" upervised learning objectives with the recently proposed pretraining framework in language modeling <ref type="bibr" target="#b0">[1]</ref>.</p><p>The overview of S 3 -Rec is presented in Fig. <ref ty multi-head self-attention function to remove all connections between Q i and K i . Inspired by BERT <ref type="bibr" target="#b0">[1]</ref>, at the pre-training stage, we remove the mask mechanism to deed observed by the model in the training process. Inspired by the masked language model like BERT <ref type="bibr" target="#b0">[1]</ref>, we propose to model the bidirectional information in item s
target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Typically, s recommendation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> capture useful sequ It has been found that such an optimization way is easy to suffer from issues such as data sparsity <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div> <div xm the interaction records by users and sort them by the interaction timestamps ascendingly. Following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, we only keep the and Mean Reciprocal Rank (MRR) to evaluate the performance, which are widely used in related works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>. Since HR@1 is equ enhance data representations instead of making predictions.</p><p>Sequential models such as GRU4Rec <ref type="bibr" target="#b20">[21]</ref> and SASRec <ref type="bibr" target="#b7">[8]</ref> mainly as a similar effect to capture sequential dependencies as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref> except that it can also utilize bidirectional sequential in
on way is easy to suffer from issues such as data sparsity <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>a ased approaches such as Factorization Machine <ref type="bibr" target="#b19">[20]</ref> and AutoInt <ref type="bibr" target="#b21">[22]</ref> mainly learn data representations through the interaction characterizes the pairwise interactions between variables using factorized model.</p><p>(3) AutoInt <ref type="bibr" target="#b21">[22]</ref> utilizes the multi-head self-attentive neural network to l
are not available <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" t #b4">[5]</ref>. The system first predicts 68 face landmarks from speech using an LSTM-based network <ref type="bibr" target="#b6">[7]</ref>, and then predicts a few talking face images from the condit
15">[16]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr hat generate talking faces directly from a condition image and the speech signal. Vougioukas et al. <ref type="bibr" target="#b17">[18]</ref> proposed a temporal-GAN method to generate more realistic
="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr 8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" erate more realistic image sequences. They further improved their methods with three discriminators <ref type="bibr" target="#b9">[10]</ref> that focus on improving the realness of video frames, the c etworks, and objective and subjective evaluations. We choose the temporal GAN approach described in <ref type="bibr" target="#b9">[10]</ref> as our baseline since it is the closest to our method. We u PS and audio to 8 kHz. We followed the same train (70%), validation (15%), and test (15%) splits as <ref type="bibr" target="#b9">[10]</ref>. We used the same files for these splits to ensure a fair c NLMD), even though our method does not use a discriminator calculating a synchronization loss as in <ref type="bibr" target="#b9">[10]</ref>, the improvement is as high as 8.9%, showing the effectiven ical emotion inputs. We evaluated our network against the ground-truth videos and a baseline system <ref type="bibr" target="#b9">[10]</ref> and validated that our method can generate emotional expres
">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> concludes that ludes that different modalities complement each other, and there are also intermodal effects. Cowie <ref type="bibr" target="#b25">[26]</ref> showed that perception is sensitive to stimuli from multip
der to provide the visual cues when they are not available <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" t om researchers in recent years. One approach is to first convert the speech input to face landmarks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" MFCCs), energy, and the first-and second-order temporal derivatives of these features. Chung et al. <ref type="bibr" target="#b5">[6]</ref> proposed a CNN that takes a condition face image and speech
ndmarks. They employ a discriminator network to improve image quality. In another work, Egor et al. <ref type="bibr" target="#b18">[19]</ref> proposed a style-based landmark-to-image conversion method
">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> concludes that ludes that different modalities complement each other, and there are also intermodal effects. Cowie <ref type="bibr" target="#b25">[26]</ref> showed that perception is sensitive to stimuli from multip
15">[16]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr hat generate talking faces directly from a condition image and the speech signal. Vougioukas et al. <ref type="bibr" target="#b17">[18]</ref> proposed a temporal-GAN method to generate more realistic
The presence of visual cues improves speech comprehension <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t
#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bib /p><p>Regarding emotional talking face generation, existing work is somewhat limited. Karras et al. <ref type="bibr" target="#b16">[17]</ref> adopted an end-to-end network to learn a latent representa www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Instead of inferring emotion from the input speech <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, in this work,
">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> concludes that different modalities complement each other,

e series regression and classification dataset benchmarks <ref type="bibr">(Tan et al., 2020a;</ref><ref type="bibr" target="#b0">Bagnall et al., 2017)</ref>, matching or even outperforming sophistica classification based on evaluations on public benchmarks <ref type="bibr">(Tan et al., 2020a;</ref><ref type="bibr" target="#b0">Bagnall et al., 2017)</ref>, followed by CNN-based deep architectures nsupervised learning for univariate and multivariate classification datasets of the UEA/UCR archive <ref type="bibr" target="#b0">(Bagnall et al., 2017)</ref>.</p><p>Transformer models for time series
t al., 2020)</ref>, HIVE-COTE <ref type="bibr" target="#b19">(Lines et al., 2018)</ref>, and ROCKET <ref type="bibr" target="#b7">(Dempster et al., 2020</ref>) currently hold the record on time series t al., 2020)</ref>, HIVE-COTE <ref type="bibr" target="#b19">(Lines et al., 2018)</ref>, and ROCKET <ref type="bibr" target="#b7">(Dempster et al., 2020)</ref> constitute the state of the art for time CHIEF, Proximity Forest, Elastic Ensembles, DTW and HIVE-COTE, but also deep learning based methods <ref type="bibr" target="#b7">(Dempster et al., 2020)</ref>. As can be seen in Table <ref type="tabl
ass of deep learning models, which were first proposed for the task of natural language translation <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> but have since come to monopolize the st the core of our method lies a transformer encoder, as described in the original transformer work by <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>; however, we do not use the decoder part pos .</formula><p>Instead of deterministic, sinusoidal encodings, which were originally proposed by <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>, we use fully learnable positional encodi block, leading to significant performance gains over batch normalization, as originally proposed by <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>. However, here we instead use batch norma
full encoder-decoder transformer architecture was employed for univariate time series forecasting: <ref type="bibr" target="#b18">Li et al. (2019)</ref> showed superior performance compared to the cl ant/compatible) all time steps which share similar values for the independent variable, as noted by <ref type="bibr" target="#b18">Li et al. (2019)</ref>.</p><p>Finally, since the transformer is a fee e of computational resources. Alternative self-attention schemes, such as sparse attention patterns <ref type="bibr" target="#b18">(Li et al., 2019)</ref>, recurrence <ref type="bibr" target="#b5">(Da
W) between time series through a matrix factorization algorithm. A distinct approach is followed by <ref type="bibr" target="#b33">Zhang et al. (2019)</ref>, who use a composite convolutional -LSTM ne
full encoder-decoder transformer architecture was employed for univariate time series forecasting: <ref type="bibr" target="#b18">Li et al. (2019)</ref> showed superior performance compared to the cl ant/compatible) all time steps which share similar values for the independent variable, as noted by <ref type="bibr" target="#b18">Li et al. (2019)</ref>.</p><p>Finally, since the transformer is a fee e of computational resources. Alternative self-attention schemes, such as sparse attention patterns <ref type="bibr" target="#b18">(Li et al., 2019)</ref>, recurrence <ref type="bibr" target="#b5">(Da
ning methods such as TS-CHIEF <ref type="bibr" target="#b28">(Shifaz et al., 2020)</ref>, HIVE-COTE <ref type="bibr" target="#b19">(Lines et al., 2018)</ref>, and ROCKET <ref type="bibr" target="#b7"> ning methods such as TS-CHIEF <ref type="bibr" target="#b28">(Shifaz et al., 2020)</ref>, HIVE-COTE <ref type="bibr" target="#b19">(Lines et al., 2018)</ref>, and ROCKET <ref type="bibr" target="#b7">
es input. They use and evaluate their method only for the task of anomaly detection.</p><p>Finally, <ref type="bibr" target="#b15">Jansen et al. (2018)</ref> rely on a triplet loss and the idea of tem

on from a limited number of time steps stored inside their hidden state (vanishing gradient problem <ref type="bibr" target="#b13">(Hochreiter, 1998;</ref><ref type="bibr" target="#b25">Pascanu et al.
nodes in GCN are inclined to converge to a certain value and thus become indistinguishable. ResNet <ref type="bibr" target="#b13">(He et al., 2016)</ref> solves a similar problem in computer vision w he -th weight matrix W ( ) . Initial residual connection. To simulate the skip connection in ResNet <ref type="bibr" target="#b13">(He et al., 2016)</ref>, <ref type="bibr" target="#b16">(Kipf &amp; W ations for introducing identity mapping into our model.</p><p>â¢ Similar to the motivation of ResNet <ref type="bibr" target="#b13">(He et al., 2016)</ref>, identity mapping ensures that a deep GCNII m


on-inspired network structure design <ref type="bibr" target="#b46">(Zhang &amp; Ghanem, 2018;</ref><ref type="bibr" target="#b30">Papyan et al., 2017)</ref>. The idea is that a feedforward neural net in the same spirit as the sparse coding problem, which has been used to design and to analyze CNNs <ref type="bibr" target="#b30">(Papyan et al., 2017)</ref>. Iterative shrinkage-thresholding algorit


earning, Vienna, Austria, PMLR 119, 2020.</ref> Copyright 2020 by the author(s).</p><p>puter vision <ref type="bibr" target="#b48">(Zhao et al., 2019;</ref><ref type="bibr" target="#b27">Ma et al., 20
Other Related Work</head><p>Spectral-based GCN has been extensively studied for the past few years. <ref type="bibr" target="#b25">(Li et al., 2018c)</ref> improves flexibility by learning a task-driv
2018)</ref> 97.6 GeniePath <ref type="bibr" target="#b26">(Liu et al., 2019)</ref> 98.5 Cluster-GCN <ref type="bibr" target="#b3">(Chiang et al., 2019)</ref> 99.36 GCNII 99.53 Â± 0.01 GCNII* 99.56 Â± 0. t (Xu et al., 2018), GeniePath <ref type="bibr" target="#b26">(Liu et al., 2019)</ref>, Cluster-GCN <ref type="bibr" target="#b3">(Chiang et al., 2019)</ref>. The metrics are summarized in Table <ref
-supervised node classification task, we apply the standard fixed training/validation/testing split <ref type="bibr" target="#b44">(Yang et al., 2016)</ref> on three datasets Cora, Citeseer, and Pubme
2018)</ref> 97.6 GeniePath <ref type="bibr" target="#b26">(Liu et al., 2019)</ref> 98.5 Cluster-GCN <ref type="bibr" target="#b3">(Chiang et al., 2019)</ref> 99.36 GCNII 99.53 Â± 0.01 GCNII* 99.56 Â± 0. t (Xu et al., 2018), GeniePath <ref type="bibr" target="#b26">(Liu et al., 2019)</ref>, Cluster-GCN <ref type="bibr" target="#b3">(Chiang et al., 2019)</ref>. The metrics are summarized in Table <ref
<head>A.2. Proof of Theorem 1</head><p>To prove Theorem 1, we need the following Cheeger Inequality <ref type="bibr" target="#b4">(Chung, 2007)</ref> for lazy random walks.</p><p>Lemma 1 ( <ref type=" Inequality <ref type="bibr" target="#b4">(Chung, 2007)</ref> for lazy random walks.</p><p>Lemma 1 ( <ref type="bibr" target="#b4">(Chung, 2007)</ref>). Let p</p><formula xml:id="formula_29">(K) i = In
/p><p>On the other hand, several methods combine deep propagation with shallow neural networks. SGC <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> attempts to capture higher-order information ks.</p><p>Second, we provide theoretical analysis for multi-layer GCN and GCNII models. It is known <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> that by stacking k layers, the vanilla GCN es a_2">H ( +1) = Ï PH ( ) W ( ) .</formula><p>(1)</p><p>Where Ï denotes the ReLU operation.</p><p>SGC <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> shows that by stacking K layers, GCN correspo rmula></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GCNII Model</head><p>It is known <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> that by stacking K layers, the vanilla GCN si mula><p>where L is the normalized Laplacian matrix of G and Î¸ k 's are the polynomial coefficients. <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> proves that a K-layer GCN simulates a polynom

earity tends to degrade the performance of these models. Such a phenomenon is called over-smoothing <ref type="bibr" target="#b24">(Li et al., 2018b)</ref>, which suggests that as the number of layers


18)</ref> have been successfully applied to a wide range of applications, including social analysis <ref type="bibr" target="#b32">(Qiu et al., 2018;</ref><ref type="bibr" target="#b21">Li &amp; Goldw
oods in the latent space of graph embedding for aggregation to extract more structural information. <ref type="bibr" target="#b5">(Dave et al., 2019</ref>) uses a single representation vector to captu
earning, Vienna, Austria, PMLR 119, 2020.</ref> Copyright 2020 by the author(s).</p><p>puter vision <ref type="bibr" target="#b48">(Zhao et al., 2019;</ref><ref type="bibr" target="#b27">Ma et al., 20


orhood aggregation function), GVAE with GCN encoder Kipf &amp; Welling (2016), DGI with GIN encoder <ref type="bibr" target="#b45">Velickovic et al. (2019)</ref>, and EGI with GIN encoder. We train GV small), our t-tests have shown the improvements of EGI to be significant. 55.56% ? 6.83% DGI (GIN) <ref type="bibr" target="#b45">Velickovic et al. (2019)</ref> 57.75% ? 4.47% 62.44% ? 4.46% 68.15% ?
type="bibr" target="#b14">Hamilton et al. (2017)</ref>; <ref type="bibr">Ying et al. (2018b)</ref>; <ref type="bibr" target="#b44">Velickovic et al. (2018)</ref>, as well as close connections to spect e Kipf &amp; Welling (2017);</p><ref type="bibr" target="#b14">Hamilton et al. (2017)</ref></p>;</p><ref type="bibr" target="#b44">Velickovic et al. (2018)</ref></p>;</p><ref type="bibr" target="#b6">
graph. In practice, the computation of eigenvalues on the small ego-graphs can be rather efficient <ref type="bibr" target="#b1">Arora et al. (2005)</ref>, and we do not need to enumerate all pairs o
n log-log scale. The class labels are between 0 to 3 reflecting the level of the airport activities <ref type="bibr" target="#b37">Ribeiro et al. (2017)</ref>. For the Gene dataset, we matched the gen bel></label><figDesc>We use two real-world network datasets with role-based node labels: (1) Airport<ref type="bibr" target="#b37">Ribeiro et al. (2017)</ref> contains three networks different regions
s and even domains in settings like few-shot learning <ref type="bibr">Vinyals et al. (2016)</ref>; <ref type="bibr" target="#b11">Finn et al. (2017)</ref>; <ref type="bibr" target="#b36">Ravi &amp; L



target="#b44">Velickovic et al. (2018)</ref>, as well as close connections to spectral graph theory <ref type="bibr" target="#b9">Defferrard et al. (2016)</ref>; <ref type="bibr" target="#b5">Bruna et rning and generalizing to unseen nodes given meaningful node features Kipf &amp; Welling (2017);</p><ref type="bibr" target="#b9">Defferrard et al. (2016)</ref></p>;</p><ref type="bibr" target="#b14">

GNNs are presented, such as the auto-encoder-based ones like GVAE Kipf &amp; Welling (2016) and GNFs<ref type="bibr" target="#b28">Liu et al. (2019)</ref>, as well as the deep-infomax-based ones likeD
two well-accepted propositions-deep neural networks learn meaningful patterns before fitting noise <ref type="bibr" target="#b2">[3]</ref> and minimum entropy regularisation principle [10]-we propose meaningful patterns before fitting noise, even when severe label noise exists in human annotations <ref type="bibr" target="#b2">[3]</ref>. (2) As a learner attains confident knowledge as time progre e meaningful patterns before fitting noise, even when severe label noise exists in human annotations<ref type="bibr" target="#b2">[3]</ref>. (2) As a learner attains confident knowledge as time progre
mum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Secondly, note imum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.Secondly, note that O n machine learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target
et="#b27">[28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" tar
f type="bibr" target="#b31">[32]</ref>. D2L monitors the subspace dimensionality change at training <ref type="bibr" target="#b26">[27]</ref>. GCE denotes generalised cross entropy <ref type="bibr" ta
ing deep neural networks. It includes label smoothing (LS) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29]</ref> and confidence penalty (CP) <ref type="bibr" target="#b32"> ovide a student model the similarity structure information <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>; (2) In Self LC, e.g., Boot-soft, a model helps itself by e inimisation against the recent confidence penalty practice <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar
t Optimisation (Jointsoft and Joint-hard) <ref type="bibr" target="#b42">[43]</ref>, and Tf-KD self <ref type="bibr" target="#b54">[55]</ref>. According to an overview in Figure <ref type="figure" tar f>, the deepest classifier provides knowledge for shallower classifiers. In a recent self KD method <ref type="bibr" target="#b54">[55]</ref>, Tf-KD self applies two-stage training. In the second stag do not modify labels, thus being less relevant for comparison. Second, the two-stage self KD method <ref type="bibr" target="#b54">[55]</ref> can be an add-on (i.e., an enhancement plugin) other than oss function of such methods can be defined to be L KD (q, p t , p) = (1 â Ç«)H(q, p) + Ç«KL(p t ||p) <ref type="bibr" target="#b54">[55]</ref>. KL(â¢||â¢) denotes the KL divergence. As KL(p t ||p) = H(p et="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55]</ref> and Non-self LC <ref type="bibr" target="#b16">[17]</ref>.
t Optimisation (Jointsoft and Joint-hard) <ref type="bibr" target="#b42">[43]</ref>, and Tf-KD self <ref type="bibr" target="#b54">[55]</ref>. According to an overview in Figure <ref type="figure" tar f>, the deepest classifier provides knowledge for shallower classifiers. In a recent self KD method <ref type="bibr" target="#b54">[55]</ref>, Tf-KD self applies two-stage training. In the second stag do not modify labels, thus being less relevant for comparison. Second, the two-stage self KD method <ref type="bibr" target="#b54">[55]</ref> can be an add-on (i.e., an enhancement plugin) other than oss function of such methods can be defined to be L KD (q, p t , p) = (1 â Ç«)H(q, p) + Ç«KL(p t ||p) <ref type="bibr" target="#b54">[55]</ref>. KL(â¢||â¢) denotes the KL divergence. As KL(p t ||p) = H(p et="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55]</ref> and Non-self LC <ref type="bibr" target="#b16">[17]</ref>.
get="#b12">13,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b24">25]</ref> and exploit their '
get="#b12">13,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b24">25]</ref> and exploit their '
to estimate in practice; (2) Exploiting an auxiliary trusted training set to differentiate examples <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" ta ally necessary for any method and differs from the methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar
aining and testing sets, respectively. The image size is 32 Ã 32. We apply simple data augmentation <ref type="bibr" target="#b14">[15]</ref>, i.e., we pad 4 pixels on every side of the image, and the with a size of 32 Ã 32. Finally, this crop is horizontally flipped with a   (2) We train ResNet-50 <ref type="bibr" target="#b14">[15]</ref> on ImageNet 2012 classification dataset, which has 1k clas k. We divide the learning rate by 10 at 15k and 22k iterations, respectively. (2) We train ResNet-50<ref type="bibr" target="#b14">[15]</ref> on ImageNet 2012 classification dataset, which has 1k clas
en two models are trained, the consistency between their predictions of a data point is promoted in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b57">58]</ref>, while the distance
f type="bibr" target="#b31">[32]</ref>. D2L monitors the subspace dimensionality change at training <ref type="bibr" target="#b26">[27]</ref>. GCE denotes generalised cross entropy <ref type="bibr" ta
ion matrix, which defines the distribution of noise labels <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" targe gnostic.</p><p>Baselines. For loss correction and estimating the noisetransition matrix, S-adaption <ref type="bibr" target="#b7">[8]</ref> uses an extra softmax layer, while Masking <ref type="bibr"
Boot-hard) <ref type="bibr" target="#b34">[35]</ref>, Joint Optimisation (Jointsoft and Joint-hard) <ref type="bibr" target="#b42">[43]</ref>, and Tf-KD self <ref type="bibr" target="#b54">[55]</ref>. o hard (SH). The other baselines have been introduced heretofore.</p><p>Training details. We follow <ref type="bibr" target="#b42">[43]</ref> to train ResNet-50 and initialise it by a trained model on C contains Self LC <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55]</ref> and Non-self LC <re et="#b39">[40,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" tar
ing deep neural networks. It includes label smoothing (LS) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29]</ref> and confidence penalty (CP) <ref type="bibr" target="#b32"> ovide a student model the similarity structure information <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>; (2) In Self LC, e.g., Boot-soft, a model helps itself by e inimisation against the recent confidence penalty practice <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar
ss are constrained to have consistent output distributions <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b55">56]</ref>. In another self KD <ref type="bibr" target="#b56">[57]</re to improve the end-to-end self LC. First, self KD methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref> maximise the consis
</ref> on ImageNet 2012 classification dataset, which has 1k classes and 50k images in the test set <ref type="bibr" target="#b38">[39]</ref>. We use SGD with a start learning rate of 2e â 3. A polyno ]</ref> on ImageNet 2012 classification dataset, which has 1k classes and 50k images in the test set<ref type="bibr" target="#b38">[39]</ref>. We use SGD with a start learning rate of 2e â 3. A polyno
are some effective approaches to define the similarity structure of data points without annotation: <ref type="bibr" target="#b0">(1)</ref> In KD, an auxiliary teacher model can provide a student mode dnn and different frameworks like Caffe <ref type="bibr" target="#b17">[18]</ref>,</p><p>Tensorflow <ref type="bibr" target="#b0">[1]</ref> and PyTorch <ref type="bibr" target="#b30">[31]</ref>) may l
ef type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b55">56]</ref>. In another self KD <ref type="bibr" target="#b56">[57]</ref>, the deepest classifier provides knowledge for shallower c t, self KD methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref> maximise the consistency of intraclass images' predictions
) Co-training strategies, which train two or more learners <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar s from the methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" tar
get="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b59">60]</ref> which use a clean d

ms of handling the large amount of vertices N .</p><p>Random-walk-based approaches such as node2vec <ref type="bibr" target="#b11">[12]</ref> with time complexity O(a 2 N ), where a is the average deg
xtension of GAT on heterogeneous information networks is Heterogeneous Graph Attention Network, HAN <ref type="bibr" target="#b39">[39]</ref>. Beside inheriting the node-level attention from GAT, it c
ibr" target="#b2">[3]</ref> are not very suitable on our datasets.</p><p>The relational-GCN (r-GCN) <ref type="bibr" target="#b32">[32]</ref> extends GCN onto heterogeneous information networks. A ver taset has |R| = R = 5, so it should be fine not to conduct a weight-matrix decomposition like r-GCN <ref type="bibr" target="#b32">[32]</ref>. We 1 MLP here refers to Multi-layer Perceptron. model the
r data (e.g. text, images) are fed into different tasks and trained jointly. Aron and Nirmal et al. <ref type="bibr" target="#b9">[10]</ref> also apply MTL on Twitter, separating the tasks by user cat
ing used: intentionally collected via strategies like survey <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">20]</ref>, and directly collected such as from news articles <ref typ
eterogeneous information network such as HetGNN <ref type="bibr" target="#b43">[43]</ref> and GATNE <ref type="bibr" target="#b2">[3]</ref> are not very suitable on our datasets.</p><p>The relational- ls. Some methods we mentioned in section 2, HetGNN <ref type="bibr" target="#b43">[43]</ref>, GATNE <ref type="bibr" target="#b2">[3]</ref> and GTN <ref type="bibr" target="#b42">[42]</ref> generally
ections, based on the targets to predict: of the politicians <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28]</ref>, and of the ordinar
aches of ideology detection on social networks focus on text <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" targ extract information from text data to do ideology-detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" targ
entered social network. Definition 3.1. (Heterogeneous Information Network) Following previous work <ref type="bibr" target="#b34">[34]</ref>, we say that an information network G = {V, E}, where numb
ms of handling the large amount of vertices N .</p><p>Random-walk-based approaches such as node2vec <ref type="bibr" target="#b11">[12]</ref> with time complexity O(a 2 N ), where a is the average deg
of sharing parameters across layers has been previously explored with the Transformer architecture <ref type="bibr" target="#b53">(Vaswani et al., 2017)</ref>, but this prior work has focused on trai <p>The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder <ref type="bibr" target="#b53">(Vaswani et al., 2017)</ref> with GELU nonlinearities <ref type="bibr
w the BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> setup in using the BOOKCORPUS <ref type="bibr" target="#b61">(Zhu et al., 2015)</ref> and English Wikipedia <ref type="bibr" targe

sks. There is empirical <ref type="bibr" target="#b51">(Szegedy et al., 2017)</ref> and theoretical <ref type="bibr" target="#b17">(Li et al., 2019)</ref> evidence showing that a combination of batch r network parameters). <ref type="foot" target="#foot_4">5</ref> Similar technique has been used in <ref type="bibr" target="#b17">Gong et al. (2019)</ref>. If we compare a 3-layer ALBERT model with a
sks. There is empirical <ref type="bibr" target="#b51">(Szegedy et al., 2017)</ref> and theoretical <ref type="bibr" target="#b17">(Li et al., 2019)</ref> evidence showing that a combination of batch r network parameters). <ref type="foot" target="#foot_4">5</ref> Similar technique has been used in <ref type="bibr" target="#b17">Gong et al. (2019)</ref>. If we compare a 3-layer ALBERT model with a




mance on language modeling and subject-verb agreement than the standard transformer. Very recently, <ref type="bibr" target="#b1">Bai et al. (2019)</ref> propose a Deep Equilibrium Model (DQE) for tra

acoustics and first-pass text hypotheses for second-pass decoding based on the deliberation network <ref type="bibr" target="#b15">[16]</ref>. The deliberation model has been used in state-of-the-art lation <ref type="bibr" target="#b17">[18]</ref>. Our deliberation model has a similar structure as <ref type="bibr" target="#b15">[16]</ref>: An RNN-T model generates the first-pass hypotheses, and d head><p>A deliberation model is typically trained from scratch by jointly optimizing all components <ref type="bibr" target="#b15">[16]</ref>. However, we find training a two-pass model from scratch t get="#b0">[1]</ref>, and a deliberation decoder, similar to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>. The shared encoder takes log-mel filterbank energies, x =
" target="#b16">[17]</ref>, or generating intermediate representation in speech-to-text translation <ref type="bibr" target="#b17">[18]</ref>. Our deliberation model has a similar structure as <ref ty
imensional vector, and then downsampled to a 30-ms frame rate. Our models are trained in Tensorflow <ref type="bibr" target="#b27">[28]</ref> using the Lingvo framework <ref type="bibr" target="#b28">
rescoring vs. beam search. We apply additional encoder (AE) layers and minimum WER (MWER) training <ref type="bibr" target="#b21">[22]</ref> to further improve quality. The results show that our MWER <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">MWER Loss</head><p>We apply the MWER loss <ref type="bibr" target="#b21">[22]</ref> in training which optimizes the expected word error rate b loss to stabilize training: L MWER (x, y * ) = LMWER(x, y * ) + Î±LCE(x, y * ), where Î± = 0.01 as in <ref type="bibr" target="#b21">[22]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
challenges compared to state-of-the-art conventional models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. To bridge the quality gap between a streaming recurrent neur
e context information for decoding. Note that the first-pass hypotheses are sequences of wordpieces <ref type="bibr" target="#b18">[19]</ref> and are usually short in VS, and thus the encoding should r). The LAS decoder has a 4,096-dimensional softmax layer to predict the same mixed-case wordpieces <ref type="bibr" target="#b18">[19]</ref> as the RNN-T.</p><p>For feature extraction, we use 128-dim
e limited impact on latency.</p><p>Our experiments are conducted using the same training data as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, which is from mul g/ns/1.0"><head n="3.1.">Datasets</head><p>For training, we use the same multidomain datasets as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> which include anon
word (or word) symbols <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target=
compared to rescoring where one leverages external language models trained with large text corpora <ref type="bibr" target="#b13">[14]</ref>. For example, a neural correction model in <ref type="bibr
word (or word) symbols <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target=
word (or word) symbols <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target=
to more sophisticated conventional systems on Google traffic <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Given its all-neural nature, an E2E model can be reasonably ix the RNN-T parameters and only train the deliberation decoder and additional encoder layers as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div> <div xmlns
e limited impact on latency.</p><p>Our experiments are conducted using the same training data as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, which is from mul g/ns/1.0"><head n="3.1.">Datasets</head><p>For training, we use the same multidomain datasets as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> which include anon
imensional vector, and then downsampled to a 30-ms frame rate. Our models are trained in Tensorflow <ref type="bibr" target="#b27">[28]</ref> using the Lingvo framework <ref type="bibr" target="#b28">
ost-process hypotheses using only the text information, and can be considered as second-pass models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta e text corpora <ref type="bibr" target="#b13">[14]</ref>. For example, a neural correction model in <ref type="bibr" target="#b10">[11]</ref> takes first-pass text hypotheses and generates new sequenc
nventional model <ref type="bibr" target="#b7">[8]</ref>, a two-pass framework has been proposed in <ref type="bibr" target="#b9">[10]</ref>, which uses a non-streaming LAS decoder to rescore the RNN- our MWER trained 8-hypothesis deliberation model performs 11% relatively better than LAS rescoring <ref type="bibr" target="#b9">[10]</ref> in VS WER, and up to 15% for proper noun recognition. Joint d s o q Z A / R</formula><p>There are two major differences between our model and the LAS rescoring <ref type="bibr" target="#b9">[10]</ref>. First, the deliberation model attends to both e and yr, wh type="bibr" target="#b9">[10]</ref>. First, the deliberation model attends to both e and yr, while <ref type="bibr" target="#b9">[10]</ref> only attends to the acoustic embedding, e. Second, our deli ends to the acoustic embedding, e. Second, our deliberation model encodes yr bidirectionally, while <ref type="bibr" target="#b9">[10]</ref> only relies on unidirectional encoding e for decoding. <ref ile <ref type="bibr" target="#b9">[10]</ref> only relies on unidirectional encoding e for decoding. <ref type="bibr" target="#b9">[10]</ref> shows that the incompatibility between an RNN-T encoder and 16]</ref>. However, we find training a two-pass model from scratch tends to be unstable in practice <ref type="bibr" target="#b9">[10]</ref>, and thus use a two-step training process: Train the RNN-T model can be further trained with MWER loss. The joint training is similar to "deep finetuning" in <ref type="bibr" target="#b9">[10]</ref> but without a pre-trained decoder.</p></div> <div xmlns="ht nal encoding from yr. In rescoring, we run the deliberation decoder on yr in a teacher-forcing mode <ref type="bibr" target="#b9">[10]</ref>. Note the difference from <ref type="bibr" target="#b9">[10 on yr in a teacher-forcing mode <ref type="bibr" target="#b9">[10]</ref>. Note the difference from <ref type="bibr" target="#b9">[10]</ref> when rescoring a hypothesis is that the deliberation networ f type="bibr" target="#b5">[6]</ref>. The SxS set contains utterances where the LAS rescoring model <ref type="bibr" target="#b9">[10]</ref> performs inferior to a state-of-the-art conventional model score first-pass RNN-T results, and expect bidirectional encoding to help compared to LAS rescoring <ref type="bibr" target="#b9">[10]</ref>. Table <ref type="table" target="#tab_6">5</ref> shows that " target="#tab_5">4</ref>, we compare deliberation models with an RNN-T [6] and LAS rescoring model <ref type="bibr" target="#b9">[10]</ref>  AE layers (E9), and a jointly trained version (E10). For L (E9), and a jointly trained version (E10). For LAS twopass model, we add AE layers to the model in <ref type="bibr" target="#b9">[10]</ref> and evaluate both rescoring (B4) and beam search (B5). We n first-pass hypotheses. However, we note that the computation can be parallelized across hypotheses <ref type="bibr" target="#b9">[10]</ref> and should have less impact on latency. Latency estimation r, an RNN-T decoder <ref type="bibr" target="#b0">[1]</ref>, and a deliberation decoder, similar to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>. The shared encoder the deliberation decoder and additional encoder layers as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n=
periments are conducted using the same training data as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, which is from multiple domains such as Voice Search, YouTu p>For training, we use the same multidomain datasets as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> which include anonymized and hand-transcribed English utter
second-pass models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. The models typically use beam search to generate new hypot correct the outputs of a connectionist temporal classification model in Mandarin ASR. In addition, <ref type="bibr" target="#b12">[13]</ref> leverages text-to-speech (TTS) audio to train an attention
kes first-pass text hypotheses and generates new sequences to improve numeric utterance recognition <ref type="bibr" target="#b14">[15]</ref>. A transformer-based spelling correction model is proposed
ess made by E2E models, they still face challenges compared to state-of-the-art conventional models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. To bridge the quality l network transducer (RNN-T) <ref type="bibr" target="#b5">[6]</ref> and a large conventional model <ref type="bibr" target="#b7">[8]</ref>, a two-pass framework has been proposed in <ref type="bibr" o RNN-T <ref type="bibr" target="#b5">[6]</ref> and has a similar WER to a large conventional model <ref type="bibr" target="#b7">[8]</ref>.</p><p>A class of neural correction models post-process hypo odel achieves a WER of 5.0% on VS, which is 21% relatively better than the large conventional model <ref type="bibr" target="#b7">[8]</ref> (6.3% VS WER). Lastly, we analyze the computational complexi <ref type="bibr" target="#b9">[10]</ref> performs inferior to a state-of-the-art conventional model <ref type="bibr" target="#b7">[8]</ref>, and one reason is due to proper nouns. The voice command te
e limited impact on latency.</p><p>Our experiments are conducted using the same training data as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, which is from mul g/ns/1.0"><head n="3.1.">Datasets</head><p>For training, we use the same multidomain datasets as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> which include anon
ollowed by 320-dimensional projection. Each of the two attention models is a multi-headed attention <ref type="bibr" target="#b26">[27]</ref> with four attention heads. The two output context vectors
ntroduction</head><p>Pre-training models <ref type="bibr" target="#b14">(Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2019;</ref><ref type="bibr">Radford et al., 2019b;</ref /head><p>The key of pre-training methods <ref type="bibr" target="#b14">(Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Song et al., rosoft/MPNet the accuracy of NLP tasks in the past years. One of the most successful models is BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, which mainly adopts masked language model ge understanding and generation. For language understanding, masked language modeling (MLM) in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and permuted language modeling (PLM) in XL his section, we briefly review MLM and PLM, and discuss their pros and cons.</p><p>MLM in BERT BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> is one of the most successful pre-training e predicted part. For the non-predicted part (x z&lt;=c , M z&gt;c ), we use bidirectional modeling <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>  Modeling Output Dependency with Two-Strea objectives mask and predict the same amount of tokens (15%), following the common practice in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and XLNet <ref type="bibr" target="#b30">( n="3.1">Experimental Setup</head><p>We conduct experiments under the BERT base setting (BERT BASE ) <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, where the model consists of 12 transforme the predicted tokens, and prepare mask tokens following the same 8:1:1 replacement strategy in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>. Additionally, we also apply whole word ma 2018)</ref>, with 160GB data size in total. We use a subword dictionary with 30K BPE codes in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> to tokenize the sentences. We limit the le in BERT BASE setting and from MNLI QNLI QQP RTE SST MRPC CoLA STS Avg Single model on dev set BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 83 )</ref> 87 </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD v1.1 EM F1</head><p>BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> 80.8 88.5 RoBERTa <ref type="bibr">(Liu et ut any data augmentation for fair comparisons. On the dev set of GLUE tasks, MPNet outperforms BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, XLNet <ref type="bibr" target="#b30">(Yan
<ref type="bibr">Radford et al., 2019b;</ref><ref type="bibr" target="#b23">Song et al., 2019;</ref><ref type="bibr" target="#b30">Yang et al., 2019;</ref><ref type="bibr" target="#b6">Dong et al., 20 bibr" target="#b4">Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Song et al., 2019;</ref><ref type="bibr" target="#b30">Yang et al., 2019;</ref><ref type="bibr" target="#b1">Clark et al., 2 masked tokens efficiently, but ignores the dependency among the masked (and to be predicted) tokens <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>.</p><p>To improve BERT, XLNet <ref type="bi type="bibr" target="#b4">(Devlin et al., 2019)</ref> and permuted language modeling (PLM) in XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> are two representative objectives. In this P (x k |x \K ; ?). (1)</formula><p>PLM in XLNet Permuted language model (PLM) is proposed in XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> to retain the benefits of autoregressive mo edict and the remaining tokens are used as condition in order to reduce the optimization difficulty <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>.</p><p>Pros and Cons of MLM and PLM We comp separately, which is not sufficient to model the complicated context dependency in natural language <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>. In contrast, PLM factorizes the predicted for normal autoregressive prediction. To this end, we follow PLM to adopt two-stream self-attention <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> to autoregressively predict the tokens, whi example, when predicting token x z 5 = x 6 , the query stream in the original two-stream attention <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> takes mask token M z 5 = [M ] and position ing the common practice in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> <ref type="foot" target="#foot_2">5</ref> . rs in total. For the pretraining objective of MPNet, we randomly permute the sentence following PLM <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>  <ref type="foot" target="#foot_4">7</ref> -6. We pre-train our model for 500K steps to be comparable with state-of-the-art models like XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>, RoBERTa <ref type="bibr">(Liu et al., 2019 ="bibr" target="#b4">(Devlin et al., 2019)</ref> 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 83.1 XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> 86.8 91.7 91.4 74.0 94.7 88.2 60.2 89.5 84. GLUE tasks, MPNet outperforms BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> and RoBERTa <ref type="bibr">(Liu et al., 2 BERT are from the RACE leaderboard 10 and the results of XL-Net are obtained from the original paper<ref type="bibr" target="#b30">(Yang et al., 2019)</ref>. "Middle" and "High" denote the accuracy on result of XLNet is ran by ourselves with only PLM pre-training objective but no long context memory<ref type="bibr" target="#b30">(Yang et al., 2019)</ref>. "*" represents pre-training only on Wikipe cted) tokens <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>.</p><p>To improve BERT, XLNet <ref type="bibr" target="#b30">(Yang et al., 2019</ref>) introduces permuted language modeling (PLM) </p><p>We pre-train MPNet on a large-scale text corpora (over 160GB data) following the practice in <ref type="bibr" target="#b30">Yang et al. (2019)</ref>; <ref type="bibr">Liu et al. (2019a)</ref>, figure" target="#fig_2">2a</ref>. For more details about two-stream self-attention, please refer to <ref type="bibr" target="#b30">Yang et al. (2019)</ref>. One drawback of two-stream self-attention i two-stream self-attention and use the original hiddens to extract context representations following <ref type="bibr" target="#b30">Yang et al. (2019)</ref>. The fine-tuning experiments on each downstr ref> in our model pretraining since these tricks have been successfully validated in previous works <ref type="bibr" target="#b30">(Yang et al., 2019;</ref><ref type="bibr">Raffel et al., 2019b)</ref> foot" n="2" xml:id="foot_0"><p>We do not consider next sentence prediction here since previous works<ref type="bibr" target="#b30">(Yang et al., 2019;</ref> Liu et al., 2019a;<ref type="bibr" target="
collection of 9 natural language understanding tasks, which include two single-sentence tasks (CoLA <ref type="bibr" target="#b28">(Warstadt et al., 2018)</ref>, SST-2 <ref type="bibr" target="#b22">(
lliams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b3">(Dagan et al., 2006)</ref>, WNLI <ref type="bibr" target="#b10">(Leves
lliams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b3">(Dagan et al., 2006)</ref>, WNLI <ref type="bibr" target="#b10">(Leves
two single-sentence tasks (CoLA <ref type="bibr" target="#b28">(Warstadt et al., 2018)</ref>, SST-2 <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>), three similarity and paraphrase tasks (
two single-sentence tasks (CoLA <ref type="bibr" target="#b28">(Warstadt et al., 2018)</ref>, SST-2 <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>), three similarity and paraphrase tasks (
g/ns/1.0"><head n="3.4">Results on RACE</head><p>The ReAding Comprehension from Examinations (RACE) <ref type="bibr" target="#b9">(Lai et al., 2017)</ref>  four choices. The task is to select the corr
ef>, STS-B <ref type="bibr" target="#b0">(Cer et al., 2017)</ref>, QQP), four inference tasks (MNLI <ref type="bibr" target="#b29">(Williams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b20">(R
ur model on SQuAD v1.1 <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref> and SQuAD v2.0 <ref type="bibr" target="#b19">(Rajpurkar et al., 2018)</ref>. SQuAD v1.1 always exists the correspo
two single-sentence tasks (CoLA <ref type="bibr" target="#b28">(Warstadt et al., 2018)</ref>, SST-2 <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>), three similarity and paraphrase tasks (
two single-sentence tasks (CoLA <ref type="bibr" target="#b28">(Warstadt et al., 2018)</ref>, SST-2 <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>), three similarity and paraphrase tasks (
>(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b3">(Dagan et al., 2006)</ref>, WNLI <ref type="bibr" target="#b10">(Levesque et al., 2012)</ref>). We follow RoBERTa hyper-parameters fo
"bibr" target="#b23">Song et al., 2019;</ref><ref type="bibr" target="#b30">Yang et al., 2019;</ref><ref type="bibr" target="#b6">Dong et al., 2019;</ref><ref type="bibr">Liu et al., 2019a;</ref><ref
"bibr" target="#b23">Song et al., 2019;</ref><ref type="bibr" target="#b30">Yang et al., 2019;</ref><ref type="bibr" target="#b6">Dong et al., 2019;</ref><ref type="bibr">Liu et al., 2019a;</ref><ref
g/ns/1.0"><head n="3.4">Results on RACE</head><p>The ReAding Comprehension from Examinations (RACE) <ref type="bibr" target="#b9">(Lai et al., 2017)</ref>  four choices. The task is to select the corr
ed in RoBERTa <ref type="bibr">(Liu et al., 2019a)</ref>, which includes: Wikipedia and BooksCorpus <ref type="bibr" target="#b31">(Zhu et al., 2015)</ref>, OpenWebText <ref type="bibr">(Radford et al
>(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b3">(Dagan et al., 2006)</ref>, WNLI <ref type="bibr" target="#b10">(Levesque et al., 2012)</ref>). We follow RoBERTa hyper-parameters fo
two single-sentence tasks (CoLA <ref type="bibr" target="#b28">(Warstadt et al., 2018)</ref>, SST-2 <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>), three similarity and paraphrase tasks (
lliams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b3">(Dagan et al., 2006)</ref>, WNLI <ref type="bibr" target="#b10">(Leves
"bibr" target="#b23">Song et al., 2019;</ref><ref type="bibr" target="#b30">Yang et al., 2019;</ref><ref type="bibr" target="#b6">Dong et al., 2019;</ref><ref type="bibr">Liu et al., 2019a;</ref><ref
ur model on SQuAD v1.1 <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref> and SQuAD v2.0 <ref type="bibr" target="#b19">(Rajpurkar et al., 2018)</ref>. SQuAD v1.1 always exists the correspo
="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. To generate diversified results, these methods either exp ef type="bibr" target="#b7">[8]</ref>, HxQuAD/HPM2 <ref type="bibr" target="#b8">[9]</ref> and DSSA <ref type="bibr" target="#b13">[14]</ref>.</p><p>Those existing approaches used greedy document sequ strategy may not lead to global optimal rankings. Based on the reinforced learning approach MDP-DIV <ref type="bibr" target="#b13">[14]</ref>, Feng <ref type="bibr" target="#b14">[15]</ref> proposed t rmula><p>Here ð ð is a learnable parameter. We use the same relevance features as the previous work <ref type="bibr" target="#b13">[14]</ref> for ð ð and ð ð ð , including BM25, TF-IDF, language model bers of incoming links and outgoing links, et al. More details about these features can be found in <ref type="bibr" target="#b13">[14]</ref> and we omit the details due to space limitation. In the fu used are actually the document embeddings. We use the subtopic embeddings released by Jiang et al. <ref type="bibr" target="#b13">[14]</ref> based on doc2vec. The subtopic embeddings is produced from t diversification task is limited, we inherit the list-pairwise sampling approach from Jiang et al. <ref type="bibr" target="#b13">[14]</ref> in order to get enough training samples. We are using pair ance features and embeddings exactly the same as the DSSA, which have been released by Jiang et al. <ref type="bibr" target="#b13">[14]</ref> in the repository on GitHub<ref type="foot" target="#foot_ ="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, all those metrics are computed on top 20 results of a doc "#b11">[12]</ref> and PAMM-NTN <ref type="bibr" target="#b12">[13]</ref>. Inspired by previous work <ref type="bibr" target="#b13">[14]</ref>, we use the metric of ð¼ â ðDCG@20 to tune the parameters. 100-dimensional vectors generated by the LDA <ref type="bibr" target="#b31">[32]</ref>.</p><p>DSSA <ref type="bibr" target="#b13">[14]</ref>. We train the DSSA model with the code and data released b lt is denoted as DSSA (doc2vec).</p><p>Since the deep reinforced learning based models e.g. MDP-DIV <ref type="bibr" target="#b13">[14]</ref> and M2DIV <ref type="bibr" target="#b14">[15]</ref> are ta target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b13">14]</ref> (i.e., explicit approaches), or directly reduce result redu
sequence. Recently, the models fully based on self-attention mechanism (denoted as self-attention  <ref type="bibr" target="#b15">[16]</ref> in the Neural Machine Translation (NMT) task, have achieve
eatures and functions <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" targe ods either explicitly model subtopic coverage of the results <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" targe as possible. Nowadays both unsupervised and supervised explicit approaches are proposed e.g. xQuAD <ref type="bibr" target="#b6">[7]</ref>, PM2 <ref type="bibr" target="#b7">[8]</ref>, HxQuAD/HPM2 <r in our experiments are using the search results of Lemur as initial ranking sequences.</p><p>xQuAD <ref type="bibr" target="#b6">[7]</ref>, PM2 <ref type="bibr" target="#b7">[8]</ref>, HxQuAD and HPM
mum additional information utility for the current selected document sequence. However, researchers <ref type="bibr" target="#b14">[15]</ref> have already proved that this greedy document selection me al ranking, the model has to search all the ranking space, which is an NP-hard problem. Feng et al. <ref type="bibr" target="#b14">[15]</ref> proposed the M2DIV model with Monte-Caro Tree Search (MCTS ult to train since MCTS is so time consuming that the M2DIV propose another raw policy without MCTS <ref type="bibr" target="#b14">[15]</ref> in adaption to some online ranking tasks.</p><p>In this pa . Based on the reinforced learning approach MDP-DIV <ref type="bibr" target="#b13">[14]</ref>, Feng <ref type="bibr" target="#b14">[15]</ref> proposed the M2DIV model with the Monte-Caro Tree Search ( imal and global optimal rankings. However, M2DIV is difficult to train since MCTS is time consuming <ref type="bibr" target="#b14">[15]</ref>, and M2DIV only models the document novelty, ignoring the p reinforced learning based models e.g. MDP-DIV <ref type="bibr" target="#b13">[14]</ref> and M2DIV <ref type="bibr" target="#b14">[15]</ref> are taking too much time to train, we do not take those mo
sed and they are based on handcrafted features and functions <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" targe enerate diversified results, these methods either explicitly model subtopic coverage of the results <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" targe are used in our experiments. Besides the metrics above, we also include the metrics of Precision-IA <ref type="bibr" target="#b5">[6]</ref> (denoted as Pre-IA) and Subtopic Recall <ref type="bibr" tar
hods in search result diversification in order to learn an optimized ranking function automatically <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta document-document similarity regardless the use of subtopics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta e. Inheriting the spirit of MMR, researchers have also proposed supervised methods, such as SVM-DIV <ref type="bibr" target="#b9">[10]</ref>, R-LTR <ref type="bibr" target="#b10">[11]</ref>), PAMM <re hts of the hierarchical subtopic layers. The parameters are tuned with cross validation and ListMLE <ref type="bibr" target="#b9">[10]</ref> is used to learn a prior relevance function with no diversi
ed by users are short <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, and these queries co
The distributed representations of documents here are 100-dimensional vectors generated by the LDA <ref type="bibr" target="#b31">[32]</ref>.</p><p>DSSA <ref type="bibr" target="#b13">[14]</ref>. We
e initial distributed representation of the document ð ð¡ . In order to avoid overfitting, we follow <ref type="bibr" target="#b21">[22]</ref> and use unsupervised methods doc2vec <ref type="bibr" targ
erfitting, we follow <ref type="bibr" target="#b21">[22]</ref> and use unsupervised methods doc2vec <ref type="bibr" target="#b22">[23]</ref> to generate the initial document representations instead o
veral deeplearning based technologies for feature extraction and document representation e.g. K-NRM <ref type="bibr" target="#b26">[27]</ref> or BERT <ref type="bibr" target="#b17">[18]</ref>.</p></di
order to learn an optimized ranking function automatically <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t the use of subtopics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t e also proposed supervised methods, such as SVM-DIV <ref type="bibr" target="#b9">[10]</ref>, R-LTR <ref type="bibr" target="#b10">[11]</ref>), PAMM <ref type="bibr" target="#b11">[12]</ref>, and PAMM ype="bibr" target="#b30">[31]</ref> (denoted as S-rec). Inheriting the spirit of the previous works <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t t="#b9">[10]</ref> is used to learn a prior relevance function with no diversification.</p><p>R-LTR <ref type="bibr" target="#b10">[11]</ref>, PAMM <ref type="bibr" target="#b11">[12]</ref> and PAMM-N
ch result diversification are unsupervised and they are based on handcrafted features and functions <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" targe the more diversified it will be. The most typical implicit model is the MMR (Max Margin Relevance) <ref type="bibr" target="#b4">[5]</ref> model:</p><formula xml:id="formula_0">Score MMR = ðscore(ð ð reduce result redundancy by comparing document-document similarity regardless the use of subtopics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" targ
sed and they are based on handcrafted features and functions <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" targe enerate diversified results, these methods either explicitly model subtopic coverage of the results <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" targe are used in our experiments. Besides the metrics above, we also include the metrics of Precision-IA <ref type="bibr" target="#b5">[6]</ref> (denoted as Pre-IA) and Subtopic Recall <ref type="bibr" tar
hods in search result diversification in order to learn an optimized ranking function automatically <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta document-document similarity regardless the use of subtopics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" ta e. Inheriting the spirit of MMR, researchers have also proposed supervised methods, such as SVM-DIV <ref type="bibr" target="#b9">[10]</ref>, R-LTR <ref type="bibr" target="#b10">[11]</ref>), PAMM <re hts of the hierarchical subtopic layers. The parameters are tuned with cross validation and ListMLE <ref type="bibr" target="#b9">[10]</ref> is used to learn a prior relevance function with no diversi
order to learn an optimized ranking function automatically <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t the use of subtopics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t e also proposed supervised methods, such as SVM-DIV <ref type="bibr" target="#b9">[10]</ref>, R-LTR <ref type="bibr" target="#b10">[11]</ref>), PAMM <ref type="bibr" target="#b11">[12]</ref>, and PAMM ype="bibr" target="#b30">[31]</ref> (denoted as S-rec). Inheriting the spirit of the previous works <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" t t="#b9">[10]</ref> is used to learn a prior relevance function with no diversification.</p><p>R-LTR <ref type="bibr" target="#b10">[11]</ref>, PAMM <ref type="bibr" target="#b11">[12]</ref> and PAMM-N
veral deeplearning based technologies for feature extraction and document representation e.g. K-NRM <ref type="bibr" target="#b26">[27]</ref> or BERT <ref type="bibr" target="#b17">[18]</ref>.</p></di
target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. While in recent years, more and more researchers tried to u target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b13">14]</ref> (i.e., explicit appr D <ref type="bibr" target="#b6">[7]</ref>, PM2 <ref type="bibr" target="#b7">[8]</ref>, HxQuAD/HPM2 <ref type="bibr" target="#b8">[9]</ref> and DSSA <ref type="bibr" target="#b13">[14]</ref>.</p><p>Th ber of the queries is 10, and the average subtopic number is about 9.48. As those previous works do <ref type="bibr" target="#b8">[9]</ref> we treat all those subtopics with uniform weights.</p><p>For " target="#foot_2">3</ref> as the non-diversified baseline. These results are released by Hu et at. <ref type="bibr" target="#b8">[9]</ref> and can be found on the website <ref type="foot" target="#fo ef type="bibr" target="#b6">[7]</ref>, PM2 <ref type="bibr" target="#b7">[8]</ref>, HxQuAD and HPM2 <ref type="bibr" target="#b8">[9]</ref>. These are the unsupervised explicit baseline approaches for
mum additional information utility for the current selected document sequence. However, researchers <ref type="bibr" target="#b14">[15]</ref> have already proved that this greedy document selection me al ranking, the model has to search all the ranking space, which is an NP-hard problem. Feng et al. <ref type="bibr" target="#b14">[15]</ref> proposed the M2DIV model with Monte-Caro Tree Search (MCTS ult to train since MCTS is so time consuming that the M2DIV propose another raw policy without MCTS <ref type="bibr" target="#b14">[15]</ref> in adaption to some online ranking tasks.</p><p>In this pa . Based on the reinforced learning approach MDP-DIV <ref type="bibr" target="#b13">[14]</ref>, Feng <ref type="bibr" target="#b14">[15]</ref> proposed the M2DIV model with the Monte-Caro Tree Search ( imal and global optimal rankings. However, M2DIV is difficult to train since MCTS is time consuming <ref type="bibr" target="#b14">[15]</ref>, and M2DIV only models the document novelty, ignoring the p reinforced learning based models e.g. MDP-DIV <ref type="bibr" target="#b13">[14]</ref> and M2DIV <ref type="bibr" target="#b14">[15]</ref> are taking too much time to train, we do not take those mo
The distributed representations of documents here are 100-dimensional vectors generated by the LDA <ref type="bibr" target="#b31">[32]</ref>.</p><p>DSSA <ref type="bibr" target="#b13">[14]</ref>. We
p>Research shows that most queries issued by users are short <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" targe
target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, and these queries could be ambiguous or vague. For example,
due to maintaining cache coherency. A notable proposed scalable system architecture is described in <ref type="bibr" target="#b33">[34]</ref> in which, NAND flash is used as main memory technology and
mplexity. However, such systems continue to struggle to scale up beyond a limited number of sockets <ref type="bibr" target="#b49">[50]</ref> due to maintaining coherence and associated inter-processo
the fundamental aspect of cache coherence is guaranteed through the serialization of memory writes <ref type="bibr" target="#b4">[5]</ref>, but as the processor count increases, so does the scalabili
anguages try to combine the advantages of a distributed memory system programming model such as MPI <ref type="bibr" target="#b71">[72]</ref>, that can benefit from hardware accelerated interconnect,
#b39">[40]</ref> supported DSM over UDP/IP, where Grappa supported both TCP and RDMA configurations <ref type="bibr" target="#b55">[56]</ref> and provided a user-level library and runtime to the user,
sfer QoS) <ref type="bibr" target="#b30">[31]</ref>  <ref type="bibr" target="#b44">[45]</ref> [59] <ref type="bibr" target="#b43">[44]</ref>  <ref type="bibr" target="#b47">[48]</ref>. The majority o
nce, however, as this is a significant research area, newer technologies such as Domain Wall Memory <ref type="bibr" target="#b69">[70]</ref>, which is the less mature of the non volatile Memory techn
mplexity. However, such systems continue to struggle to scale up beyond a limited number of sockets <ref type="bibr" target="#b49">[50]</ref> due to maintaining coherence and associated inter-processo
mplexity. However, such systems continue to struggle to scale up beyond a limited number of sockets <ref type="bibr" target="#b49">[50]</ref> due to maintaining coherence and associated inter-processo
nd HBM can reduce end-to-end application execution time by 2x-3x over DDRx and LPDDR4 architectures <ref type="bibr" target="#b46">[47]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
fill the gap between the slower main memory and the large data capacity demands of the applications <ref type="bibr" target="#b36">[37]</ref>, but due to the relative limited size of caches, memory ac
ar how to properly train deep GCN architectures, where several works have studied their limitations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" ta ref type="bibr" target="#b52">53]</ref> is an open problem in the graph learning space. Recent work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" ta causes oversmoothing, eventually leading to features of graph vertices converging to the same value <ref type="bibr" target="#b18">[19]</ref>. Due to these limitations, most state-of-the-art GCNs are is limited to a small number of layers <ref type="bibr" target="#b5">(6)</ref>. Recently, Li et al. <ref type="bibr" target="#b18">[19]</ref> studied the depth limitations of GCNs and showed that deep
mance degrades when using more than 3 layers <ref type="bibr" target="#b17">[18]</ref>. Pham et al. <ref type="bibr" target="#b25">[26]</ref> proposed Column Network (CLN) for collective classificatio
n algorithm originating from the wavelet processing domain <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>. To alleviate spatial information loss caused by pooling op
arget="#b28">29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">49]</ref>. The recent EdgeConv method by Wang et al. <ref type="bibr"
ons in social networks <ref type="bibr" target="#b35">[36]</ref>, model proteins for drug discovery <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b39">40]</ref>, enhance predictio ccurate connections between individuals. Graphs are also used to model chemical molecule structures <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b39">40]</ref>. Understanding the
ons in social networks <ref type="bibr" target="#b35">[36]</ref>, model proteins for drug discovery <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b39">40]</ref>, enhance predictio ccurate connections between individuals. Graphs are also used to model chemical molecule structures <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b39">40]</ref>. Understanding the
bibr" target="#b0">(1)</ref> We adapt residual/dense connections, and dilated convolutions to GCNs. <ref type="bibr" target="#b1">(2)</ref> We present extensive experiments on point cloud data, showin uct recommendations. Graphs are also popular modes of representation in natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>, where they are used
target="#b34">[35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref>, or by voxelization <ref type="bibr" target="#b4">[5,</ref>
attempts in creating structure from 3D data exist by either representing it with multiple 2D views <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" targ
/head><p>Dilated wavelet convolution is an algorithm originating from the wavelet processing domain <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>. To alleviate spat
o detect and segment objects in images, and also to predict semantic relations between object pairs <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" ta
ction on transformed elements in the point set to approximate a general function defined on the set <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_12">AGGREGATE({x 1 , â¢ â¢ â¢ ,
CNN, respectively, to approximate the classic weighted minimum mean square error (WMMSE) algorithm <ref type="bibr" target="#b11">[12]</ref> and accelerate the computation. Unsupervised learning and pe="bibr" target="#b6">[7]</ref>. Although several optimization-based methods have been proposed in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, they are compu onsidered. We mainly compare the proposed IGCNet with the following five benchmarks:</p><p>1) WMMSE <ref type="bibr" target="#b11">[12]</ref>: This is the most popular optimizationbased algorithm for
or non-Euclidean data. There are many sucessful applications of GNNs such as recommendation systems <ref type="bibr" target="#b17">[18]</ref> and solving combinatorial problems <ref type="bibr" target
bibr" target="#b4">[5]</ref>. The first attempts came from <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, which applied MLP and CNN, respectively, to approximate the cally, MLP <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> and CNN <ref type="bibr" target="#b1">[2]</ref> have been used to approximate the input-output mapping of th e="figure" target="#fig_3">3</ref>.</p><p>The loss function adopted is the negative sum rate, as in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>,</p><formula xml:id f>: It employs MLP and an unsupervised loss function to learn near-optimal power allocation. 4) DPC <ref type="bibr" target="#b1">[2]</ref>: CNN and the unsupervised loss function are used in this met , network realizations, to train MLP, PCNet, and DPC as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> while the number of training samples used for IGCNet is 2000 ted in the square region [0, 100] Ã [0, 100] meters. The receivers are uniformly distributed within <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> meters away from the ibr" target="#b3">[4]</ref> to set up the simulation. The link distance is uniformly distributed in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> meters during trainin he test, the link distance is uniformly distributed in [l r , u r ] meters, where l r is uniform in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> meters and u r is un
ed to apply deep learning based methods to solve NP-hard optimization problems in wireless networks <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" t ation problem, power control in the K-user interference channel has attracted most of the attention <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. The first attempts bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. The first attempts came from <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, which applied MLP </ref>, they are computationally demanding, and thus cannot be applied for real-time implementation <ref type="bibr" target="#b0">[1]</ref>. To alleviate the computation burden while achieving near-op ving near-optimal performance, machine learning based methods have been proposed. Specifically, MLP <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> and CNN <ref type=" bsection, we identify the performance deterioration phenomenon of existing methods using MLP or CNN <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>.</p><p>Fig. <ref ty /ref> illustrates MLP and CNN based approaches for power control. From the numerical experiments in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, we observe a perfo pe="bibr" target="#b2">[3]</ref>, we observe a performance loss when K gets larger. For example, in <ref type="bibr" target="#b0">[1]</ref>, the performance gap to the WMMSE algorithm is 3% when K = 1 is subsection, in order to demonstrate the effectiveness of IGCNet, we follow the system setting of <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> to set up simulatio ased algorithm for the K-user interference channel power control. It is also used as a benchmark in <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. 2) MLP <ref type=" nchmark in <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. 2) MLP <ref type="bibr" target="#b0">[1]</ref>: It leverages MLP to learn the input-output mapping of WMMSE >We generate 20000 training samples, i.e., network realizations, to train MLP, PCNet, and DPC as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> while the number of /div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Time Comparison</head><p>It was reported in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> that learning-based
several optimization-based methods have been proposed in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, they are computationally demanding, and thus cannot be ap
its effectiveness in solving such performance deterioration problems in image analysis applications <ref type="bibr" target="#b15">[16]</ref>, but it is not effective for wireless power control. Speci images, the geometric property means that adjacent pixels are meaningful to be considered together <ref type="bibr" target="#b15">[16]</ref>. In CNN, a 2D convolution kernel is applied to each patch
or non-Euclidean data. There are many sucessful applications of GNNs such as recommendation systems <ref type="bibr" target="#b17">[18]</ref> and solving combinatorial problems <ref type="bibr" target
several optimization-based methods have been proposed in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, they are computationally demanding, and thus cannot be ap
bibr" target="#b4">[5]</ref>. The first attempts came from <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, which applied MLP and CNN, respectively, to approximate the cally, MLP <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> and CNN <ref type="bibr" target="#b1">[2]</ref> have been used to approximate the input-output mapping of th e="figure" target="#fig_3">3</ref>.</p><p>The loss function adopted is the negative sum rate, as in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>,</p><formula xml:id f>: It employs MLP and an unsupervised loss function to learn near-optimal power allocation. 4) DPC <ref type="bibr" target="#b1">[2]</ref>: CNN and the unsupervised loss function are used in this met , network realizations, to train MLP, PCNet, and DPC as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> while the number of training samples used for IGCNet is 2000 ted in the square region [0, 100] Ã [0, 100] meters. The receivers are uniformly distributed within <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> meters away from the ibr" target="#b3">[4]</ref> to set up the simulation. The link distance is uniformly distributed in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> meters during trainin he test, the link distance is uniformly distributed in [l r , u r ] meters, where l r is uniform in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> meters and u r is un
bibr" target="#b4">[5]</ref>. The first attempts came from <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, which applied MLP and CNN, respectively, to approximate the cally, MLP <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> and CNN <ref type="bibr" target="#b1">[2]</ref> have been used to approximate the input-output mapping of th e="figure" target="#fig_3">3</ref>.</p><p>The loss function adopted is the negative sum rate, as in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>,</p><formula xml:id f>: It employs MLP and an unsupervised loss function to learn near-optimal power allocation. 4) DPC <ref type="bibr" target="#b1">[2]</ref>: CNN and the unsupervised loss function are used in this met , network realizations, to train MLP, PCNet, and DPC as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> while the number of training samples used for IGCNet is 2000 ted in the square region [0, 100] Ã [0, 100] meters. The receivers are uniformly distributed within <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> meters away from the ibr" target="#b3">[4]</ref> to set up the simulation. The link distance is uniformly distributed in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> meters during trainin he test, the link distance is uniformly distributed in [l r , u r ] meters, where l r is uniform in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> meters and u r is un
posed a local classification-based method based on structure similarity and side effect similarity. <ref type="bibr" target="#b4">Ferdousi et al. (2017)</ref> predicted DDIs based on drug functional s
l. (2018)</ref> developed a novel method DDINMF based on the semi-nonnegative matrix factorization. <ref type="bibr" target="#b38">Zhang et al. (2018)</ref> proposed the manifold regularized matrix fa
r from multiple diseases. The recent study <ref type="bibr" target="#b9">(Kantor et al., 2015;</ref><ref type="bibr" target="#b22">Qato et al., 2016)</ref> showed that 67% of elderly Americans took fi
e="bibr" target="#b12">Kuhn et al., 2010;</ref><ref type="bibr" target="#b15">Li et al., 2010;</ref><ref type="bibr" target="#b11">Knox et al., 2011;</ref><ref type="bibr" target="#b13">Law et al., 20 ials</head></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>DrugBank <ref type="bibr" target="#b11">(Knox et al., 2011)</ref> is a resource that provides comprehensive i
eriness, nervousness, anxiety, restlessness, and racing thoughts have been reported. The literature <ref type="bibr" target="#b21">(Prior et al., 2002)</ref> also supports this finding. More evidence
target="#b7">(Ioffe and Szegedy, 2015)</ref> to accelerate the convergence, and add dropout layers <ref type="bibr" target="#b28">(Srivastava et al., 2014)</ref> to avoid over-fitting and enhance gen
e="bibr" target="#b12">Kuhn et al., 2010;</ref><ref type="bibr" target="#b15">Li et al., 2010;</ref><ref type="bibr" target="#b11">Knox et al., 2011;</ref><ref type="bibr" target="#b13">Law et al., 20 ials</head></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>DrugBank <ref type="bibr" target="#b11">(Knox et al., 2011)</ref> is a resource that provides comprehensive i
similarity-based network to predict DDIs and integrated multiple similarities for better solutions. <ref type="bibr" target="#b18">Park et al. (2015)</ref> applied the random walk with restart on the
br" target="#b8">Kanehisa et al., 2010;</ref><ref type="bibr" target="#b12">Kuhn et al., 2010;</ref><ref type="bibr" target="#b15">Li et al., 2010;</ref><ref type="bibr" target="#b11">Knox et al., 201
br" target="#b31">Wang et al., 2009;</ref><ref type="bibr" target="#b8">Kanehisa et al., 2010;</ref><ref type="bibr" target="#b12">Kuhn et al., 2010;</ref><ref type="bibr" target="#b15">Li et al., 201
e="bibr" target="#b12">Kuhn et al., 2010;</ref><ref type="bibr" target="#b15">Li et al., 2010;</ref><ref type="bibr" target="#b11">Knox et al., 2011;</ref><ref type="bibr" target="#b13">Law et al., 20 ials</head></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>DrugBank <ref type="bibr" target="#b11">(Knox et al., 2011)</ref> is a resource that provides comprehensive i
get="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref> allow end-to-end differentable losses over data with arbitr for unsupervised losses, most work follows the semi-supervised setting for node classification from <ref type="bibr" target="#b28">[29]</ref>. For a complete introductions to the vast topic we refer i ider transductive GNNs that output a single embedding per node. Graph convolutional networks (GCNs) <ref type="bibr" target="#b28">[29]</ref> are simple yet effective <ref type="bibr" target="#b50">[5
br" target="#b4">[5]</ref>, however, a spectral relaxation of the problem can be solved efficiently <ref type="bibr" target="#b36">[37]</ref>. Let C â 0, 1 nÃk be the cluster assignment matrix and d b r iteration or Lanczos algorithm.</p><p>One can then obtain clusters by means of spectral bisection <ref type="bibr" target="#b36">[37]</ref> with iterative refinement akin to Kernighan-Lin algorithm
use the local Lloyd algorithm <ref type="bibr" target="#b33">[34]</ref> with the k-means++ seeding <ref type="bibr" target="#b0">[1]</ref> strategy.</p><p>â¢ SBM <ref type="bibr" target="#b41">[42]</r
use the local Lloyd algorithm <ref type="bibr" target="#b33">[34]</ref> with the k-means++ seeding <ref type="bibr" target="#b0">[1]</ref> strategy.</p><p>â¢ SBM <ref type="bibr" target="#b41">[42]</r
or clustering models. Works on supervised graph clustering <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b61">62]</ref> are outside of the scope of this work.</p><p>â¢ Sparse. As g
ialized deep learning architectures for dealing with graph-structured data, such as social networks <ref type="bibr" target="#b46">[47]</ref>, recommender graphs <ref type="bibr" target="#b67">[68]</r rary structure. They have been applied to an incredible range of applications, from social networks <ref type="bibr" target="#b46">[47]</ref>, to recommender systems <ref type="bibr" target="#b67">[68
<ref type="bibr" target="#b12">[13]</ref> DiffPool <ref type="bibr" target="#b68">[69]</ref> Top-k <ref type="bibr" target="#b19">[20]</ref> SAG <ref type="bibr" target="#b30">[31]</ref> MinCut <ref e clustering structure of graphs and an additional entropy loss to penalize soft assignments. Top-k <ref type="bibr" target="#b19">[20]</ref> and SAG pooling <ref type="bibr" target="#b30">[31]</ref> egation is crucial for our interpretation of graph pooling in terms of graph clustering. Both Top-k <ref type="bibr" target="#b19">[20]</ref> and SAG pooling <ref type="bibr" target="#b30">[31]</ref> e DiffPool <ref type="bibr" target="#b68">[69]</ref>, or computing A 2 , like top-k pooling methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref> .</p><p>â¢ Node agg
which normalizes the cut by the product of the number of nodes in two partitions, or normalized cut <ref type="bibr" target="#b51">[52]</ref>, which uses total edge volume of the partition as normaliz
erence d in â d out , where d in := d â d out , controls the spectral detectability of the clusters <ref type="bibr" target="#b34">[35]</ref>. Finally, we generate a power-law n-vector Î¸, where Î¸ i is es allows their estimation-based approach to overcome the spectral "detectability" limit for graphs <ref type="bibr" target="#b34">[35]</ref>, a result that we replicate in this study. Using our model
uctured data, such as social networks <ref type="bibr" target="#b46">[47]</ref>, recommender graphs <ref type="bibr" target="#b67">[68]</ref>, or molecular graphs <ref type="bibr" target="#b12">[13,</ pplications, from social networks <ref type="bibr" target="#b46">[47]</ref>, to recommender systems <ref type="bibr" target="#b67">[68]</ref>, to computational chemistry <ref type="bibr" target="#b20"
e structure. For graph-level metrics, we report average cluster conductance (as per definition from <ref type="bibr" target="#b63">[64]</ref>) and graph modularity <ref type="bibr" target="#b37">[38]<
7">8]</ref>.</p><p>Unsupervised training of GNNs is commonly done via maximizing mutual information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" targ
f the data as computational graph, allowing the information to propagate across the edges of graphs <ref type="bibr" target="#b48">[49]</ref>. When many real-wold systems are represented as graphs, th of research on graph neural networks and graph pooling methods.</p><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" ta
which normalizes the cut by the product of the number of nodes in two partitions, or normalized cut <ref type="bibr" target="#b51">[52]</ref>, which uses total edge volume of the partition as normaliz
rget="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b53">54]</ref> in a self-supervised fashion. Deep Graph Infomax (DGI) <ref
rget="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b53">54]</ref> in a self-supervised fashion. Deep Graph Infomax (DGI) <ref
Clusters can correspond to interesting phenomena in the underlying graph, for example to education <ref type="bibr" target="#b56">[57]</ref> or employment <ref type="bibr" target="#b40">[41]</ref> in
ns(features) is our baseline that only considers the feature data. We use the local Lloyd algorithm <ref type="bibr" target="#b33">[34]</ref> with the k-means++ seeding <ref type="bibr" target="#b0">[
ervised training is a desirable setting for clustering models. Works on supervised graph clustering <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b61">62]</ref> are outside of the
n <ref type="bibr" target="#b44">[45]</ref>, visualization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, genomic feature discovery <ref type="bibr" target="#b6">[7
="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, genomic feature discovery <ref type="bibr" target="#b6">[7]</ref>, anomaly detection <ref type="bibr" target="#b43">[44]</ref>
l optimization, and review some of their shortcomings.</p><p>Cut-based metrics. In his seminal work <ref type="bibr" target="#b17">[18]</ref>, Fiedler suggested that the second (Fiedler) eigenvector o
ing significantly fewer training data comparing to the few previous works which address heterophily <ref type="bibr" target="#b22">(Pei et al. 2020;</ref><ref type="bibr" target="#b38">Zhu et al. 2020 nificantly smaller fraction of training samples compared to previous works that address heterophily <ref type="bibr" target="#b22">(Pei et al. 2020;</ref><ref type="bibr" target="#b38">Zhu et al. 2020 pe="bibr" target="#b20">Namata et al. 2012)</ref>. We use the features and class labels provided by <ref type="bibr" target="#b22">Pei et al. (2020)</ref>.</p></div> <div xmlns="http://www.tei-c.org/n ch benchmark with an identity matrix I. We use the training, validation and test splits provided by <ref type="bibr" target="#b22">Pei et al. (2020)</ref>.</p><p>Heterophily. We report results on grap
(Pandit et al. 2007;</ref><ref type="bibr">Dou et al. 2020)</ref> or analysis of protein structures <ref type="bibr" target="#b6">(Fout et al. 2017)</ref>, where heterophilous connections are common.
ormation from relational data, Graph Neural Network (GNN) models have gained wide research interest <ref type="bibr" target="#b26">(Scarselli et al. 2008)</ref> and been adapted in applications includ
et="#b27">(Sen et al. 2008;</ref><ref type="bibr" target="#b18">McDowell, Gupta, and Aha 2007;</ref><ref type="bibr" target="#b24">Rossi et al. 2012</ref>) can be solved with iterative methods (J. <re
few previous works which address heterophily <ref type="bibr" target="#b22">(Pei et al. 2020;</ref><ref type="bibr" target="#b38">Zhu et al. 2020)</ref>. These experiments demonstrate the effectivene ed to previous works that address heterophily <ref type="bibr" target="#b22">(Pei et al. 2020;</ref><ref type="bibr" target="#b38">Zhu et al. 2020)</ref>. This is a more realistic assumption in many r ethods with the following baselines, some of which are reported to be competitive under heterophily <ref type="bibr" target="#b38">(Zhu et al. 2020)</ref>: GCN (Kipf and Welling 2017), GAT <ref type=" get="#b8">(Hamilton, Ying, and Leskovec 2017)</ref>, MixHop (Abu-El-Haija et al. 2019), and H 2 GCN <ref type="bibr" target="#b38">(Zhu et al. 2020)</ref>. We also consider MLP as a graph-agnostic bas rgely overlooked setting of heterophily, and many of them perform poorly in this setting. Recently, <ref type="bibr" target="#b38">Zhu et al. (2020)</ref> discussed effective designs which improve the
ch simplifies the previous work. Other GNN models that have gained wide attention include Planetoid <ref type="bibr" target="#b35">(Yang, Cohen, and</ref><ref type="bibr" target="#b35">Salakhudinov 20 have gained wide attention include Planetoid <ref type="bibr" target="#b35">(Yang, Cohen, and</ref><ref type="bibr" target="#b35">Salakhudinov 2016) and</ref><ref type="bibr">GraphSAGE (Hamilton, Yin
018)</ref>, bioinformatics <ref type="bibr" target="#b39">(Zitnik, Agrawal, and Leskovec 2018;</ref><ref type="bibr" target="#b34">Yan et al. 2019)</ref>, fraud detection <ref type="bibr">(Dou et al.
and Vandergheynst 2016;</ref><ref type="bibr" target="#b12">Kipf and Welling 2017)</ref>, GraphSAGE <ref type="bibr" target="#b8">(Hamilton, Ying, and Leskovec 2017)</ref>, MixHop (Abu-El-Haija et al. ibr" target="#b35">Salakhudinov 2016) and</ref><ref type="bibr">GraphSAGE (Hamilton, Ying, and</ref><ref type="bibr" target="#b8">Leskovec 2017)</ref>. More recent works have looked into designs which
2</ref>) can be solved with iterative methods (J. <ref type="bibr" target="#b10">Neville 2000;</ref><ref type="bibr" target="#b17">Lu and Getoor 2003)</ref>, graph-based regularization and probabilist

arkar 2019), and 3 widely adopted graphs with strong homophily, which are Cora, Pubmed and Citeseer <ref type="bibr" target="#b27">(Sen et al. 2008;</ref><ref type="bibr" target="#b20">Namata et al. 2 head><p>SSL before GNNs. The problem of semi-supervised learning (SSL) or collective classification <ref type="bibr" target="#b27">(Sen et al. 2008;</ref><ref type="bibr" target="#b18">McDowell, Gupta
presentation learning for generic graphs (GraphSAGE <ref type="bibr" target="#b5">[6]</ref>, AS-GCN <ref type="bibr" target="#b7">[8]</ref>). In general, GNNs recursively update each node's feature by icult. Although sampling methods, such as GraphSAGE <ref type="bibr" target="#b5">[6]</ref>, AS-GCN <ref type="bibr" target="#b7">[8]</ref> and FastGCN <ref type="bibr" target="#b0">[1]</ref>), have b type="bibr" target="#b9">[10]</ref>, GraphSAGE <ref type="bibr" target="#b5">[6]</ref>, and AS-GCN <ref type="bibr" target="#b7">[8]</ref>. We use a large-scale bipartite graph dataset from the Tence igh memory cost. Sampling methods like GraphSAGE <ref type="bibr" target="#b5">[6]</ref> and AS-GCN <ref type="bibr" target="#b7">[8]</ref> have been proposed to deal with this issue by reducing the n ons: GCN and MEAN aggregator. Node-wise sampling is used to address the scalability issue. â¢ AS-GCN <ref type="bibr" target="#b7">[8]</ref>: This method uses adaptive sampling between each layer to de
rom a member from one partition to a member of the other represents the user purchasing the product <ref type="bibr" target="#b12">[13]</ref>. The ability to utilize information from the graphical str
learning methods have been proposed to address this problem <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. However, they either do not apply to bipartite graphs or a target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref> try to utilize GCN to do unsupervised learning on graphs by
ated. â¢ Node2Vec <ref type="bibr" target="#b4">[5]</ref>: This approach is an extension of Word2Vec <ref type="bibr" target="#b14">[15]</ref> on graph, which learns a feature representation by simulat
ype="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>, social networks analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>, and visual unders
edded as vectors. In light of the rapid advancements of deep learning, Graph Neural Networks (GNNs) <ref type="bibr" target="#b18">[19]</ref> have exhibited tremendous progress in representation learn
mong data items in various domains, including drug discovery <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>, social networks analysis <ref type="bibr" target="#b17">[1
edded as vectors. In light of the rapid advancements of deep learning, Graph Neural Networks (GNNs) <ref type="bibr" target="#b18">[19]</ref> have exhibited tremendous progress in representation learn
t bipartite graphs as heterogeneous networks and use random walk-based methods such as Metapath2Vec <ref type="bibr" target="#b1">[2]</ref>. However, Metapath2Vec does not integrate node features into o heterogeneous graphs, where different nodes are in distinct feature domains, such as MethPath2Vec <ref type="bibr" target="#b1">[2]</ref> and PTE <ref type="bibr" target="#b21">[22]</ref>. Although
mong data items in various domains, including drug discovery <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>, social networks analysis <ref type="bibr" target="#b17">[1
ype="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>, social networks analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>, and visual unders
get="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Both persona ty-oriented search is the difficulty of query entity linking. Queries are often short and ambiguous <ref type="bibr" target="#b41">[42]</ref>, making query entity linking a challenging task: A recent quires manual annotations <ref type="bibr" target="#b11">[12]</ref> or soft linking/diversification <ref type="bibr" target="#b41">[42]</ref>. Personalization provides a natural way to help resolve th mory network that represents user's search preferences in the word-entity duet representation space <ref type="bibr" target="#b41">[42]</ref>. KEPS then conducts personalized ranking to adapt document eraction between bags of word representations and bags of entity representations is also studied in <ref type="bibr" target="#b41">[42]</ref>. Neuralbased search model EDRM <ref type="bibr" target="#b
get="#b40">[41]</ref>. Using such noisy query entities in ranking often requires manual annotations <ref type="bibr" target="#b11">[12]</ref> or soft linking/diversification <ref type="bibr" target="# h as term weight in queries according to entity descriptions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. There are also some researches using entities as connectio
d n="2">RELATED WORK</head><p>Personalized Web Search. In addition to search result diversification <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, personalized sear
features to improve personalized ranking effect. Other works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" targe
get="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>. It significantly improved personalization by learning the
tures using learning to rank.</p><p>Recently, deep learning has been applied in personalized search <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta eflect user's session search intent, while the previous history may reflect user's global interests <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Suppose a q RM <ref type="bibr" target="#b20">[21]</ref>.</p><p>Evaluation Metrics. Following the previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>, we use MAP, MRR, effect of PEDRM is similar on both types of queries. However we find that different from stated in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>, the personalizati original ranking based on BM25, which is less efficient than the ranking from search engine used in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. Compared with SLT provement on queries with click entropy no less than 1, which is consistent with the previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. KEPS has signific epresentations of user profiles and other personalized features from user's history. Ge et al. HRNN <ref type="bibr" target="#b15">[16]</ref> proposed to use a hierarchical RNN to model user's profile while that in previous sessions before the current session as long-term history ? ? following HRNN <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_1">? ? = [? ? 1 , ..., ? ? |? rsonalization model using traditional features; and the model based on deep learning: HRNN (HRNN+QA <ref type="bibr" target="#b15">[16]</ref>) using hierarchical RNN and PSGAN (we choose the document- d search is that many search logs are not publicly available <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar
and documents have been encrypted, making it impossible to link them to entities. The Sogou dataset <ref type="bibr" target="#b19">[20]</ref> contains only one month of user search log, limiting the e
d topic features are combined and studied in some researches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" targe s tend to be ambiguous. This meets our experimental expectations.   only short-term history in Tab. <ref type="bibr" target="#b5">6</ref> We can see memory networks also have certain advantages in dea
get="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>. It significantly improved personalization by learning the
get="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" tar queries for better matching. Liu et al. <ref type="bibr" target="#b18">[19]</ref> and Xiong et al. <ref type="bibr" target="#b39">[40]</ref> takes the entity as a latent space and learn query-documen
d ranking accuracy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar tations is also studied in <ref type="bibr" target="#b41">[42]</ref>. Neuralbased search model EDRM <ref type="bibr" target="#b20">[21]</ref> study the interaction between word vectors and entity vect 1,1 , ..., ? 1,? 1 , ..., ? ?,1 , ..., ? ?,? ? ].</formula><p>PEDRM is based on EDRM (EDRM-CKNRM in <ref type="bibr" target="#b20">[21]</ref>), the state-ofart ad-hoc model using interactive entity-ba ype="bibr" target="#b10">[11]</ref>, a neural model using interactive features and EDRM (EDRM-CKNRM <ref type="bibr" target="#b20">[21]</ref>), the stateof-art ad-hoc model using interactive entity-ba to 100 dimensions. The setting of CNN layers and kernel functions in PEDRM are consistent with EDRM <ref type="bibr" target="#b20">[21]</ref>.</p><p>Evaluation Metrics. Following the previous work <re
and documents have been encrypted, making it impossible to link them to entities. The Sogou dataset <ref type="bibr" target="#b19">[20]</ref> contains only one month of user search log, limiting the e
get="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Both personalized search and entity-oriented search
by Wasi et al. <ref type="bibr" target="#b0">[1]</ref>, which is based on the public AOL search log <ref type="bibr" target="#b25">[26]</ref>, makes it possible to study personalized search in the pub
by Wasi et al. <ref type="bibr" target="#b0">[1]</ref>, which is based on the public AOL search log <ref type="bibr" target="#b25">[26]</ref>, makes it possible to study personalized search in the pub

W u q 9 R j I q 3 U F N 8 c s = " &gt; A      <ref type="formula" target="#formula_32">4</ref>) and <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_53">A A C x n i c j V H L S s N ?),</formula><p>where ? ?,? = ? ? ? -? ? ? indicates the entity relation vector since we use TransE <ref type="bibr" target="#b4">[5]</ref> to pre-train entity embedding. Adding the matrix ? is becaus nd document titles in the search log as training corpus. We train the entity embedding using TransE <ref type="bibr" target="#b4">[5]</ref>, taking the entities and relations extracted from our data a
kind of relevance ranking features, such as term weight in queries according to entity descriptions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. There are also some
get="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> extracted the topic
features to improve personalized ranking effect. Other works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" targe
d ranking accuracy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar tations is also studied in <ref type="bibr" target="#b41">[42]</ref>. Neuralbased search model EDRM <ref type="bibr" target="#b20">[21]</ref> study the interaction between word vectors and entity vect 1,1 , ..., ? 1,? 1 , ..., ? ?,1 , ..., ? ?,? ? ].</formula><p>PEDRM is based on EDRM (EDRM-CKNRM in <ref type="bibr" target="#b20">[21]</ref>), the state-ofart ad-hoc model using interactive entity-ba ype="bibr" target="#b10">[11]</ref>, a neural model using interactive features and EDRM (EDRM-CKNRM <ref type="bibr" target="#b20">[21]</ref>), the stateof-art ad-hoc model using interactive entity-ba to 100 dimensions. The setting of CNN layers and kernel functions in PEDRM are consistent with EDRM <ref type="bibr" target="#b20">[21]</ref>.</p><p>Evaluation Metrics. Following the previous work <re
d document relevance. Click features and topic features are combined and studied in some researches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target </p><p>Another challenge in personalized search is that many search logs are not publicly available <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" targ ef><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Bennett et al. <ref type="bibr" target="#b3">[4]</ref> proposed SLTB to combine the two types of features using lea aditional features: P-Click <ref type="bibr" target="#b12">[13]</ref> using click features and SLTB <ref type="bibr" target="#b3">[4]</ref> using click features and topic features, which is the state-
tering transforms or certain CNN architectures have been shown to be stable to spatial deformations <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targ ze the stability of GCNs to small deformation of the underlying random graph model. Similar to CNNs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4]</ref>, studying GCNs in th es of stability are often balanced by discussions on how the representation preserves signal (e.g., <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targ ine intuitive notions of deformations and stability in the continuous world like the Euclidean case <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targ p><p>Related work on stability. The study of stability to deformations has been pioneered by Mallat <ref type="bibr" target="#b31">[32]</ref> in the context of the scattering transform for signals on ph models and to obtain deformation stability bounds that are similar to those on Euclidean domains <ref type="bibr" target="#b31">[32]</ref>. We note that <ref type="bibr" target="#b28">[29]</ref> al eformations is an essential feature for the generalization properties of deep architectures. Mallat <ref type="bibr" target="#b31">[32]</ref> studied the stability to small deformations of the wavelet for small enough âÏ â , we obtain N P (Ï ) d âÏ â , recovering the more standard quantity of Mallat <ref type="bibr" target="#b31">[32]</ref>. In this case, we also have the bound</p><formula xml:id=" ). Once again we focus on invariant c-GCNs with pooling, similar to classical scattering transform <ref type="bibr" target="#b31">[32]</ref>.</p><p>Proposition 4 (Signal deformation). Consider a GCN Ï â , the GCN is invariant to translations and stable to deformations, similar to Euclidean domains <ref type="bibr" target="#b31">[32]</ref>. We note that studies of stability are often balanced by d
[10]</ref>, semi-supervised learning <ref type="bibr" target="#b24">[25]</ref>, or graph regression <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19]</ref>, and remain one of 1 + log(1/Ï) log(n) â¼ 1, then use a union bound to conclude.</p><p>We now bound the second term in <ref type="bibr" target="#b21">(22)</ref>. Define Ï k = CÏ (k+1) 2 â d â with C such that kâ d â Ï k
rget="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref> or learning with operators <ref type="bibr" target="#b38">[
ional Networks (GCNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>) are deep architectures defined on graphs inspired by class o, for instance, node clustering <ref type="bibr" target="#b9">[10]</ref>, semi-supervised learning <ref type="bibr" target="#b24">[25]</ref>, or graph regression <ref type="bibr" target="#b21">[22,</ "bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, or GCNs with order-1 filters <ref type="bibr" target="#b24">[25]</ref> which are assimilable to message-passing networks <ref typ ef type="bibr" target="#b25">(26)</ref> and C â² as <ref type="bibr" target="#b27">(28)</ref>. Using <ref type="bibr" target="#b24">(25)</ref> and Prop 5, we obtain that</p><formula xml:id="formula_97"
esults are fully non-asymptotic, valid for relatively sparse random graphs, and unlike many studies <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39]</ref> we do not assume t s based on random walks differs greatly from ours. In general, many of these studies are asymptotic <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b39">40]</ref>, valid only in the ibr" target="#b42">[43,</ref><ref type="bibr" target="#b39">40]</ref>, valid only in the dense case <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta W (â¢, x) is (c Lip. , n X )-piecewise Lipschitz.<label>(6)</label></formula><p>Unlike other studies <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39]</ref>, we do not assume cy matrix or Laplacian in the context of spectral clustering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar
type="bibr" target="#b24">[25]</ref>, or graph regression <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19]</ref>, and remain one of the most popular variant of Graph Neural filters <ref type="bibr" target="#b24">[25]</ref> which are assimilable to message-passing networks <ref type="bibr" target="#b18">[19]</ref>, see <ref type="bibr" target="#b44">[45,</ref><ref type="b
ional Networks (GCNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>) are deep architectures defined on graphs inspired by class o, for instance, node clustering <ref type="bibr" target="#b9">[10]</ref>, semi-supervised learning <ref type="bibr" target="#b24">[25]</ref>, or graph regression <ref type="bibr" target="#b21">[22,</ "bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, or GCNs with order-1 filters <ref type="bibr" target="#b24">[25]</ref> which are assimilable to message-passing networks <ref typ ef type="bibr" target="#b25">(26)</ref> and C â² as <ref type="bibr" target="#b27">(28)</ref>. Using <ref type="bibr" target="#b24">(25)</ref> and Prop 5, we obtain that</p><formula xml:id="formula_97"
GCNs use purely discrete metrics that are less intuitive for capturing natural changes in structure <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta een in the introduction, it is not clear how to extend the notion of deformation on discrete graphs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. We show here that eory in general) has been to define a metric that minimizes over permutations Ï of the nodes (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>), thus we define M entations on discrete graphs, by considering certain well-chosen discrete perturbations and metrics <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" t y be obtained when the (c-)GCN is a structured architecture like the scattering transform on graphs <ref type="bibr" target="#b15">[16]</ref>. Convergence results can also be obtained for many other m
esults are fully non-asymptotic, valid for relatively sparse random graphs, and unlike many studies <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39]</ref> we do not assume t s based on random walks differs greatly from ours. In general, many of these studies are asymptotic <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b39">40]</ref>, valid only in the ibr" target="#b42">[43,</ref><ref type="bibr" target="#b39">40]</ref>, valid only in the dense case <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta W (â¢, x) is (c Lip. , n X )-piecewise Lipschitz.<label>(6)</label></formula><p>Unlike other studies <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39]</ref>, we do not assume cy matrix or Laplacian in the context of spectral clustering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar
rget="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref> or learning with operators <ref type="bibr" target="#b38">[
ref>). In the past few years, they have been successfully applied to, for instance, node clustering <ref type="bibr" target="#b9">[10]</ref>, semi-supervised learning <ref type="bibr" target="#b24">[2 ul in identifying large-scale structures nonetheless, e.g., for segmentation or spectral clustering <ref type="bibr" target="#b9">[10]</ref>. Under this light, a relevant notion is that of stability: he c-GCN with smoothed input function f = L W,P f .</p><p>In some cases such as spectral clustering <ref type="bibr" target="#b9">[10]</ref>, one does not have an input signal over the nodes, but has normalized) degrees of the graph Z = A1 n /n as input signal <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. In this case, using our proofs (Lemma 4 in the appendix) an a standard choice is to take the degree functions as inputs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. The next result shows that this choice also leads to the de deformations of the signal on the graph and show a bound similar to the ones in the Euclidean case <ref type="bibr" target="#b9">(10)</ref>. As can be seen in the proofs, this case is in fact a combi
he past, IBM z13 and many other systems have used FPGA based PCIe attached compression accelerators <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>. However, the FPGA >. However, the FPGA cost and limited number of PCIe slots restrict their usage to high-end servers <ref type="bibr" target="#b1">[2]</ref> and specialized applications such as storage controllers.</p hput <ref type="bibr" target="#b5">[6]</ref>. We would need 68 PCIe based compression cards such as <ref type="bibr" target="#b1">[2]</ref>, with 4GB/s peak throughput to match the NXU performance. Be ficient LZ77 encoding hardware that improves state of the art in Section IV, where the related work <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" t nd z15 to achieve highest possible compression ratio, as described in Section V, where related work <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" t oftware extensively in its products. IBM z13 and POWER systems used PCIe based Deflate accelerators <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" t jectives which is not suitable for an on-chip design. In <ref type="bibr" target="#b3">[4]</ref> In <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, the 32KB sliding nt a static table with an assumed LZ symbol distribution which typically degrades compression ratio <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" t here one of 15 different static Huffman tables yielding the smallest output is selected at run time <ref type="bibr" target="#b1">[2]</ref>. We implemented in contrast, a true "Dynamic Huffman" mode f
largest z15 system topology with 20 processor chips, 20 NXU units provide 280 GB/s total throughput <ref type="bibr" target="#b5">[6]</ref>. We would need 68 PCIe based compression cards such as <ref
largest z15 system topology with 20 processor chips, 20 NXU units provide 280 GB/s total throughput <ref type="bibr" target="#b5">[6]</ref>. We would need 68 PCIe based compression cards such as <ref
th prefix-free codes produced by the Huffman algorithm to compress an LZ77 stream by another 10-20% <ref type="bibr" target="#b19">[20]</ref>. Frequent symbols are encoded with fewer bits. For example #b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bib er of symbols in the table; n = 286 for the literal and length symbols and n = 30 for the distances <ref type="bibr" target="#b19">[20]</ref>. The algorithm is sequential and a basic hardware implemen orithm builds a binary tree with LZ symbols at the leaves arranged according to their probabilities <ref type="bibr" target="#b19">[20]</ref>. The path from the tree root is the binary code of the LZ
compatibility with Spark SQL, efficiency <ref type="bibr" target="#b62">[63]</ref> and performance <ref type="bibr" target="#b63">[64]</ref>. In the TPC-DS benchmark, we store the Parquet files in th
challenges high throughput decoders. Simple hardware designs can handle at most one code per cycle <ref type="bibr" target="#b26">[27]</ref>, because the next code's first bit position cannot be know be a speculative decoder capable of decoding 8 codes per cycle in Section VI where the related work <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> is also discuss
emented transparent compression in the memory controller <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Compression is standard on DS8000 and Storewize SAN stora type="bibr" target="#b65">[66]</ref> and memory capacity <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> and memory bandwidth <ref type="bibr" target="#b66">[67]</
k in academia <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b55">[56]</ref>- <ref type="bibr" target="#b57">[58]</ref> and industry <ref type="bibr" target="#b58">[59]</ref>- <r
ves state of the art in Section IV, where the related work <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" ssion ratio, as described in Section V, where related work <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" t to meet throughput and compression ratio objectives which is not suitable for an on-chip design. In <ref type="bibr" target="#b3">[4]</ref> In <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" ol distribution which typically degrades compression ratio <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>. For example, Fig
ompression is standard on DS8000 and Storewize SAN storage systems, and TS7700 tape storage systems <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bib ad><p>The CAM was implemented with random logic based on a design used in an IBM storage controller <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, shown in Fig.
TCC was implemented in low level firmware called millicode running on the CISC type processor cores <ref type="bibr" target="#b40">[41]</ref>. Millicode interacts with the NXU on the same chip and per
.org/ns/1.0"><head n="2.2">MAML</head><p>We give an overview of Model-Agnostic Meta-Learning method <ref type="bibr" target="#b11">[12]</ref> which is a representative algorithm of optimization-based problem, we propose to encode the information from support set into our parameter inspired by MAML <ref type="bibr" target="#b11">[12]</ref> and further we can obtain a category-specific model to acc n testing query data.</p><p>â¢Meta-Learning We select two state-of-the-art meta learning models MAML <ref type="bibr" target="#b11">[12]</ref> and Meta-SGD <ref type="bibr" target="#b20">[21]</ref> as ent based learning procedure for new task quick adaptation. In the optimization-based methods, MAML <ref type="bibr" target="#b11">[12]</ref> is a recent promising model which learns a set of model pa
category loss term. To enable distribution q Ï c (z|x q c ) differentiable, we follow previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target
alse based on a text statement. Recently, powerful neural network based models, such as Transformer <ref type="bibr" target="#b23">[24]</ref> and BERT <ref type="bibr" target="#b10">[11]</ref> have sh ainly includes two components: encoder and decoder.</p><p>Encoder The encoder in use is Transformer <ref type="bibr" target="#b23">[24]</ref>, which is a context-aware model and has been proven powerf We select three state-of-the-art models ESIM <ref type="bibr" target="#b9">[10]</ref>, Transformer <ref type="bibr" target="#b23">[24]</ref>, BERT <ref type="bibr" target="#b10">[11]</ref> as baselin get="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref> that achieve promising performance. However, NLI task usual
ities <ref type="bibr" target="#b7">[8]</ref>. The deep learning anomaly detection (DAD) approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> model the log data a
meta-learning models. The traditional models (LR, SVM, RF) are implemented by scikit-learn package <ref type="bibr" target="#b22">[23]</ref>. The best parameters are selected based on the validation
dictions and is commonly used in the semisupervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> and domain adaptati ata and labeled query data. We use the entropy minimization to define the loss on unlabeled data as <ref type="bibr" target="#b14">[15]</ref> and use the cross-entropy to define the loss on labeled da
domain adaptation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. More concretely, the loss function L c s on the support se
dictions and is commonly used in the semisupervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> and domain adaptati ata and labeled query data. We use the entropy minimization to define the loss on unlabeled data as <ref type="bibr" target="#b14">[15]</ref> and use the cross-entropy to define the loss on labeled da
dictions and is commonly used in the semisupervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> and domain adaptati ata and labeled query data. We use the entropy minimization to define the loss on unlabeled data as <ref type="bibr" target="#b14">[15]</ref> and use the cross-entropy to define the loss on labeled da
er defining our problem, we introduce our learning setting. Following the few-shot learning setting <ref type="bibr" target="#b25">[26]</ref>, in each category c â¼ C, we have a few unlabeled examples
)] Inference Loss +Î» D K L (q Ïc (z |x q c ) | | q Ïc (z |x s c ))</formula><p>Bridging Regularizer <ref type="bibr" target="#b9">(10)</ref> In this paper, we assume q Ï c (z|x q c ) and q Ï c (z|x s s related to natural language inference (NLI) problem. We select three state-of-the-art models ESIM <ref type="bibr" target="#b9">[10]</ref>, Transformer <ref type="bibr" target="#b23">[24]</ref>, BER b27">[28]</ref> (MultiNLI) corpus have promoted the development of many different neural NLI models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" tar
meta-learning models. The traditional models (LR, SVM, RF) are implemented by scikit-learn package <ref type="bibr" target="#b22">[23]</ref>. The best parameters are selected based on the validation
ities <ref type="bibr" target="#b7">[8]</ref>. The deep learning anomaly detection (DAD) approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> model the log data a
er defining our problem, we introduce our learning setting. Following the few-shot learning setting <ref type="bibr" target="#b25">[26]</ref>, in each category c â¼ C, we have a few unlabeled examples
dictions and is commonly used in the semisupervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> and domain adaptati ata and labeled query data. We use the entropy minimization to define the loss on unlabeled data as <ref type="bibr" target="#b14">[15]</ref> and use the cross-entropy to define the loss on labeled da
ld allow systems to systematically build up and re-use knowledge across different but related tasks <ref type="bibr" target="#b24">[25]</ref>. More specifically, meta-Learning approaches can be broadl
</ref>. The deep learning anomaly detection (DAD) approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> model the log data as a natural language sequence and apply
ities <ref type="bibr" target="#b7">[8]</ref>. The deep learning anomaly detection (DAD) approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> model the log data a
f type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> and domain adaptation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" ta
ld allow systems to systematically build up and re-use knowledge across different but related tasks <ref type="bibr" target="#b24">[25]</ref>. More specifically, meta-Learning approaches can be broadl
neural network based models, such as Transformer <ref type="bibr" target="#b23">[24]</ref> and BERT <ref type="bibr" target="#b10">[11]</ref> have shown promising performance towards NLI task. However e use two Transformers to take two parts separately to obtain fixed-dimensional features. Following <ref type="bibr" target="#b10">[11]</ref>, the first token of every sequence is always a special cla ef type="bibr" target="#b9">[10]</ref>, Transformer <ref type="bibr" target="#b23">[24]</ref>, BERT <ref type="bibr" target="#b10">[11]</ref> as baselines. All sublayers of ESIM produce the output wit romoted the development of many different neural NLI models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" tar
ities <ref type="bibr" target="#b7">[8]</ref>. The deep learning anomaly detection (DAD) approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> model the log data a
et="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. Algorithms that do consider object articulations <ref type ct's pose and scale relative to a category-specific canonical representation. Recently, Wang et al. <ref type="bibr" target="#b27">[28]</ref> extended the object coordinate based approach to perform c re normalized and the orientations are aligned for objects in a given category. Whereas the work by <ref type="bibr" target="#b27">[28]</ref> focuses on pose and size estimation for rigid objects, the NCSH representation is inspired by and closely related to Normalized Object Coordinate Space (NOCS) <ref type="bibr" target="#b27">[28]</ref>, which we briefly review here. NOCS is defined as a 3D spa iefly review here. NOCS is defined as a 3D space contained within a unit cube and was introduced in <ref type="bibr" target="#b27">[28]</ref> to estimate the category-level 6D pose and size of rigid o closed. In addition to normalizing the articulations, NAOCS applies the same normalization used in <ref type="bibr" target="#b27">[28]</ref> to the objects, including zero-centering, aligning orienta head><p>For each part, NPCS further zero-centers its position and uniformly scales it as is done in <ref type="bibr" target="#b27">[28]</ref>, while at the same time keeps its orientation unchanged as ime keeps its orientation unchanged as in NAOCS. In this respect, NPCS is defined similarly to NOCS <ref type="bibr" target="#b27">[28]</ref> but for individual parts instead of whole objects. NPCS pr s {p i â S (j) }, we have their corresponding NPCS predictions {c i |p i â S (j) }. We could follow <ref type="bibr" target="#b27">[28]</ref> to perform pose fitting, where the Umeyama algorithm <ref j) }, as is commonly done for bundle adjustment <ref type="bibr" target="#b1">[2]</ref>. Similar to <ref type="bibr" target="#b27">[28]</ref>, we also use RANSAC for outlier removal.</p><p>Finally, fo , t (j) , s (j) and the NPCS {c i |p i â S (j) } to compute an amodal bounding box, the same as in <ref type="bibr" target="#b27">[28]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
8]</ref>. Algorithms that do consider object articulations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar anipulation of an object to infer its articulation pattern <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar pe="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. For example, Katz et al. <ref type="bibr" target="#b13">[14]</ref>, uses a robot manipulator to interact with articulated obj
ref><ref type="bibr" target="#b19">20]</ref>. Similarly, techniques for hand pose estimation (e.g., <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11]</ref>) leverages dense c
valuate rotation error measured in degrees, translation error, and 3D intersection over union (IoU) <ref type="bibr" target="#b21">[22]</ref> of the predicted amodal bounding box.</p><p>â¢ Joint states
tes onto its CAD model for each observed object pixel, and then use voting to solve for object pose <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. These approaches are
ref><ref type="bibr" target="#b19">20]</ref>. Similarly, techniques for hand pose estimation (e.g., <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11]</ref>) leverages dense c
ref><ref type="bibr" target="#b19">20]</ref>. Similarly, techniques for hand pose estimation (e.g., <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11]</ref>) leverages dense c
et="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> often require the exact object CAD model and the associated et="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. For example, Katz
ject articulations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> often require the e ticulation pattern <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar
thetic and real-word datasets. To generate the synthetic data, we mainly use object CAD models from <ref type="bibr" target="#b28">[29]</ref> along with drawer models from <ref type="bibr" target="#b2 the joint axis prediction for unseen instances (Table <ref type="table" target="#tab_1">2</ref>).  <ref type="bibr" target="#b28">[29]</ref> and SAPIEN dataset <ref type="bibr" target="#b29">[30]</re ><p>We render synthetic depth images using the object 3D model provided in the Shape2Motion dataset <ref type="bibr" target="#b28">[29]</ref> and SAPIEN dataset <ref type="bibr" target="#b29">[30]</re ive Results. Top tow rows show test results on unseen object instances from the Shape2Motion dataset<ref type="bibr" target="#b28">[29]</ref> and SAPIEN dataset<ref type="bibr" target="#b29">[30]</ref
r" target="#b18">19]</ref>, using dense correspondence maps between 2D images and 3D surface models <ref type="bibr" target="#b2">[3]</ref>, or estimating full 3D shape through 2D supervision <ref typ

ResNet <ref type="bibr" target="#b24">[25]</ref> with 3Ã wider channels and selective kernels (SK) <ref type="bibr" target="#b27">[28]</ref>, a channel-wise attention mechanism that improves the para 0.002 Ã sqrt(BatchSize)) for larger ResNets variants (with width multiplier larger than 1 and/or SK <ref type="bibr" target="#b27">[28]</ref>). A batch size of 1024 is used. Similar to <ref type="bibr in ResNet models by varying width and depth as well as whether or not to use selective kernels (SK) <ref type="bibr" target="#b27">[28]</ref>. 4 Whenever SK is used, we also use the ResNet-D <ref type es. It shows that (1) bigger models are better, but <ref type="bibr" target="#b1">(2)</ref> with SK <ref type="bibr" target="#b27">[28]</ref>, better performance can be achieved with the same paramete arameter count. It is worth to note that, in this work, we do not leverage group convolution for SK <ref type="bibr" target="#b27">[28]</ref> and we use only 3 Ã 3 kernels. We expect further improveme
the "unsupervised pretrain, supervised fine-tune" paradigm for semi-supervised learning on ImageNet <ref type="bibr" target="#b20">[21]</ref>. During self-supervised pretraining, images are used witho We call this framework SimCLRv2. We assess the effectiveness of our method on ImageNet ILSVRC-2012 <ref type="bibr" target="#b20">[21]</ref> with only 1% and 10% of the labeled images available. Our s. Below we summarize the changes as well as their improvements of accuracy on Imagenet ILSVRC-2012 <ref type="bibr" target="#b20">[21]</ref>.</p><p>1. To fully leverage the power of general pretraini ="#b18">19,</ref><ref type="bibr">1]</ref>, we evaluate the proposed method on ImageNet ILSVRC-2012 <ref type="bibr" target="#b20">[21]</ref>. While all â¼1.28 million images are available, only a rand
generative one as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" tar
nces in self-supervised learning of visual representations <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" t on with all labels <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">1]</ref> to directly evaluate SimCLRv2 repr /1.0"><head>C The Correlation Between Linear Evaluation and Fine-tuning</head><p>Most existing work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta
There are other approaches to self-supervised learning that are based on handcrafted pretext tasks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" ta
the "unsupervised pretrain, supervised fine-tune" paradigm for semi-supervised learning on ImageNet <ref type="bibr" target="#b20">[21]</ref>. During self-supervised pretraining, images are used witho We call this framework SimCLRv2. We assess the effectiveness of our method on ImageNet ILSVRC-2012 <ref type="bibr" target="#b20">[21]</ref> with only 1% and 10% of the labeled images available. Our s. Below we summarize the changes as well as their improvements of accuracy on Imagenet ILSVRC-2012 <ref type="bibr" target="#b20">[21]</ref>.</p><p>1. To fully leverage the power of general pretraini ="#b18">19,</ref><ref type="bibr">1]</ref>, we evaluate the proposed method on ImageNet ILSVRC-2012 <ref type="bibr" target="#b20">[21]</ref>. While all â¼1.28 million images are available, only a rand
els (SK) <ref type="bibr" target="#b27">[28]</ref>. 4 Whenever SK is used, we also use the ResNet-D <ref type="bibr" target="#b35">[36]</ref> variant of ResNet. The smallest model is the standard ResN
erse set of approaches for semi-supervised learning, we refer readers to <ref type="bibr">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref> for surveys of cl
d learning, we refer readers to <ref type="bibr">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref> for surveys of classical approaches. Here we only review m
l representations <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" t Most existing work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" tar
ain model, which can help to improve the accuracy of the classifier when there are few labeled data <ref type="bibr" target="#b24">[25]</ref>. Co-training needs to analyze data from two different "per
other type of work can assess people's emotions by their behavior on the social network. Lin et al. <ref type="bibr" target="#b10">[11]</ref> use a deep sparse neural network to assess people's psycho
nal features and the degree of user frustration through linear regression methods. Bogomolov et al. <ref type="bibr" target="#b19">[20]</ref> use data collected by mobile phones (including cell phone
e neural network to assess people's psychological pressure based on people's Weibo data; Lin et al. <ref type="bibr" target="#b11">[12]</ref> use a convolutional neural network to detect people's pres


. In contrast, we utilize multiple types of data to study the stress detection problem. Wang et al. <ref type="bibr" target="#b6">[7]</ref> extract multidimensional features from the StudentLife datas aining to tackle the label insufficiency issues which is not considered by Wang's and Zhang's works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b26">[27]</ref>. This makes our a g/ns/1.0"><head>III. DATASET AND PROBLEM DEFINITION</head><p>We use open source dataset StudentLife <ref type="bibr" target="#b6">[7]</ref> to train the model. StudentLife dataset is a dataset from Da

. In contrast, we utilize multiple types of data to study the stress detection problem. Wang et al. <ref type="bibr" target="#b6">[7]</ref> extract multidimensional features from the StudentLife datas aining to tackle the label insufficiency issues which is not considered by Wang's and Zhang's works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b26">[27]</ref>. This makes our a g/ns/1.0"><head>III. DATASET AND PROBLEM DEFINITION</head><p>We use open source dataset StudentLife <ref type="bibr" target="#b6">[7]</ref> to train the model. StudentLife dataset is a dataset from Da
pressure can also be monitored by professional instruments <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. For instance, the many studies are devoted to using wearable devices to monitor people's daily psychological pressure <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Typically, these
on their tweets and use classifiers to understand the stress levels of teen's pressure. Jin et al. <ref type="bibr" target="#b14">[15]</ref> propose an approach based on co-training, which combines W
words at different window sizes. However, RNN and CNN are hard to model long-distance dependencies <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref>, which may be useful in NER, such as cor icated.</p><p>In this paper, we propose FLAT: Flat LAttice Transformer for Chinese NER. Transformer <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref> adopts fully-connected selfattention to ere W r is a learnable parameter, ? denotes the concatenation operator, and p d is calculated as in <ref type="bibr" target="#b19">Vaswani et al. (2017)</ref>,</p><formula xml:id="formula_11">p (2k) d
="bibr" target="#b19">(Vaswani et al., 2017)</ref>, which may be useful in NER, such as coreference <ref type="bibr" target="#b16">(Stanislawek et al., 2019)</ref>. Due to the dynamic lattice structur
ref type="bibr" target="#b25">Zhang and Yang (2018)</ref>, and 'LS' denotes the lexicon released by <ref type="bibr" target="#b11">Li et al. (2018)</ref>. The result of other models are from their ori are the same as <ref type="bibr" target="#b25">Zhang and Yang (2018)</ref>. When comparing with CGN <ref type="bibr" target="#b11">(Li et al., 2018)</ref>, we use the same lexicon as CGN. The way to s r lattice LSTM, our model has an average F1 improvement of 1.51 over it. When using another lexicon <ref type="bibr" target="#b11">(Li et al., 2018)</ref>, our model also outperforms CGN by 0.73 in av
="bibr" target="#b23">Yang et al., 2017;</ref><ref type="bibr" target="#b12">Liu et al., 2017;</ref><ref type="bibr" target="#b18">Sun et al., 2020)</ref>, Chinese NER is more difficult since it usual
="bibr" target="#b23">Yang et al., 2017;</ref><ref type="bibr" target="#b12">Liu et al., 2017;</ref><ref type="bibr" target="#b18">Sun et al., 2020)</ref>, Chinese NER is more difficult since it usual
raph Network (LGN) <ref type="bibr">(Gui et al., 2019b)</ref> and Collaborative Graph Network (CGN) <ref type="bibr" target="#b17">(Sui et al., 2019)</ref>. While sequential structure is still importa igure">1</ref>(a), self-matched words of "? (Drug)" are "??? ?(Renhe Pharmacy)" and "?? (Pharmacy)" <ref type="bibr" target="#b17">(Sui et al., 2019)</ref>. Experimental results show our model outperf ncy and are hard to model long-distance dependencies. <ref type="bibr">Gui et al. (2019b)</ref> and <ref type="bibr" target="#b17">Sui et al. (2019)</ref> leveraged a lexicon and character sequence to
nition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks <ref type="bibr" target="#b1">(Chen et al., 2015;</ref><ref type="bibr" target="#b4">Diefenbach et a
to 1 https://github.com/fastnlp/fastNLP indicate lattice structure. In Chinese-source translation, <ref type="bibr" target="#b21">Xiao et al. (2019)</ref> take the absolute position of nodes' first c
="bibr" target="#b19">(Vaswani et al., 2017)</ref>, which may be useful in NER, such as coreference <ref type="bibr" target="#b16">(Stanislawek et al., 2019)</ref>. Due to the dynamic lattice structur
="bibr" target="#b23">Yang et al., 2017;</ref><ref type="bibr" target="#b12">Liu et al., 2017;</ref><ref type="bibr" target="#b18">Sun et al., 2020)</ref>, Chinese NER is more difficult since it usual

le neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b3">4]</ref> could achieve our goa ural network (INN) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar generative models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b3">4]</ref>. It is equivalent to
target="#b44">45,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b39">40]</ref> show promising results on both visual effect and compressio
et="#b55">[56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b18">19]</ref>. However, details are lost and distortions appear when user
lost information during downscaling. We note that according to the Nyquist-Shannon sampling theorem <ref type="bibr" target="#b46">[47]</ref>, the lost information during downscaling an HR image amoun
get="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b33">34]</ref> and easily fit for screens with different resolution while independent tasks, most recently, there have been efforts <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> attempting to model ">14]</ref>; (2) downscaling with upscaling-optimal models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> and upscaling with trained with an upscaling decoder <ref type="bibr" target="#b25">[26]</ref> or existing SR modules <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b48">49]</ref>. Although such an the downscaling and upscaling processes are trained jointly as a united task. Similarly, Li et al. <ref type="bibr" target="#b33">[34]</ref> proposed to use a CNN to estimate downscaled compact-resol
he downscaled LR images. However, mainstream SR algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60,</ref><ref type="bibr" target="#b58">59, ype="bibr" target="#b26">27]</ref> or deep learning models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60,</ref><ref type="bibr" target="#b58">59, nterpolation and upscaling with state-of-the-art SR models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60,</ref><ref type="bibr" target="#b58">59, ref>). Furthermore, as the shortcut connection is proved to be important in the image scaling tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b49">50]</ref>, we employ the add "#b38">[39]</ref>, and Urban100 <ref type="bibr" target="#b22">[23]</ref>. Following the setting in <ref type="bibr" target="#b35">[36]</ref>, we quantitatively evaluate the peak noise-signal ratio (P
nt resolution while maintaining visually valid information <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b48">49]</ref>. Meanwhile, many of these downscaling scenarios inevitably have been efforts <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> attempting to model image downscaling and upscaling as a un ype="bibr" target="#b25">[26]</ref> or existing SR modules <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b48">49]</ref>. Although such an integrated training approach can signific oss. This practice has also been adopted in the literature <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b48">49]</ref>. HR Reconstruction Although f ? is invertible, it is not fo ing-optimal models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> and upscaling with SR models. For the method of <ref type=" and leverage a learned or specified SR model for HR image reconstruction. More recently, Sun et al. <ref type="bibr" target="#b48">[49]</ref> proposed a new contentadaptive-resampler based image downs
he downscaled LR images. However, mainstream SR algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60,</ref><ref type="bibr" target="#b58">59, ype="bibr" target="#b26">27]</ref> or deep learning models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60,</ref><ref type="bibr" target="#b58">59, nterpolation and upscaling with state-of-the-art SR models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">60,</ref><ref type="bibr" target="#b58">59, ref>). Furthermore, as the shortcut connection is proved to be important in the image scaling tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b49">50]</ref>, we employ the add "#b38">[39]</ref>, and Urban100 <ref type="bibr" target="#b22">[23]</ref>. Following the setting in <ref type="bibr" target="#b35">[36]</ref>, we quantitatively evaluate the peak noise-signal ratio (P
earning strong prior information by example-based strategy <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" tar
ation (MLE) method for existing invertible neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" tar eural Network</head><p>The invertible neural network (INN) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" tar s widely adopted by prevalent flow-based generative models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" tar /p><p>INN is composed of invertible blocks. In this study, we employ the invertible architecture in <ref type="bibr" target="#b15">[16]</ref>. For the l-th block, input h l is split into h l 1 and h l )</label></formula><p>To enhance the transformation ability, the identity branch is often augmented <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_2">h l+1 1 = h l 1 exp(?(h l verage the general coupling layer architecture proposed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, i.e. Eqs. <ref type="bibr" target="#b0">(1,</ref><ref type /1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The mini-batch size is set to<ref type="bibr" target="#b15">16</ref>. The input HR image is randomly cropped into 144 ? 144 and a
lize the bandwidth <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" tar
="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref> or for member profiles (personalization) <ref type="bibr" target="#b9">[10]</ref>. It requires a huge amount of hard disk space to store the
ortunity to understand the deep semantics of natural language data through embedding representation <ref type="bibr" target="#b12">[13]</ref>. Moreover, to enhance contextual modeling, contextual embe dels. Representation based models learn independent embeddings for the query and the document. DSSM <ref type="bibr" target="#b12">[13]</ref> averages the word embeddings as the query/document embeddi t BERT-based ranking model for industry use cases, we propose to use representation based structure <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>. Instead of applyi ents. Both source and target could have multiple fields, which is different from most previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" ta embedding layer. It is worth noting that we use word tokens instead of triletters as in prior work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>, since the latter
iple target scores as input. DeText provides the flexibility of pointwise, pairwise or listwise LTR <ref type="bibr" target="#b2">[3]</ref>, as well as Lambda rank <ref type="bibr" target="#b3">[4]</r
query and document texts. BERT is then fine tuned with ranking loss. The inherent transformer layer <ref type="bibr" target="#b26">[27]</ref> in BERT allows direct context sharing between query words ince the query string and document string are concatenated as one sentence, where transformer layer <ref type="bibr" target="#b26">[27]</ref> compares every word pair in that sentence.</p><p>In experi
. This approach, in the category of interaction based models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref>, comes with a signi ue, interaction based models compare each part of the query with each part of the document. In DRMM <ref type="bibr" target="#b10">[11]</ref>, a cosine similarity is computed for each word embedding i
any text embedding, it has comparable results as XGBoost, which is also observed in previous works <ref type="bibr" target="#b16">[17]</ref>. For DeText-CNN, it consistently outperforms the strong pr
ng work uses embedding pre-computing, either for documents <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref> or for member profiles (personalization) <ref type="bibr" t
any text embedding, it has comparable results as XGBoost, which is also observed in previous works <ref type="bibr" target="#b16">[17]</ref>. For DeText-CNN, it consistently outperforms the strong pr
ever, promoting the power of BERT in ranking is a nontrivial task. The current effective approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targ respectively. Recently, BERT <ref type="bibr" target="#b7">[8]</ref> has shown superior performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targ
. This approach, in the category of interaction based models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref>, comes with a signi ue, interaction based models compare each part of the query with each part of the document. In DRMM <ref type="bibr" target="#b10">[11]</ref>, a cosine similarity is computed for each word embedding i
side, and the document score is the cosine similarity score of the query/document embedding. NRM-F <ref type="bibr" target="#b29">[30]</ref> adds more fields in the document side and achieves better
t for the graph structure and do not leverage rich node features. For instance, Poincar? embeddings <ref type="bibr" target="#b28">[29]</ref> capture the hyperbolic properties of real graphs by learni aselines. For shallow methods, we consider Euclidean embeddings (EUC) and Poincar? embeddings (HYP) <ref type="bibr" target="#b28">[29]</ref>. We conjecture that HYP will outperform EUC on hierarchica target="#b30">31]</ref>. Shallow embedding methods have also been developed in hyperbolic geometry <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> for reconstructing </p><p>For link prediction, we use the Fermi-Dirac decoder <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>, a generalization of sigmoid, to compute probability scores
ion Networks (GAT) <ref type="bibr" target="#b40">[41]</ref> and Simplified Graph Convolution (SGC) <ref type="bibr" target="#b43">[44]</ref> <ref type="foot" target="#foot_1">3</ref> . We also consid
movs ?-hyperbolicity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17]</ref>, a notion from group theory that measures how tree-like a g
by neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar
rget="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref>. More recently, hyperbolic neural networks <ref type="bibr"
rotein interaction networks and social networks, often exhibit scale-free or hierarchical structure <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref> and Euclidean embedd
s of the graph are embedded into points in Euclidean space <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar e features to embeddings, parameterized by neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar on weights w ij can be computed using different mechanisms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>. Message passing is y a MLP to predict node labels or links. For state-of-the-art Euclidean GNN models, we consider GCN <ref type="bibr" target="#b20">[21]</ref>, GraphSAGE (SAGE) <ref type="bibr" target="#b14">[15]</ref ve NC, we use 70/15/15% splits for AIRPORT, 30/10/60% splits for DISEASE, and we use standard splits<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b45">46]</ref> with 20 train exam
urIPS 2019 announced accepted papers, we also became aware of the concurrently developed HGNN model <ref type="bibr" target="#b25">[26]</ref> for learning GNNs in hyperbolic space. The main difference s the architecture for neighborhood aggregation and uses a learnable curvature. Additionally, while <ref type="bibr" target="#b25">[26]</ref> demonstrates strong performance on graph classification ta
ns to be the same ( <ref type="formula">16</ref>) for all methods. We optimize all models with Adam <ref type="bibr" target="#b18">[19]</ref>, except Poincar? embeddings which are optimized with Riema
has been applied to neural networks, to problems of computer vision or natural language processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ
s of the graph are embedded into points in Euclidean space <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar e features to embeddings, parameterized by neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar on weights w ij can be computed using different mechanisms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>. Message passing is y a MLP to predict node labels or links. For state-of-the-art Euclidean GNN models, we consider GCN <ref type="bibr" target="#b20">[21]</ref>, GraphSAGE (SAGE) <ref type="bibr" target="#b14">[15]</ref ve NC, we use 70/15/15% splits for AIRPORT, 30/10/60% splits for DISEASE, and we use standard splits<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b45">46]</ref> with 20 train exam
ref type="bibr" target="#b52">(Zhu, 2006;</ref><ref type="bibr" target="#b22">Li et al., 2008;</ref><ref type="bibr" target="#b34">Rosenberg et al., 2005;</ref><ref type="bibr" target="#b33">Riloff an ref type="bibr" target="#b52">(Zhu, 2006;</ref><ref type="bibr" target="#b22">Li et al., 2008;</ref><ref type="bibr" target="#b34">Rosenberg et al., 2005;</ref><ref type="bibr" target="#b33">Riloff an
earning is motivated by the self-training algorithm <ref type="bibr" target="#b52">(Zhu, 2006;</ref><ref type="bibr" target="#b22">Li et al., 2008;</ref><ref type="bibr" target="#b34">Rosenberg et al. model parameters, in a way similar to self-training <ref type="bibr" target="#b52">(Zhu, 2006;</ref><ref type="bibr" target="#b22">Li et al., 2008;</ref><ref type="bibr" target="#b34">Rosenberg et al.
br" target="#b22">Li et al., 2008;</ref><ref type="bibr" target="#b34">Rosenberg et al., 2005;</ref><ref type="bibr" target="#b33">Riloff and Jones, 1999)</ref>, which uses the prediction of models wi br" target="#b22">Li et al., 2008;</ref><ref type="bibr" target="#b34">Rosenberg et al., 2005;</ref><ref type="bibr" target="#b33">Riloff and Jones, 1999)</ref>. However, in order to prevent the deep
target="#b18">Kumar et al., 2010;</ref><ref type="bibr" target="#b28">Northcutt et al., 2017;</ref><ref type="bibr" target="#b2">Chang et al., 2017)</ref>. Adding pre-defined curriculums in loss func
b6">(Goldberger and Ben-Reuven, 2017;</ref><ref type="bibr" target="#b15">Khetan et al., 2018;</ref><ref type="bibr" target="#b47">Xiao et al., 2015)</ref>. In addition, recent work also models the la
="bibr" target="#b18">Kumar et al., 2010;</ref><ref type="bibr" target="#b20">Li et al., 2018;</ref><ref type="bibr" target="#b13">Jiang et al., 2018;</ref><ref type="bibr" target="#b51">Zhao et al., ts of samples is also proved to be effective <ref type="bibr" target="#b25">(Lin et al., 2017;</ref><ref type="bibr" target="#b13">Jiang et al., 2018;</ref><ref type="bibr" target="#b1">Bengio et al., 2018)</ref>, and learning with meta-learning <ref type="bibr" target="#b20">(Li et al., 2018;</ref><ref type="bibr" target="#b13">Jiang et al., 2018)</ref>. However, existing approaches mainly focus ef> propose a meta-learning method to reweight training samples based on their gradient directions. <ref type="bibr" target="#b13">Jiang et al. (2018)</ref> train an auxiliary LSTM-based MentorNet to heme widely-used for them: random 32x32 cropping with 4-pixel padded and random horizontal flipping <ref type="bibr" target="#b13">(Jiang et al., 2018;</ref><ref type="bibr" target="#b50">Zhang and Sa ompared with several state-of-the-art methods to make DNN robust against label noise. b1) MentorNet <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref>  </p></div> <div xmlns="http://www.tei-c.o notes the ratio of wrong labels. For self-paced learning and MentorNet, we cite results reported in <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref>. For other methods, we report results of o >(Jiang et al., 2018)</ref>. For other methods, we report results of our implementations. Following <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref> For b1) and b2), we directly cite the resu target="#b13">(Jiang et al., 2018)</ref> For b1) and b2), we directly cite the results reported in <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref>. For a fair comparison, we also use ResNet e ResNets with wide filters mentioned in their paper and apply exactly the same training details as <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref>. The conv4 group of the network is duplica ivided by 10 after the 80 th and 120 th epoch, respectively.</p><p>Hyper-parameter. As MentorNet-DD <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref> uses additional samples with accurate labe
samples. Based on this assumption, a confusion matrix can be introduced to describe the label noise <ref type="bibr" target="#b39">(Sukhbaatar et al., 2018)</ref>, and many existing work is dedicated
stochastic training techniques like dropout <ref type="bibr" target="#b19">(Lan et al., 2018;</ref><ref type="bibr" target="#b41">Szegedy et al., 2016;</ref><ref type="bibr" target="#b12">Huang et al ate branches tend to make different mistakes <ref type="bibr" target="#b19">(Lan et al., 2018;</ref><ref type="bibr" target="#b41">Szegedy et al., 2016;</ref><ref type="bibr" target="#b12">Huang et al ent local minima and make different mistakes <ref type="bibr" target="#b19">(Lan et al., 2018;</ref><ref type="bibr" target="#b41">Szegedy et al., 2016)</ref>. Therefore, pseudo labels generated from
nal networks such as ResNets <ref type="bibr" target="#b9">(He et al., 2016)</ref> and Wide ResNets <ref type="bibr" target="#b48">(Zagoruyko and Komodakis, 2016)</ref>. We simply duplicate the final recommended by <ref type="bibr">Han et al. (2018)</ref>.</p><p>Training details. Wide-ResNet (WRN) <ref type="bibr" target="#b48">(Zagoruyko and Komodakis, 2016)</ref> and ResNet-32 <ref type="bibr"
at DNNs tend to learn meaningful patterns before overfitting to noise during the course of training <ref type="bibr" target="#b0">(Arpit et al., 2017)</ref>, we trigger the relabelling operation early ang et al. (2017)</ref> that DNNs are able fit random noise. We also observed similar results as in <ref type="bibr" target="#b0">(Arpit et al., 2017)</ref> that DNNs tend to learn meaningful patterns labels to learning from the pseudo labels. It is inspired by the intriguing phenomenon observed by <ref type="bibr" target="#b0">Arpit et al. (2017)</ref> that DNNs tend to learn meaningful patterns
at DNNs tend to learn meaningful patterns before overfitting to noise during the course of training <ref type="bibr" target="#b0">(Arpit et al., 2017)</ref>, we trigger the relabelling operation early ang et al. (2017)</ref> that DNNs are able fit random noise. We also observed similar results as in <ref type="bibr" target="#b0">(Arpit et al., 2017)</ref> that DNNs tend to learn meaningful patterns labels to learning from the pseudo labels. It is inspired by the intriguing phenomenon observed by <ref type="bibr" target="#b0">Arpit et al. (2017)</ref> that DNNs tend to learn meaningful patterns
ieved remarkable success in the past few years <ref type="bibr" target="#b9">(He et al., 2016;</ref><ref type="bibr" target="#b11">Huang et al., 2017;</ref><ref type="bibr" target="#b37">Simonyan and ining, 26,032 images for testing and 531,131 images for additional training are provided. Following <ref type="bibr" target="#b11">Huang et al. (2017)</ref>, we use all the training data without any d
samples. Based on this assumption, a confusion matrix can be introduced to describe the label noise <ref type="bibr" target="#b39">(Sukhbaatar et al., 2018)</ref>, and many existing work is dedicated
e. In fact, it has been proved that DNNs have the capacity to overfit even completely random labels <ref type="bibr" target="#b49">(Zhang et al., 2017)</ref>. Therefore, training DNNs reliably on data andomly labeled samples with almost 100% accuracy. This phenomenon is in line with that observed by <ref type="bibr" target="#b49">Zhang et al. (2017)</ref> that DNNs are able fit random noise. We als
ely influence each other, especially for the training set with a large portion of corrupted labels. <ref type="bibr" target="#b46">Veit et al. (2017)</ref> trains an auxiliary cleaning network jointly
stochastic training techniques like dropout <ref type="bibr" target="#b19">(Lan et al., 2018;</ref><ref type="bibr" target="#b41">Szegedy et al., 2016;</ref><ref type="bibr" target="#b12">Huang et al ate branches tend to make different mistakes <ref type="bibr" target="#b19">(Lan et al., 2018;</ref><ref type="bibr" target="#b41">Szegedy et al., 2016;</ref><ref type="bibr" target="#b12">Huang et al ent local minima and make different mistakes <ref type="bibr" target="#b19">(Lan et al., 2018;</ref><ref type="bibr" target="#b41">Szegedy et al., 2016)</ref>. Therefore, pseudo labels generated from
rget="#b4">(Frenay and Verleysen, 2014;</ref><ref type="bibr" target="#b5">Ghosh et al., 2017;</ref><ref type="bibr" target="#b31">Ren et al., 2018;</ref><ref type="bibr" target="#b18">Kumar et al., 2 robust loss functions <ref type="bibr" target="#b5">(Ghosh et al., 2017)</ref>, reweighting samples <ref type="bibr" target="#b31">(Ren et al., 2018)</ref>, and learning with meta-learning <ref type=" of noise is pretty common and challenging as it's a combination of label imbalance and label noise <ref type="bibr" target="#b31">(Ren et al., 2018)</ref>.</p><p>Baselines. Our method is compared wit and pruning. Another popular approach to handle corrupted labels is sample reweighting or pruning. <ref type="bibr" target="#b31">Ren et al. (2018)</ref> propose a meta-learning method to reweight tr MNIST and SVHN, we use all the provided training data for training.</p><p>Noise settings. Following <ref type="bibr" target="#b31">Ren et al. (2018)</ref>, we design two label noise distribution forms
ariables, EM-based algorithms are also proposed to handle the label noise with probabilistic models <ref type="bibr" target="#b6">(Goldberger and Ben-Reuven, 2017;</ref><ref type="bibr" target="#b15"> e rate has been known, and initialize the noise adaptation layer with uniform noise as suggested in <ref type="bibr" target="#b6">(Goldberger and Ben-Reuven, 2017)</ref>. For b5), we use the best Î² (Î² ing gives the highest test accuracy among all the algorithms, demonstrating remarkable robustness.  <ref type="bibr" target="#b6">(Goldberger and Ben-Reuven, 2017)</ref> Reed soft <ref type="bibr" tar
"#b39">(Sukhbaatar et al., 2018)</ref>, and many existing work is dedicated to estimate this matrix <ref type="bibr" target="#b29">(Patrini et al., 2017;</ref><ref type="bibr" target="#b10">Hendrycks arget="#b13">(Jiang et al., 2018;</ref><ref type="bibr" target="#b50">Zhang and Sabuncu, 2018;</ref><ref type="bibr" target="#b29">Patrini et al., 2017;</ref><ref type="bibr" target="#b42">Tanaka et a
target="#b18">Kumar et al., 2010;</ref><ref type="bibr" target="#b28">Northcutt et al., 2017;</ref><ref type="bibr" target="#b2">Chang et al., 2017)</ref>. Adding pre-defined curriculums in loss func
ieved remarkable success in the past few years <ref type="bibr" target="#b9">(He et al., 2016;</ref><ref type="bibr" target="#b11">Huang et al., 2017;</ref><ref type="bibr" target="#b37">Simonyan and ining, 26,032 images for testing and 531,131 images for additional training are provided. Following <ref type="bibr" target="#b11">Huang et al. (2017)</ref>, we use all the training data without any d
"#b39">(Sukhbaatar et al., 2018)</ref>, and many existing work is dedicated to estimate this matrix <ref type="bibr" target="#b29">(Patrini et al., 2017;</ref><ref type="bibr" target="#b10">Hendrycks arget="#b13">(Jiang et al., 2018;</ref><ref type="bibr" target="#b50">Zhang and Sabuncu, 2018;</ref><ref type="bibr" target="#b29">Patrini et al., 2017;</ref><ref type="bibr" target="#b42">Tanaka et a
rget="#b50">Zhang and Sabuncu, 2018;</ref><ref type="bibr" target="#b29">Patrini et al., 2017;</ref><ref type="bibr" target="#b42">Tanaka et al., 2018)</ref>. The MNIST dataset consists of 32x32 image
="bibr" target="#b18">Kumar et al., 2010;</ref><ref type="bibr" target="#b20">Li et al., 2018;</ref><ref type="bibr" target="#b13">Jiang et al., 2018;</ref><ref type="bibr" target="#b51">Zhao et al., ts of samples is also proved to be effective <ref type="bibr" target="#b25">(Lin et al., 2017;</ref><ref type="bibr" target="#b13">Jiang et al., 2018;</ref><ref type="bibr" target="#b1">Bengio et al., 2018)</ref>, and learning with meta-learning <ref type="bibr" target="#b20">(Li et al., 2018;</ref><ref type="bibr" target="#b13">Jiang et al., 2018)</ref>. However, existing approaches mainly focus ef> propose a meta-learning method to reweight training samples based on their gradient directions. <ref type="bibr" target="#b13">Jiang et al. (2018)</ref> train an auxiliary LSTM-based MentorNet to heme widely-used for them: random 32x32 cropping with 4-pixel padded and random horizontal flipping <ref type="bibr" target="#b13">(Jiang et al., 2018;</ref><ref type="bibr" target="#b50">Zhang and Sa ompared with several state-of-the-art methods to make DNN robust against label noise. b1) MentorNet <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref>  </p></div> <div xmlns="http://www.tei-c.o notes the ratio of wrong labels. For self-paced learning and MentorNet, we cite results reported in <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref>. For other methods, we report results of o >(Jiang et al., 2018)</ref>. For other methods, we report results of our implementations. Following <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref> For b1) and b2), we directly cite the resu target="#b13">(Jiang et al., 2018)</ref> For b1) and b2), we directly cite the results reported in <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref>. For a fair comparison, we also use ResNet e ResNets with wide filters mentioned in their paper and apply exactly the same training details as <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref>. The conv4 group of the network is duplica ivided by 10 after the 80 th and 120 th epoch, respectively.</p><p>Hyper-parameter. As MentorNet-DD <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref> uses additional samples with accurate labe
bibr" target="#b32">Ren et al., 2015;</ref><ref type="bibr" target="#b37">Silver et al., 2016;</ref><ref type="bibr" target="#b36">Schmidhuber, 2015)</ref>. They can learn highly generalizable represe
ork is dedicated to estimate this matrix <ref type="bibr" target="#b29">(Patrini et al., 2017;</ref><ref type="bibr" target="#b10">Hendrycks et al., 2018;</ref><ref type="bibr" target="#b14">Jindal et
rriculums in loss functions to dynamically adjust weights of samples is also proved to be effective <ref type="bibr" target="#b25">(Lin et al., 2017;</ref><ref type="bibr" target="#b13">Jiang et al.,
duction</head><p>Deep neural networks (DNNs) have achieved remarkable success in the past few years <ref type="bibr" target="#b9">(He et al., 2016;</ref><ref type="bibr" target="#b11">Huang et al., 20 p>In our experiments, the network is modified from classical convolutional networks such as ResNets <ref type="bibr" target="#b9">(He et al., 2016)</ref> and Wide ResNets <ref type="bibr" target="#b48 ide-ResNet (WRN) <ref type="bibr" target="#b48">(Zagoruyko and Komodakis, 2016)</ref> and ResNet-32 <ref type="bibr" target="#b9">(He et al., 2016)</ref> are implemented in this paper. WRN is used in
="bibr" target="#b9">(He et al., 2016;</ref><ref type="bibr" target="#b11">Huang et al., 2017;</ref><ref type="bibr" target="#b37">Simonyan and Zisserman, 2015;</ref><ref type="bibr" target="#b32">Ren get="#b37">Simonyan and Zisserman, 2015;</ref><ref type="bibr" target="#b32">Ren et al., 2015;</ref><ref type="bibr" target="#b37">Silver et al., 2016;</ref><ref type="bibr" target="#b36">Schmidhuber,

able representations when fueled by large scale datasets with precise annotations, such as ImageNet <ref type="bibr" target="#b3">(Deng et al., 2009)</ref> and COCO <ref type="bibr" target="#b23">(Lin
rget="#b50">Zhang and Sabuncu, 2018;</ref><ref type="bibr" target="#b29">Patrini et al., 2017;</ref><ref type="bibr" target="#b42">Tanaka et al., 2018)</ref>. The MNIST dataset consists of 32x32 image
="bibr" target="#b20">Li et al., 2018;</ref><ref type="bibr" target="#b13">Jiang et al., 2018;</ref><ref type="bibr" target="#b51">Zhao et al., 2019)</ref>. Popular approaches include designing robust
application-agnostic and is implemented in the Customizable Streaming Partitioner (CuSP) framework <ref type="bibr" target="#b35">[36]</ref>. The experimental results in this paper are based on the p lemented the proposed graph partitioning policy using the Customizable Streaming Partitioner (CuSP) <ref type="bibr" target="#b35">[36]</ref> framework. We modified CuSP to create edge proxies in addi
policy can be used for multi-machine CPU and GPU implementations.</p><p>Miscellaneous: Huang et al. <ref type="bibr" target="#b29">[30]</ref> implement triangle counting on an FPGA: they use the low-l
Model for Partitioning</head><p>Our graph partitioning is based on the proxy model of partitioning <ref type="bibr" target="#b34">[35]</ref> in which edges are distributed among the host machines and
synthetic graph generated by an RMAT generator <ref type="bibr" target="#b40">[41]</ref>; twitter40 <ref type="bibr" target="#b41">[42]</ref>, friendster <ref type="bibr" target="#b42">[43]</ref> uk-2
/ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and GPUs <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. We briefly dis
ef>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Triangle counting on single GPU: Green et al. <ref type="bibr" target="#b16">[17]</ref> and Voegele et al. <ref type="bibr" target="#b4">[5]</ref>
triangle counting implementation which was used in a 2018 Graph Challenge champion's implementation <ref type="bibr" target="#b28">[29]</ref>. It partitions the CSR data across multiple GPUs and strea to GPUs.</p><p>Our implementation is similar to TriCore <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> in that it uses binary search based intersection as well a instead of edgelist intersection to improve GPU locality <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. As the graph is already preprocessed before it is marshal intersection for computing the number of triangles on an edge, whereas DistTC employs binary search <ref type="bibr" target="#b28">[29]</ref>. Binary search has better locality and improved coalesced s better locality and improved coalesced memory accesses <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, so we observe an increase in performance even on a single are the performance of our approach with that of TriCore <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, which is a multi-GPU implementation of triangle counting "><head>TABLE IV :</head><label>IV</label><figDesc>Performance comparison (in seconds) with Tri-Core<ref type="bibr" target="#b28">[29]</ref>: friendster and twitter40 compares compute time; gsh-2015
triangle counting implementation which was used in a 2018 Graph Challenge champion's implementation <ref type="bibr" target="#b28">[29]</ref>. It partitions the CSR data across multiple GPUs and strea to GPUs.</p><p>Our implementation is similar to TriCore <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> in that it uses binary search based intersection as well a instead of edgelist intersection to improve GPU locality <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. As the graph is already preprocessed before it is marshal intersection for computing the number of triangles on an edge, whereas DistTC employs binary search <ref type="bibr" target="#b28">[29]</ref>. Binary search has better locality and improved coalesced s better locality and improved coalesced memory accesses <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, so we observe an increase in performance even on a single are the performance of our approach with that of TriCore <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, which is a multi-GPU implementation of triangle counting "><head>TABLE IV :</head><label>IV</label><figDesc>Performance comparison (in seconds) with Tri-Core<ref type="bibr" target="#b28">[29]</ref>: friendster and twitter40 compares compute time; gsh-2015
triangle counting implementation which was used in a 2018 Graph Challenge champion's implementation <ref type="bibr" target="#b28">[29]</ref>. It partitions the CSR data across multiple GPUs and strea to GPUs.</p><p>Our implementation is similar to TriCore <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> in that it uses binary search based intersection as well a instead of edgelist intersection to improve GPU locality <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. As the graph is already preprocessed before it is marshal intersection for computing the number of triangles on an edge, whereas DistTC employs binary search <ref type="bibr" target="#b28">[29]</ref>. Binary search has better locality and improved coalesced s better locality and improved coalesced memory accesses <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, so we observe an increase in performance even on a single are the performance of our approach with that of TriCore <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, which is a multi-GPU implementation of triangle counting "><head>TABLE IV :</head><label>IV</label><figDesc>Performance comparison (in seconds) with Tri-Core<ref type="bibr" target="#b28">[29]</ref>: friendster and twitter40 compares compute time; gsh-2015
mory. Other implementations are single GPU implementations <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, but these are even more constrained by memory limitations.
ory CPUs <ref type="bibr" target="#b6">[7]</ref>, clusters <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr"
c. We validated the performance statistics of MAESTRO against cycle-accurate RTL simulation results <ref type="bibr" target="#b4">5</ref> and reported performance in a previous work <ref type="bibr" t 's performance model against RTL simulation and reported processing delay of two accelerators-MAERI <ref type="bibr" target="#b4">5</ref> and Eyeriss 6 when running VGG16 and AlexNet, respectively. Th ch an optimization opportunity can be exploited by flexible accelerators like Flexflow 15 and MAERI <ref type="bibr" target="#b4">5</ref> or via heterogeneous accelerators that employ multiple subacce



ments (PEs) and high energy efficiency by maximizing data reuse within PEs and on-chip scratchpads. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" targe thus the prime optimization target of DNN accelerators. Data reuse pattern is dictated by dataflow, <ref type="bibr" target="#b0">1</ref> which are data/computation tile scheduling and spatial partiti

imulation results <ref type="bibr" target="#b4">5</ref> and reported performance in a previous work <ref type="bibr" target="#b5">6</ref> with the accuracy of 96.1% on average. MAESTRO provides fast c a and power constraint as 16 mm 2 and 450 mW, which is the reported chip area and power of Eyeriss. <ref type="bibr" target="#b5">6</ref> We plot the entire design space we explored in Figure <ref typ

ments (PEs) and high energy efficiency by maximizing data reuse within PEs and on-chip scratchpads. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" targe thus the prime optimization target of DNN accelerators. Data reuse pattern is dictated by dataflow, <ref type="bibr" target="#b0">1</ref> which are data/computation tile scheduling and spatial partiti


res <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref> and descriptors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" targ
r" target="#b24">25]</ref>. As a result, DTA prediction has received much attention in recent years <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ
rugs that are able to inhibit the target/protein and benefits many other bioinformatic applications <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" ta
each line represents the pre-trained vector of an atom.</p><p>Motivated by the gate function in GRU <ref type="bibr" target="#b4">[5]</ref>, we apply a 1-layer Bi-GRU on the resulting matrix to obtain
sed on the logarithm of K d as pK d = -log10( K d 1e9 ), where K d refers to the dissociation value <ref type="bibr" target="#b28">[29]</ref>. Here, we transformed the datasets into binary datasets wi
emical bond between the i-th and the j-th atoms. Then, we can learn a graph attention network (GAT) <ref type="bibr" target="#b32">[33]</ref> from the molecule graphs G. To apply GAT on molecule graph
matic applications <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>. As a result, DTA prediction has received much attention in
matic applications <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>. As a result, DTA prediction has received much attention in
ref> and descriptors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6]</ref>, and simply depend on the similarities between drugtarget pai volutional network was used to model molecule based on the extraction of their circular fingerprint <ref type="bibr" target="#b5">[6]</ref>. By learning from molecular structures and protein sequences
sed on the logarithm of K d as pK d = -log10( K d 1e9 ), where K d refers to the dissociation value <ref type="bibr" target="#b28">[29]</ref>. Here, we transformed the datasets into binary datasets wi
arget prediction. Most of the existing methods are based on topological similarity. For example, in <ref type="bibr" target="#b36">[37]</ref> they developed a Deep Belief Network (DBN) model construct
attack-agnostic manner, except a few touches on denoising <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> and obfuscating gradients <ref type="bibr" target="#b10">[1 le FPD can circumvent the structure-replaced white-box attack. Our proposal is partially related to <ref type="bibr" target="#b28">[29]</ref>, as the denoising layers in our FPD are inspired by their ng layers in our FPD are inspired by their feature denoising approach. Nevertheless, different from <ref type="bibr" target="#b28">[29]</ref>, the principle behind our FPD is to improve the intrinsic antic information. We will compare the performance between FPD-enhanced CNN and the CNN enhanced by <ref type="bibr" target="#b28">[29]</ref> in Section 4.1.</p></div> <div xmlns="http://www.tei-c.org the Gaussian filtering operator, the dot product operator helps improve the adversarial robustness <ref type="bibr" target="#b28">[29]</ref>. Meanwhile, as the dot product operator does not involve e framework structure through the exploration study. Moreover, we compare with the most related work <ref type="bibr" target="#b28">[29]</ref> as well. In the comparison experiments, we focus on compar a><p>Comparison with the Related Work As mentioned in Section 2, the denoising approach proposed in <ref type="bibr" target="#b28">[29]</ref> is similar to our denoising layers in FPD. Therefore, we c /ref> is similar to our denoising layers in FPD. Therefore, we conduct a comparison experiment with <ref type="bibr" target="#b28">[29]</ref> as well. In Table <ref type="table" target="#tab_5">1</ref /ref> as well. In Table <ref type="table" target="#tab_5">1</ref>, X represents the enhanced CNN by <ref type="bibr" target="#b28">[29]</ref>. We observe that our F 2IâMid outperforms X . Especially,
ication to image classification. Since the seminal work by <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21]</ref>, many follow-up works have demonstrated a great variety of
type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> or score-based methods <ref type="bibr" target="#b3">[4]</ref> to generate adversarial samples.</p><p>To thwart these attac
dition, we separately train a simple three layers fully-connected network as the substitute network <ref type="bibr" target="#b22">[23]</ref> for each network.</p></div> <div xmlns="http://www.tei-c.o
be kept and it helps to amend gradient update.</p><p>Moreover, we modify non-local means algorithm <ref type="bibr" target="#b2">[3]</ref> by replacing the Gaussian filtering operator with a dot prod
image quilting, total variance minimization and quantization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11]</ref>. Pixel denoising ap
dient-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> or score-based methods <ref type="bibr" target="#b3">[4]</r ten targets a specific attack, the resulting defense method can hardly be generalized, as hinted in <ref type="bibr" target="#b26">[27]</ref>. In order to defend against various attacks, a large amoun aining, proposed by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>, is an approach to
ending against the adversarial samples have been analyzed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. As stated in our following Theorem 1, the network could be
dition, we separately train a simple three layers fully-connected network as the substitute network <ref type="bibr" target="#b22">[23]</ref> for each network.</p></div> <div xmlns="http://www.tei-c.o
dient-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> or score-based methods <ref type="bibr" target="#b3">[4]</r ten targets a specific attack, the resulting defense method can hardly be generalized, as hinted in <ref type="bibr" target="#b26">[27]</ref>. In order to defend against various attacks, a large amoun aining, proposed by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>, is an approach to
66 and 1,422 images as test images  <ref type="bibr" target="#b11">[12]</ref> as well as ResNeXt-50 <ref type="bibr" target="#b29">[30]</ref> are enhanced in the following experiments. We use Pytorch
y or with only very few labeled examples <ref type="bibr" target="#b26">(Radford et al., 2019;</ref><ref type="bibr" target="#b30">Schick and SchÃ¼tze, 2020a)</ref>.</p><p>Very recently, <ref type="bib mited to a few hundred tokens.</p><p>An alternative to priming is pattern-exploiting training (PET) <ref type="bibr" target="#b30">(Schick and SchÃ¼tze, 2020a)</ref>, which combines the idea of reformu are understood well by LMs is difficult <ref type="bibr" target="#b12">(Jiang et al., 2019)</ref>, <ref type="bibr" target="#b30">Schick and SchÃ¼tze (2020a)</ref> propose PET, a method that uses know ="#b4">(Dagan et al., 2006)</ref> are textual entailment tasks like MNLI, so we use PVPs similar to <ref type="bibr" target="#b30">Schick and SchÃ¼tze (2020a)</ref>. For a premise p and hypothesis h, w ts. 5 We run PET on the FewGLUE training sets for all SuperGLUE tasks using the exact same setup as <ref type="bibr" target="#b30">Schick and SchÃ¼tze (2020a)</ref>. For COPA, WSC and ReCoRD, we use ou ref>). Given 32 examples, PET clearly outperforms both baselines, which is in line with findings by <ref type="bibr" target="#b30">Schick and SchÃ¼tze (2020a)</ref>.</p><p>We next compare PET directly t segments and mark p in s with asterisks.</p><p>MultiRC Deviating from the hyperparameters used by <ref type="bibr" target="#b30">Schick and SchÃ¼tze (2019)</ref>, we use a maximum sequence length of
used in the same sense and no for other words; for the third pattern, we use b and 2.</p><p>For WSC <ref type="bibr" target="#b16">(Levesque et al., 2011)</ref>, each example consists of a sentence s
es some modifications during training and inference that are discussed in Appendix A.</p><p>MultiRC <ref type="bibr" target="#b13">(Khashabi et al., 2018</ref>) is a QA task. Given a passage p, a ques
corresponding PVPs. We use a vertical bar (|) to mark boundaries between text segments.</p><p>BoolQ <ref type="bibr" target="#b3">(Clark et al., 2019)</ref> is a QA task where each example consists of
can deal with much longer contexts (e.g., <ref type="bibr" target="#b14">Kitaev et al., 2020;</ref><ref type="bibr" target="#b0">Beltagy et al., 2020)</ref>, it has yet to be investigated to what ext
probing the knowledge contained within LMs <ref type="bibr" target="#b33">(Trinh and Le, 2018;</ref><ref type="bibr" target="#b22">Petroni et al., 2019;</ref><ref type="bibr" target="#b32">Talmor et a
tailment to yes, disagreement to no and neutral to maybe.</p><p>Given a premise p, the task in COPA <ref type="bibr" target="#b28">(Roemmele et al., 2011)</ref> is to determine the cause or effect of
for tasks that require predicting more than one token. We then show that in combination with ALBERT <ref type="bibr" target="#b15">(Lan et al., 2020)</ref>, PET and its iterative variant (iPET) both o .tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>As underlying LM for PET we choose ALBERTxxlarge-v2 <ref type="bibr" target="#b15">(Lan et al., 2020)</ref>, the best-performing MLM on SuperGLUE when t
asily be adapted to models capable of doing conditional generation with bidirectional context (e.g.,<ref type="bibr" target="#b17">Lewis et al., 2020;</ref><ref type="bibr" target="#b27">Raffel et al.
can deal with much longer contexts (e.g., <ref type="bibr" target="#b14">Kitaev et al., 2020;</ref><ref type="bibr" target="#b0">Beltagy et al., 2020)</ref>, it has yet to be investigated to what ext
by randomly selecting 32 examples for each task using a fixed random seed. Following previous work <ref type="bibr" target="#b27">(Raffel et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al. d pronoun p and noun n, and the task is to determine whether p refers to n. We follow previous work <ref type="bibr" target="#b27">(Raffel et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al. boundary between two text segments.</p><p>WSC Unlike other SuperGLUE tasks, the WSC formulation of <ref type="bibr" target="#b27">Raffel et al. (2019)</ref> and <ref type="bibr" target="#b1">Brown et ss multiple evaluations and perform greedy decoding as described in Section 3.</p><p>We then follow <ref type="bibr" target="#b27">Raffel et al. (2019)</ref> to map the output produced by the LM to a g on FewGLUE. State-of-the-art results when using the regular, full size training sets for all tasks<ref type="bibr" target="#b27">(Raffel et al., 2019)</ref> are shown in italics.</figDesc><table><ro generation with bidirectional context (e.g.,<ref type="bibr" target="#b17">Lewis et al., 2020;</ref><ref type="bibr" target="#b27">Raffel et al., 2019)</ref>, we stick with MLMs as they are more light
l language understanding and generation tasks <ref type="bibr" target="#b9">(Dai et al., 2019;</ref><ref type="bibr" target="#b38">Shaw et al., 2018)</ref>. The proposed Disentangled Attention mechani tent, and position-to-position<ref type="foot" target="#foot_0">1</ref> .</p><p>Existing approaches <ref type="bibr" target="#b38">(Shaw et al., 2018;</ref><ref type="bibr" target="#b18">Huang et al., LEMENTATION</head><p>For an input sequence of length N , it requires a space complexity of OpN 2 dq <ref type="bibr" target="#b38">(Shaw et al., 2018;</ref><ref type="bibr" target="#b18">Huang et al.,
"1" xml:id="foot_0">In this sense, our model shares some similarity to Tensor Product Representation<ref type="bibr" target="#b40">(Smolensky, 1990;</ref><ref type="bibr" target="#b37">Schlag et al.,<
use Wikipedia (English Wikipedia dump<ref type="foot" target="#foot_2">2</ref> ; 12GB), BookCorpus <ref type="bibr" target="#b52">(Zhu et al., 2015)</ref>  We evaluate DeBERTa on additional benchmark NING DATASETFor DeBERTa pre-training, we use Wikipedia (English Wikipedia dump 6 ; 12GB), BookCorpus<ref type="bibr" target="#b52">(Zhu et al., 2015)</ref> 7 (6GB), OPENWEBTEXT (public Reddit content<
/ref>, and natural language inference (NLI) <ref type="bibr" target="#b8">(Dagan et al., 2006;</ref><ref type="bibr" target="#b0">Bar-Haim et al., 2006;</ref><ref type="bibr" target="#b14">Giampiccolo
9">(Liu et al., 2019c)</ref>, XLNet <ref type="bibr" target="#b49">(Yang et al., 2019)</ref>, UniLM <ref type="bibr" target="#b12">(Dong et al., 2019a)</ref>, ELECTRA <ref type="bibr" target="#b7">(Cl
type="bibr" target="#b11">(Dolan &amp; Brockett, 2005)</ref>, and natural language inference (NLI) <ref type="bibr" target="#b8">(Dagan et al., 2006;</ref><ref type="bibr" target="#b0">Bar-Haim et al
9">(Liu et al., 2019c)</ref>, XLNet <ref type="bibr" target="#b49">(Yang et al., 2019)</ref>, UniLM <ref type="bibr" target="#b12">(Dong et al., 2019a)</ref>, ELECTRA <ref type="bibr" target="#b7">(Cl
get="#b0">Bar-Haim et al., 2006;</ref><ref type="bibr" target="#b14">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b2">Bentivogli et al., 2009;</ref><ref type="bibr" target="#b25">Levesque
al., 2015)</ref>  We evaluate DeBERTa on additional benchmarks: (1) Question Answering: SQuAD v1.1 <ref type="bibr" target="#b35">(Rajpurkar et al., 2016)</ref>, SQuAD v2.0 <ref type="bibr" target="# tasks. As shown in Table <ref type="table" target="#tab_7">6</ref>, it includes question answering <ref type="bibr" target="#b35">(Rajpurkar et al., 2016)</ref>, linguistic acceptability <ref type="b 17)</ref>.</p><p>â SQuAD v1.1/v2.0 is the Stanford Question Answering Dataset (SQuAD) v1.1 and v2.0 <ref type="bibr" target="#b35">(Rajpurkar et al., 2016;</ref><ref type="bibr">2018)</ref> are popula
t="#b14">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b2">Bentivogli et al., 2009;</ref><ref type="bibr" target="#b25">Levesque et al., 2012;</ref><ref type="bibr" target="#b48">Williams e
as on another, thus allowing for much more parallelization than RNNs for large-scale model training <ref type="bibr" target="#b44">(Vaswani et al., 2017)</ref>. Since 2018, we have seen the rise of a MER STRUCTURE</head><p>A Transformer-based language model is composed of stacked Transformer blocks <ref type="bibr" target="#b44">(Vaswani et al., 2017)</ref>. Each block contains a multi-head self-a n our implementation.</p><p>Taking single-head attention as an example, the standard self-attention <ref type="bibr" target="#b44">(Vaswani et al., 2017)</ref> can be formulated as:</p><formula xml:id its content and position. The positional bias can be implemented using absolute position embedding <ref type="bibr" target="#b44">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b33">Radford et Finally, we apply a scaling factor of 1 ? 3d on Ã which is important for stabilizing model training <ref type="bibr" target="#b44">Vaswani et al. (2017)</ref>, especially for large-scale PLMs.</p></di
osed approach is more practical because the training data is generally inaccessible to the attacker <ref type="bibr" target="#b31">[32]</ref>. Our contributions can be summarized as follows:</p><p>â¢ W original training data. However, in practice the attacker often has no access to the training data <ref type="bibr" target="#b31">[32]</ref>. To overcome this limitation, Mopuri et al. propose to gen ome this limitation, Mopuri et al. propose to generate universal perturbation without training data <ref type="bibr" target="#b31">[32]</ref>. However, their approach is specifically designed for non- -agnostic) attacks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar
get="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5]</ref> and universal (i.e. image-agnostic) attacks <ref type="bibr" ariety of works ranging from optimization based techniques <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5]</ref> to FGSM related techniques <ref type="bibr" target="#b13">[14 ng on data availability. Note that similar techniques of clamping the logits have also been used in <ref type="bibr" target="#b4">[5]</ref>, however, their motivation is to obtain minimum-magnitude (i '. Â± 0.6 55.4 Â± 1.0 70.8 Â± 1.5 55.2 Â± 2.2 89.1 Â± 0.3 75.9 Â± 0.9 87.9 Â± 0.5 70.8 Â± 1.1 78.2 Â± 0.9 66.<ref type="bibr" target="#b4">5</ref> Â± 1.3 LL 89.2 Â± 0.4 47.1 Â± 1.1 71.6 Â± 0.8 56.9 Â± 1.1 91.0 Â± 0.

ta-free non-targeted universal attack by training a network to generate proxy images is explored in <ref type="bibr" target="#b37">[38]</ref> . No prior work is found to have achieved targeted univers ining, which have also been adopted in the context of data-free universal adversarial perturbations <ref type="bibr" target="#b37">[38]</ref>. Mopuri et al. train a generator network for crafting UAPs
DNNs of high-dimensional inputs can be trained in practice <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. One recent work <ref type="bibr" target="#b16">[17]</ref>
e="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44]</ref> to robustness under noise <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta
get="#b43">44,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3]</ref> explore the reason for the existence of adversarial examples, not fully explain the phenomenon that greater robustness is not observed in less linear classifiers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" targ
As a basic example, we show the logit vector analysis of two randomly sampled images from ImageNet <ref type="bibr" target="#b40">[41]</ref> in Figure <ref type="figure" target="#fig_1">2</ref>. The
f type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48]</ref> to motion regression <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47]</ref>. However, DNNs are a
target="#b36">37]</ref>. A wide variety of previous works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" tar reater robustness is not observed in less linear classifiers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. Another body of wo
adversarial robustness to highdimensional input properties <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" targ
cal applications pertaining to fairness, privacy, and safety <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40]</ref>. For example, we can train a GNN model to predict the effec on interpreting GNNs at the model-level. The existing study <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref> only provides example-level explanations for graph models. tudies focusing on the interpretability of deep graph models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type have no baseline to compare with. Note that existing studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref> only focus on interpreting GNNs at example-level while igno [4,</ref><ref type="bibr" target="#b39">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type="bibr" target="#b39">[40]</ref> proposes to explain deep graph models at the example-level
agation (GBP) <ref type="bibr" target="#b32">[33]</ref>, and layer-wise relevance propagation (LRP) <ref type="bibr" target="#b2">[3]</ref>. The SA and GBP methods are based on the gradients while the
target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>. Different motif sets can be found in graphs with different
target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>. Different motif sets can be found in graphs with different
graph convolution <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>, graph pooling <ref type="bibr" target="#b19">[20,</ref><re matrices. Even though there are several variants of GNNs, such as graph convolution networks (GCNs) <ref type="bibr" target="#b18">[19]</ref>, graph attention networks (GATs) <ref type="bibr" target="
/ref> or model-level <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> methods. Example-level interpretations explain the predicti Input optimization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> is the most popul ptimization methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. These methods ge input optimization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" t
ology, and engineering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>. Different motif se t motif sets can be found in graphs with different functions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>, which means different motifs may directly relate to the fu
get="#b34">[35]</ref> to train the generator. According to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref>, the loss function for the action a t at step t can be math rmula_8">10</ref>) is the final graph reward for G t +1 which can be obtained by performing Rollout <ref type="bibr" target="#b41">[42]</ref> m times on the intermediate graph G t +1 . Each time, a fi
ards different graph operations, such as graph convolution <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>, graph pooling <ref
ology, and engineering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>. Different motif se t motif sets can be found in graphs with different functions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>, which means different motifs may directly relate to the fu
mplemented using Pytorch <ref type="bibr" target="#b26">[27]</ref> and trained using Adam optimizer <ref type="bibr" target="#b17">[18]</ref>. The training accuracies of these models are reported in T mplemented using Pytorch <ref type="bibr" target="#b26">[27]</ref> and trained using Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with Î² 1 = 0.9 and Î² 2 = 0.999.  In each row, from left to
imilarity measure. A is the action space, f is the reward function, and T is the terminal condition <ref type="bibr" target="#b35">[36]</ref>. Given an initial p (l ) r , the neighbor selector choose
><ref type="bibr" target="#b38">39]</ref>, financial fraud <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37]</ref>, mobile fraud <ref from the same user <ref type="bibr" target="#b24">[25]</ref> or transactions from the same devices <ref type="bibr" target="#b23">[24]</ref>. The graph-based fraud detection problem is a semi-supervi ibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48]</ref> or devise weighting parameters <ref type="bibr" target="#b23">[24]</ref> to learn the relation weights during aggregating informati ean, and they differ from each other in Attention <ref type="bibr" target="#b33">[34]</ref>, Weight <ref type="bibr" target="#b23">[24]</ref>, and Mean <ref type="bibr" target="#b11">[12]</ref> inter- ghbor weights using LSTM and the attention mechanism <ref type="bibr" target="#b33">[34]</ref>. GEM <ref type="bibr" target="#b23">[24]</ref>, SemiGNN <ref type="bibr" target="#b36">[37]</ref>, ASA <r
ION</head><p>As Internet services thrive, they also incubate various kinds of fraudulent activities <ref type="bibr" target="#b13">[14]</ref>. Fraudsters disguise as regular users to bypass the anti-f
ture camouflage: smart fraudsters may adjust their behaviors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, add special characters in reviews <ref type="bibr" target=" ster.</p><p>Considering the agility of real-world fraudsters <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, designing GNN-based detectors that exactly capture these ca introduced various fraudster camouflage types from behavior <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> and semantic <ref type="bibr" target="#b14">[15,</ref><ref t
tect opinion fraud <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>, financial fraud <ref type="bibr" target="#b22">[23,</ref>< fraud detectors transfer the heterogeneous data into homogeneous data before applying GNNs. Fdgars <ref type="bibr" target="#b38">[39]</ref> and GraphConsis <ref type="bibr" target="#b24">[25]</ref>
f><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>, financial fraud <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" ta et="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref> or a transaction in the trading system <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref>. The node has a la egate the neighbor information from different relations. Previous methods adopt attention mechanism <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta AGE <ref type="bibr" target="#b11">[12]</ref> to represent general GNN models. We choose Ge-niePath <ref type="bibr" target="#b22">[23]</ref>, Player2Vec <ref type="bibr" target="#b47">[48]</ref>, Sem -graph based on multiple relations and employ GNNs to aggregate neighborhood information. GeniePath <ref type="bibr" target="#b22">[23]</ref> learns convolutional layers and neighbor weights using LST
e proposed other fraud datasets like Epinions <ref type="bibr" target="#b17">[18]</ref> and Bitcoin <ref type="bibr" target="#b39">[40]</ref>, they only contain graph structures and compacted features
hich have been drawing great attention from both researchers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar e="bibr" target="#b40">41]</ref> (so-called spamouflage), or employ deep language generation models <ref type="bibr" target="#b14">[15]</ref> to gloss over explicit suspicious outcomes. Like Figure <r easures could not identify the camouflaged fake reviews, which are even indistinguishable by humans <ref type="bibr" target="#b14">[15]</ref>. Therefore, we need a parameterized similarity measure to ehavior <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> and semantic <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41]</ref> perspectives. Thos
rom both researchers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">49]</ref> and practitioners < ture-based detectors <ref type="bibr" target="#b40">[41]</ref>. Relation camouflage: previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">49]</ref> show that crowd wo
d methods have become an effective approach in both academic <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38]</ref> and industrial comm
fitting, we employ mini-batch training <ref type="bibr" target="#b10">[11]</ref> and under-sampling <ref type="bibr" target="#b21">[22]</ref> techniques to train CARE-GNN and other baselines. Specific
NN architectures, which we refer to as Attention MPNN (AMPNN) and Edge Memory Neural Network (EMNN) <ref type="bibr" target="#b36">[34]</ref>, and evaluate them against published benchmark results wit
on directed, undirected, labelled, and cyclic graphs. This work was later expanded upon by Micheli <ref type="bibr" target="#b14">[12]</ref> and Scarselli et al. <ref type="bibr" target="#b15">[13]</
or activity predictions without the risk of reverse-engineering IP-sensitive structural information <ref type="bibr" target="#b33">[31]</ref><ref type="bibr" target="#b34">[32]</ref><ref type="bibr" t
es to generating representations using the graph representation of a molecule include graph kernels <ref type="bibr" target="#b9">[7]</ref>, and perhaps most importantly in the present context, ECFP (
l molecule are also desired for their potential use in, amongst other things, drug design. Duvenaud <ref type="bibr" target="#b26">[24]</ref> proposed the neural fingerprint method, describing it as a d as a framework to generalise several proposed techniques <ref type="bibr" target="#b16">[14,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" tar
to improve its efficiency from a drug discovery perspective. After Hansch proposed the QSAR concept <ref type="bibr" target="#b3">[1]</ref>, engineering molecular descriptors to build accurate models
cs. The EMNN network shares architectural similarities to the D-MPNN model published by Yang et al. <ref type="bibr" target="#b37">[35]</ref> that was developed concurrently to this work <ref type="bi ction. This model shares underlying principles with the D-MPNN architecture proposed by Yang et al. <ref type="bibr" target="#b37">[35]</ref> which also uses directed edges to improve MPNN performance
k was later expanded upon by Micheli <ref type="bibr" target="#b14">[12]</ref> and Scarselli et al. <ref type="bibr" target="#b15">[13]</ref> In 2013, the Graph Convolutional Network (GCN) was present
ems amongst others <ref type="bibr" target="#b43">[41,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b48">46]</ref> and have achieved better performance compared with normal d
or activity predictions without the risk of reverse-engineering IP-sensitive structural information <ref type="bibr" target="#b33">[31]</ref><ref type="bibr" target="#b34">[32]</ref><ref type="bibr" t
forms effectively pick up on common structural features and learn them as reported in other studies <ref type="bibr" target="#b63">[62,</ref><ref type="bibr" target="#b64">63]</ref>. However, in the c
variety of relevance match signals and shows strong performance in various ad-hoc retrieval dataset <ref type="bibr" target="#b3">(Dai and Callan, 2019)</ref>. Recent research also has shown kernels c
ER score, significantly outperforming previous BERT and Graph Neural Network (GNN) based approaches <ref type="bibr" target="#b34">(Zhou et al., 2019)</ref>. Our experiments demonstrate KGAT's strong 2018)</ref> further incorporates evidence identification to improve claim verification.</p><p>GEAR <ref type="bibr" target="#b34">(Zhou et al., 2019)</ref> formulates claim verification as a graph re tei-c.org/ns/1.0"><head n="3.1">Reasoning with Evidence Graph</head><p>Similar to previous research <ref type="bibr" target="#b34">(Zhou et al., 2019)</ref>, KGAT constructs the evidence graph G by us narios and produces a probability P (y|c, D) to predict claim label y. Different from previous work <ref type="bibr" target="#b34">(Zhou et al., 2019)</ref>, we follow the standard graph label predict sentation v p . The aggregation is done by a graph attention mechanism, the same with previous work <ref type="bibr" target="#b34">(Zhou et al., 2019)</ref>.</p><p>It first calculate the attention wei n</head><p>The per-node predictions are combined by the "readout" function in graph neural networks <ref type="bibr" target="#b34">(Zhou et al., 2019)</ref>, where KGAT uses node kernels to learn the ds without pre-training. BERT-pair, BERT-concat and GEAR are three baselines from the previous work <ref type="bibr" target="#b34">(Zhou et al., 2019)</ref>. BERT-pair and BERTconcat regard claim-evid eriments are all based on ESIM sentence retrieval, which is the one used by GEAR, our main baseline <ref type="bibr" target="#b34">(Zhou et al., 2019)</ref>.</p></div> <div xmlns="http://www.tei-c.org head n="6">Case Study</head><p>Table <ref type="table">5</ref> shows the example claim used in GEAR <ref type="bibr" target="#b34">(Zhou et al., 2019)</ref> and the evidence sentences retrieved by ESI htweight backpacker, inventor, author and global adventurer. Label: SUPPORT Table5: An example claim<ref type="bibr" target="#b34">(Zhou et al., 2019)</ref> whose verification requires multiple pieces "bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b13">Li et al., 2019;</ref><ref type="bibr" target="#b34">Zhou et al., 2019;</ref><ref type="bibr" target="#b24">Soleimani et a d is kept the same with previous work <ref type="bibr" target="#b9">(Hanselowski et al., 2018;</ref><ref type="bibr" target="#b34">Zhou et al., 2019;</ref><ref type="bibr" target="#b24">Soleimani et a l keeps the same as the previous work <ref type="bibr" target="#b9">(Hanselowski et al., 2018;</ref><ref type="bibr" target="#b34">Zhou et al., 2019)</ref>. The base version of BERT is used to impleme KGAT is the best on all testing scenarios. With ESIM sentence retrieval, same as the previous work <ref type="bibr" target="#b34">(Zhou et al., 2019;</ref><ref type="bibr" target="#b9">Hanselowski et
https: //github.com/thunlp/KernelGAT.</p><p>2 https://competitions.codalab.org/ competitions/18814 <ref type="bibr" target="#b23">et al., 2008;</ref><ref type="bibr" target="#b11">Kipf and Welling, 2
words. This observation of the scattered dotproduct attention is consistent with previous research <ref type="bibr" target="#b2">(Clark et al., 2019)</ref>. As shown in the next case study, the edge ning is conducted. This seems to be a common challenge of the dot-product attention in Transformers <ref type="bibr" target="#b2">(Clark et al., 2019)</ref>.</p></div> <div xmlns="http://www.tei-c.org
. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label <ref type="bibr" target="#b15">(Luken et al., 2018;</ref><ref type="bibr" target="#b31">Yoneda et al
gets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia <ref type="bibr" target="#b25">(Thorne et al., 2018a)</ref>. For example, as shown in Figure <ref ty reason on the evidence graph for more accurate fact verification.</p><p>In our experiments on FEVER <ref type="bibr" target="#b25">(Thorne et al., 2018a)</ref>, a large-scale fact verification benchma v> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The FEVER shared task <ref type="bibr" target="#b25">(Thorne et al., 2018a)</ref> aims to develop automatic fact verificat rom NLP community.</p><p>Existing fact verification models usually employ FEVER's official baseline <ref type="bibr" target="#b25">(Thorne et al., 2018a)</ref> with a three-step pipeline system <ref t ieval models. When using BERT (Large) as the encoder, KGAT also out-Model Prec@5 Rec@5 F1@5 FEVER   <ref type="bibr" target="#b25">(Thorne et al., 2018a)</ref>.</p><p>performs the corresponding versio ion details in our experiments.</p><p>Dataset. A large scale public fact verification dataset FEVER <ref type="bibr" target="#b25">(Thorne et al., 2018a</ref>) is used in our experiments. The FEVER co
elling, 2017)</ref>. <ref type="bibr" target="#b33">Zhong et al. (2019)</ref> further employs XLNet <ref type="bibr" target="#b28">(Yang et al., 2019)</ref> and establishes a semantic-level graph for
="bibr" target="#b10">(Hu et al., 2014;</ref><ref type="bibr" target="#b19">Pang et al., 2016;</ref><ref type="bibr" target="#b8">Guo et al., 2016;</ref><ref type="bibr" target="#b27">Xiong et al., 20
sformer, has also been used for better text representation in FEVER and achieved better performance <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b13">Li et al., 2 atenated sequence of claim, document (Wiki) title, and evidence sentence, to pre-trained BERT model <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>. Specifically, in the node n p , the claim
="bibr" target="#b8">Guo et al., 2016;</ref><ref type="bibr" target="#b27">Xiong et al., 2017;</ref><ref type="bibr" target="#b4">Dai et al., 2018)</ref>. One of the effective ways to model text match xt matches is to leverage matching kernels <ref type="bibr" target="#b27">(Xiong et al., 2017;</ref><ref type="bibr" target="#b4">Dai et al., 2018)</ref>, which summarize word or phrase interactions i </p><p>) from the translation matrix M qâp <ref type="bibr" target="#b27">(Xiong et al., 2017;</ref><ref type="bibr" target="#b4">Dai et al., 2018;</ref><ref type="bibr" target="#b22">Qiao et al., 201
. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label <ref type="bibr" target="#b15">(Luken et al., 2018;</ref><ref type="bibr" target="#b31">Yoneda et al
or NOT ENOUGH INFO by annotators. The dataset partition is kept the same with the FEVER Shared Task <ref type="bibr" target="#b26">(Thorne et al., 2018b)</ref> as shown in Table <ref type="table" targ
elling, 2017)</ref>. <ref type="bibr" target="#b33">Zhong et al. (2019)</ref> further employs XLNet <ref type="bibr" target="#b28">(Yang et al., 2019)</ref> and establishes a semantic-level graph for
sformer, has also been used for better text representation in FEVER and achieved better performance <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b13">Li et al., 2 atenated sequence of claim, document (Wiki) title, and evidence sentence, to pre-trained BERT model <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>. Specifically, in the node n p , the claim
sformer, has also been used for better text representation in FEVER and achieved better performance <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b13">Li et al., 2 atenated sequence of claim, document (Wiki) title, and evidence sentence, to pre-trained BERT model <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>. Specifically, in the node n p , the claim
CLS]" hidden state to ranking score. Pairwise loss is used to optimize the ranking model. Some work <ref type="bibr" target="#b32">(Zhao et al., 2020;</ref><ref type="bibr" target="#b29">Ye et al., 20
ps://competitions.codalab.org/ competitions/18814 <ref type="bibr" target="#b23">et al., 2008;</ref><ref type="bibr" target="#b11">Kipf and Welling, 2017)</ref>. <ref type="bibr" target="#b33">Zhong e
ve shown promising effectiveness in extracting soft match patterns from query-document interactions <ref type="bibr" target="#b10">(Hu et al., 2014;</ref><ref type="bibr" target="#b19">Pang et al., 20
. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label <ref type="bibr" target="#b15">(Luken et al., 2018;</ref><ref type="bibr" target="#b31">Yoneda et al

https: //github.com/thunlp/KernelGAT.</p><p>2 https://competitions.codalab.org/ competitions/18814 <ref type="bibr" target="#b23">et al., 2008;</ref><ref type="bibr" target="#b11">Kipf and Welling, 2
"bibr" target="#b27">(Xiong et al., 2017;</ref><ref type="bibr" target="#b4">Dai et al., 2018;</ref><ref type="bibr" target="#b22">Qiao et al., 2019;</ref><ref type="bibr" target="#b16">MacAvaney et a g rate = 5e-5 and warm up proportion = 0.1. The kernel size is set to 21, the same as previous work <ref type="bibr" target="#b22">(Qiao et al., 2019)</ref>.</p></div> <div xmlns="http://www.tei-c.org


The dominant approach to parametric text generation is based on large neural auto-regressive models <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>. These models can be trained efficiently to-regressive language models trained on large datasets have improved significantly in recent years <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>. In general however, residual learning a
EBM, a transformer language model with 12 layers, h = 16, d model = 1024, d f f = 4096 (we refer to <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref> for notations). This is also our first ba itecture We consider two architectures for our residual EBM, both of them are based on transformers <ref type="bibr" target="#b31">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b4">Devlin et a
et al., 2003;</ref><ref type="bibr" target="#b5">Du &amp; Mordatch, 2019)</ref> and Gibbs sampling <ref type="bibr" target="#b37">(Welling et al., 2005)</ref> is too slow to be practical. Generating

t="#b27">Rosenfeld et al. (2001)</ref>; <ref type="bibr" target="#b34">Wang &amp; Ou (2018b)</ref>; <ref type="bibr" target="#b23">Parshakova et al. (2019)</ref>.</p><p>This formulation has multi-fold r">2017;</ref><ref type="bibr">Wang &amp; Ou, 2017;</ref><ref type="bibr" target="#b33">2018a;</ref><ref type="bibr" target="#b23">Parshakova et al., 2019)</ref>. In particular, our residual modeling he same formulation has been proposed in <ref type="bibr" target="#b34">(Wang &amp; Ou, 2018b;</ref><ref type="bibr" target="#b23">Parshakova et al., 2019)</ref>. While <ref type="bibr" target="#b19"> pretrained autoregressive language model <ref type="bibr" target="#b34">(Wang &amp; Ou, 2018b;</ref><ref type="bibr" target="#b23">Parshakova et al., 2019)</ref>. The resulting joint model scores sequ
model distribution, which is usually approximated with Monte Carlo sampling or mean field inference <ref type="bibr" target="#b12">(Hinton, 2012;</ref><ref type="bibr" target="#b17">LeCun et al., 2006

"bibr" target="#b38">Xie et al., 2016;</ref><ref type="bibr">2017;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b22">2018;</ref><ref type="bibr" target="#b7">Gao et al., 2018;</ref><ref </formula><p>, we derive two estimators for the log-partition function log Z Î¸ based on the work of <ref type="bibr" target="#b22">Nowozin (2018)</ref>.</p><formula xml:id="formula_6">Theorem 2. Denot to note that to get lower variance estimates we use leave-one-out strategy to estimate T nâ1 . See <ref type="bibr" target="#b22">Nowozin (2018)</ref> for implementation details and methods to improv â &lt; E[T n ] &lt; Z Î¸ &lt; E[(2n â 1)T n â 2(n â 1)T nâ1 ] &lt; Z Î¸ + (7)</formula><p>Proof. From <ref type="bibr" target="#b22">Nowozin (2018)</ref> Eq. 35, we can write E[T n ] as</p><formula xml:
s have been used for sequence modeling <ref type="bibr" target="#b27">(Rosenfeld et al., 2001;</ref><ref type="bibr" target="#b35">Wang et al., 2015;</ref><ref type="bibr">2017;</ref><ref type="bibr">
f><ref type="bibr">2017;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b22">2018;</ref><ref type="bibr" target="#b7">Gao et al., 2018;</ref><ref type="bibr" target="#b21">Nijkamp et al.,
model distribution, which is usually approximated with Monte Carlo sampling or mean field inference <ref type="bibr" target="#b12">(Hinton, 2012;</ref><ref type="bibr" target="#b17">LeCun et al., 2006
The dominant approach to parametric text generation is based on large neural auto-regressive models <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>. These models can be trained efficiently to-regressive language models trained on large datasets have improved significantly in recent years <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>. In general however, residual learning a
Several variants of auto-encoders have also been investigated for representing and generating text <ref type="bibr" target="#b3">(Bowman et al., 2016;</ref><ref type="bibr">Zhao et al., 2018)</ref>,
The dominant approach to parametric text generation is based on large neural auto-regressive models <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>. These models can be trained efficiently to-regressive language models trained on large datasets have improved significantly in recent years <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>. In general however, residual learning a
e at test time it is conditioned on its own generations, a discrepancy referred to as exposure bias <ref type="bibr" target="#b26">(Ranzato et al., 2016)</ref>. Finally, while heuristics like beam sea
"bibr" target="#b38">Xie et al., 2016;</ref><ref type="bibr">2017;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b22">2018;</ref><ref type="bibr" target="#b7">Gao et al., 2018;</ref><ref </formula><p>, we derive two estimators for the log-partition function log Z Î¸ based on the work of <ref type="bibr" target="#b22">Nowozin (2018)</ref>.</p><formula xml:id="formula_6">Theorem 2. Denot to note that to get lower variance estimates we use leave-one-out strategy to estimate T nâ1 . See <ref type="bibr" target="#b22">Nowozin (2018)</ref> for implementation details and methods to improv â &lt; E[T n ] &lt; Z Î¸ &lt; E[(2n â 1)T n â 2(n â 1)T nâ1 ] &lt; Z Î¸ + (7)</formula><p>Proof. From <ref type="bibr" target="#b22">Nowozin (2018)</ref> Eq. 35, we can write E[T n ] as</p><formula xml:
"bibr" target="#b38">Xie et al., 2016;</ref><ref type="bibr">2017;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b22">2018;</ref><ref type="bibr" target="#b7">Gao et al., 2018;</ref><ref </formula><p>, we derive two estimators for the log-partition function log Z Î¸ based on the work of <ref type="bibr" target="#b22">Nowozin (2018)</ref>.</p><formula xml:id="formula_6">Theorem 2. Denot to note that to get lower variance estimates we use leave-one-out strategy to estimate T nâ1 . See <ref type="bibr" target="#b22">Nowozin (2018)</ref> for implementation details and methods to improv â &lt; E[T n ] &lt; Z Î¸ &lt; E[(2n â 1)T n â 2(n â 1)T nâ1 ] &lt; Z Î¸ + (7)</formula><p>Proof. From <ref type="bibr" target="#b22">Nowozin (2018)</ref> Eq. 35, we can write E[T n ] as</p><formula xml:
EBM, a transformer language model with 12 layers, h = 16, d model = 1024, d f f = 4096 (we refer to <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref> for notations). This is also our first ba itecture We consider two architectures for our residual EBM, both of them are based on transformers <ref type="bibr" target="#b31">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b4">Devlin et a
/ref><ref type="bibr" target="#b22">2018;</ref><ref type="bibr" target="#b7">Gao et al., 2018;</ref><ref type="bibr" target="#b21">Nijkamp et al., 2019)</ref> are not applicable when the input is disc
s, both in terms of perplexity, and through human evaluation.</p><p>Generative Adversarial Networks <ref type="bibr" target="#b8">(Goodfellow et al., 2014</ref>) also relate to EBMs, except that in EB
t="#b54">[53]</ref>. (iii) Designing a hardware for a fixed modest-sized parameter, e.g., n = 2 12  <ref type="bibr" target="#b55">[54]</ref>. However, encryption parameters determine the security-lev le-threaded Intel Xeon(R) Silver 4108 running at 1.80 GHz; which is a similar CPU used in prior art <ref type="bibr" target="#b55">[54]</ref>. The single-thread baseline is used by prior art for measu >. The single-thread baseline is used by prior art for measuring the performance (non-CKKS schemes) <ref type="bibr" target="#b55">[54]</ref>. In addition, SEAL is thread-safe but not multithreaded du ior performance compared to CPU execution.</p><p>Perhaps, the closest work to ours is by Roy et al. <ref type="bibr" target="#b55">[54]</ref> in which authors propose an architecture for BFV scheme an EAL running on Intel Xeon Silver 4108 at 1.8 GHz (note that similar processor is used compared with <ref type="bibr" target="#b55">[54]</ref> running at identical frequency).</p><p>FPGA-based Co-Proce and high throughput.</p><p>As has been shown by prior art <ref type="bibr" target="#b54">[53,</ref><ref type="bibr" target="#b55">54]</ref>, leveraging off-chip memory to store intermediate results s
OS <ref type="bibr">'20, March 16-20, 2020</ref>, Lausanne, Switzerland Most FHE schemes, i.e., BGV <ref type="bibr" target="#b11">[11]</ref>, BFV <ref type="bibr" target="#b31">[31]</ref>, and TFHE < >[60]</ref>. In <ref type="bibr" target="#b58">[58]</ref>, a GPU-based implementation of BGV scheme <ref type="bibr" target="#b11">[11]</ref> is introduced. In <ref type="bibr" target="#b6">[6]</ref>,
p memory, authors propose to leverage the idea of Cached-NTT <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref> to reduce off-chip memory accesses. In contrast, HEAX relies
GPU-based Acceleration. GPU is an alternative computing platform to accelerate evaluation functions <ref type="bibr" target="#b6">[6,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b23">24,< GPU-based implementation of BGV scheme <ref type="bibr" target="#b11">[11]</ref> is introduced. In <ref type="bibr" target="#b6">[6]</ref>, a comprehensive study is reported for multithreaded CPU exe
pe="bibr" target="#b52">[51]</ref> to the extent that it can be worse than naive software execution <ref type="bibr" target="#b54">[53]</ref>. (iii) Designing a hardware for a fixed modest-sized param d GPU-based acceleration for non-CKKS schemes.</p><p>Hardware Acceleration for non-CKKS Schemes. In <ref type="bibr" target="#b54">[53]</ref>, a system based on FPGA is proposed for BFV scheme to proc s of capacity but very fast response time and high throughput.</p><p>As has been shown by prior art <ref type="bibr" target="#b54">[53,</ref><ref type="bibr" target="#b55">54]</ref>, leveraging off-ch
ng comes with significant risks that have been analyzed in the literature over the last decade (see <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" ta
to accelerate evaluation functions <ref type="bibr" target="#b6">[6,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" tar
get="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b61">61]</ref>; high-level operati morphic operations <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" tar
#b50">49,</ref><ref type="bibr" target="#b51">50]</ref> focus on improving the performance of YASHE <ref type="bibr" target="#b10">[10]</ref> and LTV <ref type="bibr" target="#b46">[45]</ref> schemes
target="#b59">[59]</ref> have proposed the first GPU acceleration of FHE that targets Gentry-Halevi <ref type="bibr" target="#b35">[34]</ref> scheme. Subsequent improvements are reported in <ref type=
ry-Halevi <ref type="bibr" target="#b35">[34]</ref> scheme. Subsequent improvements are reported in <ref type="bibr" target="#b60">[60]</ref>. In <ref type="bibr" target="#b58">[58]</ref>, a GPU-based
t="#b16">Phang et al., 2018)</ref>.</p><p>When finetuning a big, pretrained language model, dropout <ref type="bibr" target="#b20">(Srivastava et al., 2014)</ref> has been used as a regularization tec er of that neuron as w during training, then we use (1 â p)w for that weight parameter at test time <ref type="bibr" target="#b20">(Srivastava et al., 2014)</ref>. This ensures that the expected outpu ght decay of Î» is equivalent to wdecay(0, Î»).</p><p>Probability for Dropout and Dropconnect Dropout <ref type="bibr" target="#b20">(Srivastava et al., 2014</ref>) is a regularization technique selecti

et al., 2014)</ref> has been used as a regularization technique to prevent co-adaptation of neurons <ref type="bibr" target="#b21">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b6">Devlin et a ww.tei-c.org/ns/1.0"><head n="1.1">RELATED WORK</head><p>For large-scale pretrained language models <ref type="bibr" target="#b21">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b6">Devlin et a


ian dropout <ref type="bibr" target="#b24">(Wang &amp; Manning, 2013)</ref> and variational dropout <ref type="bibr" target="#b11">(Kingma et al., 2015)</ref> use other random masks to improve dropout
ab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b26">Wiese et al. (2017)</ref>,<ref type="bibr" target="#b12">Kirkpatrick et al. (2017)</ref>, and<ref type="bibr" target="#b17">Sc
ian dropout <ref type="bibr" target="#b24">(Wang &amp; Manning, 2013)</ref> and variational dropout <ref type="bibr" target="#b11">(Kingma et al., 2015)</ref> use other random masks to improve dropout



rg/ns/1.0"><head n="1">Introduction</head><p>The majority of the research efforts on improving VAEs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> is dedicated to the st e posterior up to the (l â 1) th group. The objective is trained using the reparameterization trick <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>The main questi and bidirectional encoder networks <ref type="bibr" target="#b3">[4]</ref>.</p><p>The goal of VAEs <ref type="bibr" target="#b0">[1]</ref> is to train a generative model in the form of p(x x x, z z z
et="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, reducing the gradient noise <ref type="bibr" target="#b14"
ad><p>The majority of the research efforts on improving VAEs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> is dedicated to the statistical challenges, such as reducing The objective is trained using the reparameterization trick <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>The main question here is how to implement the conditi convolution operating in the same regime has k 2 C parameters and O(k 2 C) complexity per location.<ref type="bibr" target="#b1">2</ref> Swish cannot be done in place and it requires additional memor
target="#fig_3">4</ref> visualizes the samples generated by NVAE along with the samples from MaCow <ref type="bibr" target="#b65">[66]</ref> and Glow <ref type="bibr" target="#b60">[61]</ref> on Cele GPU, we can sample a batch of 36 images of the size 256Ã256 px in 2.03 seconds (56 ms/image). MaCow <ref type="bibr" target="#b65">[66]</ref> reports 434.2 ms/image in a similar batched-sampling exper t="#b65">[66]</ref> reports 434.2 ms/image in a similar batched-sampling experiment (â¼ 8Ã slower).  <ref type="bibr" target="#b65">[66]</ref> and Glow <ref type="bibr" target="#b60">[61]</ref> are sho Sampled images from NVAE with the temperature in prior (t). (f)-(g) A few images generated by MaCow<ref type="bibr" target="#b65">[66]</ref> and Glow<ref type="bibr" target="#b60">[61]</ref> are show
ing tighter bounds <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, reducing the gradi


xpressivity as they operate in each channel separately. To tackle this issue, following MobileNetV2 <ref type="bibr" target="#b44">[45]</ref>, we apply these convolutions after expanding the number of isualized in Fig. <ref type="figure" target="#fig_2">3(a)</ref>. Our cell is similar to MobileNetV2 <ref type="bibr" target="#b44">[45]</ref>, with three crucial differences; It has two additional BN ="#fig_2">3a</ref> requires setting an expansion ratio E.</p><p>We use E = 6 similar to MobileNetV2 <ref type="bibr" target="#b44">[45]</ref>. In a few cells, we set E = 3 to reduce the memory. Please
et="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, reducing the gradient noise <ref type="bibr" target="#b14"
nt requirements. First, VAEs maximize the mutual information between the input and latent variables <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, requiring the net
in building block of our network is depthwise convolutions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> that rapidly increase the receptive field of the network wi
><p>Inspired by the success of augmentation methods in ASR <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, as a remedy to avoid overfitting while using lowresource t
target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> have inspired the /ref>. We only focus on LSTMbased models and leave the transformer architecture as our future study <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" tar
br" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> 4 and the IWSLT TED-talks EnâDe<ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> 5 . The training d
ave inspired the end-to-end direct ST models which can be trained using a translation speech corpus <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Some appealing ad
as our future study <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. An abstract overview of the network and a summary of the m >[30]</ref> 12.2 Transformer <ref type="bibr" target="#b19">[20]</ref> 13.8 Transformer+pretraining <ref type="bibr" target="#b20">[21]</ref> 14.3 + knowledge distillation <ref type="bibr" target="#b2 3.8 Transformer+pretraining <ref type="bibr" target="#b20">[21]</ref> 14.3 + knowledge distillation <ref type="bibr" target="#b20">[21]</ref> 17.0 this work direct+pretraining+SpecAugment 2 17.0</p></
els which can be trained using a translation speech corpus <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Some appealing advantages of the direct models are: <ref t
dequate volume of training data, one remedy is generating synthetic data like back-translation (BT) <ref type="bibr" target="#b13">[14]</ref> as the most common data augmentation method to leverage mo
#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> and MT <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" targe -</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3">insensitive<ref type="bibr" target="#b6">7</ref> . The translation models are evaluated using the official scri
#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> and MT <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" targe -</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3">insensitive<ref type="bibr" target="#b6">7</ref> . The translation models are evaluated using the official scri
as our future study <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. An abstract overview of the network and a summary of the m >[30]</ref> 12.2 Transformer <ref type="bibr" target="#b19">[20]</ref> 13.8 Transformer+pretraining <ref type="bibr" target="#b20">[21]</ref> 14.3 + knowledge distillation <ref type="bibr" target="#b2 3.8 Transformer+pretraining <ref type="bibr" target="#b20">[21]</ref> 14.3 + knowledge distillation <ref type="bibr" target="#b20">[21]</ref> 17.0 this work direct+pretraining+SpecAugment 2 17.0</p></
ined on bilingual text data. Recent advancements in both ASR <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" targe
ef>, the number of authors (i.e., cluster size) is usually a pre-specified parameter. Current works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> did not efficiently ised <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, unsupervised <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref> and graph-based one of author mentions belonging to the same author and are essential in the name disambiguation task. <ref type="bibr" target="#b5">[6]</ref> first learns representation for every name mention in a pair ative attribute to separate papers into small blocks and we use the same trainset and testset as in <ref type="bibr" target="#b5">[6]</ref>.</p><p>In Semantic Scholar, the selected meta-paths of our m block as sequence s â S; 3 Construct meta-path based view {G p1</figDesc><table /><note>â¢ Aminer-AND<ref type="bibr" target="#b5">[6]</ref>: This dataset contains 70,285 records of 12,798 unique autho
size) is usually a pre-specified parameter. Current works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> did not efficiently handle the change of discriminative attr y a graph auto-encoder, but this method neglects linkage between paper and author and coauthorship. <ref type="bibr" target="#b6">[7]</ref> addresses the pairwise classification problem by extracting
s by aggregating their features. Both DeepGL and GraphSage are designed for homogeneous graphs. LAN <ref type="bibr" target="#b14">[15]</ref> aggregates neighbors with both rule-based and network-base

diverse semantic information of node type and edge type <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. GTN <ref type="bibr" target="#b22">[23]</ref> converts he
Name disambiguation methods can be divided into supervised <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, unsupervised <ref type="bibr" target="#b5">[6]</ref>, <ref
nt years. Meta-path is designed to preserve diverse semantic information of node type and edge type <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. GTN <ref type=
perceptron that directly projecting input features into a low dimensional vector.</p><p>â¢ Deepwalk <ref type="bibr" target="#b25">[26]</ref>: Deepwalk captures contextual information of neighborhood

to effective representation ability. While most GNN works <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref> focus on transductive setting, there have been some recent
sional vector while preserving graph structure and properties. Recently, Graph Neural Network (GNN) <ref type="bibr" target="#b9">[10]</ref> has attracted rising attention due to effective representat </ref> has attracted rising attention due to effective representation ability. While most GNN works <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref> focus on transdu ne score function is applied to measure the similarity of the two paper sequence representations as <ref type="bibr" target="#b9">(10)</ref>.</p><p>L sim = sim(h (1) , h (2) ) = h (1) â¢ h (2)  h (1) â¢
a set of base graph features by relational functions that can generalize across networks. GraphSage <ref type="bibr" target="#b13">[14]</ref> samples a fixed number of neighbors and generate node embe s potential to add new nodes dynamically.</p><p>For each meta-path based view, similar to GraphSage <ref type="bibr" target="#b13">[14]</ref>, node representations are generated by aggregating feature neighborhood via uniform random walks for node embedding in homogeneous network.</p><p>â¢ GraphSage <ref type="bibr" target="#b13">[14]</ref>: GraphSage samples node neighborhoods to generate node emb
edge type <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. GTN <ref type="bibr" target="#b22">[23]</ref> converts heterogeneous graph to new graph structures which
and network-based attention weights for knowledge graphs.</p><p>Heterogeneous information networks <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref> have been studi
a set of base graph features by relational functions that can generalize across networks. GraphSage <ref type="bibr" target="#b13">[14]</ref> samples a fixed number of neighbors and generate node embe s potential to add new nodes dynamically.</p><p>For each meta-path based view, similar to GraphSage <ref type="bibr" target="#b13">[14]</ref>, node representations are generated by aggregating feature neighborhood via uniform random walks for node embedding in homogeneous network.</p><p>â¢ GraphSage <ref type="bibr" target="#b13">[14]</ref>: GraphSage samples node neighborhoods to generate node emb
edge type <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. GTN <ref type="bibr" target="#b22">[23]</ref> converts heterogeneous graph to new graph structures which
nt years. Meta-path is designed to preserve diverse semantic information of node type and edge type <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. GTN <ref type=
owledge graphs.</p><p>Heterogeneous information networks <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref> have been studied in recent years. Meta-path is designed t
Fraser University. The change of discriminative attributes may lead to the paper separation problem <ref type="bibr" target="#b3">[4]</ref>, i.e., papers of an author are regarded as belonging to diff

sional vector while preserving graph structure and properties. Recently, Graph Neural Network (GNN) <ref type="bibr" target="#b9">[10]</ref> has attracted rising attention due to effective representat </ref> has attracted rising attention due to effective representation ability. While most GNN works <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref> focus on transdu ne score function is applied to measure the similarity of the two paper sequence representations as <ref type="bibr" target="#b9">(10)</ref>.</p><p>L sim = sim(h (1) , h (2) ) = h (1) â¢ h (2)  h (1) â¢
owledge graphs.</p><p>Heterogeneous information networks <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref> have been studied in recent years. Meta-path is designed t
ures</head><p>There have been a large number of works and debates on NIC offloading of TCP features <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" ta
wide-area networks <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b64">66]</ref>. L7 proxying is also widely used in middlebox applications
2]</ref>, Berkeley packet filter <ref type="bibr" target="#b47">[49]</ref>, or even keyvalue lookup <ref type="bibr" target="#b51">[53]</ref>, often achieving 2x to 3x performance improvement over CPU
2]</ref>, Berkeley packet filter <ref type="bibr" target="#b47">[49]</ref>, or even keyvalue lookup <ref type="bibr" target="#b51">[53]</ref>, often achieving 2x to 3x performance improvement over CPU
server while we port it to use AccelTCP for comparison. We test with the USR workload from Facebook <ref type="bibr" target="#b27">[29]</ref>, which consists of 99.8% GET requests and 0.2% SET request
rt Connections &amp; L7 Proxying</head><p>Short-lived TCP connections are prevalent in data centers <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b63">65]</ref> as well as in wide
rget="#b3">[5,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b57">59]</ref> or by steering the tasks into fast and slow paths <ref type e="bibr" target="#b61">[63]</ref>, Shinjuku <ref type="bibr" target="#b40">[42]</ref>, and Shenango <ref type="bibr" target="#b57">[59]</ref> further improve kernel-bypass stack by reducing the tail l
is also widely used in middlebox applications such as L7 LBs <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b34">36]</ref> and application-level gateways <ref type="bibr" target="#b1 and short RPCs: Our connection splicing is inspired by the packet tunneling mechanism of Yoda L7 LB <ref type="bibr" target="#b34">[36]</ref>. However, Yoda operates as a packet-level translator witho
it high parallelism on multicore systems by flow steering on NIC. More recently, systems like ZygOS <ref type="bibr" target="#b61">[63]</ref>, Shinjuku <ref type="bibr" target="#b40">[42]</ref>, and S
ost is as large as 60% of the entire CPU cycles (Section Â§2). An alternative might be to adopt RDMA <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b41">43]</ref> or a custom RPC pr
workloads, while we intentionally avoid the complexity of application data transfer offloading. UNO <ref type="bibr" target="#b50">[52]</ref> and Metron <ref type="bibr" target="#b43">[45]</ref> striv
h each other. Current state-of-the-art models highly depend on Graph Convoluational Networks (GCNs) <ref type="bibr" target="#b12">[13]</ref> originated from the theory of Graph Fourier Transform (GFT tp://www.tei-c.org/ns/1.0"><head>Spectral Graph Convolution</head><p>The Spectral Graph Convolution <ref type="bibr" target="#b12">[13]</ref> is composed of three steps.</p><p>(1) The multivariate tim
ized by 0.001 and decayed with rate 0.7 after every 5 epochs. We use the Mean Absolute Errors (MAE) <ref type="bibr" target="#b10">[11]</ref>, Mean Absolute Percentage Errors (MAPE) <ref type="bibr" t lute Errors (MAE) <ref type="bibr" target="#b10">[11]</ref>, Mean Absolute Percentage Errors (MAPE) <ref type="bibr" target="#b10">[11]</ref>, and Root Mean Squared Errors (RMSE) <ref type="bibr" targ entage Errors (MAPE) <ref type="bibr" target="#b10">[11]</ref>, and Root Mean Squared Errors (RMSE) <ref type="bibr" target="#b10">[11]</ref> to measure the performances, which are averaged by H steps
traffic, energy and electrocardiogram domains with other state-of-the-art models, including FC-LSTM <ref type="bibr" target="#b25">[26]</ref>, SFM <ref type="bibr" target="#b31">[32]</ref>, N-BEATS <r ></cell><cell cols="2">(b) Inter-country correlations</cell><cell></cell></row></table><note>FC-LSTM<ref type="bibr" target="#b25">[26]</ref> SFM<ref type="bibr" target="#b31">[32]</ref> N-BEATS<ref t
the theory of Graph Fourier Transform (GFT). These models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b16">17]</ref> stack GCN and temporal modules (e.g., LSTM, GRU) directly, variate techniques <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" tar <ref type="bibr" target="#b31">[32]</ref>, N-BEATS <ref type="bibr" target="#b18">[19]</ref>, DCRNN <ref type="bibr" target="#b16">[17]</ref>, LSTNet <ref type="bibr" target="#b13">[14]</ref>, ST-GCN
tions between different time-series <ref type="bibr" target="#b21">[22]</ref>. For example, FC-LSTM <ref type="bibr" target="#b29">[30]</ref> forecasts univariate time-series with LSTM and fully-conne
tirely as a tensor input and considers a large receptive field through dilated convolutions. LSTNet <ref type="bibr" target="#b13">[14]</ref> uses convolution neural network (CNN) and recurrent neural <ref type="bibr" target="#b18">[19]</ref>, DCRNN <ref type="bibr" target="#b16">[17]</ref>, LSTNet <ref type="bibr" target="#b13">[14]</ref>, ST-GCN <ref type="bibr" target="#b30">[31]</ref>, DeepSta
arget="#b30">31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar al networks and learns the parameters of the entire network through maximum log likelihood. DeepGLO <ref type="bibr" target="#b24">[25]</ref>   incorporates both spatial and temporal dependencies in t ="bibr" target="#b2">[3]</ref>, Graph Wavenet <ref type="bibr" target="#b28">[29]</ref> and DeepGLO <ref type="bibr" target="#b24">[25]</ref>. We tune the hyper-parameters on the validation data by gr type="bibr" target="#b20">[21]</ref> GraphWaveNet<ref type="bibr" target="#b28">[29]</ref> DeelpGLO<ref type="bibr" target="#b24">[25]</ref> StemGNN (ours)</note></figure> 		</body> 		<back> 			<div
variate techniques <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar get="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref> and multivariate techniques <ref type="bibr" target="#b23">
to recent surveys <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33]</ref> for more details about related works.</p></div> <div xmlns=
pplied on the spectral representation to generate final output.</p><p>Graph Fourier Transform (GFT) <ref type="bibr" target="#b7">[8]</ref> is a basic operator for Spectral Graph Convolution. It proje
r a collection of multiple time-series as a unified entity <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>. TCN <ref type="bibr" target="#b2">[3]</ref> is a representat
g loops of neural networks, blurring different dimensions of the problem. By contrast -similarly to <ref type="bibr" target="#b35">Parshakova et al. (2019a;</ref><ref type="bibr">b)</ref> in a differe ="bibr" target="#b23">(Kim &amp; Bengio, 2016;</ref><ref type="bibr" target="#b34">Owen, 2013;</ref><ref type="bibr" target="#b35">Parshakova et al., 2019a</ref>) SNIS consists in computing:</p><formu et="#b4">Belanger &amp; McCallum, 2016)</ref>. Some current applications to text generation include <ref type="bibr" target="#b35">Parshakova et al. (2019a)</ref> and <ref type="bibr" target="#b14">De

arget="#b51">(Stanovsky et al., 2019;</ref><ref type="bibr" target="#b40">Prates et al., 2020;</ref><ref type="bibr" target="#b47">Sheng et al., 2019a;</ref><ref type="bibr" target="#b7">Brown et al.,
f optimization techniques when training generative models for text, has recently been documented in <ref type="bibr" target="#b8">(Caccia et al., 2020)</ref>. So additionally, we report Self-BLEU-3,4,
it from distributional control, is that of social biases conspicuous in pretrained language models. <ref type="bibr" target="#b51">(Stanovsky et al., 2019;</ref><ref type="bibr" target="#b40">Prates e

ined LM itself the yardstick for fluency. <ref type="bibr" target="#b19">Jaques et al. (2017);</ref><ref type="bibr" target="#b61">Ziegler et al. (2019)</ref> propose a conservative fine-tuning approa ng distribution to p, this baseline tries to get sequences of maximal probability p(x). (3) ZIEGLER <ref type="bibr" target="#b61">(Ziegler et al., 2019)</ref>: an approach relying on the RL Proximal -for distinguishing diversity at the sequence level or at the distribution level. Similarly, ZIEGLER<ref type="bibr" target="#b61">(Ziegler et al., 2019)</ref> often suffers from the same lack of samp
or Machine Translation and Summarization <ref type="bibr" target="#b43">(Ranzato et al., 2016;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2017)</ref>, or hand crafted rewards <ref type="bibr" or Machine Translation and Summarization <ref type="bibr" target="#b43">(Ranzato et al., 2016;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2017)</ref> directly optimize end task rewards such a n (MT) Ranzato et al. ( <ref type="formula">2016</ref>), actor critic for Abstractive Summarization <ref type="bibr" target="#b2">(Bahdanau et al., 2017)</ref>, Image-to-Text <ref type="bibr" target="
or Machine Translation and Summarization <ref type="bibr" target="#b43">(Ranzato et al., 2016;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2017)</ref>, or hand crafted rewards <ref type="bibr" or Machine Translation and Summarization <ref type="bibr" target="#b43">(Ranzato et al., 2016;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2017)</ref> directly optimize end task rewards such a n (MT) Ranzato et al. ( <ref type="formula">2016</ref>), actor critic for Abstractive Summarization <ref type="bibr" target="#b2">(Bahdanau et al., 2017)</ref>, Image-to-Text <ref type="bibr" target="

certain a priori desirable features.</p><p>However, such an optimization process is not infallible; <ref type="bibr" target="#b30">Liu et al. (2016a)</ref> noted that it often leads to "degeneration", using a set of discriminators to ensure the quality of generated text in open-ended text generation.<ref type="bibr" target="#b30">Liu et al. (2016a)</ref>, however, show that defining a combination r
emic paper analysis.</p><p>The pipeline for creating S2ORC was used to construct the CORD-19 corpus <ref type="bibr" target="#b2">(Wang et al., 2020)</ref>, which saw fervent adoption as the canonical
on bibliometrically-enhanced derivations of these corpora, such as the ACL Anthology Network (AAN) <ref type="bibr" target="#b42">(Radev et al., 2009)</ref> <ref type="foot" target="#foot_3">6</ref> lns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>The ACL Anthology Network (AAN) <ref type="bibr" target="#b42">(Radev et al., 2009</ref>) is a bibliometric-enhanced corpus covering
exts in-clude classifying citation intent <ref type="bibr" target="#b48">(Teufel et al., 2006;</ref><ref type="bibr" target="#b23">Jurgens et al., 2018;</ref><ref type="bibr" target="#b10">Cohan et al
target="#b14">(Ding et al., 2014;</ref><ref type="bibr" target="#b50">Trujillo and Long, 2018;</ref><ref type="bibr" target="#b3">Asatani et al., 2018)</ref>. Towards these tasks, the citation context
t="#b18">(Giles et al., 1998)</ref>, <ref type="foot" target="#foot_1">4</ref>and the ACL Anthology <ref type="bibr" target="#b8">(Bird et al., 2008)</ref>, <ref type="foot" target="#foot_2">5</ref>ar orpus covering papers in the field of computational linguistics. It is built from the ACL Anthology <ref type="bibr" target="#b8">(Bird et al., 2008)</ref> and consists of 24.6k papers manually augmen
exts in-clude classifying citation intent <ref type="bibr" target="#b48">(Teufel et al., 2006;</ref><ref type="bibr" target="#b23">Jurgens et al., 2018;</ref><ref type="bibr" target="#b10">Cohan et al
target="#b48">(Teufel et al., 2006;</ref><ref type="bibr" target="#b23">Jurgens et al., 2018;</ref><ref type="bibr" target="#b10">Cohan et al., 2019)</ref>, identifying citation sentiment <ref type=" ample, by pretraining language models on S2ORC citation contexts before fine-tuning to these tasks. <ref type="bibr" target="#b10">Cohan et al. (2019)</ref> find that long citation contexts (beyond se

en used to support research over academic papers. Citation graphs like AMiner's Open Academic Graph <ref type="bibr" target="#b47">(Tang et al., 2008)</ref>, the Microsoft Academic Graph (MAG) <ref ty

et="#b48">(Teufel et al., 2006;</ref><ref type="bibr" target="#b41">Qazvinian and Radev, 2008;</ref><ref type="bibr" target="#b11">Cohan and Goharian, 2015;</ref><ref type="bibr" target="#b36">Mitrovi
s reported that these numerical errors brought about huge reputation risk, and even economic losses <ref type="bibr" target="#b0">[1]</ref>. Since the documents disclosed by the firm usually have the significance testing, presented in the academic papers in major psychology journals. A recent study <ref type="bibr" target="#b0">[1]</ref> published a system called AutoDoc, and introduced the module uch more numerical facts in tables than textual paragraphs. Therefore, as an important extension to <ref type="bibr" target="#b0">[1]</ref>, we propose Automatic Numerical Cross-Checking over Tables ( ncial, and politic fields. It has attracted a lot of research interests in recent years. Cao et al. <ref type="bibr" target="#b0">[1]</ref> propose a system to cross-check numerical facts by extractin
cit hierarchical headers. There are some studies about recognizing this type of tables. Fang et al. <ref type="bibr" target="#b2">[3]</ref> proposed a Random Forest classification to identify the comp
target="#b17">[18]</ref>, transforming complex tables to the form that can be stored in a database <ref type="bibr" target="#b13">[14]</ref>. Our task, cross-checking over numerical tables, is also a
ref> proposed a Random Forest classification to identify the complex headers in tables; Nagy et al. <ref type="bibr" target="#b10">[11]</ref> leveraged rule-based method to extract data categories and
structured information which is laborious when collecting the labelling dataset. Vlachos and Riedel <ref type="bibr" target="#b16">[17]</ref> propose a dataset to verify the claims made by public figu
elated evident paragraphs, and finally give a classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref>. In academic field,
ted numerical cross-checking systems. There are some related systems developed, such as ClaimBuster <ref type="bibr" target="#b4">[5]</ref> and StatCheck <ref type="bibr" target="#b11">[12]</ref>. Cla ade by public figures. Verifying such claims includes detecting whether a statement in check-worthy <ref type="bibr" target="#b4">[5]</ref>, retrieve information from large data source  to provide rel
elated evident paragraphs, and finally give a classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref>. In academic field,
ng text to table cells <ref type="bibr" target="#b5">[6]</ref>, table cell search for a given query <ref type="bibr" target="#b14">[15]</ref>, ad hoc search over tables <ref type="bibr" target="#b17">
give a classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref>. In academic field, Nuijten et al. <ref type="bibr" target=
cit hierarchical headers. There are some studies about recognizing this type of tables. Fang et al. <ref type="bibr" target="#b2">[3]</ref> proposed a Random Forest classification to identify the comp
body> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer models <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> have become ubiquitous for wide variety //www.tei-c.org/ns/1.0"><head>Model Architecture</head><p>Complexity per Layer Sequential Operation <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> O(n 2 ) O(1) Sparse Tansformer, <ref typ
ubiquitous for wide variety of problems in natural language processing (NLP), including translation <ref type="bibr" target="#b20">(Ott et al., 2018)</ref>, text classification, question answering, am
/ref> consumed orders of magnitude more petaflops / day to train compared to its predecessor, GPT-2 <ref type="bibr" target="#b23">(Radford et al., 2019)</ref>. Beyond training, deploying Transformer
erence) <ref type="bibr" target="#b25">(Rajpurkar et al., 2016)</ref>, and QQP (textual similarity) <ref type="bibr" target="#b4">(Chen et al., 2018)</ref> We do the same with RoBERTa, 12-layer BERT-b
erence) <ref type="bibr" target="#b25">(Rajpurkar et al., 2016)</ref>, and QQP (textual similarity) <ref type="bibr" target="#b4">(Chen et al., 2018)</ref> We do the same with RoBERTa, 12-layer BERT-b
Ott et al., 2019)</ref>. This technique can be further improved through Quantization Aware Training <ref type="bibr" target="#b11">(Jacob et al., 2018;</ref><ref type="bibr" target="#b8">Fan et al., 2
training and deploying such model are slow in practice. For example, the original BERT-Large model <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> takes four days to train on 16 Cloud TPUs, language modeling objective on a large corpus, then finetuned on target tasks using supervised data <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref><ref type="bibr" target="#b15">Liu et al., t="#b15">Liu et al., 2019;</ref><ref type="bibr" target="#b13">Lewis et al., 2019)</ref>. Following <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>, we pretrain our model on BookCorpus <ref t ef type="bibr" target="#b15">(Liu et al., 2019)</ref>, which is based on the Transformer. Following <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>, we use BookCorpus <ref type="bibr" target=
training and deploying such model are slow in practice. For example, the original BERT-Large model <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> takes four days to train on 16 Cloud TPUs, language modeling objective on a large corpus, then finetuned on target tasks using supervised data <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref><ref type="bibr" target="#b15">Liu et al., t="#b15">Liu et al., 2019;</ref><ref type="bibr" target="#b13">Lewis et al., 2019)</ref>. Following <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>, we pretrain our model on BookCorpus <ref t ef type="bibr" target="#b15">(Liu et al., 2019)</ref>, which is based on the Transformer. Following <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>, we use BookCorpus <ref type="bibr" target=
Ott et al., 2019)</ref>. This technique can be further improved through Quantization Aware Training <ref type="bibr" target="#b11">(Jacob et al., 2018;</ref><ref type="bibr" target="#b8">Fan et al., 2
formula><p>Proof. The main idea of proof is based on the distributional Johnson-Lindenstrauss lemma <ref type="bibr" target="#b14">(Lindenstrauss, 1984)</ref>. We first prove that for any row vector x
with limited efficiency gains, i.e., a 2% drop with only 20% speed up. More recently, the Reformer <ref type="bibr" target="#b12">(Kitaev et al., 2020)</ref> used locally-sensitive hashing (LSH) to r while having only limited additional speed-up, i.e., 2% drop with 20% speed up.</p><p>LSH Attention <ref type="bibr" target="#b12">(Kitaev et al., 2020)</ref>: Locally-sensitive hashing (LSH) attentio ficiency gains only appear on sequences with length &gt; 2048 (Figure <ref type="figure">5</ref> in <ref type="bibr" target="#b12">Kitaev et al. (2020)</ref>). Furthermore, the Reformer's multi-round Sparse Tansformer, <ref type="bibr" target="#b5">(Child et al., 2019)</ref> O(n ? n) O(1) Reformer, <ref type="bibr" target="#b12">(Kitaev et al., 2020</ref>) 2 Backgrounds and Related works</p><formu
simply regularize model predictions to be invariant to small noise applied to either input examples <ref type="bibr" target="#b23">(Miyato et al., 2018;</ref><ref type="bibr" target="#b34">Sajjadi et r" target="#b1">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropout noise; VAT <ref type="bibr" target="#b23">(Miyato et al., 2018;</ref><ref type="bibr">2016)</ref> defines the n type="bibr" target="#b34">(Sajjadi et al., 2016;</ref><ref type="bibr">Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b23">Miyato et al., 2018)</ref>. But different from existing work, we focu has been shown to be beneficial <ref type="bibr" target="#b15">(Grandvalet &amp; Bengio, 2005;</ref><ref type="bibr" target="#b23">Miyato et al., 2018)</ref>, we sharpen predictions when computing the current parameters Î¸ indicating that the gradient is not propagated through Î¸, as suggested by VAT <ref type="bibr" target="#b23">(Miyato et al., 2018)</ref>. We set Î» to 1 for most of our experiment cally, we compare UDA with two highly competitive baselines: (1) Virtual adversarial training (VAT) <ref type="bibr" target="#b23">(Miyato et al., 2018)</ref>, an algorithm that generates adversarial ="#b41">(Tarvainen &amp; Valpola, 2017)</ref> Conv-Large 3.1M 12.31 Â± 0.28 3.95 Â± 0.19 VAT + EntMin <ref type="bibr" target="#b23">(Miyato et al., 2018)</ref> Conv-Large 3.1M 10.55 Â± 0.05 3.86 Â± 0.11
ss <ref type="bibr" target="#b36">(Salimans et al., 2016)</ref> works very well in practice. Later, <ref type="bibr" target="#b11">Dai et al. (2017)</ref> shows that this can be seen as an instantiati
al., 2011)</ref>, which are often used to compare semi-supervised algorithms, as well as Ima-geNet <ref type="bibr" target="#b12">(Deng et al., 2009)</ref> of a larger scale to test the scalability o
bserve that advanced data augmentation methods, specifically those work best in supervised learning <ref type="bibr" target="#b39">(Simard et al., 1998;</ref><ref type="bibr">Krizhevsky et al., 2012;<
are mismatched with those of in-domain data, which can result in performance loss if directly used <ref type="bibr" target="#b26">(Oliver et al., 2018)</ref>. To obtain data relevant to the domain fo nchmarks CIFAR-10 and SVHN.</p><p>Vary the size of labeled data. Firstly, we follow the settings in <ref type="bibr" target="#b26">(Oliver et al., 2018)</ref> and employ Wide-ResNet-28-2 <ref type="bi
)</ref>. Invariant representation learning <ref type="bibr" target="#b16">(Liang et al., 2018;</ref><ref type="bibr" target="#b35">Salazar et al., 2018)</ref> applies the consistency loss not only to
2017;</ref><ref type="bibr" target="#b48">Ye et al., 2019)</ref>. Invariant representation learning <ref type="bibr" target="#b16">(Liang et al., 2018;</ref><ref type="bibr" target="#b35">Salazar et a
type="bibr" target="#b23">(Miyato et al., 2018)</ref> Conv-Large 3.1M 10.55 Â± 0.05 3.86 Â± 0.11 SNTG <ref type="bibr" target="#b17">(Luo et al., 2018)</ref> Conv-Large 3.1M 10.93 Â± 0.14 3.86 Â± 0.27 VAd f type="bibr" target="#b0">(Athiwaratkun et al., 2018)</ref> and Smooth Neighbors on Teacher Graphs <ref type="bibr" target="#b17">(Luo et al., 2018)</ref>. For a complete version of related work, ple to explore a diverse set of plausible parameters. In addition to parameter-level consistency, SNTG <ref type="bibr" target="#b17">(Luo et al., 2018)</ref>  There are also recent works on generating d
Db, Yelp-2, Yelp-5, Amazon-2 and Amazon-5 sentiment classification and DBPedia topic classification <ref type="bibr" target="#b19">(Maas et al., 2011;</ref><ref type="bibr">Zhang et al., 2015)</ref>.<
Db, Yelp-2, Yelp-5, Amazon-2 and Amazon-5 sentiment classification and DBPedia topic classification <ref type="bibr" target="#b19">(Maas et al., 2011;</ref><ref type="bibr">Zhang et al., 2015)</ref>.<
stantially scaled in Word2Vec <ref type="bibr" target="#b21">(Mikolov et al., 2013)</ref> and Glove <ref type="bibr" target="#b29">(Pennington et al., 2014)</ref>  <ref type="formula">2018</ref>) have
target="#b14">Dosovitskiy et al., 2014;</ref><ref type="bibr" target="#b41">Oord et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019)</ref>.</p><p>In this work, we introduce a simple rget="#b33">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b22">HÃ©naff et al., 2019;</ref><ref type="bibr" target="#b1">Bachman et al., 2019)</ref>, it has not been considered as a systemati br" target="#b22">HÃ©naff et al., 2019;</ref><ref type="bibr" target="#b24">Hjelm et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019)</ref>. However, it is not clear if the success o tation learning methods:</p><p>â¢ DIM/AMDIM <ref type="bibr" target="#b24">(Hjelm et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019)</ref> achieve global-to-local/local-to-neighbor gure" target="#fig_0">1</ref>), but it is also simpler, requiring neither specialized architectures <ref type="bibr" target="#b1">(Bachman et al., 2019;</ref><ref type="bibr" target="#b22">HÃ©naff et a ibr" target="#b20">(Zhang et al., 2016;</ref><ref type="bibr" target="#b41">Oord et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b29">Kolesnikov e n is useful for self-supervised learning <ref type="bibr" target="#b11">(Doersch et al., 2015;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b22">HÃ©naff et al the default nonlinear projection with one additional hidden layer (and ReLU activation), similar to <ref type="bibr" target="#b1">Bachman et al. (2019)</ref>. We observe that a nonlinear projection is tch size. The best self-supervised model that reports linear evaluation result on CIFAR-10 is AMDIM <ref type="bibr" target="#b1">(Bachman et al., 2019)</ref>, which achieves 91.2% with a model 25Ã la
omise, achieving state-of-theart results <ref type="bibr" target="#b19">(Hadsell et al., 2006;</ref><ref type="bibr" target="#b14">Dosovitskiy et al., 2014;</ref><ref type="bibr" target="#b41">Oord et ches learn representations by contrasting positive pairs against negative pairs. Along these lines, <ref type="bibr" target="#b14">Dosovitskiy et al. (2014)</ref> proposes to treat each instance as a 38">Misra &amp; van der Maaten, 2019)</ref> generalize the Exemplar approach originally proposed by <ref type="bibr" target="#b14">Dosovitskiy et al. (2014)</ref> and leverage an explicit memory bank.

ge classification datasets, SimCLR performs on par with or better than a strong supervised baseline <ref type="bibr" target="#b30">(Kornblith et al., 2019)</ref> on 10 out of 12 datasets.</p></div> <d ge datasets in both linear evaluation (fixed feature extractor) and fine-tuning settings. Following <ref type="bibr" target="#b30">Kornblith et al. (2019)</ref>, we perform hyperparameter tuning for e ntly long to achieve near-maximal accuracy, as demonstrated in Figure <ref type="figure">8</ref> of <ref type="bibr" target="#b30">Kornblith et al. (2019)</ref>.</p><p>On Birdsnap, there are no statis re we allow all weights to vary during training. In both cases, we follow the approach described by <ref type="bibr" target="#b30">Kornblith et al. (2019)</ref>, although our preprocessing differs sli
<cell>compares our results with previ-</cell></row></table><note>Semi-supervised learning. We follow<ref type="bibr" target="#b56">Zhai et al. (2019)</ref> and sample 1% or 10% of the labeled ILSVRC-1
et="#b10">(Doersch &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b54">Ye et al., 2019;</ref><ref type="bibr" target="#b27">Ji et al., 2019)</ref>.</p><p>Recent literature has attempted to rela
>, the SUN397 scene dataset <ref type="bibr" target="#b52">(Xiao et al., 2010)</ref>, Stanford Cars <ref type="bibr" target="#b31">(Krause et al., 2013)</ref>, FGVC Aircraft <ref type="bibr" target="#

hman et al., 2019;</ref><ref type="bibr" target="#b22">HÃ©naff et al., 2019)</ref> nor a memory bank <ref type="bibr" target="#b51">(Wu et al., 2018;</ref><ref type="bibr" target="#b49">Tian et al., 20 lized cross-entropy-based objective. We use simpler data augmentation.</p><p>â¢ InstDisc, MoCo, PIRL <ref type="bibr" target="#b51">(Wu et al., 2018;</ref><ref type="bibr" target="#b21">He et al., 2019 ni-batch. This loss has been used in previous work <ref type="bibr" target="#b46">(Sohn, 2016;</ref><ref type="bibr" target="#b51">Wu et al., 2018;</ref><ref type="bibr" target="#b41">Oord et al., 201 <head n="2.2.">Training with Large Batch Size</head><p>We do not train the model with a memory bank <ref type="bibr" target="#b51">(Wu et al., 2018)</ref>. Instead, we vary the training batch size N f + â u T v â &lt; m else 0</formula><p>(2) linear projection, as used by several previous approaches <ref type="bibr" target="#b51">(Wu et al., 2018)</ref>; and (3) the default nonlinear projection wit proposes to treat each instance as a class represented by a feature vector (in a parametric form). <ref type="bibr" target="#b51">Wu et al. (2018)</ref> proposes to use a memory bank to store the ins
such as logistic loss <ref type="bibr" target="#b37">(Mikolov et al., 2013)</ref>, and margin loss <ref type="bibr" target="#b44">(Schroff et al., 2015)</ref>. Table <ref type="table" target="#tab_3" igh the negatives by their relative hardness. As a result, one must apply semi-hard negative mining <ref type="bibr" target="#b44">(Schroff et al., 2015)</ref> for these loss functions: instead of com
e learning in the latent space have recently shown great promise, achieving state-of-theart results <ref type="bibr" target="#b19">(Hadsell et al., 2006;</ref><ref type="bibr" target="#b14">Dosovitski rality of learned representations.</p><p>Contrastive visual representation learning. Dating back to <ref type="bibr" target="#b19">Hadsell et al. (2006)</ref>, these approaches learn representations b
target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" targe going through an intermediate text transcript. There are many advantages of end-to-end SLU systems <ref type="bibr" target="#b4">[5]</ref>, the most significant of which is that E2E systems can direc trained on increasingly relevant data until it is fine-tuned on the actual domain data. Similarly, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref> advocate pre-trainin
intent-labeled speech data, and such data is usually scarce. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> address this problem using a curriculum and transfer learni
timized separately or jointly (also with end-to-end criteria <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>). One key advantage of n-ended first utterances by customers describing the reasons for their calls, which is described in <ref type="bibr" target="#b7">[8]</ref>. The 8kHz telephony speech data was manually transcribed and
mited amount of S2I data, in the same spirit as learning a shared representation between modalities <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" t
rit as learning a shared representation between modalities <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" t
etween modalities <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. We employ the fo
TS system</head><p>The TTS system architecture is similar to the single speaker system described in <ref type="bibr" target="#b24">[25]</ref>. It is a modular system based on three neural-net models:
pochs of guided training <ref type="bibr" target="#b20">[21]</ref>. We use sequence noise injection <ref type="bibr" target="#b21">[22]</ref> and SpecAugment <ref type="bibr" target="#b22">[23]</ref>
em that interprets the meaning, or intent, of the text. In contrast, an end-to-end (E2E) SLU system <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" targe
than 10 hours of speech, 21 VCTK <ref type="bibr" target="#b26">[27]</ref> voices, and 138 LibriTTS <ref type="bibr" target="#b27">[28]</ref> voices. During synthesis, for each sentence we select a ra
pochs of guided training <ref type="bibr" target="#b20">[21]</ref>. We use sequence noise injection <ref type="bibr" target="#b21">[22]</ref> and SpecAugment <ref type="bibr" target="#b22">[23]</ref>
rit as learning a shared representation between modalities <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" t
ad><p>Leveraging text embedding (TE) from models pre-trained on large amounts of data, such as BERT <ref type="bibr" target="#b13">[14]</ref> and GPT-2 <ref type="bibr" target="#b14">[15]</ref>, has r l model. (A) T2I model pre-training: As in the standard process outlined in the original BERT paper <ref type="bibr" target="#b13">[14]</ref>, we first fine-tune BERT on the available text-to-intent d /ns/1.0"><head n="4.2.2.">BERT based T2I model</head><p>We start with pre-trained BERTbase model of <ref type="bibr" target="#b13">[14]</ref>. Using the implementation introduced in <ref type="bibr" t
TS system</head><p>The TTS system architecture is similar to the single speaker system described in <ref type="bibr" target="#b24">[25]</ref>. It is a modular system based on three neural-net models:
timized separately or jointly (also with end-to-end criteria <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>). One key advantage of n-ended first utterances by customers describing the reasons for their calls, which is described in <ref type="bibr" target="#b7">[8]</ref>. The 8kHz telephony speech data was manually transcribed and
mited amount of S2I data, in the same spirit as learning a shared representation between modalities <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" t
rform or just barely outperform traditional cascaded systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. One reason is that deep learning models require a large amo
ad><p>Leveraging text embedding (TE) from models pre-trained on large amounts of data, such as BERT <ref type="bibr" target="#b13">[14]</ref> and GPT-2 <ref type="bibr" target="#b14">[15]</ref>, has r l model. (A) T2I model pre-training: As in the standard process outlined in the original BERT paper <ref type="bibr" target="#b13">[14]</ref>, we first fine-tune BERT on the available text-to-intent d /ns/1.0"><head n="4.2.2.">BERT based T2I model</head><p>We start with pre-trained BERTbase model of <ref type="bibr" target="#b13">[14]</ref>. Using the implementation introduced in <ref type="bibr" t
until it is fine-tuned on the actual domain data. Similarly, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref> advocate pre-training an ASR model on a large amount of tra
ad><p>Leveraging text embedding (TE) from models pre-trained on large amounts of data, such as BERT <ref type="bibr" target="#b13">[14]</ref> and GPT-2 <ref type="bibr" target="#b14">[15]</ref>, has r l model. (A) T2I model pre-training: As in the standard process outlined in the original BERT paper <ref type="bibr" target="#b13">[14]</ref>, we first fine-tune BERT on the available text-to-intent d /ns/1.0"><head n="4.2.2.">BERT based T2I model</head><p>We start with pre-trained BERTbase model of <ref type="bibr" target="#b13">[14]</ref>. Using the implementation introduced in <ref type="bibr" t
timized separately or jointly (also with end-to-end criteria <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>). One key advantage of n-ended first utterances by customers describing the reasons for their calls, which is described in <ref type="bibr" target="#b7">[8]</ref>. The 8kHz telephony speech data was manually transcribed and
sually limited.</p><p>In our work, we use a phone-based connectionist temporal classification (CTC) <ref type="bibr" target="#b12">[13]</ref> acoustic model (AM) trained on general speech data as the
"bibr" target="#b8">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b22">Wu et al., 2018;</ref><ref type="bibr" target="#b6">Fey et al., 2020)</ref>. Graph neural networks have been proven to be ibr" target="#b0">Abu-El-Haija et al., 2019;</ref><ref type="bibr" target="#b16">Loukas, 2019;</ref><ref type="bibr" target="#b6">Fey et al., 2020)</ref>. In the context of molecular property predicti olution on both the input graph and a coarser version of it from which all cycles have been removed <ref type="bibr" target="#b6">(Fey et al., 2020)</ref>. Despite interesting results, this approach g >Fey et al., 2020)</ref>. In the context of molecular property prediction, we highlight the work of <ref type="bibr" target="#b6">Fey et al. (2020)</ref> who proposed to perform message passing betwee


systems, traffic predictions, and much more <ref type="bibr" target="#b25">(Zhou et al., 2018;</ref><ref type="bibr" target="#b23">Wu et al., 2020)</ref>. Among those problems, supervised whole graph
is to extend the current framework, to make it equivalent to higher order Weisfeiler Leman kernels <ref type="bibr" target="#b19">(Morris et al., 2019)</ref>, or to increase the receptive field of ea
e while increasing the predictive accuracy <ref type="bibr" target="#b8">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b22">Wu et al., 2018;</ref><ref type="bibr" target="#b6">Fey et al., 2020) have been proven to be particularly effective in this task <ref type="bibr">(Hu et al., 2020;</ref><ref type="bibr" target="#b22">Wu et al., 2018)</ref>, exceeding other approaches, both based on oth ATASET</head><p>We performed experiments on five molecule classification datasets, from MoleculeNet <ref type="bibr" target="#b22">Wu et al. (2018)</ref> and Open Graph Benchmark (OGB) <ref type="bibr
amentally diverging from the standard convolution based framework, such as invariant graph networks <ref type="bibr" target="#b18">(Maron et al., 2019;</ref><ref type="bibr">2018)</ref> or relational
ion can be seen as a computationally simple first-order approximation of spectral graph convolution <ref type="bibr" target="#b1">(Bruna et al., 2014;</ref><ref type="bibr" target="#b4">Defferrard et
n the context of chemistry. We propose a very simple correction to the now standard GIN convolution <ref type="bibr" target="#b24">(Xu et al., 2019)</ref> that enables the network to detect small cycl <p>A second problem is that the most common framework, namely message passing neural network (MPNN) <ref type="bibr" target="#b24">(Xu et al., 2019)</ref>, is not an universal approximator of function that the operation is injective on multisets, or at least on a large finite ensemble of multisets. <ref type="bibr" target="#b24">(Xu et al., 2019)</ref> have proposed graph isomorphism network(GIN), ations encoding structure and node features, some drawbacks have been pointed by followup works. In <ref type="bibr" target="#b24">Xu et al. (2019)</ref> it is shown that GCNs are limited in its capab
instead of handcrafting them or using other representation is an idea that emerged a few years ago <ref type="bibr" target="#b5">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b8">Gilmer et a
is to extend the current framework, to make it equivalent to higher order Weisfeiler Leman kernels <ref type="bibr" target="#b19">(Morris et al., 2019)</ref>, or to increase the receptive field of ea
et="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref>, regularizes the spectral norm of the weight matrix at each i et="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref>. <ref type="bibr" target="#b7">[8]</ref> utilizes Lipschitz p et="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref>, keep Î² constant across layers. These harder constraints over get="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref>. <ref type="bibr" target="#b7">[8]</ref> utilizes Lipschitz properties of the DNN to improve robustne utilizes Lipschitz properties of the DNN to improve robustness against adversarial attacks. Unlike <ref type="bibr" target="#b7">[8]</ref>, our approach does not require a predetermined set of hyper- 11]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" their papers: Î² = 1.0, 1.6, 2.0. The 2 works given in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b7">[8]</ref> may be seen as subsets of the works given in <ref type="bibr
se, but their method covered only a toy adversary constrained to perturbing the input by a constant <ref type="bibr" target="#b28">[29]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
ef>, attacks of this form have proven difficult to prevent <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target=
identity matrix, then the matrix is said to be row-or column-diagonally dominant.</p><p>Corollary 1 <ref type="bibr" target="#b32">[33]</ref> Every symmetric quasi-dominant matrix is positive definite
s are only applicable to parts of the input space for which feasible solutions exist. Works such as <ref type="bibr" target="#b38">[39]</ref> have empirically shown that bounding a layer's response to
es' freedom to alter the input. While this does not capture the full scope of potential adversaries <ref type="bibr" target="#b3">[4]</ref>, attacks of this form have proven difficult to prevent <ref
s are only applicable to parts of the input space for which feasible solutions exist. Works such as <ref type="bibr" target="#b38">[39]</ref> have empirically shown that bounding a layer's response to
show that all the data points inside an p ball around a sample data point have the same prediction <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta ovides a theoretical backing for the empirical findings in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref> which state that Leaky ReLu, a modified version of the ReLu in the previous sections support the findings presented in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref> that the use of Leaky Relu activation in the architecture o
difficult to prevent <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31]</ref>. While optimal defens
for the above architecture and data set combinations. The experiments are implemented in TensorFlow <ref type="bibr" target="#b0">[1]</ref> and the code will be made readily available. We test the DNN
to the input generally improves robustness. Works such as <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref> focus on certifying robustness for a DNN by calculating the
sampling technique. Furthermore, we adapt the higher-order graph node information under the CoreSet <ref type="bibr" target="#b30">[31]</ref> for a new sampling technique by introducing latent space d 1]</ref> for a new sampling technique by introducing latent space distancing. In principle, CoreSet <ref type="bibr" target="#b30">[31]</ref> uses risk minimisation between core-sets on the learner fe , the first work applying it for CNNs as an active learning problem, CoreSet, has been presented in <ref type="bibr" target="#b30">[31]</ref>. The key principle depends on minimising the difference be ods: UncertainGCN and CoreGCN. UncertainGCN is based on the standard AL method uncertainty sampling <ref type="bibr" target="#b30">[31]</ref> which tracks the confidence scores of the designed graph n dence scores of the designed graph nodes. Furthermore, CoreGCN adapts the highly successful CoreSet <ref type="bibr" target="#b30">[31]</ref> on the induced graph embeddings by the sequentially traine formation between the labelled and unlabelled graph representation, we approach a CoreSet technique <ref type="bibr" target="#b30">[31]</ref> in our sampling stage. This has shown better performance i r performance in comparison to uncertainty-based methods <ref type="bibr" target="#b37">[38]</ref>. <ref type="bibr" target="#b30">[31]</ref> shows how bounding the difference between the loss of the ype="bibr" target="#b32">[33]</ref> Learning Loss <ref type="bibr" target="#b41">[42]</ref> CoreSet <ref type="bibr" target="#b30">[31]</ref> CoreGCN(Ours) FeatProp <ref type="bibr" target="#b37">[38] s which we describe here. Random sampling is by default the most common sampling technique. CoreSet <ref type="bibr" target="#b30">[31]</ref> on learner feature space is one of the best performing geo spectrum of baselines. One is random sampling which is the default mechanism. The other is CoreSet <ref type="bibr" target="#b30">[31]</ref>, one of the best performing baselines from the previous ex ng system due to its successful deployment in recent methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar
L has no way to communicate between the learner and the sampler. Graph Convolutional Networks(GCNs) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5]</ref> are capable of shari les by sharing information through message-passing operations in GCN. GCNs in active learning. GCNs <ref type="bibr" target="#b17">[18]</ref> have opened new active learning methods that have been suc ">A = D -1 (S -I) + I.<label>(4)</label></formula><p>To avoid over-smoothing of the features in GCN <ref type="bibr" target="#b17">[18]</ref>, we adopt a two-layer architecture. The first GCN layer ca ertainDiscriminator. This approach over-fits at early selection stages. Although, GCN with 2 layers <ref type="bibr" target="#b17">[18]</ref> has been a de-facto optimal design choice, we also report
target="#b21">22]</ref> and 3D Hand Pose Estimation (HPE) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b24">25]</ref>. This has been poss
our method on a challenging dataset for 3D Hand Pose Estimation benchmarks from depth images. ICVL <ref type="bibr" target="#b33">[34]</ref>  joints from depth images. Thus, we replace ResNet-18 by c baselines on one of the most challenging, widely been used and first of depth based datasets, ICVL <ref type="bibr" target="#b33">[34]</ref>. This is composed of 16,004 images for training and 1,600
e in comparison to other networks with comparable parameter complexity. Any other model like VGG-11 <ref type="bibr" target="#b31">[32]</ref> can also be easily deployed (refer to Supplementary Materi 7,500 images. The experiment architecture and settings are similar to the one on the full scale.    <ref type="bibr" target="#b31">[32]</ref>. Therefore, we analyse how the AL methods are affected in
ness of the data <ref type="bibr" target="#b3">[4]</ref>. To overcome these issues, active learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> has been successful out (MC Dropout) adapted architecture. Hence, it is successfully integrated into active learning by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar e learner. Majority of the previous works in active learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target
. Some of the recent works such as VAAL <ref type="bibr" target="#b32">[33]</ref> and Learning Loss <ref type="bibr" target="#b41">[42]</ref> tackle such a problem. VAAL trains a variational auto-enco of the most representative data. Our method is based on this category. One of the first approaches <ref type="bibr" target="#b41">[42]</ref> attached a loss-learning module so that loss can be predic y on CIFAR10 Random UncertainGCN(Ours) VAAL <ref type="bibr" target="#b32">[33]</ref> Learning Loss <ref type="bibr" target="#b41">[42]</ref> CoreSet <ref type="bibr" target="#b30">[31]</ref> CoreGCN( other competitive baseline for us. VAAL <ref type="bibr" target="#b32">[33]</ref> and Learning Loss <ref type="bibr" target="#b41">[42]</ref> are two state-of-the-art baselines from task-agnostic fram rget="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>. In this scenario, from a pool of unlabeled dataset D U , w h is a 100-class benchmark. Similar to the existing works of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42]</ref>, we apply our selection on randomly selected subsets D S ?
its associated labels are not yet suitable to train a downstream discriminative model. Recent study <ref type="bibr" target="#b2">[3]</ref> recommends sub-sampling the The only difference will be in t e fourth selection stage. Only a small fraction of synthetic examples are useful to train the model <ref type="bibr" target="#b2">[3]</ref>.</p><p>After the fourth stage, we force sampler to select mo
="#b3">[4]</ref>. To overcome these issues, active learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> has been successfully deployed to efficiently select the mo here have been studies exploring the data space through the representations of the learning model ( <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" ta
. Some of the recent works such as VAAL <ref type="bibr" target="#b32">[33]</ref> and Learning Loss <ref type="bibr" target="#b41">[42]</ref> tackle such a problem. VAAL trains a variational auto-enco of the most representative data. Our method is based on this category. One of the first approaches <ref type="bibr" target="#b41">[42]</ref> attached a loss-learning module so that loss can be predic y on CIFAR10 Random UncertainGCN(Ours) VAAL <ref type="bibr" target="#b32">[33]</ref> Learning Loss <ref type="bibr" target="#b41">[42]</ref> CoreSet <ref type="bibr" target="#b30">[31]</ref> CoreGCN( other competitive baseline for us. VAAL <ref type="bibr" target="#b32">[33]</ref> and Learning Loss <ref type="bibr" target="#b41">[42]</ref> are two state-of-the-art baselines from task-agnostic fram rget="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>. In this scenario, from a pool of unlabeled dataset D U , w h is a 100-class benchmark. Similar to the existing works of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42]</ref>, we apply our selection on randomly selected subsets D S ?
nty exploration of the convolutional neural networks (CNNs). A Bayesian approximation introduced in <ref type="bibr" target="#b8">[9]</ref> produce meaningful uncertainty measurements by variational i
bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b31">Liu et al., 2019;</ref><ref type="bibr" target="#b27">Lewis et al., 2020</ref>), yet they are far from perfect. In generati eration framework, built upon the pre-trained sequence-to-sequence (seq2seq) Transformer model BART <ref type="bibr" target="#b27">(Lewis et al., 2020)</ref>. As shown in Figure <ref type="figure" tar et al., 2019;</ref><ref type="bibr" target="#b25">Lawrence et al., 2019)</ref>. Our work uses BART <ref type="bibr" target="#b27">(Lewis et al., 2020)</ref>, a state-of-the-art seq2seq model that off U card with 24 GB memory.</p><p>Model Sizes. Our generation model has the same architecture as BART <ref type="bibr" target="#b27">(Lewis et al., 2020)</ref> with 406M parameters. The content planner
tone of many state-of-the-art models in various natural language understanding and generation tasks <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b31">Liu et al., els to generate more rele-vant and coherent text. We first study a planning model trained from BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> to produce the initial content plan, which
ment has been studied in machine translation <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b11">Freitag et al., 2019;</ref><ref type="bibr" target="#b33">Mansimov et

-autoregressive generation outputs <ref type="bibr" target="#b12">(Ghazvininejad et al., 2019;</ref><ref type="bibr" target="#b25">Lawrence et al., 2019)</ref>. Our work uses BART <ref type="bibr" tar cceleration in non-autoregressive generation <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b25">Lawrence et al., 2019)</ref>, though their refinement mostly focuses

nd JurÄÃ­Äek, 2016;</ref><ref type="bibr" target="#b13">Goyal and Durrett, 2020)</ref> and semantics <ref type="bibr" target="#b49">(Wen et al., 2015;</ref><ref type="bibr" target="#b3">Chen et al., 20
ment has been studied in machine translation <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b11">Freitag et al., 2019;</ref><ref type="bibr" target="#b33">Mansimov et
ment has been studied in machine translation <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b11">Freitag et al., 2019;</ref><ref type="bibr" target="#b33">Mansimov et
A draft is first produced and then refined, with updated words highlighted in italics.</p><p>2001; <ref type="bibr" target="#b47">Stent et al., 2004)</ref>. Specially designed control codes and auxil
., 2017;</ref><ref type="bibr" target="#b43">Rohrbach et al., 2018)</ref>, often with low relevance <ref type="bibr" target="#b28">(Li et al., 2016)</ref> and poor discourse structure <ref type="bibr"
are known to produce low-quality content <ref type="bibr" target="#b51">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b43">Rohrbach et al., 2018)</ref>, often with low relevance <ref type="bib
sequently, planning modules are designed and added into neural systems to enhance content relevance <ref type="bibr" target="#b52">(Wiseman et al., 2018;</ref><ref type="bibr" target="#b35">Moryossef
ions to attend to both left and right. To resolve this discrepancy, we apply causal attention masks <ref type="bibr" target="#b6">(Dong et al., 2019)</ref> over m to disallow attending to the future (
arget="#b1">Callaway, 2003)</ref> and have received dedicated research efforts in rulebased systems <ref type="bibr" target="#b42">(Reed et al., 2018;</ref><ref type="bibr" target="#b0">Balakrishnan e
ed entities <ref type="bibr" target="#b10">(Fan et al., 2018)</ref>, or display a certain attribute <ref type="bibr" target="#b18">(Hu et al., 2017;</ref><ref type="bibr" target="#b32">Luo et al., 201
http://www.tei-c.org/ns/1.0"><head n="5.1">Automatic Evaluation</head><p>We report scores with BLEU <ref type="bibr" target="#b37">(Papineni et al., 2002)</ref>, which is based on n-gram precision (up
ive progress made in many generation tasks, neural systems are known to produce low-quality content <ref type="bibr" target="#b51">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b43">Rohrbach e
omains and adaptability to large pre-trained Transformers. One exception is the Plug and Play model <ref type="bibr" target="#b4">(Dathathri et al., 2020)</ref>, which directly modifies the key and va
standing, it naturally benefits our method.</p><p>Decoding. We employ the nucleus sampling strategy <ref type="bibr" target="#b16">(Holtzman et al., 2019)</ref>, which is shown to yield superior outpu
ourse markers are crucial for coherence <ref type="bibr" target="#b14">(Grote and Stede, 1998;</ref><ref type="bibr" target="#b1">Callaway, 2003)</ref> and have received dedicated research efforts in
ases are identified as noun phrases and verb phrases that contain at least one topic signature word <ref type="bibr" target="#b30">(Lin and Hovy, 2000)</ref>, which is determined by a log-likelihood r
m . On March 7 , 2012 , he was named one of five finalists for the Naismith Award , which is 0.064  <ref type="bibr" target="#b0">(Baevski and Auli, 2019;</ref><ref type="bibr" target="#b19">Radford e
r" target="#b19">(Radford et al., 2019)</ref>, language modeling (for k = 0), and dialogue modeling <ref type="bibr" target="#b29">(Zhang et al., 2018)</ref> where x 1:k is a dialogue history and a co
nt variants of beam search have been explored <ref type="bibr" target="#b14">(Li et al., 2016;</ref><ref type="bibr" target="#b26">Vijayakumar et al., 2018;</ref><ref type="bibr" target="#b12">Kulikov
nt variants of beam search have been explored <ref type="bibr" target="#b14">(Li et al., 2016;</ref><ref type="bibr" target="#b26">Vijayakumar et al., 2018;</ref><ref type="bibr" target="#b12">Kulikov
get="#b26">Vijayakumar et al., 2018;</ref><ref type="bibr" target="#b12">Kulikov et al., 2018;</ref><ref type="bibr" target="#b9">Holtzman et al., 2018)</ref> which can decrease a model's level of rep
get="#b26">Vijayakumar et al., 2018;</ref><ref type="bibr" target="#b12">Kulikov et al., 2018;</ref><ref type="bibr" target="#b9">Holtzman et al., 2018)</ref> which can decrease a model's level of rep
e rare words can add engaging specificity <ref type="bibr" target="#b27">(Weston et al., 2018;</ref><ref type="bibr" target="#b22">See et al., 2019)</ref>.</p></div> <div xmlns="http://www.tei-c.org/n
e rare words can add engaging specificity <ref type="bibr" target="#b27">(Weston et al., 2018;</ref><ref type="bibr" target="#b22">See et al., 2019)</ref>.</p></div> <div xmlns="http://www.tei-c.org/n
s that operate on model-generated sequences <ref type="bibr" target="#b3">(DaumÃ© et al., 2009;</ref><ref type="bibr" target="#b21">Ross et al., 2011;</ref><ref type="bibr" target="#b20">Ranzato et al.
plications. However, the standard approach -training a sequence to sequence model, e.g. Transformer <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref>, to maximize log-likelihood and approxim s are based on the Transformer architecture, a multi-layer feed-forward network with self-attention <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref>. We use a 16-layer Transformer with 8 at
e with maximum-likelihood training, motivating objectives that operate on model-generated sequences <ref type="bibr" target="#b3">(DaumÃ© et al., 2009;</ref><ref type="bibr" target="#b21">Ross et al.,
rable to different noise distributions at test time? Inspired by recent research in computer vision <ref type="bibr" target="#b51">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref typ model using a mixture of noisy and clean samples.</p><p>â¢ We implement a stability training method <ref type="bibr" target="#b51">(Zheng et al., 2016)</ref>, adapted to the sequence labeling scenario r method to improve robustness is to design a representation that is less sensitive to noisy input. <ref type="bibr" target="#b51">Zheng et al. (2016)</ref> presented a general method to stabilize mod
vision <ref type="bibr" target="#b51">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref type="bibr" target="#b12">Cheng et al., 2018)</ref>, and ASR <ref type="bibr" target="#b45">(Sp 6)</ref> presented a general method to stabilize model predictions against small input distortions. <ref type="bibr" target="#b12">Cheng et al. (2018)</ref> continued their work and developed the adve
Neural Machine Translation (NMT; <ref type="bibr" target="#b12">Cheng et al., 2018)</ref>, and ASR <ref type="bibr" target="#b45">(Sperber et al., 2017)</ref>, we propose two Noise-Aware Training (NA mputer vision <ref type="bibr" target="#b27">(Krizhevsky et al., 2012)</ref> and speech recognition <ref type="bibr" target="#b45">(Sperber et al., 2017)</ref>.</p><p>During training, we artificially
i et al., 2016;</ref><ref type="bibr" target="#b42">Schmaltz et al., 2017)</ref> and hybrid systems <ref type="bibr" target="#b43">(Schulz and Kuhn, 2017)</ref>.</p><p>In this paper, we have taken a d
target="#b32">(Miyato et al., 2017;</ref><ref type="bibr" target="#b50">Yasunaga et al., 2018;</ref><ref type="bibr" target="#b5">Bekoulis et al., 2018)</ref>, we do not have a good intuition how it c

e evaluate our methods on real OCR errors and misspellings against state-of-the-art baseline models <ref type="bibr" target="#b37">(Peters et al., 2018;</ref><ref type="bibr" target="#b2">Akbik et al. word units to represent textual tokens. We use the BERT BASE model in our experiments.</p><p>â¢ ELMo <ref type="bibr" target="#b37">(Peters et al., 2018)</ref> utilizes a linear combination of hidden s
and to classify them into pre-defined categories, e.g., person names, organizations, and locations <ref type="bibr" target="#b48">(Tjong Kim Sang and De Meulder, 2003)</ref>. NER systems are often tr ary loss objectives ( Â§3.3, Â§3.4)<ref type="foot" target="#foot_8">9</ref> . We used the CoNLL 2003 <ref type="bibr" target="#b48">(Tjong Kim Sang and De Meulder, 2003)</ref> and the GermEval 2014 <re ortunately, such data is scarce. On the other hand, labeled clean text corpora are widely available <ref type="bibr" target="#b48">(Tjong Kim Sang and De Meulder, 2003;</ref><ref type="bibr" target="#
y sequence of characters. We used settings recommended by the authors and combined FLAIR with GloVe <ref type="bibr" target="#b36">(Pennington et al., 2014</ref>; FLAIR + GloVe) for English and Wikipe
, voting <ref type="bibr" target="#b49">(Wemhoener et al., 2013)</ref>, sequence to sequence models <ref type="bibr" target="#b0">(Afli et al., 2016;</ref><ref type="bibr" target="#b42">Schmaltz et al
ced in an upstream task, like OCR <ref type="bibr" target="#b3">(Alex and Burns, 2014)</ref> or ASR <ref type="bibr" target="#b10">(Chen et al., 2017)</ref>, causing the errors to be propagated downst
s, in contrast to the black-box scenario <ref type="bibr" target="#b4">(Alzantot et al., 2018;</ref><ref type="bibr" target="#b21">Gao et al., 2018)</ref>, where the attacker can only sample model pre
/ref>. In a white-box attack scenario <ref type="bibr" target="#b22">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b18">Ebrahimi et al., 2018)</ref> we assume that the attacker has access t
CR technology was more advanced than several years ago when many historical archives were digitized <ref type="bibr" target="#b26">(Kim and Cassidy, 2015;</ref><ref type="bibr" target="#b34">Neudecker
br" target="#b37">(Peters et al., 2018;</ref><ref type="bibr" target="#b2">Akbik et al., 2018;</ref><ref type="bibr" target="#b16">Devlin et al., 2019)</ref> and demonstrate the effectiveness of our a ref type="bibr" target="#b8">(Bojanowski et al., 2017</ref>; FLAIR + Wiki) for German.</p><p>â¢ BERT <ref type="bibr" target="#b16">(Devlin et al., 2019)</ref> employs a Transformer encoder to learn a
i et al., 2016;</ref><ref type="bibr" target="#b42">Schmaltz et al., 2017)</ref> and hybrid systems <ref type="bibr" target="#b43">(Schulz and Kuhn, 2017)</ref>.</p><p>In this paper, we have taken a d
ed distortions at the level of embeddings <ref type="bibr" target="#b32">(Miyato et al., 2017;</ref><ref type="bibr" target="#b50">Yasunaga et al., 2018;</ref><ref type="bibr" target="#b5">Bekoulis et s on given examples. Adversarial training <ref type="bibr" target="#b32">(Miyato et al., 2017;</ref><ref type="bibr" target="#b50">Yasunaga et al., 2018)</ref>, on the other hand, aims to improve the
nts between the pairs (x, x) â P of noisy and clean sentences using the Levenshtein distance metric <ref type="bibr" target="#b30">(Levenshtein, 1966)</ref>, where P is a corpus of paired noisy and ma
"#b36">(Pennington et al., 2014</ref>; FLAIR + GloVe) for English and Wikipedia FastText embeddings <ref type="bibr" target="#b8">(Bojanowski et al., 2017</ref>; FLAIR + Wiki) for German.</p><p>â¢ BERT

the Beginning, Inside, Outside, End-of-entity and Single-tag-entity subtags are also distinguished <ref type="bibr" target="#b39">(Ratinov and Roth, 2009)</ref>. This method introduces strong depende
ced in an upstream task, like OCR <ref type="bibr" target="#b3">(Alex and Burns, 2014)</ref> or ASR <ref type="bibr" target="#b10">(Chen et al., 2017)</ref>, causing the errors to be propagated downst
nts between the pairs (x, x) â P of noisy and clean sentences using the Levenshtein distance metric <ref type="bibr" target="#b30">(Levenshtein, 1966)</ref>, where P is a corpus of paired noisy and ma
beling is also often performed on user-generated text, which may contain spelling mistakes or typos <ref type="bibr" target="#b15">(Derczynski et al., 2013)</ref>. Errors introduced in an upstream tas rated text is a rich source of informal language containing misspellings, typos, or scrambled words <ref type="bibr" target="#b15">(Derczynski et al., 2013)</ref>. Noise can also be introduced in an u
r Î· train from 10 to 30%, which roughly corresponds to the label-preserving noise range. Similar to <ref type="bibr" target="#b23">Heigold et al. (2018)</ref> and <ref type="bibr" target="#b11">Cheng stness to noise is to augment the training data with samples perturbed using a similar noise model. <ref type="bibr" target="#b23">Heigold et al. (2018)</ref>  </p><formula xml:id="formula_5">LD = 0 L
against state-of-the-art baseline models <ref type="bibr" target="#b37">(Peters et al., 2018;</ref><ref type="bibr" target="#b2">Akbik et al., 2018;</ref><ref type="bibr" target="#b16">Devlin et al., and should also be able to handle misspelled text and out-of-vocabulary (OOV) tokens:</p><p>â¢ FLAIR <ref type="bibr" target="#b2">(Akbik et al., 2018</ref>) learns a Bidi-rectional Language Model (BiL
assume that the attacker has access to the model parameters, in contrast to the black-box scenario <ref type="bibr" target="#b4">(Alzantot et al., 2018;</ref><ref type="bibr" target="#b21">Gao et al.
beling is also often performed on user-generated text, which may contain spelling mistakes or typos <ref type="bibr" target="#b15">(Derczynski et al., 2013)</ref>. Errors introduced in an upstream tas rated text is a rich source of informal language containing misspellings, typos, or scrambled words <ref type="bibr" target="#b15">(Derczynski et al., 2013)</ref>. Noise can also be introduced in an u
corpus <ref type="bibr" target="#b33">(Namysl and Konya, 2019)</ref> using the Tesseract OCR engine <ref type="bibr" target="#b44">(Smith, 2007)</ref>. Moreover, we employed two sets of misspellings r


out relying on prior error correction, which, in case of OCR errors, is still far from being solved <ref type="bibr" target="#b13">(Chiron et al., 2017;</ref><ref type="bibr" target="#b41">Rigaud et a
pes to narrow the list of feasible correction candidates. Another application is data anonymization <ref type="bibr" target="#b31">(Mamede et al., 2016)</ref>.</p><p>Future work will involve improveme
tworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying <ref type="bibr" target="#b22">[24]</ref>. The node-copying model can be used to produce sample grap type="bibr" target="#b21">[23]</ref> uses a non-parametric model for the graph generative model and <ref type="bibr" target="#b22">[24]</ref> proposes a node copying model to achieve flexibility in th rnative, we use a more general generative model for graphs based on copying nodes, as introduced in <ref type="bibr" target="#b22">[24]</ref>. We demonstrate in the following sections that this model setting.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Node Copying</head><p>In <ref type="bibr" target="#b22">[24]</ref>, Pal et al. introduce the node copying model for ð (G). Sa etworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying<ref type="bibr" target="#b22">[24]</ref>. The node-copying model can be used to produce sample grap ode classification when there are very few training labels <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" tar node classification when there are very few training labels<ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" tar s. These limitations were addressed in the follow-up works <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24]</ref>, where <ref type="bibr" target="#b21">[23]</ref> uses a non
for alleviating the data sparsity and cold start problems and improves the recommendation relevance <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" ta 4,</ref><ref type="bibr" target="#b36">38]</ref>, user-user and (or) item-item co-occurrence graphs <ref type="bibr" target="#b17">[19]</ref> and heterogeneous graphs <ref type="bibr" target="#b2">[3, (Î, G, G ððð  ) now takes into account the embeddings derived using G (and G ððð  ), as expressed in <ref type="bibr" target="#b17">(19)</ref>.</p><p>Adopting a Monte-Carlo approximation to the integra iginal BPR framework, but change our choice of xð¢ð ð (Î, G ððð  ). Let â ð¢,G denote the embedding in <ref type="bibr" target="#b17">(19)</ref> for user ð¢ under sampled graph G, and let â ð,G be the cor
for alleviating the data sparsity and cold start problems and improves the recommendation relevance <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" ta 4,</ref><ref type="bibr" target="#b36">38]</ref>, user-user and (or) item-item co-occurrence graphs <ref type="bibr" target="#b17">[19]</ref> and heterogeneous graphs <ref type="bibr" target="#b2">[3, (Î, G, G ððð  ) now takes into account the embeddings derived using G (and G ððð  ), as expressed in <ref type="bibr" target="#b17">(19)</ref>.</p><p>Adopting a Monte-Carlo approximation to the integra iginal BPR framework, but change our choice of xð¢ð ð (Î, G ððð  ). Let â ð¢,G denote the embedding in <ref type="bibr" target="#b17">(19)</ref> for user ð¢ under sampled graph G, and let â ð,G be the cor
n employed for the specific task of social recommendation in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" tar
ion bipartite graph. GNNs have also been employed for the specific task of social recommendation in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" targ Î ||Î|| 2 (13)</formula><p>Equation <ref type="bibr" target="#b10">(11)</ref>, in conjunction with <ref type="bibr" target="#b6">(7)</ref>, illustrates that our final prediction of a ranking involves
oblems. Recently, neural networks have been incorporated into collaborative filtering architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">13]</ref>. These use a combina
d start problems and improves the recommendation relevance <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38]</ref>. The proposed syste The proposed systems exploit user-item interaction graphs <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38]</ref>, user-user and (or) switched focus to applying GNNs in recommendation systems <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38]</ref>. Graph Convolutiona formance gains for the Pinterest recommendation system. Neural Graph Collaborative Filtering (NGCF) <ref type="bibr" target="#b32">[34]</ref> designs a novel information propagation layer which enable target="#b29">[31]</ref>, PinSAGE <ref type="bibr" target="#b36">[38]</ref>, PinSAGE-LSTM, and NGCF <ref type="bibr" target="#b32">[34]</ref>.</p><p>Please refer to the supplementary material for the it has better expressive capability and can model more complicated neighborhood relationships.â¢ NGCF<ref type="bibr" target="#b32">[34]</ref>: NGCF is the state-of-the-art graph-based CF method.It exp
iate the data sparsity problem is to leverage side information. For example, factorization machines <ref type="bibr" target="#b23">[25]</ref> can provide a mechanism for incorporating side information
n employed for the specific task of social recommendation in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" tar
diversity of the top-ð recommended items (i.e., the mutual influence between items). As observed in <ref type="bibr" target="#b20">[22]</ref>, ignoring diversity of the recommended list leads to sub-o ecause accuracy alone does not guarantee satisfactory recommendations, we also assess serendipity@k <ref type="bibr" target="#b20">[22]</ref>, which factors in how surprising and relevant a recommenda
ype="bibr" target="#b17">[19]</ref> and heterogeneous graphs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> coming from heterogeneous interaction types (search, guide, c s between users and items in large-scale e-commerce networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. This problem setting is not in the scope of this paper since
"bibr" target="#b55">[56]</ref>, CoLT <ref type="bibr" target="#b45">[46]</ref>, and Clustered TLBs <ref type="bibr" target="#b44">[45]</ref> combine near virtual-to-physical page translations into si
eservation-based Transparent Huge Pages (THP). We also implemented and evaluated the impact of CoLT <ref type="bibr" target="#b45">[46]</ref> and RMM <ref type="bibr" target="#b33">[34]</ref> on the L xplicit application changes.</p><p>Sub-blocked TLBs <ref type="bibr" target="#b55">[56]</ref>, CoLT <ref type="bibr" target="#b45">[46]</ref>, and Clustered TLBs <ref type="bibr" target="#b44">[45]</r
s for decades <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Virtual memory provides each application with a very larg
ast-level TLBs <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b39">[40]</ref> seek to reduce the number of page walks and improve TLB re
ibr" target="#b40">[41]</ref> and previously proposed in <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Reserved frames are neither free nor in use; they can tra ccess permissions. However, DVMT also requires explicit application changes.</p><p>Sub-blocked TLBs <ref type="bibr" target="#b55">[56]</ref>, CoLT <ref type="bibr" target="#b45">[46]</ref>, and Clust
.</p><p>Address translation overhead can be lowered by reducing the TLB miss rate. Synergistic TLBs <ref type="bibr" target="#b53">[54]</ref> and shared last-level TLBs <ref type="bibr" target="#b9">[
ry protection <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref> identify similarity and contiguity across many conventiona
7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bib 7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bib <ref type="bibr" target="#b38">[39]</ref>. Other processors supported both segmentation and paging <ref type="bibr" target="#b30">[31]</ref>. Unlike past segmentation approaches, TPS adheres to the p
us financial support of the HPS Research Group. Prior work <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bib rformance in applications suffering from limited TLB reach <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bib ce the number of page walks and improve TLB reach. Prior work has proposed hardware PTE prefetchers <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bib
agement component of modern computer systems for decades <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Virtual memory
ual memory has been a fundamental memorymanagement component of modern computer systems for decades <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bib
e="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>, with adversarial training <ref type="bibr" target="#b20">[21]</ref> being one of the most effective methods. It formulates tra Typically, using more attack iterations (higher value of k) produces stronger adversarial examples <ref type="bibr" target="#b20">[21]</ref>. However, each attack iteration needs to compute the gradi ct to traditional PGD-40.</p><p>We apply our technique on Madry's Adversarial Training method (MAT) <ref type="bibr" target="#b20">[21]</ref> and TRADES <ref type="bibr" target="#b38">[39]</ref> and e arget="#b38">39]</ref> focuse on analyzing and improving adversarial machine learning. Madry et al. <ref type="bibr" target="#b20">[21]</ref> first formulate adversarial training as a min-max optimiza higher value of k (more attack iterations), PGDk can generate adversarial examples with higher loss <ref type="bibr" target="#b20">[21]</ref> els. This property is named as transferability. This prope rbations from previous epochs. To compare the attack strength of two attacks, we use Madry's method <ref type="bibr" target="#b20">[21]</ref> to adversarially train two models on MNIST and CIFAR10 and we integrate ATTA with two popular adversarial training methods: Madry's Adversarial Training (MAT) <ref type="bibr" target="#b20">[21]</ref> and TRADES <ref type="bibr" target="#b38">[39]</ref>. By e efficiency</head><p>We select four state-of-the-art adversarial training methods as baselines: MAT <ref type="bibr" target="#b20">[21]</ref>, TRADES <ref type="bibr" target="#b38">[39]</ref>, YOPO <r ed in <ref type="bibr" target="#b15">[16]</ref> and is formulated as a min-max optimization problem <ref type="bibr" target="#b20">[21]</ref>. As one of the most effective defense methods, lots of wor , are widely adopted in various adversarial training methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar rget="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar arget="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar jected back to S, k-step projected gradient descent method <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref> (PGDk) has been widely adopted to generate adversarial exam gradient descent <ref type="bibr" target="#b15">[16]</ref>) is adopted to conduct iterative attack <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" ta div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Following the literature <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta convolutional layers followed by three full-connected layers which is same architecture as used in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39]</ref>. The adversarial p with size = 0.3.</p><p>For the CIFAR10 dataset, we use the wide residual network  which is same as <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39]</ref>. The perturbation a new perspective (e.g., improving transferability between epochs). which is same with other works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" ta re, and hyper-parameters used in this work.</p><p>MNIST. We use the same model architecture used in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta step size and set decay factor as 1 for M-PGD (momentum PGD).</p><p>CIFAR10. Following other works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" ta
ently, lots of works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" tar
/p><p>Recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> show that adversarial examples can be transferred between m et="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. To attack a targeted model f t , the attacker generates tr benchmark label y, f s is trained with f t (x) which is the prediction result of the targeted model <ref type="bibr" target="#b23">[24]</ref> to achieve a higher black-box attack success rate. While o rget="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. Lie et al. <ref ty
get="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> focuse on analyzing
of adversarial training can be close to 100 times larger than natural training.</p><p>Recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta roperty is usually leveraged to perform a black-box attack <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar ttack between models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" tar ,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. Lie et al. <ref type="bibr" target="#b19">[20]</ref> show that adversarial examples generated by an ensemble of
adversarial accuracy can lower natural accuracy. This trade-off has been observed and explained in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref>. A recent work <re
get="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. Lie et al. <ref type="bibr" target="#b19">[20]</ref> show
a model trained with adversarial examples achieves considerable robustness. Recently, lots of works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ f type="bibr" target="#b20">[21]</ref>. As one of the most effective defense methods, lots of works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target
ion on the image can fool a well-trained model. Recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> also show that adve
evaluate the model robustness, we perform the PGD <ref type="bibr" target="#b15">[16]</ref>, M-PGD <ref type="bibr" target="#b7">[8]</ref> and CW <ref type="bibr" target="#b2">[3]</ref> attack with a
get="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> focuse on analyzing
e="bibr" target="#b31">(Weinberger &amp; Saul, 2009)</ref>. In this paradigm, selecting the hardest <ref type="bibr" target="#b5">(Bucher et al., 2016)</ref> or harder <ref type="bibr" target="#b23">(
es has improved both the rate of learning and final performance. Similar to our findings about MoCo,<ref type="bibr" target="#b32">Wu et al. (2017)</ref> find that mining the very hardest negatives hu
et al., 2013)</ref>. Work in object detection has also benefited from efforts to find hard examples <ref type="bibr" target="#b25">(Sung, 1996;</ref><ref type="bibr">CanÃ©vet &amp; Fleuret, 2015;</ref>


rget="#b11">(Chen et al., 2020c)</ref> and the downstream task of linear classification on ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. We make the following contributions (see F e query). To evaluate semantic similarity we used the ImageNet class hierarchy derived from WordNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. For each negative, we computed the tree de
n active learning, for example, it is common to favor examples on which the model is most uncertain <ref type="bibr" target="#b14">(Fu et al., 2013)</ref>. Work in object detection has also benefited

rget="#b11">(Chen et al., 2020c)</ref> and the downstream task of linear classification on ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. We make the following contributions (see F e query). To evaluate semantic similarity we used the ImageNet class hierarchy derived from WordNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. For each negative, we computed the tree de
rget="#b11">(Chen et al., 2020c)</ref> and the downstream task of linear classification on ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. We make the following contributions (see F e query). To evaluate semantic similarity we used the ImageNet class hierarchy derived from WordNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. For each negative, we computed the tree de

b7">(Caron et al., 2018)</ref>, SwAV <ref type="bibr" target="#b8">(Caron et al., 2020)</ref>, SeLa <ref type="bibr" target="#b2">(Asano et al., 2020)</ref>, PCL <ref type="bibr" target="#b20">(Li et
rget="#b11">(Chen et al., 2020c)</ref> and the downstream task of linear classification on ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. We make the following contributions (see F e query). To evaluate semantic similarity we used the ImageNet class hierarchy derived from WordNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. For each negative, we computed the tree de
s for self-supervised have been proposed that do not work within the CID paradigm, including RotNet <ref type="bibr" target="#b15">(Gidaris et al., 2018)</ref>, Jigsaw <ref type="bibr" target="#b22">(
et al., 2013)</ref>. Work in object detection has also benefited from efforts to find hard examples <ref type="bibr" target="#b25">(Sung, 1996;</ref><ref type="bibr">CanÃ©vet &amp; Fleuret, 2015;</ref>
n active learning, for example, it is common to favor examples on which the model is most uncertain <ref type="bibr" target="#b14">(Fu et al., 2013)</ref>. Work in object detection has also benefited
paradigm, selecting the hardest <ref type="bibr" target="#b5">(Bucher et al., 2016)</ref> or harder <ref type="bibr" target="#b23">(Schroff et al., 2015)</ref>  </p></div> <div xmlns="http://www.tei-c
ve curriculum is beyond the scope of this work, curricula have shown utility in many other contexts <ref type="bibr" target="#b3">(Bengio et al., 2009)</ref>.</p></div> <div xmlns="http://www.tei-c.or
paradigm, selecting the hardest <ref type="bibr" target="#b5">(Bucher et al., 2016)</ref> or harder <ref type="bibr" target="#b23">(Schroff et al., 2015)</ref>  </p></div> <div xmlns="http://www.tei-c
<ref type="bibr" target="#b25">(Sung, 1996;</ref><ref type="bibr">CanÃ©vet &amp; Fleuret, 2015;</ref><ref type="bibr" target="#b24">Shrivastava et al., 2016)</ref>. However, none of the aforementioned


strained RL problem and to use a primal-dual algorithm as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> that chooses the parameters automatically. The main advanta dual algorithms for constrained reinforcement learning, e.g. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, in fact converge to the optimal solution under mild assump type="bibr" target="#b20">[21]</ref>. Primal-dual algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, allow us to choose dynamically the multipliers by find the ning as well, where a policy gradient -or actor critic as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> -update is followed by an update of the multipliers along t ="bibr" target="#b0">(1)</ref>. In particular, the proofs in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> rely on the fact that this different time-scale is such tha primal dual algorithm considered here and those proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> provide a manner to solve constrained policy optimization p
dually, by multiplying each signal by its own coefficient, which controls the emphasis placed on it <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" targe problems can be solved through by maximizing an unconstrained Lagrangian, for a specific multiplier <ref type="bibr" target="#b1">[2]</ref>. The combination of different rewards with manually selected
el>(18)</label></formula><p>And therefore reinforcement learning algorithms such as policy gradient <ref type="bibr" target="#b31">[32]</ref> or actor-critic methods <ref type="bibr" target="#b32">[33
neural network-which are universal function approximators <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" t
="#b11">[12,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>-the loss in optim

lgorithms such as policy gradient <ref type="bibr" target="#b31">[32]</ref> or actor-critic methods <ref type="bibr" target="#b32">[33]</ref> can be used to find the parameters ? such that they maximi
ality bound also holds when the parametrization is a universal approximator, e.g., a neural network <ref type="bibr" target="#b11">[12]</ref>). 3. We leverage these theoretical results to establish th eproducing kernel Hilbert spaces <ref type="bibr" target="#b29">[30]</ref> and deep neural networks <ref type="bibr" target="#b11">[12]</ref>. Notice that the objective function and the constraints in ation is. If we consider, for instance, a neural network-which are universal function approximators <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" t
">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Although effective, the multi-objective problem <ref type="bibr" target="#b4">[5]</ref> has several downsides. First, for each set of penalty coeffi
ts own coefficient, which controls the emphasis placed on it <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Although effective, ype="bibr" target="#b13">[14]</ref>, networking <ref type="bibr" target="#b14">[15]</ref>, robotics <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" targ ? 2 ) = ?P ? 1 + (1 -?)P ? 2 . (4)</formula><p>If such policy exists, the previous equation implies <ref type="bibr" target="#b2">(3)</ref>. Thus, to complete the proof of the result we need to establ

supervised tasks, e.g., discriminating whether two subsequences come from the same user's behaviors <ref type="bibr" target="#b35">[36]</ref>. We further improve upon CLRec and propose Multi-CLRec, wh the regular task where ð¥ is a sequence of clicks and ð¦ is the next click to be predicted. Task u2u <ref type="bibr" target="#b35">[36]</ref> adds an auxiliary loss where ð¥ and ð¦ are both sequences fr /div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Complex Pretext Tasks</head><p>In task u2u <ref type="bibr" target="#b35">[36]</ref>, ð¥ and ð¦ are both sequences from the same user, before and
topic of reducing the bias in training and evaluating recommender systems has been explored before <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe s. We note that our fomulation is different from the existing literature on debiasing a recommender <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" targe
rget="#b25">26]</ref> usually outperforms the binary-cross-entropy based approximations such as NCE <ref type="bibr" target="#b17">[18]</ref> and negative sampling <ref type="bibr" target="#b37">[38]< with each positive example. Sampled softmax in general outperforms other approximations such as NCE <ref type="bibr" target="#b17">[18]</ref> and negative sampling <ref type="bibr" target="#b37">[38]<
b30">[31]</ref> 0.7155 0.8371 GRU4Rec <ref type="bibr" target="#b20">[21]</ref> 0.7760 0.8471 Caser <ref type="bibr" target="#b46">[47]</ref> 0.7582 0.8745 DIEN <ref type="bibr" target="#b59">[60]</re
b30">[31]</ref> 0.7155 0.8371 GRU4Rec <ref type="bibr" target="#b20">[21]</ref> 0.7760 0.8471 Caser <ref type="bibr" target="#b46">[47]</ref> 0.7582 0.8745 DIEN <ref type="bibr" target="#b59">[60]</re
anced expressiveness.</p><p>Typical large-scale DCG models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar
duct offline experiments and compare CLRec with sampled softmax. We report the aggregated diversity <ref type="bibr" target="#b1">[2]</ref> in Table <ref type="table" target="#tab_3">1</ref>, and the s clicked by them, divided by the total number of clicks on our platform. â¢ The aggregate diversity <ref type="bibr" target="#b1">[2]</ref>, measured on a sampled subset of users for testing, is the n /1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Aggregate diversity<ref type="bibr" target="#b1">[2]</ref>, i.e. the number of distinct items recommended to a randomly
b30">[31]</ref> 0.7155 0.8371 GRU4Rec <ref type="bibr" target="#b20">[21]</ref> 0.7760 0.8471 Caser <ref type="bibr" target="#b46">[47]</ref> 0.7582 0.8745 DIEN <ref type="bibr" target="#b59">[60]</re
ch size, our implementation is then equivalent to sampling negative examples from the present batch <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta
tive Loss. We study the following type of contrastive loss <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43]</ref> under a negative sampler ð ð (ð¦ | ð¥): where {ð¦ ð } ð¿ ð=1 ar esigning a well-performing proposal distribution ð ð (ð¦ | ð¥) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43]</ref>. InfoNCE <ref type="bibr" target="#b38">[39]</ref> demonstr the present batch <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43]</ref> (see Figure <ref type="figure" target="#fig_13">1a</ref>).
ch size, our implementation is then equivalent to sampling negative examples from the present batch <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta
assignment, non-maximum suppression (NMS) post-processing. They are not fully end-to-end. Recently, <ref type="bibr" target="#b2">Carion et al. (2020)</ref> proposed DETR to eliminate the need for suc d attention module suffers from a quadratic complexity growth with the feature map size. DETR. DETR <ref type="bibr" target="#b2">(Carion et al., 2020)</ref> is built upon the Transformer encoder-deco ng different feature levels. Other hyper-parameter setting and training strategy mainly follow DETR <ref type="bibr" target="#b2">(Carion et al., 2020)</ref>, except that Focal Loss <ref type="bibr" t or 50 epochs and the learning rate is decayed at the 40-th epoch by a factor of 0.1. Following DETR <ref type="bibr" target="#b2">(Carion et al., 2020)</ref>, we train our models using Adam optimizer
="bibr">) 43.8 62.6 47.8 26.5 47.3 58.1 4 BiFPN (Tan et al., 2020) 43.9 62.5 47.7 25.6 47.4</ref>   <ref type="bibr" target="#b36">(Xie et al., 2017)</ref>, our method achieves 48.7 AP and 49.0 AP wit
e="bibr" target="#b11">Huang et al., 2019;</ref><ref type="bibr" target="#b9">Ho et al., 2019;</ref><ref type="bibr" target="#b10">Hu et al., 2019;</ref><ref type="bibr" target="#b23">Parmar et al., 2 ibr" target="#b11">Huang et al. (2019)</ref>; <ref type="bibr" target="#b9">Ho et al. (2019)</ref>; <ref type="bibr" target="#b10">Hu et al. (2019)</ref>; <ref type="bibr" target="#b23">Parmar et al. te the theoretically reduced complexity, <ref type="bibr" target="#b23">Parmar et al. (2019)</ref>; <ref type="bibr" target="#b10">Hu et al. (2019)</ref> admit such approaches are much slower in imple eature of query elements. Different from <ref type="bibr" target="#b23">Parmar et al. (2019)</ref>; <ref type="bibr" target="#b10">Hu et al. (2019)</ref>, deformable attention is just slightly slower
y rises to 50.1 AP. With additional test-time augmentations, the proposed method achieves 52.3 AP.  <ref type="bibr" target="#b32">(Tian et al., 2019)</ref> ResNeXt-101 44.7 64.1 48.4 27.6 47.5 55.6 A
pe="bibr" target="#b21">(Liu et al., 2018b)</ref> further adds an bottom-up path on the top of FPN. <ref type="bibr" target="#b15">Kong et al. (2018)</ref> combines features from all scales by a globa
ainly follow DETR <ref type="bibr" target="#b2">(Carion et al., 2020)</ref>, except that Focal Loss <ref type="bibr" target="#b18">(Lin et al., 2017b)</ref> with loss weight of 2 is used for bounding
. The module can be naturally extended to aggregating multi-scale features, without the help of FPN <ref type="bibr" target="#b17">(Lin et al., 2017a)</ref>. In Deformable DETR , we utilize (multi-sca ctors usually exploit multi-scale features to accommodate this. As one of the pioneering works, FPN <ref type="bibr" target="#b17">(Lin et al., 2017a)</ref> proposes a top-down path to combine multi-s . All the multi-scale feature maps are of C = 256 channels. Note that the top-down structure in FPN <ref type="bibr" target="#b17">(Lin et al., 2017a)</ref> is not used, because our proposed multi-sca /ref> is utilized as the backbone for ablations. Multi-scale feature maps are extracted without FPN <ref type="bibr" target="#b17">(Lin et al., 2017a)</ref>. M = 8 and K = 4 are set for deformable att e cost of unordered memory access. Thus, it is still slightly slower than traditional convolution.  <ref type="bibr" target="#b17">(Lin et al., 2017a</ref><ref type="bibr">) 43.8 62.6 47.8 26.5 47.3 5
pe="bibr" target="#b21">(Liu et al., 2018b)</ref> further adds an bottom-up path on the top of FPN. <ref type="bibr" target="#b15">Kong et al. (2018)</ref> combines features from all scales by a globa
y rises to 50.1 AP. With additional test-time augmentations, the proposed method achieves 52.3 AP.  <ref type="bibr" target="#b32">(Tian et al., 2019)</ref> ResNeXt-101 44.7 64.1 48.4 27.6 47.5 55.6 A
s follows.</p><p>Given the input feature maps x â R CÃHÃW extracted by a CNN backbone (e.g., ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref>), DETR exploits a standard Transformer encoder eature maps {x l } Lâ1 l=1 (L = 4) from the output feature maps of stages C 3 through C 5 in ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref> (transformed by a 1 Ã 1 convolution), where C ion Details. ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009)</ref> pre-trained ResNet-50 <ref type="bibr" target="#b8">(He et al., 2016)</ref> is utilized as the backbone for ablations. Mul
pe="bibr" target="#b21">(Liu et al., 2018b)</ref> further adds an bottom-up path on the top of FPN. <ref type="bibr" target="#b15">Kong et al. (2018)</ref> combines features from all scales by a globa
es a relational graph convolutional network to link prediction task and entity classification task. <ref type="bibr" target="#b35">[36]</ref> propose a heterogeneous graph neural network model which c
s. Plenty of researches have been conducted to solve the name ambiguity problem. Supervised methods <ref type="bibr" target="#b0">[1]</ref>,</p><p>The research is supported by the National Key Researc representations play a critical role to quantify distinctions and similarities between publications <ref type="bibr" target="#b0">[1]</ref>. The majority of existing solutions utilize biographical fea lustering (HAC) method works well for skewed data and is widely in many name disambiguation methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" t ected components to generate the clustering results for each name to be disambiguated. Zhang et al. <ref type="bibr" target="#b0">[1]</ref>: This method uses a global metric learning and local linkage For example, <ref type="bibr" target="#b4">[5]</ref> need to specify the number of distinct author, <ref type="bibr" target="#b0">[1]</ref> need labeled data to estimate the number. For a fair compari d co-occurrence information of text and loss a certain amount of semantic information. Zhang et al. <ref type="bibr" target="#b0">[1]</ref> also use a graph convolutional network based encoder-decoder based framework to extract multiple types of characteristics and relations in publication database. <ref type="bibr" target="#b0">[1]</ref> use a global metric learning and local linkage graph auto-en
number of clusters reaches the specified K.</p><p>When K is unknown, we adopt an optimal modularity <ref type="bibr" target="#b21">[22]</ref> partitioning mechanism to determine the partition of publi
to learn the similarity between publications. Some methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> use the the word c real world we actually have no clue about the number. Some <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref> propose different strategies to estimate the number. However ications, then cluster them into disjoint clusters, each of the which belongs to a distinct person. <ref type="bibr" target="#b7">[8]</ref> propose a Markov random fields based framework to extract mu
="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, the H
n the network embedding technology. DeepWalk <ref type="bibr" target="#b15">[16]</ref> and Node2Vec <ref type="bibr" target="#b28">[29]</ref> use random walk strategy on network and skip-gram <ref typ
l features u (0) i by HGCN in eq.2. Then, we adopt the popular negative sampling method proposed in <ref type="bibr" target="#b17">[18]</ref> to sample negative nodes to increase the optimization effi
ef type="bibr" target="#b4">[5]</ref> or heuristic methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> to learn the similarity between publications. Some methods <
="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> are implemented
to learn the similarity between publications. Some methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> use the the word c real world we actually have no clue about the number. Some <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref> propose different strategies to estimate the number. However ications, then cluster them into disjoint clusters, each of the which belongs to a distinct person. <ref type="bibr" target="#b7">[8]</ref> propose a Markov random fields based framework to extract mu
<ref type="bibr" target="#b6">[7]</ref> to learn the similarity between publications. Some methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" t ous author K is known in advance, but in real world we actually have no clue about the number. Some <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref> propose different s using a recurrent neural network and use HAC to determine the assignment of publications. Xu et al. <ref type="bibr" target="#b3">[4]</ref>: For an ambiguous name, this method build several publicatio n, we assume the number of clusters is set to real value and choose HAC as the clustering method of <ref type="bibr" target="#b3">[4]</ref>. We use pairwise F1-score <ref type="bibr" target="#b13">[14 b4">[5]</ref> utilize a network representation learning based approach on three anonymized network, <ref type="bibr" target="#b3">[4]</ref> construct five relationship networks among publications and
model that use a rich set of metadata and classifies new publications to existing author entities. <ref type="bibr" target="#b27">[28]</ref> combines several domain-specific heuristics in order to au
biguation methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" the representation of publications, but it requires lots of human labeled data to train the model. <ref type="bibr" target="#b8">[9]</ref> propose a hierarchical agglomerative clustering based approa
eveloped in recent years, GNNs can also be used as a node encoder to learn network embeddings. DNGR <ref type="bibr" target="#b32">[33]</ref> proposes a deep neural networks based method to learn a lo
ef type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> or heuristic methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> to learn the simila mes. Many methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr"
l features u (0) i by HGCN in eq.2. Then, we adopt the popular negative sampling method proposed in <ref type="bibr" target="#b17">[18]</ref> to sample negative nodes to increase the optimization effi
node in PHNet to a high quality representation. Inspired by the network embedding methods DeepWalk <ref type="bibr" target="#b15">[16]</ref> and Metapath2Vec <ref type="bibr" target="#b16">[17]</ref> networks with same construction as PHNet, and use HAC to generate the clustering results: DeepWalk <ref type="bibr" target="#b15">[16]</ref>: DeepWalk is a network embedding method based on random wa mbedding. Recently, there has been a growing interest in the network embedding technology. DeepWalk <ref type="bibr" target="#b15">[16]</ref> and Node2Vec <ref type="bibr" target="#b28">[29]</ref> use
parameters are independent with the size of publication set. Besides, we use the alias table method <ref type="bibr" target="#b18">[19]</ref> to sample neighbors and negative nodes, which only takes O
of relationships and the details of network structure to learn representations of nodes. GraphSAGE <ref type="bibr" target="#b24">[25]</ref>: GraphSAGE learn node embeddings through different aggrega uring the graph structural information. GCN <ref type="bibr" target="#b33">[34]</ref> and GraphSage <ref type="bibr" target="#b24">[25]</ref> use graph convolution neural networks to obtain node repre
="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, the H
y the network embedding methods DeepWalk <ref type="bibr" target="#b15">[16]</ref> and Metapath2Vec <ref type="bibr" target="#b16">[17]</ref> which use a random walk strategy and the skip-gram model t central nodes and deviation of different relation quantities via the ordered guidance of meta-path <ref type="bibr" target="#b16">[17]</ref>. Besides, our designed random walk strategy considers the d secondorder proximities of nodes and is applicable for homogeneous weighted network. Metapath2Vec <ref type="bibr" target="#b16">[17]</ref>: Metapath2Vec is applicable for heterogeneous network with nodes, and generate incorporated node paths <ref type="bibr" target="#b31">[32]</ref>. Metapath2Vec <ref type="bibr" target="#b16">[17]</ref> proposes an embedding method on heterogeneous network base
="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, the H
model.</p><p>Given a PHNet G = (V, E, R), we start with initialize node attributes, we use Doc2vec <ref type="bibr" target="#b14">[15]</ref> to encode the text information (i.e, title or abstract) of
ibr" target="#b24">[25]</ref> use graph convolution neural networks to obtain node representations. <ref type="bibr" target="#b34">[35]</ref> introduces a relational graph convolutional network to lin
e in some digital libraries, especially the affiliation information usually has the synonym problem <ref type="bibr" target="#b13">[14]</ref> and hard to be cleaned, so in this paper, we only use the e HAC as the clustering method of <ref type="bibr" target="#b3">[4]</ref>. We use pairwise F1-score <ref type="bibr" target="#b13">[14]</ref> to evaluate the clustering results of our method and the c
the new introduced publications in DLs, some works design incremental name disambiguation methods, <ref type="bibr" target="#b26">[27]</ref> propose a probabilistic model that use a rich set of metad
n the network embedding technology. DeepWalk <ref type="bibr" target="#b15">[16]</ref> and Node2Vec <ref type="bibr" target="#b28">[29]</ref> use random walk strategy on network and skip-gram <ref typ
the new introduced publications in DLs, some works design incremental name disambiguation methods, <ref type="bibr" target="#b26">[27]</ref> propose a probabilistic model that use a rich set of metad
od of a node and then leverages a heterogeneous skip-gram model to perform node embeddings. Hin2Vec <ref type="bibr" target="#b23">[24]</ref>: Hin2Vec is also an unweighted heterogeneous network embed
<ref type="bibr" target="#b6">[7]</ref> to learn the similarity between publications. Some methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" t ous author K is known in advance, but in real world we actually have no clue about the number. Some <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref> propose different s using a recurrent neural network and use HAC to determine the assignment of publications. Xu et al. <ref type="bibr" target="#b3">[4]</ref>: For an ambiguous name, this method build several publicatio n, we assume the number of clusters is set to real value and choose HAC as the clustering method of <ref type="bibr" target="#b3">[4]</ref>. We use pairwise F1-score <ref type="bibr" target="#b13">[14 b4">[5]</ref> utilize a network representation learning based approach on three anonymized network, <ref type="bibr" target="#b3">[4]</ref> construct five relationship networks among publications and
ween publications by these characteristics and construct publication networks, then use graph-based <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> or heuristic method y process the new published publications that may contain potentially ambiguous names. Many methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" t
biguation methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" the representation of publications, but it requires lots of human labeled data to train the model. <ref type="bibr" target="#b8">[9]</ref> propose a hierarchical agglomerative clustering based approa
model that use a rich set of metadata and classifies new publications to existing author entities. <ref type="bibr" target="#b27">[28]</ref> combines several domain-specific heuristics in order to au
rogeneous textrich network. Meta-paths <ref type="bibr" target="#b32">[31]</ref> and motif patterns <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b19">18]</ref> have been widely ado b31">[30]</ref>, bioinformatics <ref type="bibr" target="#b19">[18]</ref>, and information networks <ref type="bibr" target="#b6">[5]</ref>. In the context of heterogeneous information networks, netwo
rns in our framework to extract useful features from the heterogeneous textrich network. Meta-paths <ref type="bibr" target="#b32">[31]</ref> and motif patterns <ref type="bibr" target="#b6">[5,</ref> phs, can offer more flexibility and capture richer network semantics than the widely used meta-path <ref type="bibr" target="#b32">[31]</ref> patterns. Recent studies have shown that incorporating mot of two authors (i.e., "Jure Leskovec" and "Jon Kleinberg").</p><p>It is worth noting that meta-path <ref type="bibr" target="#b32">[31]</ref> can be viewed as a special case of motif patterns when the
<ref type="bibr" target="#b32">[31]</ref> and motif patterns <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b19">18]</ref> have been widely adopted to extract useful structural infor oss various domains, such as neuroscience <ref type="bibr" target="#b31">[30]</ref>, bioinformatics <ref type="bibr" target="#b19">[18]</ref>, and information networks <ref type="bibr" target="#b6">[5
ments, we form the term set T by extracting high-quality phrases from the corpus D using AutoPhrase <ref type="bibr" target="#b27">[26]</ref>.</p><p>â¢ Network Structure: A heterogeneous information ne s performance is limited due to (1) the poor phrase quality compared to the state-of-the-art method <ref type="bibr" target="#b27">[26]</ref> and</p><p>(2) the poor term clustering results compared to are the extracted from raw texts by the state-of-the-art distantly supervised phrase mining method <ref type="bibr" target="#b27">[26]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
n term similarity search <ref type="bibr" target="#b18">[17]</ref>, we apply vMF mixture clustering <ref type="bibr" target="#b4">[3]</ref> in NetTaxo. It is a classical, effective soft clustering met for vMF mixture clustering is manually selected by incrementally increasing k by 1 in the range of <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b7">6]</ref> until coherent cluster
ely adopted in information retrieval to measure the informativeness of a term within a given corpus <ref type="bibr" target="#b30">[29]</ref>. At each taxonomy node c, we calculate the weighted invers
t corpora. In pioneer studies, hierarchical topic modeling <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" tar s many strong baselines, such as hierarchical topic models <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" tar sters in NetTaxo. We have tested our enhanced Hierarchical Latent Dirichlet Allocation (HLDA) model <ref type="bibr" target="#b13">[12]</ref> and its performance is quite similar to HPAM++. Therefore,
rmation networks (i.e., networks of typed nodes and edges) <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33]</ref>. For example, NetClus <ref type="bibr" target="#b34">[33]</ type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33]</ref>. For example, NetClus <ref type="bibr" target="#b34">[33]</ref> starts with user-provided seed nodes and applies authority
nodes of businesses, users, and reviews.</p><p>While most existing methods solely rely on text data <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" targ tion in the embedding space <ref type="bibr" target="#b12">[11]</ref> and neural network classifier <ref type="bibr" target="#b3">[2]</ref>. In our setting, there are no hyponymy labels.</p><p>Term Cl
A high-quality topic taxonomy benefits various downstream applications, such as search and indexing <ref type="bibr" target="#b44">[43]</ref>, personalized content recommendation <ref type="bibr" targ
ments, we form the term set T by extracting high-quality phrases from the corpus D using AutoPhrase <ref type="bibr" target="#b27">[26]</ref>.</p><p>â¢ Network Structure: A heterogeneous information ne s performance is limited due to (1) the poor phrase quality compared to the state-of-the-art method <ref type="bibr" target="#b27">[26]</ref> and</p><p>(2) the poor term clustering results compared to are the extracted from raw texts by the state-of-the-art distantly supervised phrase mining method <ref type="bibr" target="#b27">[26]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
ng tree. The lexical patterns are either manually designed <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" tar
rget="#b3">[2,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b45">44]</ref>, incorporating network structures can bring additional, val ng term embedding, topdown hierarchical clustering methods <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b45">44]</ref> achieve the state-ofthe-art. For example, TaxoGen <ref type per, and term-paper relations. Note that, previous methods <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref> choose five areas from this dataset too, for example in <re y the state-of-the-art work on topic taxonomy construction <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref> and recent work on topic modeling <ref type="bibr" target=" ng nodes. Following previous taxonomy construction methods <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref>, we perform the term intrusion test. Specifically, for each 16,</ref><ref type="bibr" target="#b45">44]</ref> achieve the state-ofthe-art. For example, TaxoGen <ref type="bibr" target="#b45">[44]</ref> learns local term embedding from the documents associated leveraging both text data and network structures.</p><p>Based on our observations and previous work <ref type="bibr" target="#b45">[44]</ref>, using term embedding learned from textual contexts alone zation to our subsequent motif instance selection step. Specifically, we first follow previous work <ref type="bibr" target="#b45">[44]</ref> to learn local term embedding and obtain initial term clus dition the term embeddings to the current taxonomy node.</p><p>To this end, we follow previous work <ref type="bibr" target="#b45">[44]</ref> and adopt the idea of local embedding <ref type="bibr" tar h node according to its own associated (weighted) documents. Its effectiveness has been verified in <ref type="bibr" target="#b45">[44]</ref> through ablation tests.</p><p>We use skip-gram with negati ent taxonomy node. Therefore, our loss function slightly differs from the ones in the previous work <ref type="bibr" target="#b45">[44]</ref> as well as the original local embedding work <ref type="bi aggregating clustering probability from their connected terms. This process is the same as that in <ref type="bibr" target="#b45">[44]</ref>. The aggregated probabilities of a document, multiplied by ef><ref type="bibr" target="#b45">44]</ref> choose five areas from this dataset too, for example in <ref type="bibr" target="#b45">[44]</ref>, information retrieval, computer vision, robotics, securit rmance is quite similar to HPAM++. Therefore, we only present the results of HPAM++ here. â¢ TaxoGen <ref type="bibr" target="#b45">[44]</ref> is the state-of-the-art topic taxonomy construction method level and k = 4 for the second level of the taxonomy in both the DBLP and Yelp dataset. In TaxoGen <ref type="bibr" target="#b45">[44]</ref>, this number is set to 5 for all levels, which is not far
ng tree. The lexical patterns are either manually designed <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" tar
atic topic taxonomy construction from text corpora. In pioneer studies, hierarchical topic modeling <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" ta ta. As demonstrated in its paper, it beats many strong baselines, such as hierarchical topic models <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" ta
rget="#b3">[2,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b45">44]</ref>, incorporating network structures can bring additional, val ng term embedding, topdown hierarchical clustering methods <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b45">44]</ref> achieve the state-ofthe-art. For example, TaxoGen <ref type per, and term-paper relations. Note that, previous methods <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref> choose five areas from this dataset too, for example in <re y the state-of-the-art work on topic taxonomy construction <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref> and recent work on topic modeling <ref type="bibr" target=" ng nodes. Following previous taxonomy construction methods <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref>, we perform the term intrusion test. Specifically, for each 16,</ref><ref type="bibr" target="#b45">44]</ref> achieve the state-ofthe-art. For example, TaxoGen <ref type="bibr" target="#b45">[44]</ref> learns local term embedding from the documents associated leveraging both text data and network structures.</p><p>Based on our observations and previous work <ref type="bibr" target="#b45">[44]</ref>, using term embedding learned from textual contexts alone zation to our subsequent motif instance selection step. Specifically, we first follow previous work <ref type="bibr" target="#b45">[44]</ref> to learn local term embedding and obtain initial term clus dition the term embeddings to the current taxonomy node.</p><p>To this end, we follow previous work <ref type="bibr" target="#b45">[44]</ref> and adopt the idea of local embedding <ref type="bibr" tar h node according to its own associated (weighted) documents. Its effectiveness has been verified in <ref type="bibr" target="#b45">[44]</ref> through ablation tests.</p><p>We use skip-gram with negati ent taxonomy node. Therefore, our loss function slightly differs from the ones in the previous work <ref type="bibr" target="#b45">[44]</ref> as well as the original local embedding work <ref type="bi aggregating clustering probability from their connected terms. This process is the same as that in <ref type="bibr" target="#b45">[44]</ref>. The aggregated probabilities of a document, multiplied by ef><ref type="bibr" target="#b45">44]</ref> choose five areas from this dataset too, for example in <ref type="bibr" target="#b45">[44]</ref>, information retrieval, computer vision, robotics, securit rmance is quite similar to HPAM++. Therefore, we only present the results of HPAM++ here. â¢ TaxoGen <ref type="bibr" target="#b45">[44]</ref> is the state-of-the-art topic taxonomy construction method level and k = 4 for the second level of the taxonomy in both the DBLP and Yelp dataset. In TaxoGen <ref type="bibr" target="#b45">[44]</ref>, this number is set to 5 for all levels, which is not far
target="#b2">[1,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" tar
g is typically conducted on the entire document collection <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b23">22]</ref>. However, such learning paradigm faces a major drawback in
A high-quality topic taxonomy benefits various downstream applications, such as search and indexing <ref type="bibr" target="#b44">[43]</ref>, personalized content recommendation <ref type="bibr" targ
search and indexing <ref type="bibr" target="#b44">[43]</ref>, personalized content recommendation <ref type="bibr" target="#b47">[46]</ref>, and question answering <ref type="bibr" target="#b43">[42
get="#b13">12,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b39">38]</ref> and bottom-up agglomerative clusteringbased <ref type="bibr get="#b13">12,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b39">38]</ref>. It utilizes the same local embedding idea as our model, bu o significantly improve the quality of clustering.</p><p>Network Clustering-based Methods. CATHYHIN <ref type="bibr" target="#b39">[38]</ref> is arguably the state-of-the-art method solely based on ne but ignores network structures. â¢ CATHYHIN++ is a method enhanced by us from the original CATHYHIN <ref type="bibr" target="#b39">[38]</ref> method. CATHYHIN <ref type="bibr" target="#b39">[38]</ref> nhanced by us from the original CATHYHIN <ref type="bibr" target="#b39">[38]</ref> method. CATHYHIN <ref type="bibr" target="#b39">[38]</ref> is a topic taxonomy construction method using network data r, venue-paper, year-paper, year range-paper, and term-paper relations. Note that, previous methods <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref> choose five areas been a very challenging task. Inspired by the state-of-the-art work on topic taxonomy construction <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref> and recent work on should be distinguishable from its sibling nodes. Following previous taxonomy construction methods <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref>, we perform the te
#b38">37,</ref><ref type="bibr" target="#b39">38]</ref> and bottom-up agglomerative clusteringbased <ref type="bibr" target="#b9">[8]</ref> methods are arguably the most popular and effective framewor
e supervision or seeds <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" tar
arget="#b7">6,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b48">47]</ref>. Such patterns have
g vectors from text using word2vec <ref type="bibr" target="#b18">[17]</ref> and network using LINE <ref type="bibr" target="#b35">[34]</ref> separately, where every embedding vector has a dimension o
xonomy node.</p><p>Term embedding learning is typically conducted on the entire document collection <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b23">22]</ref>. However, such lea rget="#b45">[44]</ref> through ablation tests.</p><p>We use skip-gram with negative sampling (SGNS) <ref type="bibr" target="#b18">[17]</ref> as our base embedding model. At each taxonomy node, we use be how to select Mc in section 4.5.</p><p>The probabilities are approximated with negative sampling <ref type="bibr" target="#b18">[17]</ref>.</p><formula xml:id="formula_2">log P(m | t) = log Ï (r T sine similarity between term embedding has demonstrated its effectiveness in term similarity search <ref type="bibr" target="#b18">[17]</ref>, we apply vMF mixture clustering <ref type="bibr" target=" the network structure. Specifically, we first learn term embedding vectors from text using word2vec <ref type="bibr" target="#b18">[17]</ref> and network using LINE <ref type="bibr" target="#b35">[34]
et="#b11">[10,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b39">38]</ref> and bottom-up agglo et="#b11">[10,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b39">38]</ref>. It utilizes the sa
rmation networks (i.e., networks of typed nodes and edges) <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33]</ref>. For example, NetClus <ref type="bibr" target="#b34">[33]</ type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33]</ref>. For example, NetClus <ref type="bibr" target="#b34">[33]</ref> starts with user-provided seed nodes and applies authority
ely adopted in information retrieval to measure the informativeness of a term within a given corpus <ref type="bibr" target="#b30">[29]</ref>. At each taxonomy node c, we calculate the weighted invers
target="#b2">[1,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" tar
.</p><p>While most existing methods solely rely on text data <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" tar an be identified through supervised models, for example, semantic projection in the embedding space <ref type="bibr" target="#b12">[11]</ref> and neural network classifier <ref type="bibr" target="#b3
xonomy node.</p><p>Term embedding learning is typically conducted on the entire document collection <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b23">22]</ref>. However, such lea rget="#b45">[44]</ref> through ablation tests.</p><p>We use skip-gram with negative sampling (SGNS) <ref type="bibr" target="#b18">[17]</ref> as our base embedding model. At each taxonomy node, we use be how to select Mc in section 4.5.</p><p>The probabilities are approximated with negative sampling <ref type="bibr" target="#b18">[17]</ref>.</p><formula xml:id="formula_2">log P(m | t) = log Ï (r T sine similarity between term embedding has demonstrated its effectiveness in term similarity search <ref type="bibr" target="#b18">[17]</ref>, we apply vMF mixture clustering <ref type="bibr" target=" the network structure. Specifically, we first learn term embedding vectors from text using word2vec <ref type="bibr" target="#b18">[17]</ref> and network using LINE <ref type="bibr" target="#b35">[34]
ments, we form the term set T by extracting high-quality phrases from the corpus D using AutoPhrase <ref type="bibr" target="#b27">[26]</ref>.</p><p>â¢ Network Structure: A heterogeneous information ne s performance is limited due to (1) the poor phrase quality compared to the state-of-the-art method <ref type="bibr" target="#b27">[26]</ref> and</p><p>(2) the poor term clustering results compared to are the extracted from raw texts by the state-of-the-art distantly supervised phrase mining method <ref type="bibr" target="#b27">[26]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
usal model which could measure this causal relationship i.e. Linear Structual Equation Models (SEM) <ref type="bibr" target="#b20">(Shimizu et al., 2006)</ref>. Existing methods for disentangled repre et="#b6">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b24">Zhang &amp; Hyvarinen, 2012;</ref><ref type="bibr" target="#b20">Shimizu et al., 2006)</ref>. <ref type="bibr" target="#b19">Pearl (20 2009)</ref> introduce a probabilistic graphical model based framework to learn causality from data. <ref type="bibr" target="#b20">Shimizu et al. (2006)</ref> proposed an effective method called LiNGA
usal graph from pure observational data has attracted large amount of attention in the past decades <ref type="bibr" target="#b6">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b24">Zhang &amp; H
ural language processing, recommender systems <ref type="bibr" target="#b8">(Hsu et al., 2017;</ref><ref type="bibr" target="#b16">Ma et al., 2019;</ref><ref type="bibr" target="#b7">Hsieh et al., 201

codes to be independent of each other. <ref type="bibr" target="#b12">Kim &amp; Mnih (2018)</ref>; <ref type="bibr" target="#b2">Chen et al. (2018)</ref> further improve the independent by reducing t lance between independence of disentangled factors and reconstruction performance. While factor VAE <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> proposes a new frame work which focuses sole


s applications such as speech, object recognition, natural language processing, recommender systems <ref type="bibr" target="#b8">(Hsu et al., 2017;</ref><ref type="bibr" target="#b16">Ma et al., 2019
s applications such as speech, object recognition, natural language processing, recommender systems <ref type="bibr" target="#b8">(Hsu et al., 2017;</ref><ref type="bibr" target="#b16">Ma et al., 2019

pe="bibr" target="#b8">(Hsu et al., 2017;</ref><ref type="bibr" target="#b16">Ma et al., 2019;</ref><ref type="bibr" target="#b7">Hsieh et al., 2018)</ref>. The reason is that it would help enhancing
ts decrease along with neighbor hops increasing. Inspired by the architecture design of Transformer <ref type="bibr" target="#b30">[31]</ref>, we leverage attention mechanism to learn the weight of so
eural Networks</head><p>Graph neural networks (GNNs) , especially gated graph neural network (GGNN) <ref type="bibr" target="#b18">[19]</ref>, graph convolutional network (GCN) <ref type="bibr" target
m overwhelming alternatives. Collaborative filtering (CF) based methods (e.g., matrix factorization <ref type="bibr" target="#b17">[18]</ref>) have been extensively used for recommendation, assuming u
GCN) <ref type="bibr" target="#b16">[17]</ref>, graph inductive representation learning (GraphSAGE) <ref type="bibr" target="#b8">[9]</ref> and graph attention network (GAT) <ref type="bibr" target="# on, so the padding would lead to both high space and time complexity. Followed by the previous work <ref type="bibr" target="#b8">[9]</ref>, we sample fixed number of source nodes and pad zeros when s
tion learning (GraphSAGE) <ref type="bibr" target="#b8">[9]</ref> and graph attention network (GAT) <ref type="bibr" target="#b31">[32]</ref>, have been attracting considerable attention recently. The performs multi-head attention to jointly attend information from different representation subspaces <ref type="bibr" target="#b31">[32]</ref>, while treats each subspace equally, and can be seen as a
GCN) <ref type="bibr" target="#b16">[17]</ref>, graph inductive representation learning (GraphSAGE) <ref type="bibr" target="#b8">[9]</ref> and graph attention network (GAT) <ref type="bibr" target="# on, so the padding would lead to both high space and time complexity. Followed by the previous work <ref type="bibr" target="#b8">[9]</ref>, we sample fixed number of source nodes and pad zeros when s
t, i.e., bring enhanced generalization ability as well as improved robustness to adversarial attack <ref type="bibr" target="#b1">[2]</ref>, but also make the downstream process more interpretable whi
then used by supervised learning models to predict the score <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>. However, these methods always treat each user-item record
="bibr" target="#b12">13]</ref>, texts <ref type="bibr" target="#b15">[16]</ref> and user behaviors <ref type="bibr" target="#b21">[22]</ref>. For graph-structure data, GAT performs multi-head attenti
paths between users and items. Cao et al. <ref type="bibr" target="#b3">[4]</ref> and Zhang et al. <ref type="bibr" target="#b41">[42]</ref> jointly learn latent representations in CF as well as item
Ns framework to obtain better user and item representations by aggregating neighborhood information <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" targ sponding semantic relations. Recently, some HIN based GNNs are proposed for information propagation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targ
g future lightning occurrences. However, although extrapolationbased methods for weather nowcasting <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> can be migrated to re sensitive to different dimensions are assembled to predict mobile events in the city. Shi et al. <ref type="bibr" target="#b0">[1]</ref> proposed convolutional LSTM (ConvLSTM) for precipitation now tâ1 .</formula><p>The ConvLSTM in this paper does not include peephole connections, as mentioned in <ref type="bibr" target="#b0">[1]</ref>. The data first enters the CNN modules, where sequentially a
rence) from these parameters, researchers have proposed a lot of lightning parameterization schemes <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>. Unfortunately, the gh lightning is the result of multiple factors, it tends to be more closely related to some of them <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" t ice, and graupel. But they did not provide a direct way to acquire lightning density. McCaul et al. <ref type="bibr" target="#b4">[5]</ref> proposed two threats that can be converted to the total flas e of our model.</p><p>1 https://keras.io We choose parameters that are closely related to lightning <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" t at F2 is one of the state of the art lightning parameterization methods in the field of meteorology <ref type="bibr" target="#b4">[5]</ref>. It poses a lightning threat that calculated as:</p><formula at height level z, respectively. The threat can be converted into flash rate by a threshold = 0.40 <ref type="bibr" target="#b4">[5]</ref>.</p><p>GBDT. The Gradient Boost Decision Tree (GBDT) is a ki rs are exactly meteorological factors leveraged by the lightning parameterization schemes threat F1 <ref type="bibr" target="#b4">[5]</ref> and PR92 <ref type="bibr" target="#b5">[6]</ref>.</p><p>Rese
historical data. In StepDeep <ref type="bibr" target="#b29">[30]</ref>, a series of 3D convolution <ref type="bibr" target="#b30">[31]</ref> filters that are sensitive to different dimensions are ass
cal forecasts <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>: probability of detection (POD), false alarm ratio (FAR),

although extrapolationbased methods for weather nowcasting <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> can be migrated to lightning forecast tasks, they encounter encoder and a CNN content encoder cooperate to predict future frames in video sequences. In PredRNN <ref type="bibr" target="#b2">[3]</ref>, a vertical data flow is added in the stacked ConvLSTM, maki
e encoder and the decoder. Moreover, drawing on the creativity of the previous attention mechanisms <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref> and the depthwi itical information. Previous works show a variety of design strategies for attention mechanisms. In <ref type="bibr" target="#b20">[21]</ref>, attention mechanism is introduced into image captioning. ="http://www.tei-c.org/ns/1.0"><head>C. CHANNEL-WISE ATTENTION</head><p>Referring to previous works <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref>, we introduce a
e encoder and the decoder. Moreover, drawing on the creativity of the previous attention mechanisms <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref> and the depthwi itical information. Previous works show a variety of design strategies for attention mechanisms. In <ref type="bibr" target="#b20">[21]</ref>, attention mechanism is introduced into image captioning. ="http://www.tei-c.org/ns/1.0"><head>C. CHANNEL-WISE ATTENTION</head><p>Referring to previous works <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref>, we introduce a
f commonly used skill-scores in meteorological forecasts <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>: probability of
long-term spatiotemporal features of gestures from vedios, respectively. Similar design appears in <ref type="bibr" target="#b33">[34]</ref> for travel demand prediction. These works provide us with
e encoder and the decoder. Moreover, drawing on the creativity of the previous attention mechanisms <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref> and the depthwi itical information. Previous works show a variety of design strategies for attention mechanisms. In <ref type="bibr" target="#b20">[21]</ref>, attention mechanism is introduced into image captioning. ="http://www.tei-c.org/ns/1.0"><head>C. CHANNEL-WISE ATTENTION</head><p>Referring to previous works <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref>, we introduce a

rget="#b41">(Paszke et al., 2017)</ref>, NumPy <ref type="bibr" target="#b36">(Oliphant, 2006;</ref><ref type="bibr" target="#b49">Walt et al., 2011;</ref><ref type="bibr" target="#b18">Harris et al.,
<ref type="bibr" target="#b46">Thomas et al., 2018;</ref><ref type="bibr">Kondor et al., 2018;</ref><ref type="bibr" target="#b54">Weiler et al., 2018b;</ref><ref type="bibr">a;</ref><ref type="bibr"

<ref type="bibr" target="#b27">Kitaev et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020;</ref><ref type="bibr" target="#b25">Katharopoulos et al., 2020)</ref>. An alternative approach is to inco
rget="#b41">(Paszke et al., 2017)</ref>, NumPy <ref type="bibr" target="#b36">(Oliphant, 2006;</ref><ref type="bibr" target="#b49">Walt et al., 2011;</ref><ref type="bibr" target="#b18">Harris et al.,

the physical properties of the modelled system. Indeed, a famous result known as Noether's theorem <ref type="bibr" target="#b35">(Noether, 1971)</ref> states that if the Hamiltonian function has a s
ww.tei-c.org/ns/1.0"><head>E.2. QM9</head><p>For the QM9 experiment setup we follow the approach of <ref type="bibr" target="#b0">Anderson et al. (2019)</ref> for parameterising the inputs and for the

orks that use stand-alone selfattention on images to achieve competitive performance to convolutions<ref type="bibr" target="#b39">(Parmar et al., 2019a;</ref><ref type="bibr" target="#b9">Dosovitskiy
target="#b10">[11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38]</ref>, we are the first to discuss their importance under heterop COMBINE functions that leverage each representation separately-e.g., concatenation, LSTM-attention <ref type="bibr" target="#b37">[38]</ref>. This design is introduced in jumping knowledge networks < <ref type="bibr" target="#b37">[38]</ref>. This design is introduced in jumping knowledge networks <ref type="bibr" target="#b37">[38]</ref> and shown to increase the representation power of GCNs und ons from two rounds with the embedded ego-representation (following the jumping knowledge framework <ref type="bibr" target="#b37">[38]</ref>), GCN's accuracy increases to 58.93%Â±3.17 for h = 0.1, a 2 label>(7)</label></formula><p>where we empirically find concatenation works better than max-pooling <ref type="bibr" target="#b37">[38]</ref> as the COMBINE function.</p><p>In the classification stage e compare GraphSAGE, GCN-Cheby and GCN to their corresponding variants enhanced with JK connections <ref type="bibr" target="#b37">[38]</ref>. GCN and GCN-Cheby benefit significantly from D3 in hetero with and without JK connections is similar (gaps mostly less than 2%), matching the observations in <ref type="bibr" target="#b37">[38]</ref>.</p><p>While other design choices and implementation detai K, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type="bibr" target="#b37">[38]</ref> without changing the number of layers or other hyperparame K, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type="bibr" target="#b37">[38]</ref> without changing the number of layers or other hyperparame
hborhoods via different mechanisms (e.g., averaging, LSTM) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">36]</ref>. However, in the re ifferent neighbors more precisely as a weighted average of the ego-and neighbor-features. GraphSAGE <ref type="bibr" target="#b10">[11]</ref> generalizes the aggregation beyond averaging, and models t -and the aggregated neighbor-embeddings without 'mixing' them is with concatenation as in GraphSAGE <ref type="bibr" target="#b10">[11]</ref>-rather than averaging all of them as in the GCN model by K ef type="bibr" target="#b35">[36]</ref> GCN-Cheby <ref type="bibr" target="#b6">[7]</ref> GraphSAGE <ref type="bibr" target="#b10">[11]</ref> MixHop <ref type="bibr" target="#b0">[1]</ref> H2GCN (prop ding transformations per round in H 2 GCN? GCN <ref type="bibr" target="#b16">[17]</ref>, GraphSAGE <ref type="bibr" target="#b10">[11]</ref> and other GNN models embed the intermediate representation &amp; GCN-Cheby <ref type="bibr" target="#b16">[17]</ref>: https://github.com/tkipf/gcn â¢ GraphSAGE <ref type="bibr" target="#b10">[11]</ref>: https://github.com/williamleif/graphsage-simple (PyTorch : 3 * early_stopping: 40 We report the best performance, for Set 1 with a = 64, b = 5e-4.â¢ GraphSAGE<ref type="bibr" target="#b10">[11]</ref>:-hid_units: a â {64, 128} lr: b â {0.1, 0.7} epochs: 500</ led the default feature normalization in the official implementation for this baseline. â¢ GraphSAGE <ref type="bibr" target="#b10">[11]</ref>:</p><p>-hid_units: a â {64, 128} lr: b â {0.1, 0.7} epochs led the default feature normalization in the official implementation for this baseline. â¢ GraphSAGE <ref type="bibr" target="#b10">[11]</ref>:</p><p>-hid_units: 64 lr: {0.1, 0.7} epochs: 500</p><p>â¢ M intermediate representations. While these designs have been utilized separately in some prior works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" targ 1)d)</formula><p>Solving the above inequality for Î´ 1 , we get the amount of perturbation needed as <ref type="bibr" target="#b10">(11)</ref> and the least absolute amount of perturbation needed is |Î´
cy matrix with a sparsified version of a diffusion matrix (e.g., heat kernel or PageRank). Geom-GCN <ref type="bibr" target="#b25">[26]</ref> precomputes unsupervised node embeddings and uses neighbor 0.7), and across the full spectrum ("Overall"). The "*" denotes ranks based on results reported in <ref type="bibr" target="#b25">[26]</ref>. Real datasets &amp; setup We now evaluate the performance of nodes per class for train/validation/test<ref type="foot" target="#foot_2">2</ref> ) provided by <ref type="bibr" target="#b25">[26]</ref>.</p><p>For Cora-Full, we generate 3 random splits, with 25 the best results among the three recentlyproposed GEOM-GCN variants ( Â§ 4), directly from the paper <ref type="bibr" target="#b25">[26]</ref>: other models (including ours) outperform this method sign ng universities, originally collected by the CMU WebKB project. We used the preprocessed version in <ref type="bibr" target="#b25">[26]</ref>. In these networks, nodes are web pages, which are classif br" target="#b28">[29]</ref>. For the classification task, we utilize the class labels generated by <ref type="bibr" target="#b25">[26]</ref>, where the nodes are categorized into 5 classes based on t erage traffic. â¢ Actor is a graph representing actor co-occurrence in Wikipedia pages, processed by <ref type="bibr" target="#b25">[26]</ref> based on the film-director-actor-writer network in <ref ty ter network in <ref type="bibr" target="#b34">[35]</ref>. We also use the class labels generated by <ref type="bibr" target="#b25">[26]</ref>. â¢ Cora, Pubmed and Citeseer are citation graphs originall fferent data splits. Best model per benchmark highlighted in gray. The "*" results are obtained from<ref type="bibr" target="#b25">[26]</ref> and "N/A" denotes non-reported results.</figDesc><table><r ix of A + I.</note> 			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><ref type="bibr" target="#b25">[26]</ref> claims that the ratios are 60%/20%/20%, which is different atent space to define graph convolution. Some of these works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12]</ref> acknowledge the cha
Y t as before, with the modification that in this case we have for signal Y s (similarly for Y t ): <ref type="bibr" target="#b19">(20)</ref>. The rest of the proof is similar to Proof 3.</p><formula imate inference algorithms (e.g., iterative classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>, loopy belief propagation) are used to solve the problem. B
nodes often belong to the same class or have similar features ("birds of a feather flock together") <ref type="bibr" target="#b20">[21]</ref>. For example, friends are likely to have similar political
get="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref> with edge homophily r tion as the feature vector for each node. â¢ Cora Full is an extended version of Cora, introduced in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>, which contain more
We now evaluate the performance of our model and existing GNNs on a variety of real-world datasets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" ta sed by <ref type="bibr" target="#b25">[26]</ref> based on the film-director-actor-writer network in <ref type="bibr" target="#b34">[35]</ref>. We also use the class labels generated by <ref type="bibr
et="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targe and Citeseer are citation graphs originally introduced in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b21">22]</ref>, which are among the most widely used benchmarks for semi-s
/ref>. In general, the synthetic graphs are generated by a modified preferential attachment process <ref type="bibr" target="#b2">[3]</ref>: The number of class labels |Y| in the synthetic graph is pr
arget="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref> with edge homophily ratio h ranging from strong heterophily e. â¢ Cora Full is an extended version of Cora, introduced in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>, which contain more papers and research fields than Cora. T >22]</ref>, which are among the most widely used benchmarks for semi-supervised node classification <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>. Each node is assi of words representation as the feature vector for each node.</p><p>Data Limitations As discussed in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>, Cora, Pubmed and
om the corresponding class in a real benchmark (e.g., Cora <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref> or ogbn-products <ref type="bibr" target="#b12">[13]</ref>)
We now evaluate the performance of our model and existing GNNs on a variety of real-world datasets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" ta sed by <ref type="bibr" target="#b25">[26]</ref> based on the film-director-actor-writer network in <ref type="bibr" target="#b34">[35]</ref>. We also use the class labels generated by <ref type="bibr
cross neighboring nodes (especially within a community or cluster) for any set of original features <ref type="bibr" target="#b27">[28]</ref>. While this may work well in the case of homophily, where
aph information, proposing diagnostic measurements based on feature smoothness and label smoothness <ref type="bibr" target="#b11">[12]</ref> that may guide the learning process. To capture more graph Some of these works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12]</ref> acknowledge the challenges of learning from graphs with het
eal-world datasets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar of node classification by leveraging the correlations between the node labels and their attributes <ref type="bibr" target="#b29">[30]</ref>. Since exact inference is NP-hard, approximate inference a d by sampling feature vectors of nodes from the corresponding class in a real benchmark (e.g., Cora <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref> or ogbn-products < target="#b25">[26]</ref>. â¢ Cora, Pubmed and Citeseer are citation graphs originally introduced in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b21">22]</ref>, which are among t
" target="#b18">[19]</ref> and has fast linearized versions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8]</ref>. Different from the setup where GNNs are employed, BP does no
al random field, trained with expectation maximization using GNNs; Correlated Graph Neural Networks <ref type="bibr" target="#b14">[15]</ref> model the correlation structure in the residuals of a regr
get="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref> with edge homophily r tion as the feature vector for each node. â¢ Cora Full is an extended version of Cora, introduced in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>, which contain more
filtering-, the higher order polynomials of the normalized adjacency matrix A is a low-pass filter <ref type="bibr" target="#b36">[37]</ref>, so intermediate outputs from earlier rounds contain highe eighbor-embeddings. We found that removing the usual nonlinear transformations per round, as in SGC <ref type="bibr" target="#b36">[37]</ref>, works better (App. D.2), in which case we only need to in a learnable matrix. Our design in Eq. 5 aggregates different neighborhoods in a similar way to SGC <ref type="bibr" target="#b36">[37]</ref>, which has shown that removing non-linearities does not ne
" target="#b18">[19]</ref> and has fast linearized versions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8]</ref>. Different from the setup where GNNs are employed, BP does no
" target="#b18">[19]</ref> and has fast linearized versions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8]</ref>. Different from the setup where GNNs are employed, BP does no
apart from MixHop <ref type="bibr" target="#b0">[1]</ref> (cf. Â§ 3.1), Graph Diffusion Convolution <ref type="bibr" target="#b17">[18]</ref> replaces the adjacency matrix with a sparsified version of
rs are more likely to connect to accomplices than to other fraudsters in online purchasing networks <ref type="bibr" target="#b23">[24]</ref>. Since many existing GNNs assume strong homophily, they fa tendency of connection between each pair of classes. For instance, in an online purchasing network <ref type="bibr" target="#b23">[24]</ref> with three classes-fraudsters, accomplices, and honest use
Since exact inference is NP-hard, approximate inference algorithms (e.g., iterative classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>, loopy belief prop
apart from MixHop <ref type="bibr" target="#b0">[1]</ref> (cf. Â§ 3.1), Graph Diffusion Convolution <ref type="bibr" target="#b17">[18]</ref> replaces the adjacency matrix with a sparsified version of
get="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref> with edge homophily r tion as the feature vector for each node. â¢ Cora Full is an extended version of Cora, introduced in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>, which contain more
Since exact inference is NP-hard, approximate inference algorithms (e.g., iterative classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>, loopy belief prop
filtering-, the higher order polynomials of the normalized adjacency matrix A is a low-pass filter <ref type="bibr" target="#b36">[37]</ref>, so intermediate outputs from earlier rounds contain highe eighbor-embeddings. We found that removing the usual nonlinear transformations per round, as in SGC <ref type="bibr" target="#b36">[37]</ref>, works better (App. D.2), in which case we only need to in a learnable matrix. Our design in Eq. 5 aggregates different neighborhoods in a similar way to SGC <ref type="bibr" target="#b36">[37]</ref>, which has shown that removing non-linearities does not ne
ay have complex relationships that should be modeled directly. For instance, Graph Agreement Models <ref type="bibr" target="#b32">[33]</ref> augment the classification task with an agreement task, co
eal-world datasets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" tar of node classification by leveraging the correlations between the node labels and their attributes <ref type="bibr" target="#b29">[30]</ref>. Since exact inference is NP-hard, approximate inference a d by sampling feature vectors of nodes from the corresponding class in a real benchmark (e.g., Cora <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref> or ogbn-products < target="#b25">[26]</ref>. â¢ Cora, Pubmed and Citeseer are citation graphs originally introduced in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b21">22]</ref>, which are among t
arget="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref> with edge homophily ratio h ranging from strong heterophily e. â¢ Cora Full is an extended version of Cora, introduced in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>, which contain more papers and research fields than Cora. T >22]</ref>, which are among the most widely used benchmarks for semi-supervised node classification <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>. Each node is assi of words representation as the feature vector for each node.</p><p>Data Limitations As discussed in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>, Cora, Pubmed and
et="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" targe and Citeseer are citation graphs originally introduced in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b21">22]</ref>, which are among the most widely used benchmarks for semi-s
ggregating them within various graph neighborhoods via different mechanisms (e.g., averaging, LSTM) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" ta borhood is N 1 (v)-i.e., the 1-hop neighbors of v. As for f , in graph convolutional networks (GCN) <ref type="bibr" target="#b16">[17]</ref> each node repeatedly averages its own features and those o ssification (differences in accuracy of MLP for different h are due to randomness). Especially, GCN <ref type="bibr" target="#b16">[17]</ref> and GAT <ref type="bibr" target="#b35">[36]</ref> show up target="#b10">[11]</ref>-rather than averaging all of them as in the GCN model by Kipf and Welling <ref type="bibr" target="#b16">[17]</ref>.</p><p>Intuition. In heterophily settings, by definition ( (v)) may be different. However, the typical GCN design that mixes the embeddings through an average <ref type="bibr" target="#b16">[17]</ref> or weighted average <ref type="bibr" target="#b35">[36]</r ility matrix notion from belief propagation <ref type="bibr" target="#b9">[10]</ref> into GNNs. GCN <ref type="bibr" target="#b16">[17]</ref> GAT <ref type="bibr" target="#b35">[36]</ref> GCN-Cheby <r ditional conceptual and mechanism differences.</p><p>As we have mentioned, H 2 GCN differs from GCN <ref type="bibr" target="#b16">[17]</ref> in a number of ways: <ref type="bibr" target="#b0">(1)</re t are critical in heterophily.</p><p>Non-linear embedding transformations per round in H 2 GCN? GCN <ref type="bibr" target="#b16">[17]</ref>, GraphSAGE <ref type="bibr" target="#b10">[11]</ref> and o c.org/ns/1.0"><head>F Experimental Setup &amp; Hyperparameter Tuning</head><p>â¢ GCN &amp; GCN-Cheby <ref type="bibr" target="#b16">[17]</ref>: https://github.com/tkipf/gcn â¢ GraphSAGE <ref type="bibr" Function Ï: ReLU -Dropout Rate: a â {0, 0.5}</p><p>We report the best performance, for a = 0. â¢ GCN <ref type="bibr" target="#b16">[17]</ref>:</p><p>-hidden1: a â {16, 32, 64} -early_stopping: b â {40 â {40, 100, 200} epochs: 2000</p><p>We report the best performance, for a = 32, b = 40. â¢ GCN-Cheby <ref type="bibr" target="#b16">[17]</ref>:</p><p>-Set 1:</p><p>-hidden1: 64 -early_stopping: {40, 10 method hurts the performance significantly. We report the best performance, for a = 40. â¢ GCN-Cheby <ref type="bibr" target="#b16">[17]</ref>:</p><p>-hidden1: 64 -max_degree: 2 -early_stopping: 40 epo unction Ï: {ReLU, None} -Dropout Rate: {0, 0.5} -L2 Regularization Weight: {1e-5, 1e-6}</p><p>â¢ GCN <ref type="bibr" target="#b16">[17]</ref>: </p></div>			</div> 			<div type="references">  				<list
high memory footprint, both of which grow quadratically with respect to the image height (or width) <ref type="bibr" target="#b37">[38]</ref>. In real-world applications like content-based image searc ployed for higher efficiency. This differentiates our method from early recurrent attention methods <ref type="bibr" target="#b37">[38]</ref> which adopt pure recurrent models. In addition, we focus o ">8]</ref>.</p><p>One similar work to our GFNet is the recurrent visual attention model proposed in <ref type="bibr" target="#b37">[38]</ref>. However, our method differs from it in two important aspe ults with only a few class-discriminative patches, such as the head of a dog or the wings of a bird <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" ta
tion objective:</p><p>Herein, S Ï (s t ) denotes the entropy bonus to ensure sufficient exploration <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" ta
configurations as <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b56">57]</ref>. In our implementation, we estimate the confidence threshol
rrectly classified with high confidence at the glance step, which is inline with the observation in <ref type="bibr" target="#b63">[64]</ref>. When the glance step fails to produce sufficiently high c <ref type="bibr" target="#b19">[20]</ref> and its variants <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b63">64]</ref> introduce a multi-scale architecture with multiple classifi ax j p tj , (treated as confidence following earlier works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b63">64]</ref>) is compared to a pre-defined threshold Î· t . If max j p tj
e="bibr" target="#b34">35]</ref> or quantizing the weights <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Another technique
/ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>. Recent works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b22">23]</ref> scale up the image tNets, we first train the networks from scratch following all the details mentioned in their papers <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b15">16]</ref> to match the repor <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b35">36]</ref> and EfficientNet <ref type="bibr" target="#b49">[50]</ref>, can be deployed for higher efficiency. This differentiate ype="bibr" target="#b15">[16]</ref>, RegNet <ref type="bibr" target="#b39">[40]</ref>, EfficientNet <ref type="bibr" target="#b49">[50]</ref>, etc.) in the budgeted batch classification setting <ref t <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b35">36]</ref> and EfficientNet <ref type="bibr" target="#b49">[50]</ref>. Since deep networks typically have a considerable number e="bibr" target="#b15">[16]</ref>, RegNet-Y <ref type="bibr" target="#b39">[40]</ref>, EfficientNet <ref type="bibr" target="#b49">[50]</ref>, ResNet <ref type="bibr" target="#b13">[14]</ref> and Dens target="#b39">[40]</ref>, MobileNets-V3 <ref type="bibr" target="#b15">[16]</ref> and EfficientNets <ref type="bibr" target="#b49">[50]</ref>, we use a gated recurrent unit (GRU) with 256 hidden units th 1024 hidden units. For MobileNets-V3 <ref type="bibr" target="#b15">[16]</ref> and EfficientNets <ref type="bibr" target="#b49">[50]</ref>, we find that although a GRU classifier with a large numbe et="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b15">16]</ref>. Notably, when Mobi
or 320Ã320 images <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar
ask-relevant regions <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar
="bibr" target="#b15">16]</ref>, CondenseNet <ref type="bibr" target="#b20">[21]</ref>, ShuffleNets <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b35">36]</ref> and EfficientNet < ="bibr" target="#b15">16]</ref>, CondenseNet <ref type="bibr" target="#b20">[21]</ref>, ShuffleNets <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b35">36]</ref> and EfficientNet <
et="#b49">[50]</ref>. Since deep networks typically have a considerable number of redundant weights <ref type="bibr" target="#b10">[11]</ref>, some other approaches focus on pruning <ref type="bibr" t
="bibr" target="#b15">16]</ref>, CondenseNet <ref type="bibr" target="#b20">[21]</ref>, ShuffleNets <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b35">36]</ref> and EfficientNet < ="bibr" target="#b15">16]</ref>, CondenseNet <ref type="bibr" target="#b20">[21]</ref>, ShuffleNets <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b35">36]</ref> and EfficientNet <
ef type="bibr" target="#b49">(Rogers et al., 2020)</ref>, which specifically studies the BERT model <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>.</p><p>In this work, we adapt and extend In NLP, transformers are the backbone of state-of-the-art pre-trained language models such as BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>. BERTology focuses on interpreting what t

decreasing cost of sequencing technology has enabled vast databases of naturally occurring proteins <ref type="bibr" target="#b12">(El-Gebali et al., 2019a)</ref>, which are rich in information for de
al link between attention and model behavior <ref type="bibr" target="#b65">(Vig et al., 2020;</ref><ref type="bibr" target="#b19">Grimsley et al., 2020)</ref>, nor to explain model predictions <ref t

a dataset of 2.1B protein sequences, while the other ProtTrans models were pretrained on UniRef100 <ref type="bibr" target="#b57">(Suzek et al., 2014)</ref>, which includes 216M protein sequences. A




properties of attention in Transformers <ref type="bibr" target="#b61">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b28">Kovaleva et al., 2019;</ref><ref type="bibr" target="#b22">Hoover et
tation models (PLMs) such as ELMo <ref type="bibr" target="#b29">(Peters et al., 2018a)</ref>, BERT <ref type="bibr" target="#b9">(Devlin et al., 2019a)</ref> and XLNet <ref type="bibr" target="#b46">
t="#b22">(Madotto et al., 2018)</ref>, but also some early works use text as additional information <ref type="bibr" target="#b44">(Xie et al., 2016;</ref><ref type="bibr" target="#b22">An et al., 201 target="#b37">(Sun et al., 2019a)</ref> combines the advantages of both of them. Among these works, <ref type="bibr" target="#b44">Xie et al. (2016)</ref> propose to utilize entity descriptions as an
Radford et al. (2018)</ref> demonstrate a pre-trained generative model (GPT) and its effects, while <ref type="bibr" target="#b10">Devlin et al. (2019b)</ref> release a pre-trained deep Bidirectional entation from Transformers (BERT), achieving state-of-the-arts on dozens of benchmarks.</p><p>After <ref type="bibr" target="#b10">Devlin et al. (2019b)</ref>, similar pre-trained encoders spring up r many alternatives for pre-trained language representation can be used, e.g., masked language model <ref type="bibr" target="#b10">(Devlin et al., 2019b)</ref>. Note that those two tasks only share th output at [CLS] is often used as the sentence representation.</p><p>PLM Objective Inspired by BERT <ref type="bibr" target="#b10">(Devlin et al., 2019b)</ref>, MLM randomly selects 15% of input token LS] output for sentence-level prediction and the outputs of all tokens for sequence labelling tasks <ref type="bibr" target="#b10">(Devlin et al., 2019b)</ref> </p></div> <div xmlns="http://www.tei-c. use the transformer architecture <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref> as in <ref type="bibr" target="#b10">(Devlin et al., 2019b;</ref><ref type="bibr" target="#b20">Liu et al.
ypically lack of factual world knowledge <ref type="bibr" target="#b32">(Petroni et al., 2019;</ref><ref type="bibr" target="#b21">Logan et al., 2019)</ref>.</p><p>Recent works <ref type="bibr" target
0"><head n="1">Introduction</head><p>Pre-trained language representation models (PLMs) such as ELMo <ref type="bibr" target="#b29">(Peters et al., 2018a)</ref>, BERT <ref type="bibr" target="#b9">(Dev
ype="bibr" target="#b12">Han et al., 2016;</ref><ref type="bibr" target="#b4">Cao et al., 2017</ref><ref type="bibr" target="#b3">Cao et al., , 2018))</ref>.</p><p>In this paper, we propose to learn k
one of the major differences between RoBERTa and BERT is that RoBERTa uses Byte-Pair Encoding (BPE) <ref type="bibr" target="#b35">(Sennrich et al., 2016)</ref> to better tokenize rare words.</p><p>Gi

beddings into NLP models to improve the performance of NLP applications such as machine translation <ref type="bibr" target="#b47">(Zaremoodi et al., 2018)</ref>, reading comprehension <ref type="bibr
remoodi et al., 2018)</ref>, reading comprehension <ref type="bibr">(Mihaylov and Frank, 2018;</ref><ref type="bibr" target="#b50">Zhong et al., 2019)</ref> and dialogue system <ref type="bibr" target
0"><head n="1">Introduction</head><p>Pre-trained language representation models (PLMs) such as ELMo <ref type="bibr" target="#b29">(Peters et al., 2018a)</ref>, BERT <ref type="bibr" target="#b9">(Dev
one of the major differences between RoBERTa and BERT is that RoBERTa uses Byte-Pair Encoding (BPE) <ref type="bibr" target="#b35">(Sennrich et al., 2016)</ref> to better tokenize rare words.</p><p>Gi
commonlyused datasets: TACRED <ref type="bibr" target="#b48">(Zhang et al., 2017)</ref> and FewRel <ref type="bibr" target="#b13">(Han et al., 2018)</ref>. TACRED covers 42 relation types and contain to" indicates Prototypical Networks<ref type="bibr" target="#b36">(Snell et al., 2017)</ref> used in<ref type="bibr" target="#b13">Han et al. (2018)</ref>. "PAIR" is proposed in<ref type="bibr" target
mbeddings for their ability to capture syntactic and semantic information from large-scale corpora. <ref type="bibr" target="#b30">Peters et al. (2018b)</ref> push this trend a step forward by using a
of our model. However, we still keep MLM as one of our objectives to avoid catastrophic forgetting <ref type="bibr" target="#b23">(McCloskey and Cohen, 1989)</ref> while training towards the KRL loss
e-trained word embeddings as input features, there is another trend exploring pre-trained encoders. <ref type="bibr" target="#b7">Dai and Le (2015)</ref> first propose to train an auto-encoder on unla
dels have superior performance over others. We have also compared with current state-of-the-art MTP <ref type="bibr" target="#b1">(Baldini Soares et al., 2019)</ref>, which outperforms us a little. Bu >. For supervised relation extraction and fewshot relation extraction, we follow the approaches from<ref type="bibr" target="#b1">(Baldini Soares et al., 2019)</ref> and<ref type="bibr" target="#b11">
irst propose to train an auto-encoder on unlabeled data, and then fine-tune it on downstream tasks. <ref type="bibr" target="#b15">Howard and Ruder (2018)</ref> propose a universal language model (ULM
(h, r, t) and predict head or tail entities with scores of candidate entities. For example, TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> treats tail entities as translations of he gin, Ï is the sigmoid function, and d r is the score function, for which we choose to follow TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> for its simplicity and efficiency,</p><for is trained on the training set and evaluated on the link Method MR MRR HITS@1 HITS@3 HITS@10 TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> 109370 0.253 0.170 0.311 0.392 DistMult <r 016)</ref>  prediction task.</p><p>We conduct 5 knowledge graph embedding models , including TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, Dist-Mult <ref type="bibr" target="#b45">
mbeddings for their ability to capture syntactic and semantic information from large-scale corpora. <ref type="bibr" target="#b30">Peters et al. (2018b)</ref> push this trend a step forward by using a
beddings into NLP models to improve the performance of NLP applications such as machine translation <ref type="bibr" target="#b47">(Zaremoodi et al., 2018)</ref>, reading comprehension <ref type="bibr
one of the major differences between RoBERTa and BERT is that RoBERTa uses Byte-Pair Encoding (BPE) <ref type="bibr" target="#b35">(Sennrich et al., 2016)</ref> to better tokenize rare words.</p><p>Gi
oni et al., 2019;</ref><ref type="bibr" target="#b21">Logan et al., 2019)</ref>.</p><p>Recent works <ref type="bibr" target="#b49">(Zhang et al., 2019;</ref><ref type="bibr" target="#b28">Peters et al h Wikipedia corpus to save time and also for a fair comparison with previous knowledgeenhanced PLMs <ref type="bibr" target="#b49">(Zhang et al., 2019;</ref><ref type="bibr" target="#b31">Peters et al instances for each relation.</p><p>Here we follow the relation extraction finetuning procedure from <ref type="bibr" target="#b49">Zhang et al. (2019)</ref>, where four special tokens are added before ls on OpenEntity <ref type="bibr" target="#b5">(Choi et al., 2018)</ref> following the setting from <ref type="bibr" target="#b49">Zhang et al. (2019)</ref>, which focuses on nine general entity types
et. We use Prototypical Networks <ref type="bibr" target="#b36">(Snell et al., 2017)</ref> and PAIR <ref type="bibr" target="#b11">(Gao et al., 2019)</ref> as the base frameworks and try out different , we follow the approaches from<ref type="bibr" target="#b1">(Baldini Soares et al., 2019)</ref> and<ref type="bibr" target="#b11">(Gao et al., 2019)</ref> respectively. Statistics of Wikidata5m compa l., 2017)</ref> used in<ref type="bibr" target="#b13">Han et al. (2018)</ref>. "PAIR" is proposed in<ref type="bibr" target="#b11">Gao et al. (2019)</ref> and "MTB" is from Baldini Soares et al. (2019
ype="bibr" target="#b12">Han et al., 2016;</ref><ref type="bibr" target="#b4">Cao et al., 2017</ref><ref type="bibr" target="#b3">Cao et al., , 2018))</ref>.</p><p>In this paper, we propose to learn k
rison with previous knowledgeenhanced PLMs <ref type="bibr" target="#b49">(Zhang et al., 2019;</ref><ref type="bibr" target="#b31">Peters et al., 2019)</ref>  </p></div> <div xmlns="http://www.tei-c.o
0"><head n="1">Introduction</head><p>Pre-trained language representation models (PLMs) such as ELMo <ref type="bibr" target="#b29">(Peters et al., 2018a)</ref>, BERT <ref type="bibr" target="#b9">(Dev
an et al., 2019)</ref>.</p><p>Recent works <ref type="bibr" target="#b49">(Zhang et al., 2019;</ref><ref type="bibr" target="#b28">Peters et al., 2017;</ref><ref type="bibr">Liu et al., 2019a)</ref> u
an et al., 2019)</ref>.</p><p>Recent works <ref type="bibr" target="#b49">(Zhang et al., 2019;</ref><ref type="bibr" target="#b28">Peters et al., 2017;</ref><ref type="bibr">Liu et al., 2019a)</ref> u
he powerful Transformer <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref> as its encoder, <ref type="bibr" target="#b33">Radford et al. (2018)</ref> demonstrate a pre-trained generative mode
t="#b22">An et al., 2018)</ref> or jointly train the knowledge and text embedding in the same space <ref type="bibr" target="#b43">(Wang et al., 2014;</ref><ref type="bibr" target="#b39">Toutanova et
wledge and text embedding in the same space <ref type="bibr" target="#b43">(Wang et al., 2014;</ref><ref type="bibr" target="#b39">Toutanova et al., 2015;</ref><ref type="bibr" target="#b12">Han et al
mbeddings for their ability to capture syntactic and semantic information from large-scale corpora. <ref type="bibr" target="#b30">Peters et al. (2018b)</ref> push this trend a step forward by using a
ch that it covers a predefined set of downsampling kernels <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b12">13]</ref>; using DNNs to capture only a natural-image prior which is adation model and assumes that the downsampling kernels belong to a certain set of Gaussian filters <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref>. Another approach estimation methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Finally, we wo f>.</p><p>Finally, we would like to highlight major differences be-tween this paper and the work in <ref type="bibr" target="#b12">[13]</ref>, whose "kernel correction" approach may be misunderstood a >[13]</ref>, whose "kernel correction" approach may be misunderstood as our "correction filter". In <ref type="bibr" target="#b12">[13]</ref>, three different DNNs (super-resolver, kernel estimator, a ned existing DNN methods (other than SRMD <ref type="bibr" target="#b37">[38]</ref>) can be used in <ref type="bibr" target="#b12">[13]</ref>. Secondly, their approach is restricted by the offline tra ur approach. Thirdly, the concepts of these works are very different: The (iterative) correction in <ref type="bibr" target="#b12">[13]</ref> modifies the estimated downsampling kernel, while our corr erformed with the official code of each method. Unfortunately, such code has not been available for <ref type="bibr" target="#b12">[13]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
for image processing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>. These works typically propose linear interpolation methods
tion comparison on Set14. Each cell displays PSNR [dB] (left), SSIM (middle) and perceptual distance<ref type="bibr" target="#b39">[40]</ref> (right).</figDesc><table><row><cell></cell><cell cols="2"> ion comparison on BSD100. Each cell displays PSNR [dB] (left), SSIM (middle) and perceptual distance<ref type="bibr" target="#b39">[40]</ref> (right).</figDesc><table><row><cell></cell><cell cols="2">
get="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar get="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" tar into existing leading super-resolvers, such as DBPN <ref type="bibr" target="#b13">[14]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, and proSR <ref type="bibr" target="#b32">[33]</ref>, thus SR <ref type="bibr" target="#b25">[26]</ref>, proSR <ref type="bibr" target="#b32">[33]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, DBPN <ref type="bibr" target="#b13">[14]</ref>, proSR wit /p><p>For the experiments in this paper we use DBPN <ref type="bibr" target="#b13">[14]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, and proSR <ref type="bibr" target="#b32">[33]</ref>, but ) in <ref type="bibr" target="#b9">(10)</ref>: DBPN <ref type="bibr" target="#b13">[14]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, and proSR <ref type="bibr" target="#b32">[33]</ref>. We c SR <ref type="bibr" target="#b25">[26]</ref>, proSR <ref type="bibr" target="#b32">[33]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, DBPN <ref type="bibr" target="#b13">[14]</ref>, proSR wit lGAN <ref type="bibr" target="#b1">[2]</ref>, proSR <ref type="bibr" target="#b32">[33]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, DBPN <ref type="bibr" target="#b13">[14]</ref>, proSR wit PN <ref type="bibr" target="#b13">[14]</ref>, proSR <ref type="bibr" target="#b32">[33]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, and our proposed approach). Therefore, it is not displaye ZSSR<ref type="bibr" target="#b25">[26]</ref>, proSR<ref type="bibr" target="#b32">[33]</ref>, RCAN<ref type="bibr" target="#b40">[41]</ref>, DBPN<ref type="bibr" target="#b13">[14]</ref>, proSR with rnelGAN<ref type="bibr" target="#b1">[2]</ref>, proSR<ref type="bibr" target="#b32">[33]</ref>, RCAN<ref type="bibr" target="#b40">[41]</ref>, DBPN<ref type="bibr" target="#b13">[14]</ref>, proSR with get="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref> and the perceptual quality <ref type="bibr" target="#b2">[3
e performance drop <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Such scenarios include a downsampling kernel which is not for one acquisition model and then been tested on another <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Recently, di ><ref type="bibr" target="#b38">39]</ref>. Recently, the two last approaches have been incorporated <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1]</ref>. In all these meth-
natural-image prior which is decoupled from the SISR task <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4]</ref>; or completely avoid any offline training and instead train a ep denoiser or GAN <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b38">39]</ref>. Recently, the two la
ingle Image Super-Resolution (SISR) is one of the most examined inverse problems in the past decade <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" tar .</p><p>After an estimator of h is obtained using Algorithm 1, the HR image can be reconstructed by <ref type="bibr" target="#b9">(10)</ref>, similarly to the non-blind setting. </p></div> <div xmlns= and blind settings, using three different off-the-shelf DNN super-resolvers that serve as f (â¢) in <ref type="bibr" target="#b9">(10)</ref>: DBPN <ref type="bibr" target="#b13">[14]</ref>, RCAN <ref te the correction filter using Algorithm 1, and then use this estimation to restore the HR image by <ref type="bibr" target="#b9">(10)</ref>. In this setting we compare our method to kernelGAN <ref ty
rget="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar pe="bibr" target="#b40">41]</ref> and the perceptual quality <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" tar rget="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar
ne of the most examined inverse problems in the past decade <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar non-local similarity <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar
include: modifying the training phase such that it covers a predefined set of downsampling kernels <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b12">13]</ref>; using DNNs to cap ght and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD <ref type="bibr" target="#b37">[38]</ref>, ZSSR <ref type="bibr" target="#b25">[26]</ref>, proSR <re sumption that the downsampling kernel belongs to a certain family of Gaussian filters (similarly to <ref type="bibr" target="#b37">[38]</ref>), and the CNN super-resolver gets the estimated kernel as r difference is that contrary to our approach, no pre-trained existing DNN methods (other than SRMD <ref type="bibr" target="#b37">[38]</ref>) can be used in <ref type="bibr" target="#b12">[13]</ref>. timation in the blind setting) as an input: ZSSR <ref type="bibr" target="#b25">[26]</ref> and SRMD <ref type="bibr" target="#b37">[38]</ref>. We also compare our method to DPSR <ref type="bibr" targe ght and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD <ref type="bibr" target="#b37">[38]</ref>, ZSSR <ref type="bibr" target="#b25">[26]</ref>, proSR <re as produced by its official code is lower by more than 10 dB than the other examined methods (SRMD <ref type="bibr" target="#b37">[38]</ref>, ZSSR <ref type="bibr" target="#b25">[26]</ref>, DBPN <ref ight and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD<ref type="bibr" target="#b37">[38]</ref>, ZSSR<ref type="bibr" target="#b25">[26]</ref>, proSR<ref mpling kernels belong to a certain set of Gaussian filters <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref>. Another approach builds on the structural prior of CNNs, w
get="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Recently, several works have shown that in practical
as a single table, but starting with z15 it exploits a variation of the TAGE algorithm based off of <ref type="bibr" target="#b8">[8]</ref>. Two TAGE PHT tables are employed in z15-a short and a long wever, a weak TAGE PHT prediction can sometimes be detrimental, particularly after a context switch <ref type="bibr" target="#b8">[8]</ref>. As such, weak filtering is employed. Before allowing a weak
a Pattern History Table (PHT). The tagged PHT has been used on z branch predictions since the z196 <ref type="bibr" target="#b15">[15]</ref> as a single table, but starting with z15 it exploits a var
he current address space undergoing search. The CTB has been present since the z10 branch predictor <ref type="bibr" target="#b20">[20]</ref>. In the z15, there are four, 512 entry SRAM arrays that co
elp augment the patternbased auxiliary direction prediction <ref type="bibr" target="#b9">[9]</ref> <ref type="bibr" target="#b18">[18]</ref>. The z15 perceptron carries this design forward.</p><p>As
allows the IDU to know when branch prediction has fallen behind. Starting with the IBM z13 machine <ref type="bibr" target="#b17">[17]</ref>, the branch predictor employs such strict synchronization.
direction prediction was introduced to help augment the patternbased auxiliary direction prediction <ref type="bibr" target="#b9">[9]</ref> <ref type="bibr" target="#b18">[18]</ref>. The z15 perceptro br" target="#b18">[18]</ref>. The z15 perceptron carries this design forward.</p><p>As described in <ref type="bibr" target="#b9">[9]</ref> and in patents <ref type="bibr" target="#b13">[13]</ref>[14] imilar components -detection and prediction. The z14 introduced a basic call/return stack predictor <ref type="bibr" target="#b9">[9]</ref>, which was further enhanced on z15.</p><p>A mechanism at the

direction prediction was introduced to help augment the patternbased auxiliary direction prediction <ref type="bibr" target="#b9">[9]</ref> <ref type="bibr" target="#b18">[18]</ref>. The z15 perceptro br" target="#b18">[18]</ref>. The z15 perceptron carries this design forward.</p><p>As described in <ref type="bibr" target="#b9">[9]</ref> and in patents <ref type="bibr" target="#b13">[13]</ref>[14] imilar components -detection and prediction. The z14 introduced a basic call/return stack predictor <ref type="bibr" target="#b9">[9]</ref>, which was further enhanced on z15.</p><p>A mechanism at the
to various papers describing ways to reduce branch prediction latency. The Alpha EV8 line predictor <ref type="bibr" target="#b11">[11]</ref> fetches and predicts up to 16 conditional branches every c
allows the IDU to know when branch prediction has fallen behind. Starting with the IBM z13 machine <ref type="bibr" target="#b17">[17]</ref>, the branch predictor employs such strict synchronization.
he current address space undergoing search. The CTB has been present since the z10 branch predictor <ref type="bibr" target="#b20">[20]</ref>. In the z15, there are four, 512 entry SRAM arrays that co
ges during training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b55">56]</ref>. An alternative to approximate the loss is to approximate t set as its own class <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b55">56]</ref>. Dosovitskiy et al. <ref type="bibr" target="#b15">[16]</re ata. In addition, SwAV works with small and large batch sizes and does not need a large memory bank <ref type="bibr" target="#b55">[56]</ref> or a momentum encoder <ref type="bibr" target="#b23">[24]< h as many classes as images in the dataset. As this approach becomes quickly intractable, Wu et al. <ref type="bibr" target="#b55">[56]</ref> mitigate this issue by replacing the classifier with a mem fferent augmentations of the same image. This solution is inspired by contrastive instance learning <ref type="bibr" target="#b55">[56]</ref> as we do not consider the codes as a target, but only enfo feature. A similar comparison appears in contrastive learning where features are compared directly <ref type="bibr" target="#b55">[56]</ref>. In Fig. <ref type="figure" target="#fig_0">1</ref>, we il bel>2</label></formula><formula xml:id="formula_3">)</formula><p>where Ï is a temperature parameter <ref type="bibr" target="#b55">[56]</ref>. Taking this loss over all the images and pairs of data au cating pass forwards to the assignments. This is similar to the memory bank introduced by Wu et al. <ref type="bibr" target="#b55">[56]</ref>, without momentum.</p><p>Assignment phase in DeepCluster-v .6 Image classification with KNN classifiers on ImageNet</head><p>Following previous work protocols <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b65">66]</ref>, we evaluate the q
get="#b56">57,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b65">66]</ref>. Caron et al. <ref type="bibr" target="#b6">[7]</ref> show ers on ImageNet</head><p>Following previous work protocols <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b65">66]</ref>, we evaluate the quality of our unsupervised features with
hat considers each image of the dataset (or "instance") and its transformations as a separate class <ref type="bibr" target="#b15">[16]</ref>. This task yields representations that are able to discrim ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b55">56]</ref>. Dosovitskiy et al. <ref type="bibr" target="#b15">[16]</ref> assign a class explicitly to each image and learn a linear ification considers each image in a dataset as its own class <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b55">56]</ref>. Dosovitskiy et al.
get="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" tar
" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" targe is method scales to large uncurated dataset and can be used for pre-training of supervised networks <ref type="bibr" target="#b7">[8]</ref>. However, their formulation is not principled and recently, ting it into a hard assignment. Besides, unlike Caron et al. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and Asano et al. <ref type="bibr" target="#b1">[2]</ref>, we ng with SwAV on random images significantly improves over training from scratch on ImageNet (+1.3%) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>. In Fig. <ref type="
the shorter side, before taking a 224 Ã 224 center crop. Then, we train a linear SVM with LIBLINEAR <ref type="bibr" target="#b17">[18]</ref> on top of corresponding global average pooled final repres
ferent. They use a curated set of Instagram images, filtered by hashtags similar to ImageNet labels <ref type="bibr" target="#b38">[39]</ref>. We compare SwAV with a randomly initialized network and w variants of the ResNeXt architecture <ref type="bibr" target="#b58">[59]</ref> as in Mahajan et al. <ref type="bibr" target="#b38">[39]</ref>. We compare SwAV with supervised models trained from scrat age of the increased model capacity. For reference, we also include the results from Mahajan et al. <ref type="bibr" target="#b38">[39]</ref> obtained with a weakly-supervised model pretrained by pred
get="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b63">64]</ref>. We refer the reade
="bibr" target="#b1">[2]</ref> we keep the soft assignment produced by the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b12">[13]</ref> instead of approximating it into a hard assignment. Beside are the solution of Prob. (3) over the set Q and takes the form of a normalized exponential matrix <ref type="bibr" target="#b12">[13]</ref>:  where u and v are renormalization vectors in R K and R B omputed using a small number of matrix multiplications using the iterative Sinkhorn-Knopp algorithm <ref type="bibr" target="#b12">[13]</ref>. In practice, we observe that using only 3 iterations is f vestigate the impact of the number of normalization steps performed during Sinkhorn-Knopp algorithm <ref type="bibr" target="#b12">[13]</ref> on the performance of SwAV. We observe that using only 3 i
e datasets. Second, we report network finetuning on object detection on VOC07+12 using Faster R-CNN <ref type="bibr" target="#b47">[48]</ref> and on COCO <ref type="bibr" target="#b35">[36]</ref> with Ã 224 center crop on the validation set.</p><p>Object Detection on VOC07+12. We use a Faster R-CNN <ref type="bibr" target="#b47">[48]</ref> model as implemented in Detec-tron2 <ref type="bibr" targe ight panel we evaluate the features by finetuning a ResNet-50 on object detection with Faster R-CNN <ref type="bibr" target="#b47">[48]</ref> and DETR <ref type="bibr" target="#b5">[6]</ref>. Overall, ith our model. (2) Object detection with finetuned features on VOC07+12 trainval using Faster R-CNN <ref type="bibr" target="#b47">[48]</ref> and on COCO <ref type="bibr" target="#b35">[36]</ref> usin ">8</ref>, we evaluate the features by finetuning a ResNet-50 on object detection with Faster R-CNN <ref type="bibr" target="#b47">[48]</ref> and DETR <ref type="bibr" target="#b5">[6]</ref> and repor we report mAP. (2) Object detection with finetuned features on VOC07+12 trainval using Faster R-CNN<ref type="bibr" target="#b47">[48]</ref> and on COCO<ref type="bibr" target="#b35">[36]</ref> using c>More detection metrics for object detection on VOC07+12 with finetuned features using Faster R-CNN<ref type="bibr" target="#b47">[48]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">AP
get="#b28">29,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b65">66]</ref>. Caron et al. <ref
While previous learning simulation approaches <ref type="bibr" target="#b17">(Li et al., 2018;</ref><ref type="bibr" target="#b33">Ummenhofer et al., 2020)</ref> have been highly specialized for parti lution 3D water scenario with randomized water position, initial velocity and volume, comparable to <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref>'s containers of water. We used SPlisHS d boundary particles, a loss function that weights slow particles with few neighbors more heavily). <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref> reported CConv outperformed DPI, so we is to, during training, provide the model with its own predictions by rolling out short sequences. <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref>, for example, train with two-step pred e="bibr" target="#b14">(Kipf &amp; Welling, 2016)</ref> work. The full CConv update as described in <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref> is,</p><formula xml:id="formula_19">f e comparisons.</head><p>We implemented the CConv model, loss and training procedure as described by <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref>. For simplicity, we only tested the CC appended a particle type learned embedding to the input node features.</p><p>To be consistent with <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref>, we used their batch size of 16, learn "><head>D. Supplementary baseline comparisons D.1. Continuous convolution (CConv)</head><p>Recently <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref> presented Continuous Convolution (CCon eral tasks.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretation.</head><p>While <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref> state that "Unlike previous approaches
ferent node, edge, and graph-level attributes, and can be trained to learn a form of messagepassing <ref type="bibr" target="#b6">(Gilmer et al., 2017)</ref>, where latent information is propagated be
es' velocities and positions accordingly. Other techniques, such as "position-based dynamics" (PBD) <ref type="bibr" target="#b23">(MÃ¼ller et al., 2007)</ref> and "material point method" (MPM) <ref ty

ks (GN) <ref type="bibr" target="#b2">(Battaglia et al., 2018</ref>)-a type of graph neural network <ref type="bibr" target="#b26">(Scarselli et al., 2008)</ref>-have recently proven effective at lear
One popular particle-based method for simulating fluids is "smoothed particle hydrodynamics" (SPH) <ref type="bibr" target="#b21">(Monaghan, 1992)</ref>, which evaluates pressure and viscosity forces
(PBD) <ref type="bibr" target="#b23">(MÃ¼ller et al., 2007)</ref> and "material point method" (MPM) <ref type="bibr" target="#b28">(Sulsky et al., 1995)</ref>, are more suitable for interacting, defor
ks (GN) <ref type="bibr" target="#b2">(Battaglia et al., 2018</ref>)-a type of graph neural network <ref type="bibr" target="#b26">(Scarselli et al., 2008)</ref>-have recently proven effective at lear
ks (GN) <ref type="bibr" target="#b2">(Battaglia et al., 2018</ref>)-a type of graph neural network <ref type="bibr" target="#b26">(Scarselli et al., 2008)</ref>-have recently proven effective at lear
"#b24">Sanchez-Gonzalez et al., 2018;</ref><ref type="bibr" target="#b22">Mrowca et al., 2018;</ref><ref type="bibr" target="#b18">Li et al., 2019;</ref><ref type="bibr" target="#b25">Sanchez-Gonzalez
aph settings <ref type="bibr" target="#b32">(Trivedi et al., 2019;</ref><ref type="bibr">2017;</ref><ref type="bibr" target="#b36">Yan et al., 2018;</ref><ref type="bibr" target="#b20">Manessi et al.,
">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. The main concept behind these approaches is to interpret s successfully transferred from NLP to protein sequences <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b51">[52]</ref>, with the excep f their surrounding context (residues next to it). As previously established for another protein LM <ref type="bibr" target="#b23">[24]</ref>, the t-SNE projections (e.g. ProtBert Fig. <ref type="figu ">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, we might expect an upper limit for what protein LMs can l
">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bib ig_0">1</ref>. Overall, BFD was about eight times larger than the largest data sets used previously <ref type="bibr" target="#b18">[19]</ref>. Despite the 8-fold increase in data, the number of tokens previous work <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bib
dvanced libraries <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" t
d on proteins <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bib iven the experiments described here and in previous work <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bib
">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bib parameters). Largely, we used configurations successfully transferred from NLP to protein sequences <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bib ">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bib
">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> usually have to
ioinformatics <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bib
">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bib parameters). Largely, we used configurations successfully transferred from NLP to protein sequences <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bib ">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bib
dels (including Bert and Albert). With the average length of an English sentence around 15-30 words <ref type="bibr" target="#b50">[51]</ref>, an upper sentence length limit is no problem for sentence
emory by using 16-bit precision only or a mix of 16-bit and 32-bit precision. Nvidia's APEX library <ref type="bibr" target="#b56">[57]</ref> was used for mixed precision training of ProtTXL, due to i
ghts. While existing solutions in Protein Bioinformatics <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bib ref>).</p><p>Per-protein prediction: For the prediction of features of entire proteins, the DeepLoc <ref type="bibr" target="#b25">[26]</ref> data set was used to classify proteins into membrane-bound ProtXLNet (SOM 13E) produce visually easier separable clusters.</p><p>Using a different protein set <ref type="bibr" target="#b25">[26]</ref>, we analyzed whether or not the embeddings captured aspect single amino acids during pre-training (Panel A). A redundancy reduced version (30%) of the DeepLoc <ref type="bibr" target="#b25">[26]</ref> dataset was used to assess whether the LM learnt to classi into membrane-bound and soluble (blue bars) using the dataset of an existing approach, i.e. DeepLoc <ref type="bibr" target="#b25">[26]</ref>). A simple two-layer neural network is trained on top of f
se approach <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> and its layer-dependent variant <ref type="bibr" target="#b13">(Huang et al., 2018)</ref>. Specifically, GAT <ref type="bibr" target on et al., 2017)</ref>, FastGCN <ref type="bibr" target="#b2">(Chen et al., 2018)</ref>, and AS-GCN <ref type="bibr" target="#b13">(Huang et al., 2018)</ref>. We name this category of approaches as Dr e <ref type="table" target="#tab_1">2</ref>; for the SOTA methods, we reuse the results reported in <ref type="bibr" target="#b13">Huang et al. (2018)</ref>.</p><p>We have these findings: (1) Clearly, ing the testing nodes are unseen for training. We apply the full-supervised training fashion used in<ref type="bibr" target="#b13">Huang et al. (2018)</ref> and<ref type="bibr" target="#b2">Chen et al
which develops graph convolution based on spectral graph theory. Later, Kipf &amp; Welling (2017); <ref type="bibr" target="#b3">Defferrard et al. (2016)</ref>; <ref type="bibr" target="#b10">Henaff
volution on graphs under the umbrella of GCNs. The first prominent research on GCNs is presented in <ref type="bibr" target="#b1">Bruna et al. (2013)</ref>, which develops graph convolution based on s

sks on graphs, such as node classification <ref type="bibr" target="#b0">(Bhagat et al., 2011;</ref><ref type="bibr" target="#b34">Zhang et al., 2018)</ref>, social recommendation <ref type="bibr" tar
(2019)</ref> as well as the concept of mixing time that has been studied in the random walk theory <ref type="bibr" target="#b21">(LovÃ¡sz et al., 1993)</ref>. We provide the full details in the suppl ve s l Î» &lt; 1.</formula><p>We also need to adopt some concepts of the conductance of a graph from <ref type="bibr" target="#b21">LovÃ¡sz et al. (1993)</ref> in proving Theorem 1. Consider the graph G aph is defined as</p><formula xml:id="formula_15">Ï = min S Î¦(S)</formula><p>By the graph theory in <ref type="bibr" target="#b21">LovÃ¡sz et al. (1993)</ref>, the conductance of the graph is bounded b layer since log d M (X) &lt; 0. To reduce the conductance of a graph, we recall the graph theory in <ref type="bibr" target="#b21">LovÃ¡sz et al. (1993)</ref> which implies that the conductance of the
(2019)</ref> as well as the concept of mixing time that has been studied in the random walk theory <ref type="bibr" target="#b21">(LovÃ¡sz et al., 1993)</ref>. We provide the full details in the suppl ve s l Î» &lt; 1.</formula><p>We also need to adopt some concepts of the conductance of a graph from <ref type="bibr" target="#b21">LovÃ¡sz et al. (1993)</ref> in proving Theorem 1. Consider the graph G aph is defined as</p><formula xml:id="formula_15">Ï = min S Î¦(S)</formula><p>By the graph theory in <ref type="bibr" target="#b21">LovÃ¡sz et al. (1993)</ref>, the conductance of the graph is bounded b layer since log d M (X) &lt; 0. To reduce the conductance of a graph, we recall the graph theory in <ref type="bibr" target="#b21">LovÃ¡sz et al. (1993)</ref> which implies that the conductance of the
et="#b3">Defferrard et al. (2016)</ref>; <ref type="bibr" target="#b10">Henaff et al. (2015)</ref>; <ref type="bibr" target="#b19">Li et al. (2018b)</ref>; <ref type="bibr" target="#b16">Levie et al.
c>(1) classifying the research topic of papers in three citation datasets: Cora, Citeseer and Pubmed<ref type="bibr" target="#b27">(Sen et al., 2008)</ref>; (2) predicting which community different po
" target="#b10">Henaff et al. (2015)</ref>; <ref type="bibr" target="#b19">Li et al. (2018b)</ref>; <ref type="bibr" target="#b16">Levie et al. (2017)</ref> apply improvements, extensions, and approxi
Feature Modeling We also implement a variant of graph convolution layer with self feature modeling <ref type="bibr" target="#b5">(Fout et al., 2017)</ref>:</p><formula xml:id="formula_28">H (l+1) = Ï
sity model typically involves perturbing the search results to collect examples of counterfactuals, <ref type="bibr" target="#b0">[1]</ref> describes methods to construct the propensity model without
sses were added to the wide part.</p><p>is was followed by variants of a ention based networks from <ref type="bibr" target="#b15">[16]</ref>.</p><p>e intention there was to make the hidden layer deri
g was a major milestone in the evolution of search ranking at Airbnb. Our account of the journey in <ref type="bibr" target="#b5">[6]</ref> brought us in conversation with many industry practitioners, h. In this paper we capture the major enhancements that followed the launch of the DNN described in <ref type="bibr" target="#b5">[6]</ref>. In addition to delving into the core machine learning techn , we started with the observation that the series of successful ranking model launches described in <ref type="bibr" target="#b5">[6]</ref> were not only associated with an increase in bookings, but a during ranking.</p><p>When tested online as an A/B experiment against the two hidden layer DNN from <ref type="bibr" target="#b5">[6]</ref>, average price of search results dropped by -5.7%, in con rm ype="figure" target="#fig_2">4</ref>. e plots suggested that the fully connected two layer DNN from <ref type="bibr" target="#b5">[6]</ref> already understood cheaper was be er. Repeating the ICE anal lts</head><p>When tested online in an A/B experiment against the fully connected two layer DNN from <ref type="bibr" target="#b5">[6]</ref>, the two tower architecture recorded a bookings gain of +0.6
isting with a capacity of two.</p><p>is is conceptually similar to the Naive Bayes recommender from <ref type="bibr" target="#b10">[11]</ref> which used a generative method to estimate the missing inf
g era. But as we sought to replicate the bene ts of scaling data and adding layers as summarized in <ref type="bibr" target="#b13">[14]</ref>, we met nothing but neutral test results. Trying to deciph
architectures that were monotonic with respect to some of its inputs. La ice networks described in <ref type="bibr" target="#b18">[19]</ref> presented an elegant solution to the problem. But pivoting
showing any gains led us to borrow more ideas from the literature, like applying residual learning <ref type="bibr" target="#b6">[7]</ref> and batch normalization <ref type="bibr" target="#b7">[8]</r
architectures that were monotonic with respect to some of its inputs. La ice networks described in <ref type="bibr" target="#b18">[19]</ref> presented an elegant solution to the problem. But pivoting
t vector of the query tower. Since the 100-d vectors were not human interpretable, we applied t-SNE <ref type="bibr" target="#b16">[17]</ref> to reduce them to 2-d vectors, which are shown in Figure <
sses were added to the wide part.</p><p>is was followed by variants of a ention based networks from <ref type="bibr" target="#b15">[16]</ref>.</p><p>e intention there was to make the hidden layer deri
t vector of the query tower. Since the 100-d vectors were not human interpretable, we applied t-SNE <ref type="bibr" target="#b16">[17]</ref> to reduce them to 2-d vectors, which are shown in Figure <
rget="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>The most common paradigm for CF is to learn latent f et="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref> that typically aggregate extended neighbors and need to han edding learning.</p><p>To deepen the use of subgraph structure with high-hop neighbors, Wang et al. <ref type="bibr" target="#b38">[39]</ref> recently proposes NGCF and achieves state-of-the-art perfo <div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We first introduce NGCF <ref type="bibr" target="#b38">[39]</ref>, a representative and state-of-the-art GCN model for recom the scores of NGCF are directly copied from the Table <ref type="table" target="#tab_4">3</ref> of <ref type="bibr" target="#b38">[39]</ref>. As can be seen, removing feature transformation (i.e., NG >transformation and nonlinear activation. The graph convolution operation (a.k.a., propagation rule <ref type="bibr" target="#b38">[39]</ref>) in LightGCN is defined as:</p><formula xml:id="formula_5" ms) that have overlap on interacted items (users), and higher-layers capture higher-order proximity <ref type="bibr" target="#b38">[39]</ref>. Thus combining them will make the representation more com TS</head><p>We first describe experimental settings, and then conduct detailed comparison with NGCF <ref type="bibr" target="#b38">[39]</ref>, the method that is most relevant with LightGCN but more c e experiment workload and keep the comparison fair, we closely follow the settings of the NGCF work <ref type="bibr" target="#b38">[39]</ref>. We request the experimented datasets (including train/tes ation is that increasing the layer number from 0 (i.e., the matrix factorization model, results see <ref type="bibr" target="#b38">[39]</ref>) to 1 leads to the largest performance gain, and  using a " target="#b26">27]</ref>. Motivated by the strength of graph convolution, recent efforts like NGCF <ref type="bibr" target="#b38">[39]</ref>, GC-MC <ref type="bibr" target="#b32">[33]</ref>, and PinS ifferent layers. The scores of NGCF on Gowalla and Amazon-Book are directly copied from the Table3of<ref type="bibr" target="#b38">[39]</ref>; the scores of NGCF on Yelp2018 are re-run by us.</figDesc
torization is an early such model, which directly projects the single ID of a user to her embedding <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>. Later on, several ing historical useritem interactions. For example, earlier CF models like matrix factorization (MF) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref> project the ID of
h the her interaction history as the input can improve the quality of embedding. For example, SVD++ <ref type="bibr" target="#b22">[23]</ref> demonstrates the benefits of user interaction history in p better user representations. For example, FISM <ref type="bibr" target="#b18">[19]</ref> and SVD++ <ref type="bibr" target="#b22">[23]</ref> use the weighted average of the ID embeddings of historica
state-of-the-art performance for CF. It takes inspiration from the Graph Convolution Network (GCN) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>, following the sam de itself (i.e., selfconnection). This is different from most existing graph convolution operations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta a light on modeling graph structure, especially high-hop neighbors, to guide the embedding learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>. Early studies def data in real applications, graph-based models are becoming increasingly important in recommendation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref>; by explicitly exp and GIN <ref type="bibr" target="#b41">[42]</ref>, mean aggregator and LSTM aggregator in GraphSAGE <ref type="bibr" target="#b11">[12]</ref>, etc. However, most of the work ties feature transformatio s <ref type="bibr" target="#b5">[6]</ref>, which are computationally expensive. Later on, GraphSage <ref type="bibr" target="#b11">[12]</ref> and GCN <ref type="bibr" target="#b20">[21]</ref> re-defin
mes a prevalent formulation of GNNs and is being widely used <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>. Motivated by the strength of graph convolution, recent eff
ieve the prediction, remains to be a fundamental task towards effective personalized recommendation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" targ 45">[46]</ref>, neural network-based models NeuMF <ref type="bibr" target="#b17">[18]</ref> and CMN <ref type="bibr" target="#b7">[8]</ref>, and factorization-based models MF <ref type="bibr" target="
the web, recommender system has been widely deployed to perform personalized information filtering <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46]</ref>. The core of recomme tering</head><p>Collaborative Filtering (CF) is a prevalent technique in modern recommender systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46]</ref>. One common paradigm
nal supervised learning scheme like factorization machines <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref> that model the relations implicitly. For example, a recent
ch as the hard negative sampling <ref type="bibr" target="#b28">[29]</ref> and adversarial sampling <ref type="bibr" target="#b6">[7]</ref>. We leave this extension in the future since it is not the f
can generalize to unknown data. We have also tried to learn Î± from validation data, as inspired by <ref type="bibr" target="#b3">[4]</ref> that learns hyper-parameters on validation data. The perform
resentation for nodes by smoothing features over the graph <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref>. To achieve this, it performs graph convolution iteratively insights into GNNs <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40]</ref>, which inspire us developing LightGCN. Particularly, Wu et ehind the simple design of LightGCN. First we discuss the connection with the Simplified GCN (SGCN) <ref type="bibr" target="#b39">[40]</ref>, which is a recent linear GCN model that integrates self-c providing more insights into the working mechanism of LightGCN.</p><p>3.2.1 Relation with SGCN. In <ref type="bibr" target="#b39">[40]</ref>, the authors argue the unnecessary complexity of GCN for n type="bibr" target="#b39">40]</ref>, which inspire us developing LightGCN. Particularly, Wu et al. <ref type="bibr" target="#b39">[40]</ref> argues the unnecessary complexity of GCN, developing a sim
ph signal processing literature to measure the smoothness of a signal defined over nodes of a graph <ref type="bibr" target="#b4">(Chen et al., 2015)</ref>. More specifically, given a graph with the a
en used in BNNs <ref type="bibr" target="#b1">(Boluki et al., 2020)</ref> and information retrieval <ref type="bibr" target="#b6">(Dadaneh et al., 2020b;</ref><ref type="bibr">a)</ref>. In the next se
m since the drop masks are binary. Moreover, score-function gradient estimators, such as REIN-FORCE <ref type="bibr" target="#b38">(Williams, 1992;</ref><ref type="bibr" target="#b7">Fu, 2006)</ref>,
to their irregular neighborhood connection structures. In order to account for uncertainty in GNNs, <ref type="bibr" target="#b40">Zhang et al. (2019)</ref> present a Bayesian framework where the obse
l nodes representations will converge to a stationary point, making them unrelated to node features <ref type="bibr" target="#b26">(Li et al., 2018)</ref>. While it has been shown in <ref type="bibr" oving node classification performance. However, it also brings potential concerns of over-smoothing <ref type="bibr" target="#b26">(Li et al., 2018)</ref>. If a GCN is deep with many convolutional lay
ccesses, GNNs have There exist a variety of methods to address these problems. For example, DropOut <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref> is a popular regularisation technique mpling-Graph DropConnect (GDC). We show that existing GNN regularization techniques such as DropOut <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref>, DropEdge <ref type="bibr" target="#b mprove the deep GNN performance. For example, stochastic regularization techniques, such as DropOut <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref> and DropEdge <ref type="bibr" target= constant drop rate for all layers in previous methods.</p><p>As shown in (1), (2), and (3), DropOut <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref>, DropEdge <ref type="bibr" target="#b /ref> N S ) â {0, 1} nÃn be the random binary matrices corresponding to the ones adopted in DropOut <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref>, DropEdge <ref type="bibr" target="#b ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Schematic of DropOut<ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref>. Each circle is a feature and each sq a desired distribution, such as Bernoulli <ref type="bibr" target="#b18">(Hinton et al., 2012;</ref><ref type="bibr" target="#b36">Srivastava et al., 2014)</ref> and Gaussian <ref type="bibr" target=" ivastava et al., 2014)</ref> and Gaussian <ref type="bibr" target="#b22">(Kingma et al., 2015;</ref><ref type="bibr" target="#b36">Srivastava et al., 2014)</ref>. Bernoulli dropout and its extensions
l nodes representations will converge to a stationary point, making them unrelated to node features <ref type="bibr" target="#b26">(Li et al., 2018)</ref>. While it has been shown in <ref type="bibr" oving node classification performance. However, it also brings potential concerns of over-smoothing <ref type="bibr" target="#b26">(Li et al., 2018)</ref>. If a GCN is deep with many convolutional lay

m since the drop masks are binary. Moreover, score-function gradient estimators, such as REIN-FORCE <ref type="bibr" target="#b38">(Williams, 1992;</ref><ref type="bibr" target="#b7">Fu, 2006)</ref>,
pable of enforcing sparsity in random masks <ref type="bibr" target="#b41">(Zhou et al., 2009;</ref><ref type="bibr" target="#b13">Hajiramezanali et al., 2018)</ref>, which has been shown to be necess
2017;</ref><ref type="bibr">2016;</ref><ref type="bibr" target="#b17">Hasanzadeh et al., 2019;</ref><ref type="bibr" target="#b14">Hajiramezanali et al., 2019)</ref>. Despite their successes, GNNs hav
urbations to the input node features with graph structures unchanged. FLAG leverages "free" methods <ref type="bibr" target="#b27">(Shafahi et al., 2019)</ref> to conduct efficient adversarial trainin de feature space.</p><p>Augmentation for "free". We leverage the "free" adversarial training method <ref type="bibr" target="#b27">(Shafahi et al., 2019)</ref> to craft adversarial data augmentations.
put features will lead to different effects. To illustrate, we provide a simple example on the Cora <ref type="bibr" target="#b11">(Getoor, 2005)</ref> dataset. We choose FGSM to craft adversarial aug
get="#b14">Hamilton et al., 2017;</ref><ref type="bibr" target="#b31">VeliÄkoviÄ et al., 2017;</ref><ref type="bibr" target="#b37">Xu et al., 2019)</ref> and show that FLAG brings consistent and signi
ef>, meta-learning <ref type="bibr" target="#b10">(Garcia &amp; Bruna, 2017)</ref>, social analysis <ref type="bibr" target="#b25">(Qiu et al., 2018;</ref><ref type="bibr" target="#b20">Li &amp; Goldw
2018;</ref><ref type="bibr" target="#b20">Li &amp; Goldwasser, 2019)</ref>, and recommender systems <ref type="bibr" target="#b38">(Ying et al., 2018)</ref>. However, the training of GNNs on large-sca
-distribution samples, and its effectiveness has been verified in domains including computer vision <ref type="bibr" target="#b36">(Xie et al., 2020)</ref>, language understanding <ref type="bibr" tar t of attention paid to leverage adversarial training for better clean performance in varied domains <ref type="bibr" target="#b36">(Xie et al., 2020;</ref><ref type="bibr" target="#b43">Zhu et al., 20 s to generalize GAT, and FLAG works to push the improvement further. In the computer vision domain, <ref type="bibr" target="#b36">Xie et al. (2020)</ref> proposed a new batch norm method that makes a vely augment graph data using adversarial perturbations. On large-scale image classification tasks, <ref type="bibr" target="#b36">Xie et al. (2020)</ref> leveraged adversarial perturbations, along wi
n. To clarify, FLAG is intrinsically different from the previous graph adversarial training methods <ref type="bibr" target="#b8">(Feng et al., 2019;</ref><ref type="bibr" target="#b5">Deng et al., 20 " target="#b5">Deng et al., 2019;</ref><ref type="bibr" target="#b17">Jin &amp; Zhang, 2019)</ref>. <ref type="bibr" target="#b8">Feng et al. (2019)</ref> proposed to reinforce local smoothness to mak

get="#b14">Hamilton et al., 2017;</ref><ref type="bibr" target="#b31">VeliÄkoviÄ et al., 2017;</ref><ref type="bibr" target="#b37">Xu et al., 2019)</ref> and show that FLAG brings consistent and signi
ef>, meta-learning <ref type="bibr" target="#b10">(Garcia &amp; Bruna, 2017)</ref>, social analysis <ref type="bibr" target="#b25">(Qiu et al., 2018;</ref><ref type="bibr" target="#b20">Li &amp; Goldw

>12]</ref>.</p><p>Our approach has some similarities with Predictions of Bootstrapped Latents (PBL, <ref type="bibr" target="#b48">[49]</ref>), a selfsupervised representation learning technique for r
get="#b19">20,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" tar


ng on downstream tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Many different trainin
and representation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Generative methods
rder to provide smoother changes in the target representation.</p><p>In the semi-supervised setting <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>, an unsupervised l
reproducing the setup in <ref type="bibr" target="#b8">[9]</ref> using a Faster R-CNN architecture <ref type="bibr" target="#b81">[82]</ref>, as detailed in Appendix E.5. We fine-tune on trainval2007


for its improved robustness. While previous methods based on bootstrapping have used pseudo-labels <ref type="bibr" target="#b15">[16]</ref>, cluster indices <ref type="bibr" target="#b16">[17]</ref>
he teacher and that of the student. To this end, RRD adopts the list-wise learning-to-rank approach <ref type="bibr" target="#b28">[29]</ref> and learns to ensure the student to preserve the ranking o hat of the student model. To this end, RRD adopts the classical list-wise learning-to-rank approach <ref type="bibr" target="#b28">[29]</ref>. Its core idea is to define a probability of a permutation d of the ground-truth ranking order. For more details about the list-wise approach, please refer to <ref type="bibr" target="#b28">[29]</ref>.</p><p>However, merely adopting the list-wise loss can hav p>Relaxed permutation probability. Then, RRD defines a relaxed permutation probability motivated by <ref type="bibr" target="#b28">[29]</ref>. For user ð¢, ð ð denotes a ranked list of all the sampled
atterns make the size of the model continuously increasing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar required for the inference are also increasing accordingly <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar distillation (KD) in the computer vision field, a few work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> have employed KD for RS to reduce the size of models while eacher, and also has a lower latency due to its small size <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>The core idea behind this process is that the soft l sions from the teacher model, the state-of-the-art methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> have achieved comparable or even better performance to the p>However, there are still limitations in existing methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. First, the learning of the student is only guided by the t at Figure <ref type="figure">1</ref>: The existing methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> distill the knowledge only based on the teacher's predictio of ranking orders among items. Unlike the existing methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> that distill the knowledge of an item at a time, RRD formul ation performance compared to the state-of-the-art methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. An unified framework. We propose a novel framework-DE-RRD- uge success of KD in the computer vision field, a few work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillati sting items, we adopt a ranking position importance scheme <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> that places more emphasis on the higher positions in the ra .1, 0.5, 1.0}. Following the notation of the previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, we call the student model trained without the help of the recommendation list would have strong correlations to the items that the user has interacted before <ref type="bibr" target="#b24">[25]</ref>. Also, the soft labels provide guidance for distinguishing e="bibr" target="#b24">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillation (RD) <ref type="bibr" target="#b24">[25]</ref> which applies KD for the ranking problem; Providing recomm items); the high-ranked items in the recommendation list would have strong correlations to the user <ref type="bibr" target="#b24">[25]</ref>. By using such additional supervisions from the teacher, t s. The proposed framework is compared with the following methods:</p><p>â¢ Ranking Distillation (RD) <ref type="bibr" target="#b24">[25]</ref>: A KD method for recommender system that uses items with t
head n="5.1">Experimental Setup</head><p>Datasets. We use two public real-world datasets: CiteULike <ref type="bibr" target="#b26">[27]</ref>, Foursquare <ref type="bibr" target="#b16">[17]</ref>. We
problems such as applicable only to specific models (e.g., k-d tree for metric learningbased models <ref type="bibr" target="#b11">[12]</ref>), or easily falling into a local optimum due to the local
d on accelerating the inference of the existing recommenders <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. Specifically, tree
problems such as applicable only to specific models (e.g., k-d tree for metric learningbased models <ref type="bibr" target="#b11">[12]</ref>), or easily falling into a local optimum due to the local
problems such as applicable only to specific models (e.g., k-d tree for metric learningbased models <ref type="bibr" target="#b11">[12]</ref>), or easily falling into a local optimum due to the local
ompact" model (student) by transferring knowledge from a previously trained "large" model (teacher) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target the one-hot class label, which leads to improved learning of the student model. Subsequent methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref> have focused on dist e layers. Because teacher's intermediate layers are generally bigger than that of the student, they <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref> utilize additional l
ompact" model (student) by transferring knowledge from a previously trained "large" model (teacher) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target the one-hot class label, which leads to improved learning of the student model. Subsequent methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref> have focused on dist e layers. Because teacher's intermediate layers are generally bigger than that of the student, they <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref> utilize additional l
ly, it has a limitation in accurately maintaining the ranking orders predicted by the teacher model <ref type="bibr" target="#b23">[24]</ref>, which leads to degraded recommendation performance.</p><p sly, it has a limitation in accurately maintaining the ranking orders in the teacher's ranking list <ref type="bibr" target="#b23">[24]</ref>. This can lead to limited recommendation performance.</p>< ms (e.g., pair-wise, list-wise) can achieve better ranking performance than the point-wise approach <ref type="bibr" target="#b23">[24]</ref>. RRD enables the model to capture the ranking orders among
s have adopted hash techniques to reduce the inference cost <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar
s have adopted hash techniques to reduce the inference cost <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar
endation performance. Since a user is interested in only a few items among the numerous total items <ref type="bibr" target="#b9">[10]</ref>, learning the detailed ranking orders of all items is not o anking performance. Because a user is interested in only a few items among the numerous total items <ref type="bibr" target="#b9">[10]</ref>, learning the detailed ranking orders of all the unobserved ctiveness and Efficiency. Several methods have adopted hash techniques to reduce the inference cost <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" tar three ranking metrics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>: hit ratio (H@ð ), normalized discounted cumulative gain (N@
ut evaluation protocol <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>. For each user, we leave out a single interacted item for t
s have adopted hash techniques to reduce the inference cost <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" tar
ings for CiteULike, twenty ratings for Foursquare as done in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21]</ref>. Data statistics are We follow the widely used leave-one-out evaluation protocol <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>. For each user, we le rmance of each method with widely used three ranking metrics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>: hit ratio (H@ð ), nor
ly, it has a limitation in accurately maintaining the ranking orders predicted by the teacher model <ref type="bibr" target="#b23">[24]</ref>, which leads to degraded recommendation performance.</p><p sly, it has a limitation in accurately maintaining the ranking orders in the teacher's ranking list <ref type="bibr" target="#b23">[24]</ref>. This can lead to limited recommendation performance.</p>< ms (e.g., pair-wise, list-wise) can achieve better ranking performance than the point-wise approach <ref type="bibr" target="#b23">[24]</ref>. RRD enables the model to capture the ranking orders among
="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. Specifically, tree-based data structures <ref type="bibr" target="#b1">[2]</ref>, data compression techniques <ref type="bibr" target="#b25">
ings for CiteULike, twenty ratings for Foursquare as done in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21]</ref>. Data statistics are We follow the widely used leave-one-out evaluation protocol <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>. For each user, we le rmance of each method with widely used three ranking metrics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>: hit ratio (H@ð ), nor
d on accelerating the inference of the existing recommenders <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. Specifically, tree
ings for CiteULike, twenty ratings for Foursquare as done in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21]</ref>. Data statistics are We follow the widely used leave-one-out evaluation protocol <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>. For each user, we le rmance of each method with widely used three ranking metrics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>: hit ratio (H@ð ), nor
ly, it has a limitation in accurately maintaining the ranking orders predicted by the teacher model <ref type="bibr" target="#b23">[24]</ref>, which leads to degraded recommendation performance.</p><p sly, it has a limitation in accurately maintaining the ranking orders in the teacher's ranking list <ref type="bibr" target="#b23">[24]</ref>. This can lead to limited recommendation performance.</p>< ms (e.g., pair-wise, list-wise) can achieve better ranking performance than the point-wise approach <ref type="bibr" target="#b23">[24]</ref>. RRD enables the model to capture the ranking orders among
ering what causes deep networks to be fragile to adversarial examples and how to improve robustness <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" ta t has been theoretically shown that decreasing the input dimensionality of data improves robustness <ref type="bibr" target="#b45">[47]</ref>. Adversarial training <ref type="bibr" target="#b33">[35]< ining performance on natural examples.</p><p>Using the first order vulnerability of neural networks <ref type="bibr" target="#b45">[47]</ref>, we theoretically show that increasing output dimensionali ethods can improve the model's robustness without compromising clean accuracy. Simon-Gabriel et al. <ref type="bibr" target="#b45">[47]</ref> conducted a theoretical analysis of the vulnerability of n ref>. We denote the multitask predictor as F and each individual task predictor as F c . Prior work <ref type="bibr" target="#b45">[47]</ref> showed that the norm of gradients captures the vulnerabili rial noise is imperceptible, i.e., r â 0, we can approximate âL with a first-order Taylor expansion <ref type="bibr" target="#b45">[47]</ref>.</p><p>Lemma 1. For a given neural network F that predicts l></formula><p>Remark 1. By increasing the number of output tasks M , the first order vulnerability <ref type="bibr" target="#b45">[47]</ref> of network decreases. In the ideal case, if the model has s we add more tasks, the norm of the joint gradient decreases, indicating improvement to robustness <ref type="bibr" target="#b45">[47]</ref>. The only exception is the depth estimation task, which we e same dimension for baselines and ours during comparison because input dimension impacts robustness<ref type="bibr" target="#b45">[47]</ref>.</note> 		</body> 		<back>  			<div type="acknowledgement"
in the literature. While past theoretical work showed the hardness of multi-objective optimization <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b42">44]</ref>, we leverage this
we use semantic segmentation. The experiment uses a pre-trained Dilated Residual Network (DRN-105) <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref> model on the Citys
sks on two natural image datasets, Cityscapes <ref type="bibr" target="#b6">[8]</ref> and Taskonomy <ref type="bibr" target="#b58">[60]</ref>. When all tasks are under attack, multitask learning incre ne) and multitask models.<ref type="foot" target="#foot_0">1</ref> Taskonomy. The Taskonomy dataset <ref type="bibr" target="#b58">[60]</ref> consists of images of indoor scenes. We train on up to 11 s, we use DRN-105 model as the architecture for encoder and decoder; on Taskonomy, we use Resnet-18 <ref type="bibr" target="#b58">[60]</ref>. Each task has its own decoder. For the Cityscapes dataset
as demonstrated that images with human-imperceptible noise <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target
et="#b22">[24,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b25">27]</ref>, where all the task k uses shared backbone network with task-specific branches <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b25">27]</ref>. We denote the mult cks</head><p>Following the setup for multitask learning in <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b25">27]</ref>, we train the multi
eously attacked. We experiment with up to 11 vision tasks on two natural image datasets, Cityscapes <ref type="bibr" target="#b6">[8]</ref> and Taskonomy <ref type="bibr" target="#b58">[60]</ref>. Whe ns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Cityscapes. The Cityscapes dataset <ref type="bibr" target="#b6">[8]</ref> consists of images of urban driving scenes. We study three t
ss. While previous work shows that multitask learning can improve the performance of specific tasks <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b46">48]</ref>, we show that it inc ed work in multitask learning and adversarial attacks.</p><p>Multitask Learning: Multitask learning <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" targ k learning improves the performance of select tasks by introducing a knowledge-based inductive bias <ref type="bibr" target="#b2">[4]</ref>. However, multi-objective functions are hard to optimize, wh
zation procedure. For instance, more training data -both labeled and unlabeled -improves robustness <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b50">52]</ref>. It has been theor
duction method, which preserves the correlation and structure for high dimensional, structured data <ref type="bibr" target="#b20">[22]</ref>. Figure <ref type="figure">4a</ref> shows that the model's
eously attacked. We experiment with up to 11 vision tasks on two natural image datasets, Cityscapes <ref type="bibr" target="#b6">[8]</ref> and Taskonomy <ref type="bibr" target="#b58">[60]</ref>. Whe ns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Cityscapes. The Cityscapes dataset <ref type="bibr" target="#b6">[8]</ref> consists of images of urban driving scenes. We study three t
nches remain the single most important category of mispredictions to tackle. Prior works like EXACT <ref type="bibr" target="#b8">[9]</ref>, SLB <ref type="bibr" target="#b17">[18]</ref> have proposed ei-c.org/ns/1.0"><head n="2.2">Existing Techniques to Handle Data Dependent Branches</head><p>EXACT <ref type="bibr" target="#b8">[9]</ref> is a branch prediction technique proposed to lower mispredic pairs that need to get tracked quickly increase the storage requirement over 10KB. As mentioned in <ref type="bibr" target="#b8">[9]</ref>, EXACT predictor does not win over an similarly sized TAGE p leading to the branch. To enable this, we employ a scheme similar to EXACT predictor ID generation <ref type="bibr" target="#b8">[9]</ref> where the ARF is extended to track the load addresses. Inste hat wrote to the same address and use the store data value for override. EXACT's active update unit <ref type="bibr" target="#b8">[9]</ref> and SLB <ref type="bibr" target="#b17">[18]</ref> use this o en. But, when data has high entropy, it affects their accuracy. Among recent works, EXACT predictor <ref type="bibr" target="#b8">[9]</ref> proposes associating data dependent branches with their corr BT. We assume that opcode and other instruction metadata is available with the ROB entry similar to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>1) 3D-Branch
rom a prior store that wrote to same address as the feeder load using a memory dependence predictor <ref type="bibr" target="#b39">[40]</ref>, iii) Predict the load address and prefetch the load value the hardware requirements, we enhance 3D-Branch Overrider with a Memory Dependence Predictor (MDP) <ref type="bibr" target="#b39">[40]</ref> to capture the strongly correlated store PC-load PC pairs
n extended ARF, we require 14 bytes per entry. Since x64 ISA <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref>, only has 16 general purpose architectural registers and on
CTION</head><p>Given the continued importance of single thread performance in general-purpose cores <ref type="bibr" target="#b29">[30]</ref>, architects are looking to extract more instruction level
allowing the loads value to be available earlier as well. We leverage from prior proposals such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref> to build a simple a ranch misprediction penalty rather than improving the branch accuracy. For unconfident predictions, <ref type="bibr" target="#b9">[10]</ref> proposes to prioritize the issue of the instructions on the an be implemented on top of the 3D-Branch Overrider for additional gains. Even though the target of <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref> and our work is
1.0"><head n="8">RELATED WORK</head><p>Prior works such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" tar ues is higher as compared to using the load to read the value in timely manner.</p><p>Works such as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref> use data dependenc
chain are ready, a predictor table is looked up to obtain a prediction. Branch Outcome Anticipation <ref type="bibr" target="#b16">[17]</ref> also looked at reducing the branch misprediction penalty r Overrider for additional gains. Even though the target of <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref> and our work is to reduce the branch misprediction penalty
allowing the loads value to be available earlier as well. We leverage from prior proposals such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref> to build a simple a ranch misprediction penalty rather than improving the branch accuracy. For unconfident predictions, <ref type="bibr" target="#b9">[10]</ref> proposes to prioritize the issue of the instructions on the an be implemented on top of the 3D-Branch Overrider for additional gains. Even though the target of <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref> and our work is
s direction. Branch predictor proposals in literature track specific branches in the global history <ref type="bibr" target="#b26">[27]</ref> or only use the specific branch's prior history (referred
div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK</head><p>Prior works such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta
xtend the 3D-Branch Overrider, our proposal will work with any other load address predictor such as <ref type="bibr" target="#b28">[29]</ref> as well. The load address predictor generates a load prefe
r/item embeddings by mimicking the meta-learning setting via episode based training, as proposed in <ref type="bibr" target="#b33">[34]</ref>. Specifically, we pick the users/items with sufficient int rget="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref>, which consists of metric-based recommendation <ref type="b
, social trust path <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> and knowledge graph

<ref type="bibr" target="#b43">44]</ref>, social trust path <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar
9 N x c 2 W x s o + P R l L R e X k o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h <ref type="bibr" target="#b5">6</ref>  </p><formula xml:id="formula_2">O U B z E d K R E J R t F K f 9 N x c 2 W x s o + P R l L R e X k o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h <ref type="bibr" target="#b5">6</ref>  </p><formula xml:id="formula_3">O U B z E d K R E J R t F K f 9 N x c 2 W x s o + P R l L R e X k o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h <ref type="bibr" target="#b5">6</ref>  </p><formula xml:id="formula_4">O U B z E d K R E J R t F K f
-item interactions. One kind of the methods is meta-learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" tar
eve this goal, we cast the task of neighbor sampler as a hierarchical Markov Decision Process (MDP) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47]</ref>. Specifically, we
ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref>, social trust path <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" tar
the underlying patterns behind the user-item interactions. One kind of the methods is meta-learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" targ
9 N x c 2 W x s o + P R l L R e X k o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h <ref type="bibr" target="#b5">6</ref>  </p><formula xml:id="formula_2">O U B z E d K R E J R t F K f 9 N x c 2 W x s o + P R l L R e X k o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h <ref type="bibr" target="#b5">6</ref>  </p><formula xml:id="formula_3">O U B z E d K R E J R t F K f 9 N x c 2 W x s o + P R l L R e X k o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h <ref type="bibr" target="#b5">6</ref>  </p><formula xml:id="formula_4">O U B z E d K R E J R t F K f
ntal Setup</head><p>Dataset. We evaluate on three public datasets including MovieLens-1M (Ml-1M) 3  <ref type="bibr" target="#b11">[12]</ref>, MOOCs 4 <ref type="bibr" target="#b46">[47]</ref> and Las
rained language models (e.g., ELMo <ref type="bibr" target="#b29">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, XLnet <ref type="bibr" target="#b45">(Yan e achieved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b22">Liu et al., entations of the fully supervised NER methods attain very close to the state-of-the-art performance <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b19">Limsopatham
kens s = [x i , ..., x j ] (0 â¤ i â¤ j â¤ N ) associated with an entity type. Based on the BIO schema <ref type="bibr" target="#b18">(Li et al., 2012)</ref>, NER is typically formulated as a sequence la
uire a considerable amount of annotated tokens or external tools. To address the label noise issue, <ref type="bibr" target="#b27">Ni et al. Ni et al. (2017)</ref> use heuristic rules to filter out se
kens s = [x i , ..., x j ] (0 â¤ i â¤ j â¤ N ) associated with an entity type. Based on the BIO schema <ref type="bibr" target="#b18">(Li et al., 2012)</ref>, NER is typically formulated as a sequence la

t="#b37">Tarvainen and Valpola, 2017;</ref><ref type="bibr" target="#b26">Miyato et al., 2018;</ref><ref type="bibr" target="#b25">Meng et al., 2018;</ref><ref type="bibr" target="#b3">Clark et al., 2
extra training data. Therefore, we only compare with the results reported in their papers. (i) KALM <ref type="bibr" target="#b20">(Liu et al., 2019a)</ref> augments a traditional language model with rove the performance, as shown in the ablation study in our experiments.</p><p>Other related works: <ref type="bibr" target="#b20">Liu et al. (2019a)</ref> propose a language model-based method -KALM
uire a considerable amount of annotated tokens or external tools. To address the label noise issue, <ref type="bibr" target="#b27">Ni et al. Ni et al. (2017)</ref> use heuristic rules to filter out se
stic gradient-type algorithms, e.g., ADAM <ref type="bibr" target="#b14">(Kingma and Ba, 2014;</ref><ref type="bibr" target="#b21">Liu et al., 2020)</ref>. Following <ref type="bibr" target="#b30">Raf
er all possible labels for unlabeled tokens <ref type="bibr" target="#b44">(Yang et al., 2018;</ref><ref type="bibr" target="#b35">Shang et al., 2018)</ref>, but they still require a considerable amou </ref><ref type="bibr" target="#b36">Strauss et al., 2016)</ref> and Biomedical Domain NER Datasets <ref type="bibr" target="#b35">(Shang et al., 2018)</ref>.</p></div> <div xmlns="http://www.tei-c.or "#b24">(Ma and Hovy, 2016)</ref> is trained using the distant labels matched from KBs; (ii) AutoNER <ref type="bibr" target="#b35">(Shang et al., 2018)</ref> trains the model by assigning ambiguous to
/ns/1.0"><head n="5">Related Work and Discussion</head><p>Our work is related to low-resource NER.  <ref type="bibr" target="#b33">(Rosenberg et al., 2005;</ref><ref type="bibr" target="#b37">Tarvaine
i-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformer models were originally introduced by <ref type="bibr" target="#b36">Vaswani et al. (2017)</ref> in the context of neural machine translat p><p>Initially, in Â§ 3.1, we introduce a formulation for the transformer architecture introduced in <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>. Subsequently, in Â§ 3.2 and Â§ 3.3 we pre the memory consumption with respect to the self attention layer. In all experiments, we use softmax <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref> to refer to the standard transformer arc
ring knowledge to tasks with limited or no supervision when they are pretrained with autoregressive <ref type="bibr" target="#b26">(Radford et al., 2018;</ref><ref type="bibr">2019)</ref> or masked la
ning previous contexts in memory introduces significant additional computational cost. In contrast, <ref type="bibr" target="#b33">Sukhbaatar et al. (2019)</ref> extended the context length significan
mp; Hinton, 2009)</ref>. Recent works <ref type="bibr" target="#b1">(Blanc &amp; Rendle, 2017;</ref><ref type="bibr" target="#b28">Rawat et al., 2019)</ref>, have approximated softmax with a linear do
</ref> and have demonstrated impressive results on a variety of tasks dealing with natural language <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, audio <ref type="bibr" target="#b32">(Spe 26">(Radford et al., 2018;</ref><ref type="bibr">2019)</ref> or masked language modeling objectives <ref type="bibr" target="#b8">(Devlin et al., 2019;</ref><ref type="bibr" target="#b37">Yang et al.,
asks dealing with natural language <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, audio <ref type="bibr" target="#b32">(Sperber et al., 2018)</ref>, and images <ref type="bibr" target="#b2
ugh weight pruning <ref type="bibr" target="#b20">(Michel et al., 2019)</ref>, weight factorization <ref type="bibr" target="#b16">(Lan et al., 2020)</ref>, weight quantization <ref type="bibr" target
ring knowledge to tasks with limited or no supervision when they are pretrained with autoregressive <ref type="bibr" target="#b26">(Radford et al., 2018;</ref><ref type="bibr">2019)</ref> or masked la
</ref> and have demonstrated impressive results on a variety of tasks dealing with natural language <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, audio <ref type="bibr" target="#b32">(Spe 26">(Radford et al., 2018;</ref><ref type="bibr">2019)</ref> or masked language modeling objectives <ref type="bibr" target="#b8">(Devlin et al., 2019;</ref><ref type="bibr" target="#b37">Yang et al.,
ring knowledge to tasks with limited or no supervision when they are pretrained with autoregressive <ref type="bibr" target="#b26">(Radford et al., 2018;</ref><ref type="bibr">2019)</ref> or masked la
called replaced token detection that is more sample efficient and reduces the overall computation. <ref type="bibr" target="#b15">Lample et al. (2019)</ref> used product-key attention to increase the
ion.</p><p>Accurately predicting (black-box) workload criticality is itself a challenge. Prior work <ref type="bibr" target="#b5">[6]</ref> associated a diurnal utilization pattern with user interacti system</head><p>To integrate predictions into VM scheduling in practice, we target Resource Central <ref type="bibr" target="#b5">[6]</ref>, the existing ML and predictionserving system in Azure. The e workload of each VM is performance-critical or not, before we can train a model. As in prior work <ref type="bibr" target="#b5">[6]</ref>, we consider a workload critical if it is user-facing, i.e. he problem reduces to identifying VMs whose time series of CPU utilizations exhibit 24-hour periods <ref type="bibr" target="#b5">[6]</ref>.</p><p>Obviously, some background VMs may exhibit 24-hour pe identifying periods in time series, such as FFT or the autocorrelation function (ACF). For example, <ref type="bibr" target="#b5">[6]</ref> assumes a workload is user-facing if the FFT indicates a 24- /task length for provisioning or scheduling purposes, e.g. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr"
execute, e.g. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bib
">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Both modeling/optimization and feedback techniques have b

"#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bib

="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bib
">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b27">[28]</ref>. In contrast, we introduce a new algorithm and ML models f
meet a tight power budget as applications execute, e.g. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bib
orrelated peaks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Our work exten ave studied hierarchical capping in production datacenters <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Our paper focu

limitations of deep SISR. SISR performance was boosted right after the non-local attention modules <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" targ <ref type="bibr" target="#b19">[20]</ref>, RNAN <ref type="bibr" target="#b36">[37]</ref> and SAN <ref type="bibr" target="#b1">[2]</ref>, incorporate non-local operation into their networks in orde o the fusion structure. For the IS-NL branch, it contains a non-local attention module adopted from <ref type="bibr" target="#b1">[2]</ref> and a deconvolution layer for upscaling the module outputs. ution layer for upscaling the module outputs. The IS-NL module is region-based in this paper. As in <ref type="bibr" target="#b1">[2]</ref>, we divide the feature maps into region grids, where the int N <ref type="bibr" target="#b17">[18]</ref>, OISR <ref type="bibr" target="#b11">[12]</ref> and SAN <ref type="bibr" target="#b1">[2]</ref>.</p><p>Quantitative Evaluations In Table <ref type="table" t f> RDN <ref type="bibr" target="#b38">[39]</ref> RCAN <ref type="bibr" target="#b36">[37]</ref> SAN <ref type="bibr" target="#b1">[2]</ref> Ours Urban100 (4Ã): img 078 HR Bicubic LapSRN <ref type="bib f> RDN <ref type="bibr" target="#b38">[39]</ref> RCAN <ref type="bibr" target="#b36">[37]</ref> SAN <ref type="bibr" target="#b1">[2]</ref> Ours Urban100 (4Ã): img 047 HR Bicubic LapSRN <ref type="bib f> RDN <ref type="bibr" target="#b38">[39]</ref> RCAN <ref type="bibr" target="#b36">[37]</ref> SAN <ref type="bibr" target="#b1">[2]</ref> Ours which only needs 20% parameters of RCAN and SAN, but ac
discriminative ability of the network.</p><p>Motivated by the traditional Image SR and recent DBPN <ref type="bibr" target="#b10">[11]</ref>, we adopt the back-projection approach to incorporate loca ncorporate local information to regularize the feature and correct reconstruction errors. Following <ref type="bibr" target="#b10">[11]</ref>, the final fused feature H is computed by,</p><formula xml Net <ref type="bibr" target="#b26">[27]</ref>, EDSR <ref type="bibr" target="#b18">[19]</ref>, DBPN <ref type="bibr" target="#b10">[11]</ref>, RDN <ref type="bibr" target="#b38">[39]</ref>, RCAN <ref orms other state-of-the-arts by a large margin. EDSR <ref type="bibr" target="#b18">[19]</ref> DBPN <ref type="bibr" target="#b10">[11]</ref> OISR <ref type="bibr" target="#b11">[12]</ref> RDN <ref ty apSRN <ref type="bibr" target="#b16">[17]</ref> EDSR <ref type="bibr" target="#b18">[19]</ref> DBPN <ref type="bibr" target="#b10">[11]</ref> OISR <ref type="bibr" target="#b11">[12]</ref> RDN <ref ty apSRN <ref type="bibr" target="#b16">[17]</ref> EDSR <ref type="bibr" target="#b18">[19]</ref> DBPN <ref type="bibr" target="#b10">[11]</ref> OISR <ref type="bibr" target="#b11">[12]</ref> RDN <ref ty
<ref type="bibr" target="#b16">[17]</ref>, SRMDNF <ref type="bibr" target="#b35">[36]</ref>, MemNet <ref type="bibr" target="#b26">[27]</ref>, EDSR <ref type="bibr" target="#b18">[19]</ref>, DBPN <ref
get="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>. The mapping between LR and HR image is not bijective, whic
bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, we use 800 images from DIV2K <ref type="bibr" target="#b27">[28]</ref> dataset to train our models. For testing, we report the pe
get="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>. The mapping between LR and HR image is not bijective, whic
ocal limitation, methods based on non-local mean filtering <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref> start to globally search similar patches over the whole LR
arget="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. In the pioneering s also been applied to solve SR with blur and noisy images <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Deep CNNs for Image SR The first work that introduce
nt performance improvement. To further take the advantages of deep SISR, for several years, efforts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ
ithful, accurate and high-quality reconstructions.</p><p>Since the first deep learning-based method <ref type="bibr" target="#b3">[4]</ref> was proposed, discriminative learning based methods make it .</p><p>Deep CNNs for Image SR The first work that introduced CNN to solve image SR was proposed by <ref type="bibr" target="#b3">[4]</ref>, where they interpret the three consecutive convolution laye
AN <ref type="bibr" target="#b36">[37]</ref>, NLRN <ref type="bibr" target="#b19">[20]</ref>, SRFBN <ref type="bibr" target="#b17">[18]</ref>, OISR <ref type="bibr" target="#b11">[12]</ref> and SAN <r
ve to automatically generate prompts given the few-shot training data using the generative T5 model <ref type="bibr" target="#b30">(Raffel et al., 2020)</ref>. This allows us to cheaply obtain effecti lly from a fixed set of label words M(Y). To address this challenging problem, we propose to use T5 <ref type="bibr" target="#b30">(Raffel et al., 2020)</ref>, a large pre-trained text-to-text Transfo e <ref type="table">B</ref>.1.</p><p>For automatic template search with T5, we take the T5-3B model <ref type="bibr" target="#b30">(Raffel et al., 2020)</ref>, which is the largest publicly available
lliams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b9">(Dagan et al., 2005;</ref><ref type="bibr" target="#b3">Bar-Haim et al
ginal development sets for testing. For the datasets which require a cross-validation evaluation-MR <ref type="bibr" target="#b25">(Pang and Lee, 2005)</ref>, CR <ref type="bibr" target="#b18">(Hu and
tworks <ref type="bibr" target="#b39">(Vinyals et al., 2016)</ref>, and more specifically BERT-PAIR <ref type="bibr" target="#b14">(Gao et al., 2019)</ref>, used in metalearning, but here we do not ex
rget="#b45">Xie et al., 2020)</ref>, where a set of unlabeled examples are given; (2) meta-learning <ref type="bibr" target="#b16">(Yu et al., 2018;</ref><ref type="bibr" target="#b16">Han et al., 201 labeled examples are given; (2) meta-learning <ref type="bibr" target="#b16">(Yu et al., 2018;</ref><ref type="bibr" target="#b16">Han et al., 2018;</ref><ref type="bibr">Bansal et al., 2020a,b;</ref>
practical scenario in which we only assume access to a moderatelysized language model such as BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> or RoBERTa <ref type="bibr" target="#b21"
mple examples that are semantically close to x in . Specifically, we use a pretrained Sentence-BERT <ref type="bibr" target="#b32">(Reimers and Gurevych, 2019)</ref> model to obtain embeddings for all
knowledge from pre-trained language models <ref type="bibr" target="#b38">(Trinh and Le, 2018;</ref><ref type="bibr" target="#b26">Petroni et al., 2019;</ref><ref type="bibr" target="#b10">Davison et

r" target="#b9">(Dagan et al., 2005;</ref><ref type="bibr" target="#b3">Bar-Haim et al., 2006;</ref><ref type="bibr" target="#b15">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b5">Bentivog
">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b5">Bentivogli et al., 2009)</ref>, MRPC <ref type="bibr" target="#b13">(Dolan and Brockett, 2005)</ref>, QQP<ref type="foot" target="#foot_9
nd efficiently segmenting large point clouds <ref type="bibr" target="#b45">[Wang et al., 2018</ref><ref type="bibr" target="#b29">, Li et al., 2019b]</ref>. Recent works have looked at frameworks to 20">, Huang et al., 2017</ref><ref type="bibr" target="#b53">, Yu and Koltun, 2016]</ref>, DeepGCNs <ref type="bibr" target="#b29">[Li et al., 2019b]</ref> propose to train very deep GCNs (56 layers) to be either SoftMax_Agg Î² (â¢) or PowerMean_Agg p (â¢).</p><p>Better Residual Connections. DeepGCNs <ref type="bibr" target="#b29">[Li et al., 2019b]</ref> show residual connections <ref type="bibr" t ., 2016]</ref> is used in every layer before the activation function ReLU.</p><p>ResGCN. Similar to <ref type="bibr" target="#b29">Li et al. [2019b]</ref>, we construct ResGCN by adding residual conne

cing predictions of recommendation engines <ref type="bibr" target="#b35">[Monti et al., 2017b</ref><ref type="bibr" target="#b52">, Ying et al., 2018]</ref>, and efficiently segmenting large point cl
ed out by <ref type="bibr">Li et al. [2018]</ref>. Recent works focus on addressing this phenomenon <ref type="bibr" target="#b24">[Klicpera et al., 2019</ref><ref type="bibr" target="#b41">, Rong et rget="#b41">, Rong et al., 2020</ref><ref type="bibr" target="#b57">, Zhao and Akoglu, 2020]</ref>. <ref type="bibr" target="#b24">Klicpera et al. [2019]</ref> proposes a PageRank-based message passin



ef>, Node2Vec <ref type="bibr" target="#b13">[Grover and Leskovec, 2016]</ref>, Chebyshev graph CNN <ref type="bibr" target="#b8">[Defferrard et al., 2016]</ref>, GCN <ref type="bibr" target="#b23">[K
et al. [2019b]</ref> show the universality of invariant GNNs to any continuous invariant function. <ref type="bibr" target="#b22">Keriven and PeyrÃ© [2019]</ref> further extend it to the equivariant c

al., 2017]</ref> and 3D point cloud processing <ref type="bibr" target="#b39">[Qi et al., 2017</ref><ref type="bibr" target="#b46">, Wang et al., 2019]</ref>. To further go beyond these simple aggrega
t="#b28">(Navarin, Tran, and Sperduti 2018;</ref><ref type="bibr" target="#b18">Hu et al. 2019</ref><ref type="bibr" target="#b17">Hu et al. , 2020) )</ref> is to learn transferable prior knowledge fr t="#b18">Hu et al. 2019)</ref>, or still require supervised information for graph-level pretraining <ref type="bibr" target="#b17">(Hu et al. 2020</ref>). While at the node level, predicting links bet is to learn a generic initialization for model parameters using readily available graph structures <ref type="bibr" target="#b17">(Hu et al. 2020</ref><ref type="bibr" target="#b18">(Hu et al. , 2019 atasets. We conduct experiments on data from two domains: biological function prediction in biology <ref type="bibr" target="#b17">(Hu et al. 2020</ref>) and research field prediction in bibliography. ers with three unsupervised tasks to capture different aspects of a graph. More recently, Hu et al. <ref type="bibr" target="#b17">(Hu et al. 2020)</ref> propose different strategies to pre-train grap subgraph is centered at a paper and contains the associated information of For biology data, as in <ref type="bibr" target="#b17">(Hu et al. 2020)</ref>, we use 306,925 unlabeled protein ego-networks that correspond to 40 binary classification tasks. We split the downstream data with species split <ref type="bibr" target="#b17">(Hu et al. 2020)</ref>, and evaluate the test performance with averag across the graph's patch representations; (3) Context Prediction strategy (denoted by ContextPred) <ref type="bibr" target="#b17">(Hu et al. 2020)</ref> to explore graph structures and (4) Attribute 2020)</ref> to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) <ref type="bibr" target="#b17">(Hu et al. 2020)</ref> to learn the regularities of the node and edge which harms the generalization of the pre-trained GNNs. This finding confirms previous observations <ref type="bibr" target="#b17">(Hu et al. 2020;</ref><ref type="bibr" target="#b31">Rosenstein et al
a few fine-tuning updates. The proposed learning to pre-train can be deemed a form of meta-learning <ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017)</ref>, also known as learning to lear ermore, our strategy is a form of meta-learning, in particular, model agnostic meta-learning (MAML) <ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017)</ref>. Meta-learning aims to learn pr hods directly adjust the optimization algorithm to enable quick adaptation with just a few examples <ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b43">Y
rget="#b7">(Devlin et al. 2019;</ref><ref type="bibr">Mikolov et al. 2013</ref>) and image encoders <ref type="bibr" target="#b13">(Girshick et al. 2014;</ref><ref type="bibr" target="#b8">Donahue et uning, which is widely known in the literature <ref type="bibr" target="#b25">(Lv et al. 2020;</ref><ref type="bibr" target="#b13">Gururangan et al. 2020</ref>), whether it is on the graph data, or in
res as well as node or edge features <ref type="bibr" target="#b47">(Zhang, Cui, and Zhu 2020;</ref><ref type="bibr" target="#b42">Wu et al. 2020;</ref><ref type="bibr" target="#b10">Dwivedi et al. 20 ="bibr" target="#b5">Bruna et al. 2014;</ref><ref type="bibr" target="#b22">Levie et al. 2019;</ref><ref type="bibr" target="#b42">Xu et al. 2019a</ref>) and message passing architectures to aggregate al. 2019)</ref>. For a more comprehensive understanding of GNNs, we refer readers to the literature <ref type="bibr" target="#b42">(Wu et al. 2020;</ref><ref type="bibr" target="#b2">Battaglia et al.
target="#b30">Qu, Bengio, and Tang 2019;</ref><ref type="bibr" target="#b29">Pei et al. 2020;</ref><ref type="bibr" target="#b27">Munkhdalai and Yu 2017)</ref>. Empirically, these GNNs have achieved ion over tasks, while model-based methods <ref type="bibr" target="#b32">(Santoro et al. 2016;</ref><ref type="bibr" target="#b27">Munkhdalai and Yu 2017)</ref> aim to design an architecture or traini
rk</head><p>GNNs have received significant attention due to the prevalence of graph-structured data <ref type="bibr" target="#b4">(Bronstein et al. 2017)</ref>. Originally proposed <ref type="bibr" ta
Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b39">Velickovic et al. 2018;</ref><ref type="bibr" target="#b45">Ying et al. 2018b;</ref><ref type="bibr" target="#b15">Hasanzadeh et t al. 2015)</ref> or more complex approaches <ref type="bibr" target="#b5">(Bruna et al. 2014;</ref><ref type="bibr" target="#b45">Ying et al. 2018b</ref>). We abstract READOUT as a parameterized func
ge across similar learning tasks, so that the learned knowledge can be quickly adapted to new tasks <ref type="bibr" target="#b41">(Vilalta and Drissi 2002;</ref><ref type="bibr" target="#b38">Vanscho
et="#b12">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b43">Yao et al. 2019;</ref><ref type="bibr" target="#b21">Lee et al. 2019;</ref><ref type="bibr" target="#b24">Lu, Fang, and Sh
works on meta-learning, metric-based methods <ref type="bibr" target="#b35">(Sung et al. 2018;</ref><ref type="bibr" target="#b34">Snell, Swersky, and Zemel 2017</ref>) learn a metric or distance func
ge across similar learning tasks, so that the learned knowledge can be quickly adapted to new tasks <ref type="bibr" target="#b41">(Vilalta and Drissi 2002;</ref><ref type="bibr" target="#b38">Vanscho
et="#b12">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b43">Yao et al. 2019;</ref><ref type="bibr" target="#b21">Lee et al. 2019;</ref><ref type="bibr" target="#b24">Lu, Fang, and Sh
target="#b30">Qu, Bengio, and Tang 2019;</ref><ref type="bibr" target="#b29">Pei et al. 2020;</ref><ref type="bibr" target="#b27">Munkhdalai and Yu 2017)</ref>. Empirically, these GNNs have achieved ion over tasks, while model-based methods <ref type="bibr" target="#b32">(Santoro et al. 2016;</ref><ref type="bibr" target="#b27">Munkhdalai and Yu 2017)</ref> aim to design an architecture or traini
ref><ref type="bibr" target="#b14">Hamilton, Ying, and Leskovec 2017)</ref>, recommendation systems <ref type="bibr" target="#b11">(Fan et al. 2019;</ref><ref type="bibr" target="#b44">Ying et al. 201
o notice that some baselines give surprisingly limited performance gain and yield negative transfer <ref type="bibr" target="#b31">(Rosenstein et al. 2005</ref>) on the downstream task (i.e., EdgePred s. This finding confirms previous observations <ref type="bibr" target="#b17">(Hu et al. 2020;</ref><ref type="bibr" target="#b31">Rosenstein et al. 2005</ref>) that negative transfer results in limit
ky, and Zemel 2017</ref>) learn a metric or distance function over tasks, while model-based methods <ref type="bibr" target="#b32">(Santoro et al. 2016;</ref><ref type="bibr" target="#b27">Munkhdalai
ph-structured data <ref type="bibr" target="#b4">(Bronstein et al. 2017)</ref>. Originally proposed <ref type="bibr" target="#b26">(Marco, Gabriele, and Franco 2005;</ref><ref type="bibr" target="#b33 m a public repository<ref type="foot" target="#foot_0">1</ref> , covering 394,925 protein subgraphs <ref type="bibr" target="#b26">(Marinka et al. 2019)</ref>. We further present a new collection of b
" target="#b45">Ying et al. 2018b;</ref><ref type="bibr" target="#b15">Hasanzadeh et al. 2019;</ref><ref type="bibr" target="#b30">Qu, Bengio, and Tang 2019;</ref><ref type="bibr" target="#b29">Pei et " target="#b41">(Vilalta and Drissi 2002;</ref><ref type="bibr" target="#b38">Vanschoren 2018;</ref><ref type="bibr" target="#b30">Peng 2020</ref>). Among previous works on meta-learning, metric-based
res as well as node or edge features <ref type="bibr" target="#b47">(Zhang, Cui, and Zhu 2020;</ref><ref type="bibr" target="#b42">Wu et al. 2020;</ref><ref type="bibr" target="#b10">Dwivedi et al. 20 ="bibr" target="#b5">Bruna et al. 2014;</ref><ref type="bibr" target="#b22">Levie et al. 2019;</ref><ref type="bibr" target="#b42">Xu et al. 2019a</ref>) and message passing architectures to aggregate al. 2019)</ref>. For a more comprehensive understanding of GNNs, we refer readers to the literature <ref type="bibr" target="#b42">(Wu et al. 2020;</ref><ref type="bibr" target="#b2">Battaglia et al.
ref><ref type="bibr" target="#b14">Hamilton, Ying, and Leskovec 2017)</ref>, recommendation systems <ref type="bibr" target="#b11">(Fan et al. 2019;</ref><ref type="bibr" target="#b44">Ying et al. 201
works on meta-learning, metric-based methods <ref type="bibr" target="#b35">(Sung et al. 2018;</ref><ref type="bibr" target="#b34">Snell, Swersky, and Zemel 2017</ref>) learn a metric or distance func
Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b39">Velickovic et al. 2018;</ref><ref type="bibr" target="#b45">Ying et al. 2018b;</ref><ref type="bibr" target="#b15">Hasanzadeh et t al. 2015)</ref> or more complex approaches <ref type="bibr" target="#b5">(Bruna et al. 2014;</ref><ref type="bibr" target="#b45">Ying et al. 2018b</ref>). We abstract READOUT as a parameterized func
Bayesian active learning algorithm for deep learning in image data is proposed based on the idea in <ref type="bibr" target="#b41">[42]</ref>.</p><p>Most works that apply active learning to recommende basic equation of Variational Inference <ref type="bibr" target="#b51">[52]</ref>.</p><p>Following <ref type="bibr" target="#b41">[42]</ref>, we use the distribution of the network parameter with dro nsidered as the smoothed version of hinge loss.</p><p>As for the second term in (2), it's proved in <ref type="bibr" target="#b41">[42]</ref> that it can be approximated by L2 regularization term</p><
pwords like of and on. Therefore, a better mechanism for local weights is needed.</p><p>Inspired by <ref type="bibr" target="#b59">[60]</ref>, we propose a local weight network based on distributed wo
wise approach.</p><p>â¢ Combining the proposed model with densityweighted Expected Loss Optimization <ref type="bibr" target="#b16">[17]</ref>, we introduce active learning into POLAR <ref type="bibr" ediction the parameters under the posterior disagree about are selected. Expected Loss Optimization <ref type="bibr" target="#b16">[17]</ref> selects the instance that maximizes the expected loss base ected Loss Optimization</head><p>The active learning metric we choose is Expected Loss Optimization <ref type="bibr" target="#b16">[17]</ref>. The basic idea is to choose the instance that maximizes t
ampled from optimal dropout distribution. The optimization is done through standard backpropagation <ref type="bibr" target="#b61">[62]</ref> and stochastic gradient descent method with mini-batches.
hot deep learning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we propose to learn a one-shot deep matching metric for per onal layers used before the fully-connected layers and the top-level energy function. Matching Nets <ref type="bibr" target="#b7">[8]</ref> take as input not only the new sample but also a small suppo gy between oneshot learning and our problem because S is of minimal size or even empty. Inspired by <ref type="bibr" target="#b7">[8]</ref>, our model computes f Ï (d q , S, d i ) as:</p><formula xml:
://www.tei-c.org/ns/1.0"><head>Configurable Convolution</head><p>For visual question answering task <ref type="bibr" target="#b56">[57]</ref>, configurable convolutional kernels are generated by trans
luding Memory-Augmented Network <ref type="bibr" target="#b6">[7]</ref> and LSTM-based meta-learner <ref type="bibr" target="#b29">[30]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
]</ref> selects the instance that maximizes the expected loss based on Bayesian decision theory. In <ref type="bibr" target="#b40">[41]</ref> a Bayesian active learning algorithm for deep learning in
est. Giles et al. introduced the first research-article recommender as part of the CiteSeer project <ref type="bibr" target="#b17">[18]</ref>. Content-based filtering <ref type="bibr" target="#b3">[4] s <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b20">[21]</ref>, and citations <ref type="bibr" target="#b17">[18]</ref>. Collaborative filtering <ref type="bibr" target="#b4">[5]
measures the relevance of a term to the subject of the document. For example, in the following text <ref type="bibr" target="#b58">[59]</ref>:</p><p>Example 1. We propose a low-complexity audio-visual
. Based on the decision theory, Expected Error Reduction <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> aims to maximize the expected reduction of the generalizat
ive features.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Matrix</head><p>In <ref type="bibr" target="#b55">[56]</ref>, an attention matrix is employed to give different attenti
nsists of several convolutional layers and max-pooling layers. Similar to CNNs in image recognition <ref type="bibr" target="#b57">[58]</ref>, the filters in low-level convolutional layers can capture
ng is the corresponding author.  <ref type="bibr" target="#b3">[4]</ref> or Collaborative Filtering <ref type="bibr" target="#b4">[5]</ref> in this scenario. Inspired by the recent success of one-shot "#b20">[21]</ref>, and citations <ref type="bibr" target="#b17">[18]</ref>. Collaborative filtering <ref type="bibr" target="#b4">[5]</ref> makes recommendation predictions by utilizing the explicit o
">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, but they often treat all the words in an article indiscri action of two articles, usually based on word embeddings <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b27">[28]</ref>. These models l
arity score between two articles is produced by the cosine similarity of their representations.â¢ WMD<ref type="bibr" target="#b65">[66]</ref>:The Word Mover's Distance (WMD) is the minimum distance re
ive features.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Matrix</head><p>In <ref type="bibr" target="#b55">[56]</ref>, an attention matrix is employed to give different attenti
arity score between two articles is produced by the cosine similarity of their representations.â¢ WMD<ref type="bibr" target="#b65">[66]</ref>:The Word Mover's Distance (WMD) is the minimum distance re
</ref> in the statistic literature. There are three different scenarios: membership query synthesis <ref type="bibr" target="#b31">[32]</ref>, stream-based sampling <ref type="bibr" target="#b32">[33]
refore, content-based recommendation methods are preferred <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Articles often contain highly representative texts, like t wever, in article recommendation, collaborative filtering often suffers from the cold-start problem <ref type="bibr" target="#b9">[10]</ref>. Some works also use graph-based methods to explore the inh
ive filtering <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. The active learning method is also called the Ask-To-Rate
ive features.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Matrix</head><p>In <ref type="bibr" target="#b55">[56]</ref>, an attention matrix is employed to give different attenti
type="bibr" target="#b47">[48]</ref> or error reduction <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. For example, in <ref type="bibr" target="#b43">[44]</ref>
]</ref> selects the instance that maximizes the expected loss based on Bayesian decision theory. In <ref type="bibr" target="#b40">[41]</ref> a Bayesian active learning algorithm for deep learning in
arity score between two articles is produced by the cosine similarity of their representations.â¢ WMD<ref type="bibr" target="#b65">[66]</ref>:The Word Mover's Distance (WMD) is the minimum distance re
bibr" target="#b44">[45]</ref>. The active learning method is also called the Ask-To-Rate technique <ref type="bibr" target="#b45">[46]</ref>. A comprehensive survey can be found in <ref type="bibr" t
ion retrieval <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> and semantic matching <ref type="bibr" target="#b11">[12]< ]</ref>. Traditional methods for measuring the similarity between two pieces of texts, such as BM25 <ref type="bibr" target="#b25">[26]</ref> and TF-IDF <ref type="bibr" target="#b24">[25]</ref>, are cument, such as the term frequency (TF) in TF-IDF <ref type="bibr" target="#b24">[25]</ref> or BM25 <ref type="bibr" target="#b25">[26]</ref> ranking function:</p><formula xml:id="formula_11">BM25(d,
[10]</ref>. Some works also use graph-based methods to explore the inherent connections in academia <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>Our work c(d i , d j ) is the similarity between d i and d j predicted by our CNN model. The latter part in <ref type="bibr" target="#b22">(23)</ref> is the average similarity between d i and articles in U. < <ref type="bibr" target="#b22">(23)</ref> is the average similarity between d i and articles in U. <ref type="bibr" target="#b22">(23)</ref> aims to find the instance that is representative of most u
ods are based on uncertainty reduction <ref type="bibr" target="#b47">[48]</ref> or error reduction <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. For example, i
ods are based on uncertainty reduction <ref type="bibr" target="#b47">[48]</ref> or error reduction <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. For example, i
[10]</ref>. Some works also use graph-based methods to explore the inherent connections in academia <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>Our work c(d i , d j ) is the similarity between d i and d j predicted by our CNN model. The latter part in <ref type="bibr" target="#b22">(23)</ref> is the average similarity between d i and articles in U. < <ref type="bibr" target="#b22">(23)</ref> is the average similarity between d i and articles in U. <ref type="bibr" target="#b22">(23)</ref> aims to find the instance that is representative of most u
bibr" target="#b44">[45]</ref>. The active learning method is also called the Ask-To-Rate technique <ref type="bibr" target="#b45">[46]</ref>. A comprehensive survey can be found in <ref type="bibr" t
ion retrieval <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> and semantic matching <ref type="bibr" target="#b11">[12]< ]</ref>. Traditional methods for measuring the similarity between two pieces of texts, such as BM25 <ref type="bibr" target="#b25">[26]</ref> and TF-IDF <ref type="bibr" target="#b24">[25]</ref>, are cument, such as the term frequency (TF) in TF-IDF <ref type="bibr" target="#b24">[25]</ref> or BM25 <ref type="bibr" target="#b25">[26]</ref> ranking function:</p><formula xml:id="formula_11">BM25(d,
machine translation <ref type="bibr" target="#b10">(Gu et al. 2018)</ref>, task-oriented dialogues <ref type="bibr" target="#b24">(Qian and Yu 2019;</ref><ref type="bibr" target="#b20">Mi et al. 2019
ically, which has gained increasing attention <ref type="bibr" target="#b16">(Lin et al. 2019;</ref><ref type="bibr" target="#b33">Wei et al. 2018;</ref><ref type="bibr" target="#b35">Xu et al. 2019)< sirable to study how to transfer the diagnostic experience among diseases. dialogue policy learning <ref type="bibr" target="#b33">(Wei et al. 2018)</ref>, dialogue management <ref type="bibr" target= 14">Li et al. 2017</ref>) focus on reinforcement learning (RL) based task-oriented dialogue system. <ref type="bibr" target="#b33">Wei et al. (2018)</ref> proposed to learn dialogue policy with RL to
enario directly in absence of medical knowledge. Recently, large-scale pre-training language models <ref type="bibr" target="#b3">(Devlin et al. 2019;</ref><ref type="bibr" target="#b25">Radford et al


t al. , 2020) )</ref> has achieved promising results in many NLP areas, such as machine translation <ref type="bibr" target="#b10">(Gu et al. 2018)</ref>, task-oriented dialogues <ref type="bibr" targ
<head n="4.1">Hierarchical Context Encoder</head><p>We first utilize a hierarchical context encoder <ref type="bibr" target="#b27">(Serban et al. 2016)</ref> to encode the dialogue history and obtain
high-resource diseases to others of data scarcity. Besides, existing knowledge-grounded approaches <ref type="bibr" target="#b17">(Liu et al. 2018;</ref><ref type="bibr" target="#b15">Lian et al. 201 nes. We first compare our base dialogue model MGR with two knowledge-grounded dialogue systems, NKD <ref type="bibr" target="#b17">(Liu et al. 2018)</ref> and POKS <ref type="bibr" target="#b15">(Lian test it directly on target diseases. We test the above three base models and denote them as PT-NKD <ref type="bibr" target="#b17">(Liu et al. 2018)</ref>, PT-POKS <ref type="bibr" target="#b15">(Lian


et al. 2018)</ref>, task-oriented dialogues <ref type="bibr" target="#b24">(Qian and Yu 2019;</ref><ref type="bibr" target="#b20">Mi et al. 2019), and</ref><ref type="bibr">text classification (2019;
directly incorporate multi-hop neighborhood information of a node without explicit message-passing <ref type="bibr" target="#b32">[33]</ref>. Intuitively, propagation based on personalized PageRank c here the node influence decays exponentially with each layer. However, as proposed, Klicpera et al. <ref type="bibr" target="#b32">[33]</ref>'s approach does not easily scale to large graphs since it ng that can reduce predictive performance.</p><p>To tackle both of these challenges Klicpera et al. <ref type="bibr" target="#b32">[33]</ref> suggest decoupling the feature transformation from the pro instead. Unfortunately, even a moderate number of power iteration evaluations (e.g. Klicpera et al. <ref type="bibr" target="#b32">[33]</ref> used K = 10 to achieve a good approximation) is prohibitiv ency of the subsequent learning step. Both of these approaches are a special case of the PPNP model <ref type="bibr" target="#b32">[33]</ref> which experimentally shows higher classification performan . In addition to the two scalable baselines, we also evaluate how PPRGo compares to the APPNP model <ref type="bibr" target="#b32">[33]</ref> which we build upon. The results are summarized in Table < ich experimentally shows higher classification performance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Approximating PageRank. Recent approaches combine ba
get="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar get="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar get="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" tar et="#b13">[14]</ref> use the historical activations of the nodes as a control variate. Huang et al. <ref type="bibr" target="#b26">[27]</ref> propose an adaptive sampling strategy with a trainable sam
the message passing into a single step. It therefore has less theoretical expressiveness than GNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b50">51]</ref>, even if it practica
get="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" tar PageRank. Recent approaches combine basic techniques to create algorithms with enhanced guarantees <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" ta
et="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar et="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar h, e.g. based on different importance scores for the nodes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar rget="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" tar ibr" target="#b53">54]</ref>. <ref type="foot" target="#foot_1">3</ref> Beyond sampling, Gao et al. <ref type="bibr" target="#b20">[21]</ref> collect the representations from a node's neighborhood int
l <ref type="bibr" target="#b6">[7]</ref> (18.7K nodes, 62.4K edges, 8.7K node features) and PubMed <ref type="bibr" target="#b52">[53]</ref> (19.7K nodes, 44.3K edges, 0.5K node features) -as well as
get="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" tar PageRank. Recent approaches combine basic techniques to create algorithms with enhanced guarantees <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" ta
get="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" tar c techniques to create algorithms with enhanced guarantees <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. For example Wei et
">44,</ref><ref type="bibr" target="#b54">55]</ref> to community detection and graph classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targ
rget="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37]</ref>. The success of GNNs on academic datasets has generated sig
ing step by utilizing the (strong) localization properties <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref> of personalized PageRank vectors for real-world graphs. The mall number of nodes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>. Thus, we can approximate Ï (i) with a sparse vector and in
synthesis and physicochemical property prediction may overcome these limitations in the near future <ref type="bibr" target="#b5">(Coley et al., 2018;</ref><ref type="bibr" target="#b16">Gao et al., 2
e rapid expansion of chemical spaces that are accessible by the derivatization of complex scaffolds <ref type="bibr" target="#b38">(Ortholand and Ganesan, 2004)</ref>, engineering next-generation vers
ore, many antibiotic discovery programs have turned to screening large synthetic chemical libraries <ref type="bibr" target="#b55">(Tommasi et al., 2015)</ref>. However, these libraries, which can con raries should contain molecules with physicochemical properties consistent with antibacterial drugs <ref type="bibr" target="#b55">(Tommasi et al., 2015)</ref>, yet sufficiently diverse such that the
of clusters for our dataset. Transcript cluster enrichment was performed using EcoCyc Pathway Tools <ref type="bibr" target="#b22">(Karp, 2001;</ref><ref type="bibr" target="#b23">Karp et al., 2016;</
e rapid expansion of chemical spaces that are accessible by the derivatization of complex scaffolds <ref type="bibr" target="#b38">(Ortholand and Ganesan, 2004)</ref>, engineering next-generation vers
ber of bacterial phenotypes that could theoretically result in efficacious antibiotics is expansive <ref type="bibr" target="#b13">(Farha and Brown, 2015;</ref><ref type="bibr" target="#b25">Kohanski
g dataset, and predicted toxicity using a deep neural network model trained on the ClinTox database <ref type="bibr" target="#b17">(Gayvert et al., 2016;</ref><ref type="bibr" target="#b60">Wu et al.,
ed models for molecular property prediction <ref type="bibr" target="#b35">(Mayr et al., 2018;</ref><ref type="bibr" target="#b60">Wu et al., 2017)</ref>. However, the accuracy of these models has bee rk model trained on the ClinTox database <ref type="bibr" target="#b17">(Gayvert et al., 2016;</ref><ref type="bibr" target="#b60">Wu et al., 2017)</ref> (Table <ref type="table">S2B</ref>). Specifica
which new antibiotics are urgently required <ref type="bibr" target="#b27">(Lee et al., 2017;</ref><ref type="bibr" target="#b40">Perez et al., 2007)</ref>. In addition to halicin, from a distinct se
s against specific biological targets ( <ref type="bibr" target="#b48">Stokes and Brown, 2015;</ref><ref type="bibr" target="#b49">Stokes et al., 2016</ref><ref type="bibr" target="#b50">Stokes et al.
in efficacious antibiotics is expansive <ref type="bibr" target="#b13">(Farha and Brown, 2015;</ref><ref type="bibr" target="#b25">Kohanski et al., 2010)</ref>, and so long as it is possible to gather
re-ranking model's effectiveness and its efficiency. While IR-specific networks are reasonably fast <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" targ rms in a single interaction match matrix, followed by softhistogram scoring based on kernel-pooling <ref type="bibr" target="#b35">[36]</ref>. This allows us to explain scoring reasons by probing the of a hard histogram method and the resulting lack of fine-tuned word representations. Xiong et al. <ref type="bibr" target="#b35">[36]</ref> improve on the idea and propose the kernel-pooling techniq qi, dj)<label>(4)</label></formula><p>Then, we transform each entry in M with a set of RBF-kernels <ref type="bibr" target="#b35">[36]</ref>. Each kernel focuses on a specific similarity range with c similarity range with center Âµ k . The size of all ranges is set by Ï. In contrast to Xiong et al. <ref type="bibr" target="#b35">[36]</ref> we do not employ an exact match kernel -as contextualized alysis unfeasible.</p><p>The differences of TK to previous kernel-pooling methods are:</p><p>â¢ KNRM <ref type="bibr" target="#b35">[36]</ref> uses only word embeddings, therefore a match does not have improves the robustness of PACRR's pooling strategy with randomization during training.</p><p>KNRM <ref type="bibr" target="#b35">[36]</ref> uses a soft-histogram (differentiable Gaussian kernel func
bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15]</ref>, large Transformer based models <ref type="bibr" target="#b31">[32]</ref>, such as BERT <ref type="bibr" target="#b5">[6]</ref>, sho il: markus.zlabinger@tuwien.ac.at 3 TU Wien, Austria, email: hanbury@ifs.tuwien.ac.at former layers <ref type="bibr" target="#b31">[32]</ref> (we evaluate up to three) can effectively contextualize qu learning -a local contextualization, fixed by the n-gram size hyperparameter.</p><p>Vaswani et al. <ref type="bibr" target="#b31">[32]</ref> proposed the Transformer architecture in the context of la intensity of the contextualization. We calculate the context(t1:n) with a set of Transformer layers <ref type="bibr" target="#b31">[32]</ref>. First, the input sequence is fused with a positional enco
ype="bibr" target="#b29">[30]</ref>, whereas pattern-based methods are harder to interpret post-hoc <ref type="bibr" target="#b8">[9]</ref>.</p><p>Contextualization allows neural IR models to vary the
it offers many benefits in practice. Word embeddings are easy to pre-train on domain specific data <ref type="bibr" target="#b13">[14]</ref>. They require only one id per term, making the index consu
ype="bibr" target="#b29">[30]</ref>, whereas pattern-based methods are harder to interpret post-hoc <ref type="bibr" target="#b8">[9]</ref>.</p><p>Contextualization allows neural IR models to vary the
independence <ref type="bibr" target="#b22">[23]</ref> or by approximating interaction similarities <ref type="bibr" target="#b16">[17]</ref>. When a large number of pre-trained Transformer layers is
results instead <ref type="bibr" target="#b19">[20]</ref> or by pruning unnecessary attention-heads <ref type="bibr" target="#b32">[33]</ref>. In this work we speed up Transformer contextualization by
et="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4]</ref>. This includes applying a temporal constraint on the number o ">[34]</ref>, and comparing the effectiveness and efficiency of various learning-to-rank algorithms <ref type="bibr" target="#b3">[4]</ref>. In web search the speed of a response is crucial as determi
r n-gram interaction modelling <ref type="bibr" target="#b14">[15]</ref>, recurrent neural networks <ref type="bibr" target="#b7">[8]</ref>, and position independent counting methods. Guo et al. <ref
ral IR models to vary the importance of otherwise identical term matches. The neural CO-PACRR model <ref type="bibr" target="#b15">[16]</ref> provides a lightweight contextualization. It averages word ly contextualization methods used in CONV-KNRM <ref type="bibr" target="#b4">[5]</ref> and CO-PACRR <ref type="bibr" target="#b15">[16]</ref>.  </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><hea In contrast to MatchPyramid, the single CNN layer focuses on different n-gram sizes.</p><p>CO-PACRR <ref type="bibr" target="#b15">[16]</ref> extends the PACRR model with additional contextualized sim
it offers many benefits in practice. Word embeddings are easy to pre-train on domain specific data <ref type="bibr" target="#b13">[14]</ref>. They require only one id per term, making the index consu
>, show substantially better effectiveness at the cost of orders of magnitude longer inference time <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" ta ><p>Recently, the issue of efficiency gained traction in the neural IR community. HofstÃ¤tter et al. <ref type="bibr" target="#b11">[12]</ref> establish efficiency baselines for common neural IR models
nal learning-to-rank the trade-off between effectiveness and efficiency has been thoroughly studied <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" ta ibr" target="#b34">[35]</ref>, incorporating an efficiency metric in the training of linear rankers <ref type="bibr" target="#b33">[34]</ref>, and comparing the effectiveness and efficiency of various
atrix of term similarities to the matching score: using stacked Convolutional Neural Networks (CNN) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22]</ref>, parallel single-l ethod baselines. In the following we give an overview over our neural baselines:</p><p>MatchPyramid <ref type="bibr" target="#b25">[26]</ref> applies several stacked CNN layers with max-pooling on top
neural IR models unsuccessfully tried to match single vector representations per query and document <ref type="bibr" target="#b20">[21]</ref>. Then, interaction-focused models moved to a more fine-gra
target="#b1">[2]</ref>, MSMARCO-Document <ref type="bibr" target="#b1">[2]</ref>, and TREC CAR 2017 <ref type="bibr" target="#b6">[7]</ref>. We evaluate a broad range of traditional and neural ranking ) it is deemed relevant in the retrieval task as well as the document containing it.</p><p>TREC CAR <ref type="bibr" target="#b6">[7]</ref> is created as part of the TREC Complex Answer Retrieval (CAR
g and evaluation code, as well as clear and documented neural network implementations using PyTorch <ref type="bibr" target="#b26">[27]</ref> and Al-lenNLP <ref type="bibr" target="#b9">[10]</ref>.</p ation inputs for the neural models. For our neural re-ranking training and inference we use PyTorch <ref type="bibr" target="#b26">[27]</ref> and AllenNLP <ref type="bibr" target="#b9">[10]</ref>. For
et="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4]</ref>. This includes applying a temporal constraint on the number o ">[34]</ref>, and comparing the effectiveness and efficiency of various learning-to-rank algorithms <ref type="bibr" target="#b3">[4]</ref>. In web search the speed of a response is crucial as determi
neural IR models unsuccessfully tried to match single vector representations per query and document <ref type="bibr" target="#b20">[21]</ref>. Then, interaction-focused models moved to a more fine-gra
nal learning-to-rank the trade-off between effectiveness and efficiency has been thoroughly studied <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" ta ibr" target="#b34">[35]</ref>, incorporating an efficiency metric in the training of linear rankers <ref type="bibr" target="#b33">[34]</ref>, and comparing the effectiveness and efficiency of various
ype="bibr" target="#b29">[30]</ref>, whereas pattern-based methods are harder to interpret post-hoc <ref type="bibr" target="#b8">[9]</ref>.</p><p>Contextualization allows neural IR models to vary the
">15]</ref>, large Transformer based models <ref type="bibr" target="#b31">[32]</ref>, such as BERT <ref type="bibr" target="#b5">[6]</ref>, show substantially better effectiveness at the cost of orde hese Transformer layers are the building blocks of versatile multi-task architectures, such as BERT <ref type="bibr" target="#b5">[6]</ref> and XLNet <ref type="bibr" target="#b38">[39]</ref>. These m
end-to-end ASR under cross-lingual transfer learning setting. To this end, we extend our prior work <ref type="bibr" target="#b0">[1]</ref>, and propose a hybrid Transformer-LSTM based architecture. T ot only require external language models but also lead to a slow inference. To tackle this problem, <ref type="bibr" target="#b0">[1]</ref> has proposed long short term memory (LSTM)-based encoderdeco <p>In this work, we propose a hybrid Transformer-LSTM architecture which combines the advantages of <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b5">[6]</ref>. It not only has l.</p><p>The paper is organized as follows. Section 2 describes baseline architectures mentioned in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b5">[6]</ref>. Then, the propo res 2.1. LSTM-based encoder-decoder architecture</head><p>A LSTM-based encoder-decoder architecture <ref type="bibr" target="#b0">[1]</ref>, denoted as A1 in the rest of this paper, consists of a Bidi ayers respectively. Figure <ref type="figure">1</ref>: LSTM-based encoder-decoder architecture (A1) <ref type="bibr" target="#b0">[1]</ref>, where the decoder acts as an independent language model.</p ords, the LSTM acts as an independent language model that can be easily updated with text-only data <ref type="bibr" target="#b0">[1]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n= tune the transferred model. This avoids a so-called catastrophic forgetting problem as mentioned in <ref type="bibr" target="#b0">[1]</ref>. Specifically, at each training iteration, we mix a batch of cond step, the model is further fine-tuned with the labeled data of the target language. Similar to <ref type="bibr" target="#b0">[1]</ref>, we empirically found that the second step is necessary to i
></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">E2E setting</head><p>ESPnet toolkit <ref type="bibr" target="#b20">[21]</ref> is used to train our E2E architectures. We use 80 mel-scal
esource problem in ASR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target
ployed to train language models (LM) applied during decoding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> and re-scoring stages <
pplied during decoding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> and re-scoring stages <ref type="bibr" target="#b4">[5]</ref>
language processing tasks, then adopted to the ASR task in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. The model architecture, denoted as A2, is shown in Fig. <r
ngual transfer learning, which is a popular approach to address the limited resource problem in ASR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> and re-scoring stages <ref type="bibr" target="#b4">[5]</ref>. Such techniques not only require external language models b
ular approach to address the limited resource problem in ASR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target=
ngual transfer learning, which is a popular approach to address the limited resource problem in ASR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
is shown in Table <ref type="table">1</ref>. We perform speed perturbation based data augmentation <ref type="bibr" target="#b18">[19]</ref> on the training data. For extra text data, we examine two
tative set of latency-critical applications, we use the benchmarks of the TailBench benchmark suite <ref type="bibr" target="#b18">[20]</ref>. This suite includes eight representative applications of . However, we found that these values are in line with the results of this application presented in <ref type="bibr" target="#b18">[20]</ref>.</p><p>Once we have defined the QoS requirements for each saturation QPS (i.e., point at which the LQoS is achieved). This issue has been also pointed out in <ref type="bibr" target="#b18">[20]</ref>, where authors realized of this unexpected behavior and de
those works that do not consider virtualization. With the aim of improving system fairness, CoPart <ref type="bibr" target="#b2">[3]</ref> leverages Intel's CAT and MBA technologies and characterizes
iques. In the experimental evaluation, mostly high performance benchmarks are employed. Finally, in <ref type="bibr" target="#b4">[5]</ref>, the Quasar cluster management system is presented. The main
Hyper-Threading</head><p>The processor of the experimental platform implements the Hyper-Threading <ref type="bibr" target="#b16">[18]</ref> technology, that is, Intel's implementation of the simulta
ing dominated by huge queuing delays, latency-critical workloads are typically run at low CPU loads <ref type="bibr" target="#b22">[24]</ref>. To make our experiments representative, we make sure that
latency-critical application are studied in this work. A similar methodology is followed in HyPart <ref type="bibr" target="#b3">[4]</ref>, which is a hybrid partitioning approach that combines Intel both the memory bandwidth of img-dnn to 4000 MB/s and the LLC to 2 cache 0.2 0.4 0.6 0.8 1.0 1.2 1. <ref type="bibr" target="#b3">4</ref>    ways, which corresponds to approximately one third of the m
those works that do not consider virtualization. With the aim of improving system fairness, CoPart <ref type="bibr" target="#b2">[3]</ref> leverages Intel's CAT and MBA technologies and characterizes
nsider just LXC virtualization <ref type="bibr" target="#b5">[6]</ref> (i.e., Linux containers). In <ref type="bibr" target="#b6">[7]</ref>, D. Lo et al. aim to increase server utilization by tolerati
those works that do not consider virtualization. With the aim of improving system fairness, CoPart <ref type="bibr" target="#b2">[3]</ref> leverages Intel's CAT and MBA technologies and characterizes
ds. However, authors do not consider reducing unpredictability through resource partitioning. AVMMC <ref type="bibr" target="#b9">[10]</ref>, on the other hand, determines dynamically the best VM to c
ient and server nodes. This configuration is similar to that used by OpenStack on its compute nodes <ref type="bibr" target="#b14">[16]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
w York Times (NYT) <ref type="bibr" target="#b13">(Riedel, Yao, and McCallum 2010)</ref> and WebNLG <ref type="bibr" target="#b6">(Gardent et al. 2017)</ref>. NYT comes from the distant supervised rel ramework <ref type="bibr" target="#b15">(Sutskever, Vinyals, and Le 2014)</ref> with copy mechanism <ref type="bibr" target="#b6">(Gu et al. 2016)</ref>. But it cannot predict the entire entities. In
method for solving the overlapping relation problem, which extracts triplets by a Seq2Seq framework <ref type="bibr" target="#b15">(Sutskever, Vinyals, and Le 2014)</ref> with copy mechanism <ref type
en. To solve this problem, the followers <ref type="bibr" target="#b16">(Takanobu et al. 2018;</ref><ref type="bibr" target="#b4">Dai et al. 2019</ref>) run tagging on a sentence for multiple turns, w bibr" target="#b16">(Takanobu et al. 2018)</ref>, based on the reinforcement learning framework and <ref type="bibr" target="#b4">(Dai et al. 2019</ref>) leverage attention mechanism. Although these m
ling, Tagging, and Sequence-to-Sequence (Seq2Seq). Among these approaches, the table filling method <ref type="bibr" target="#b7">(Gupta, SchÃ¼tze, and Andrassy 2016;</ref><ref type="bibr" target="#b0" neural models also focus on pipeline methods, include (1) Fully-Supervised Relation Classification <ref type="bibr" target="#b7">(Hendrickx et al. 2009)</ref> (2) Distant Supervised Relation Extracti elations. The models include history-based searching (Miwa and Sasaki 2014), neuralbased prediction <ref type="bibr" target="#b7">(Gupta, SchÃ¼tze, and Andrassy 2016)</ref> and global normalization (Ad
the recent progress of neural models <ref type="bibr" target="#b1">(Cai, Zhang, and Wang 2016;</ref><ref type="bibr" target="#b17">Zeng et al. 2014;</ref><ref type="bibr" target="#b3">Christopoulou, M
target="#b1">(Cai, Zhang, and Wang 2016;</ref><ref type="bibr" target="#b17">Zeng et al. 2014;</ref><ref type="bibr" target="#b3">Christopoulou, Miwa, and Ananiadou 2018;</ref><ref type="bibr" target=
rget="#b12">Qin, Xu, and Wang 2018)</ref>, the pipeline methods introduce error propagation problem <ref type="bibr" target="#b9">(Li and Ji 2014)</ref>, which does harm to the overall performance.</p
(DSRE), which aims to leverage the strength of the knowledge base to generate a largescale dataset <ref type="bibr" target="#b10">(Mintz et al. 2009</ref>). To make joint extraction more challenging f type="bibr" target="#b7">(Hendrickx et al. 2009)</ref> (2) Distant Supervised Relation Extraction <ref type="bibr" target="#b10">(Mintz et al. 2009)</ref>. In spite of the recent progress of neural
al. 2018)</ref>, NovelTagging <ref type="bibr" target="#b20">(Zheng et al. 2017)</ref> and GraphRel <ref type="bibr" target="#b5">(Fu, Li, and Ma 2019)</ref>. NovelTagging uses sequence labeling to as
method for solving the overlapping relation problem, which extracts triplets by a Seq2Seq framework <ref type="bibr" target="#b15">(Sutskever, Vinyals, and Le 2014)</ref> with copy mechanism <ref type
tand the meaning and then point out the entity-relation pairs sequentially.</p><p>Currently, CopyRE <ref type="bibr" target="#b19">(Zeng et al. 2018)</ref> is the most powerful Seq2Seq based joint ext ww.tei-c.org/ns/1.0"><head>Baselines and Evaluation Metrics</head><p>We compare CopyMTL with CopyRE <ref type="bibr" target="#b19">(Zeng et al. 2018)</ref>, NovelTagging <ref type="bibr" target="#b20" >(Mintz et al. 2009</ref>). To make joint extraction more challenging than DSRE experiment setting, <ref type="bibr" target="#b19">Zeng et al. (2018)</ref> additionally modified the data to include mo ng relation problem, their nature and complexities are akin to table filling.</p><p>Seq2Seq: CopyRE <ref type="bibr" target="#b19">(Zeng et al. 2018</ref>) is another method for solving the overlappin
for other network analysis tasks, such as link prediction <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b40">41]</ref> and graph classification <ref type="bibr" target="#b16">[17
nal GNNs can operate on the transformed homogeneous graphs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43]</ref>. This is a two-stage approach and requires hand-crafted met <head n="3.2">Meta-Path Generation</head><p>Previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43]</ref> require manually defined meta-paths and perform Graph Neura
<ref type="bibr" target="#b33">[34]</ref>, and engineered features from a local neighbor structure <ref type="bibr" target="#b22">[23]</ref>. These features are not flexible and suffer from poor perf
h as social networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>, citation networks <ref type="bibr" target="#b18">[19,</ref
Networks (GNNs) have been widely adopted in various tasks over graphs, such as graph classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta
ive in achieving state-ofthe-art performance in a variety of graph datasets such as social networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" targ type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38]</ref> and nonspectral methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" targ ,</ref><ref type="bibr" target="#b32">33]</ref>, subsampling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and inductive representation for a large graph <ref type="bib
Networks (GNNs) have been widely adopted in various tasks over graphs, such as graph classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" ta
data. To improve performance or scalability, generalized convolution based on spectral convolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, attention mechanism
graph datasets have been recently studied for other network analysis tasks, such as link prediction <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b40">41]</ref> and graph classifi
ave been used such as simple graph statistics <ref type="bibr" target="#b1">[2]</ref>, graph kernel <ref type="bibr" target="#b33">[34]</ref>, and engineered features from a local neighbor structure <
d for decades. Conventionally, hand-crafted features have been used such as simple graph statistics <ref type="bibr" target="#b1">[2]</ref>, graph kernel <ref type="bibr" target="#b33">[34]</ref>, and
>, link prediction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42]</ref> and node classification <ref type="bibr" target="#b2">[3,</
data. To improve performance or scalability, generalized convolution based on spectral convolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, attention mechanism
ef type="bibr" target="#b19">[20]</ref>, recommender systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref>. The underlying gra
ave been used such as simple graph statistics <ref type="bibr" target="#b1">[2]</ref>, graph kernel <ref type="bibr" target="#b33">[34]</ref>, and engineered features from a local neighbor structure <
recommender systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref>. The underlying graph structure is utilized by GNNs to oper
node classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>. The representation learnt by GNNs has been proven to be ef ref type="bibr" target="#b34">35]</ref>, citation networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>, functional structure of brains <ref type="bibr" target="#b get="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>. Based on spectral graph theory, Bruna et al. <ref type="bi target="#b25">26]</ref>, attention mechanism on neighbors <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33]</ref>, subsampling <ref type="bibr" target="#b5">[6,</ref><ref ty rations directly on the graph, utilizing spatially close neighbors. For instance, VeliÄkoviÄ et al. <ref type="bibr" target="#b32">[33]</ref> applies different weight matrices for nodes with different .0"><head>GNN-based methods</head><p>We used the GCN <ref type="bibr" target="#b18">[19]</ref>, GAT <ref type="bibr" target="#b32">[33]</ref>, and HAN <ref type="bibr" target="#b36">[37]</ref> as GNN
pe="bibr" target="#b40">41]</ref> and graph classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>, applying our GTNs to the other tasks can be interesting fu
of node types and edge types respectively. The input graphs can be viewed as a heterogeneous graph <ref type="bibr" target="#b30">[31]</ref> G = (V, E), where V is a set of nodes, E is a set of obser
data. To improve performance or scalability, generalized convolution based on spectral convolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, attention mechanism
>, link prediction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42]</ref> and node classification <ref type="bibr" target="#b2">[3,</
s the heterogeneous graph into a homogeneous graph and then learns representation. The metapath2vec <ref type="bibr" target="#b9">[10]</ref> learns graph representations by using meta-path based rando have been studied and recently DeepWalk <ref type="bibr" target="#b27">[28]</ref> and metapath2vec <ref type="bibr" target="#b9">[10]</ref> have shown predominant performance among random walk based
ef type="bibr" target="#b45">[43,</ref><ref type="bibr" target="#b56">54]</ref>. Based on Cycle-GAN <ref type="bibr" target="#b58">[56]</ref>, Yuan et al. <ref type="bibr" target="#b45">[43]</ref> pro scheme has also been used to perform image translation without paired training data, e.g., CycleGAN <ref type="bibr" target="#b58">[56]</ref> and DualGAN <ref type="bibr" target="#b44">[42]</ref>. Spe avoid the possible mode collapse issue when solving the under-constrained image translation problem <ref type="bibr" target="#b58">[56]</ref>. Unlike these methods, we seek to improve the performance es collected from YouTube. Thus, there are 3 DRN-adapt models in total. And We also train a CinCGAN <ref type="bibr" target="#b58">[56]</ref> model for each kind of unpaired data for comparison. Based Specifically, a cycle consistency loss is proposed to avoid the mode collapse issue of GAN methods <ref type="bibr" target="#b58">[56,</ref><ref type="bibr" target="#b6">4,</ref><ref type="bibr" targ CycleGAN based SR methods. First, Cycle-GAN based methods <ref type="bibr" target="#b45">[43,</ref><ref type="bibr" target="#b58">56]</ref> use a cycle consistency loss to avoid the possible mode col
ow-resolution (LR) images. Based on DNNs, many methods have been proposed to improve SR performance <ref type="bibr" target="#b53">[51,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" ta <ref type="bibr" target="#b28">[26]</ref>, DBPN <ref type="bibr" target="#b18">[16]</ref>, and RCAN <ref type="bibr" target="#b53">[51]</ref>. However, these methods still suffer from the large space nsists of several up-and down-sampling layers to iteratively produce LR and HR images. Zhang et al. <ref type="bibr" target="#b53">[51]</ref> propose the channel attention mechanism to build a deep mo nlike the baseline U-Net, we build each basic block using B residual channel attention block (RCAB) <ref type="bibr" target="#b53">[51]</ref> to improve the model capacity. Following <ref type="bibr" are 2 dual models for 4Ã SR and 3 dual models for 8Ã SR, respectively. Let B be the number of RCABs <ref type="bibr" target="#b53">[51]</ref> and F be the number of base feature channels. For 4Ã SR, w ctionbased methods <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b53">51]</ref>. Haris et al. <ref type="bibr" target="#b18">[16]</ref> pro ata, and augment the training data following the method in <ref type="bibr" target="#b28">[26,</ref><ref type="bibr" target="#b53">51]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head>A
many methods have been proposed to improve SR performance <ref type="bibr" target="#b53">[51,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b12">10,</ref><ref type="bibr" tar ve the SR performance, one can design effective models by increasing the model capacity, e.g., EDSR <ref type="bibr" target="#b28">[26]</ref>, DBPN <ref type="bibr" target="#b18">[16]</ref>, and RCAN 39">[37]</ref>, we train our models on DIV2K <ref type="bibr" target="#b36">[34]</ref> and Flickr2K <ref type="bibr" target="#b28">[26]</ref> datasets.</p></div> <div xmlns="http://www.tei-c.org/ns/1. use Conv s2 to represent the convolution layer with the stride of 2. Following the settings of EDSR <ref type="bibr" target="#b28">[26]</ref>, we build the Upsampler with one convolution layer and one b39">[37]</ref>, we train our model on DIV2K <ref type="bibr" target="#b36">[34]</ref> and Flickr2K <ref type="bibr" target="#b28">[26]</ref> datasets, which contain 800 and 2650 training images separ ta, we follow the learning scheme of supervised SR methods <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b28">26]</ref> and train model by minimizing Eqn. ( <ref type="formula" ta nding HR patches as the paired training data, and augment the training data following the method in <ref type="bibr" target="#b28">[26,</ref><ref type="bibr" target="#b53">51]</ref>.</p></div> <div xm
pe="bibr" target="#b46">44,</ref><ref type="bibr" target="#b8">6]</ref> and many other applications <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" targ
get="#b28">26,</ref><ref type="bibr" target="#b12">10,</ref><ref type="bibr" target="#b14">12,</ref><ref type="bibr" target="#b51">49]</ref>. However, these methods may suffer from two limitations.</p
ong to a different domain from the unpaired data (See more discussions in supplementary). Following <ref type="bibr" target="#b34">[32]</ref>, we randomly choose 3k images from ImageNet (called ImageN <p>Training data. To obtain the unpaired synthetic data, we randomly choose 3k images from ImageNet <ref type="bibr" target="#b34">[32]</ref> (called Ima-geNet3k) and obtain the LR images using differ
" target="#b21">[19]</ref> and reconstructionbased methods <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b53">51]</ref>. Haris et al. <ref
adopted as the metrics, such as PSNR and SSIM <ref type="bibr" target="#b40">[38]</ref>. Following <ref type="bibr" target="#b39">[37]</ref>, we train our models on DIV2K <ref type="bibr" target="#b3 ages. Thus, the shape of output images should be 8h Ã 8w for the 8Ã model. Training data. Following <ref type="bibr" target="#b39">[37]</ref>, we train our model on DIV2K <ref type="bibr" target="#b36
ere we evaluate SR models on the LR images with different degradation methods (e.g., Nearest and BD <ref type="bibr" target="#b50">[48]</ref>). During training, we can only access the LR images but no
f possible mapping functions, resulting in the limited performance without producing sharp textures <ref type="bibr" target="#b26">[24]</ref> (See Figure <ref type="figure">1</ref>). Thus, how to redu function. We also evaluate the impact of our dual regression scheme on other models, e.g., SRResNet <ref type="bibr" target="#b26">[24]</ref> based network (See more details in the supplementary).</p>
ion block (RCAB) <ref type="bibr" target="#b53">[51]</ref> to improve the model capacity. Following <ref type="bibr" target="#b41">[39,</ref><ref type="bibr" target="#b25">23]</ref>, we add additional
" target="#b28">[26]</ref>, we build the Upsampler with one convolution layer and one pixel-shuffle <ref type="bibr" target="#b35">[33]</ref> layer to upscale the feature maps. Moreover, we use h and
t methods on five benchmark datasets, including SET5 <ref type="bibr" target="#b5">[3]</ref>, SET14 <ref type="bibr" target="#b49">[47]</ref>, BSDS100 <ref type="bibr" target="#b3">[1]</ref>, UR-BAN10 models using five benchmark datasets, including SET5 <ref type="bibr" target="#b5">[3]</ref>, SET14 <ref type="bibr" target="#b49">[47]</ref>, BSDS100 <ref type="bibr" target="#b3">[1]</ref>, URBAN100
l learning methods <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b55">53]</ref> contain a primal mo
synthetic data.</p><p>Dual learning. Dual learning methods <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" tar d of the dual regression scheme, where the loss function is bounded by [0, C], is more general than <ref type="bibr" target="#b42">[40]</ref>. Moreover, this generalization bound is tight when trainin the loss function L P (P (x), y) + Î»L D (D(P (x)), x) is bounded by [0, C], which is different from <ref type="bibr" target="#b42">[40]</ref>. </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
5 <ref type="bibr" target="#b5">[3]</ref>, SET14 <ref type="bibr" target="#b49">[47]</ref>, BSDS100 <ref type="bibr" target="#b3">[1]</ref>, UR-BAN100 <ref type="bibr" target="#b23">[21]</ref> and MAN 5 <ref type="bibr" target="#b5">[3]</ref>, SET14 <ref type="bibr" target="#b49">[47]</ref>, BSDS100 <ref type="bibr" target="#b3">[1]</ref>, URBAN100 <ref type="bibr" target="#b23">[21]</ref> and MANG on DRN</head><p>We conduct an experiment to investigate the impact of the hyper-parameter Î» in Eqn. <ref type="bibr" target="#b3">(1)</ref>. From Table <ref type="table" target="#tab_5">4</ref>, when
for 8Ã SR. Our dual regression scheme is able to produce sharper images than the baseline methods. <ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b8">6]</ref> and many other applic
g. Thus, we design the dual model with only two convolution layers and a LeakyReLU activation layer <ref type="bibr" target="#b30">[28]</ref>, which has much lower computation cost than the primal mod
g. Thus, we design the dual model with only two convolution layers and a LeakyReLU activation layer <ref type="bibr" target="#b30">[28]</ref>, which has much lower computation cost than the primal mod
ementation Details</head><p>We compare different methods on five benchmark datasets, including SET5 <ref type="bibr" target="#b5">[3]</ref>, SET14 <ref type="bibr" target="#b49">[47]</ref>, BSDS100 <r rison on paired data, we evaluate different SR models using five benchmark datasets, including SET5 <ref type="bibr" target="#b5">[3]</ref>, SET14 <ref type="bibr" target="#b49">[47]</ref>, BSDS100 <r mply remove one basic block from the 8Ã model. <ref type="table">A</ref>, we use Conv(1,1) and Conv <ref type="bibr" target="#b5">(3,</ref><ref type="bibr" target="#b5">3)</ref> to represent the convo model. <ref type="table">A</ref>, we use Conv(1,1) and Conv <ref type="bibr" target="#b5">(3,</ref><ref type="bibr" target="#b5">3)</ref> to represent the convolution layer with the kernel size of 1
get="#b52">50,</ref><ref type="bibr" target="#b54">52,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b22">20]</ref>. Recently, image super-resolution (SR) has become an import
synthetic data.</p><p>Dual learning. Dual learning methods <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" tar d of the dual regression scheme, where the loss function is bounded by [0, C], is more general than <ref type="bibr" target="#b42">[40]</ref>. Moreover, this generalization bound is tight when trainin the loss function L P (P (x), y) + Î»L D (D(P (x)), x) is bounded by [0, C], which is different from <ref type="bibr" target="#b42">[40]</ref>. </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
="#b3">[5]</ref>, <ref type="bibr" target="#b4">[6]</ref>, <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr een these sub-problems. For example, coauthors, which are used as a strong evidence in many methods <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr Zhang" in this example. More troubles may appear when multi-hop coauthorships are used as features <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b30">[32]</ref>. For instance, " icient, compared with the state-ofthe-art methods CE <ref type="bibr" target="#b5">[7]</ref>, GHOST <ref type="bibr" target="#b9">[11]</ref>, CSLR <ref type="bibr" target="#b17">[19]</ref>, MIX <ref t d the redundant information, we only consider valid 2-hop coauthorship paths connecting two authors <ref type="bibr" target="#b9">[11]</ref>. Specifically, a valid 2-hop coauthorship path in G is an A fficiency of NDCC versus state-of-the-art methods CE <ref type="bibr" target="#b5">[7]</ref>, GHOST <ref type="bibr" target="#b9">[11]</ref>, CSLR <ref type="bibr" target="#b17">[19]</ref>, MIX <ref t greedy agglomerative clustering method is used to merge the most similar clusters.</p><p>(2) GHOST <ref type="bibr" target="#b9">[11]</ref> is a graph-based method employing coauthorship only. Its si n baselines, only CE and GHOST analyze the time complexity <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b9">[11]</ref>. The time complexity of CE is O(|A (0) |k log |A (0) |), wh and unsupervised <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b17">[19]</ref>, <ref type="bibr pe="bibr" target="#b17">[19]</ref>, <ref type="bibr" target="#b37">[39]</ref>, affinity propagation <ref type="bibr" target="#b9">[11]</ref> and Markov clustering <ref type="bibr" target="#b39">[41]</
b7">[9]</ref>, <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr" target="#b13">[15]</ref>, <ref type="bibr" target="#b15">[17]</ref>, <ref type="bib scores are no less than t, and updates the network G (equally, the matrix W AP ) accordingly (lines <ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref>. It also needs to disambiguation can be divided into two classes: supervised <ref type="bibr" target="#b2">[4]</ref>, <ref type="bibr" target="#b13">[15]</ref>, <ref type="bibr" target="#b15">[17]</ref>, <ref type="bib >195,</ref><ref type="bibr" target="#b17">19)</ref> times faster than (CE, CSLR and MIX) on AMiner, <ref type="bibr" target="#b13">(15,</ref><ref type="bibr" target="#b6">8)</ref> times faster than (C
"#b4">[6]</ref>, <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr" target="#b13">[15]</ref>, <ref type="bib hor similarity scores in A n , and finds K-th largest score t by using a K-size minimum heap (lines <ref type="bibr" target="#b11">[13]</ref><ref type="bibr" target="#b12">[14]</ref>. Then it merges a
guation problem <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" target="#b25">[27]</ref>. However, they are not designed for scholar name disambigu mes. Most of them need another clean knowledge base (KB) <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" target="#b25">[27]</ref>, which is unavailable in most cases. <ref type="bibr" targ
">[19]</ref>, <ref type="bibr" target="#b27">[29]</ref>, <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" target="#b33">[35]</ref>, <ref type="bibr" target="#b34">[36]</ref>, <ref type="bib ">[15]</ref>, <ref type="bibr" target="#b15">[17]</ref>, <ref type="bibr" target="#b16">[18]</ref>, <ref type="bibr" target="#b33">[35]</ref>, <ref type="bibr" target="#b36">[38]</ref>, <ref type="bib andom forests <ref type="bibr" target="#b15">[17]</ref>, <ref type="bibr" target="#b16">[18]</ref>, <ref type="bibr" target="#b33">[35]</ref>, which is then used to assign publications to different au
that our method NDCC is both effective and efficient, compared with the state-ofthe-art methods CE <ref type="bibr" target="#b5">[7]</ref>, GHOST <ref type="bibr" target="#b9">[11]</ref>, CSLR <ref t strategies, such as rule based methods, are used widely in the previous name disambiguation methods <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" riments to evaluate (1) the effectiveness and efficiency of NDCC versus state-of-the-art methods CE <ref type="bibr" target="#b5">[7]</ref>, GHOST <ref type="bibr" target="#b9">[11]</ref>, CSLR <ref t hich can determine the author numbers automatically and use citation information only.</p><p>(1) CE <ref type="bibr" target="#b5">[7]</ref> is a collective entity resolution method for relational data y performance comparison. Among the chosen baselines, only CE and GHOST analyze the time complexity <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b9">[11]</ref>. The time complexi 0) | is the number of atomic authors, and k is largest number of buckets that a buckets connects to <ref type="bibr" target="#b5">[7]</ref>. It is difficult to exactly compare CE and our method becaus ef type="bibr" target="#b38">[40]</ref>, <ref type="bibr" target="#b42">[44]</ref> and unsupervised <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" t explored by disambiguation methods <ref type="bibr" target="#b10">[12]</ref>: citation information <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b36">[38]</ref>, web information collective entity resolution methods that can be used to solve multiple name disambiguation problem <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" t="#b12">[14]</ref>, <ref type="bibr" target="#b25">[27]</ref>, which is unavailable in most cases. <ref type="bibr" target="#b5">[7]</ref> is a collective entity resolution method without a KB. Howev ess of Eq. ( <ref type="formula">8</ref>) can be proved by combining the definition of W AA and Eq. <ref type="bibr" target="#b5">(7)</ref>.</p><p>As the merging of two author nodes incorporates new 2
">[37]</ref>, <ref type="bibr" target="#b37">[39]</ref>, <ref type="bibr" target="#b39">[41]</ref>, <ref type="bibr" target="#b40">[42]</ref>. Supervised methods use labeled data to train a classifier
even more challenging to attack.</p><p>Most existing methods tackle name disambiguation separately <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr" target="#b4">[6]</ref>, <ref type="bibr" t r" target="#b36">[38]</ref>, web information <ref type="bibr" target="#b17">[19]</ref>, affiliation <ref type="bibr" target="#b3">[5]</ref>, and implicit evidence <ref type="bibr" target="#b26">[28]</ e vary ? from 1 to 100 <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target
">[37]</ref>, <ref type="bibr" target="#b36">[38]</ref>, <ref type="bibr" target="#b37">[39]</ref>, <ref type="bibr" target="#b38">[40]</ref>, <ref type="bibr" target="#b42">[44]</ref>. For each name ">[18]</ref>, <ref type="bibr" target="#b33">[35]</ref>, <ref type="bibr" target="#b36">[38]</ref>, <ref type="bibr" target="#b38">[40]</ref>, <ref type="bibr" target="#b42">[44]</ref> and unsupervise
used real-life datasets AMiner (http://www. aminer.org) <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" target="#b31">[33]</ref>, <ref type="bibr" target="#b32">[34]</ref>, <ref type="bib
guation problem <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" target="#b25">[27]</ref>. However, they are not designed for scholar name disambigu mes. Most of them need another clean knowledge base (KB) <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" target="#b25">[27]</ref>, which is unavailable in most cases. <ref type="bibr" targ
user packets are not detoured in transmission, numerous network attack surfaces are opened up today <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. For example, an at d path validation that fill the void, such as ICING <ref type="bibr" target="#b2">[3]</ref> and OPT <ref type="bibr" target="#b1">[2]</ref>. However, for the targeting environment which is adversarial performance of the Click router as the baseline, and compare our PSVM with the-state-of-the-art OPT <ref type="bibr" target="#b1">[2]</ref>.  Method and parameter setup. For fairness, we use the same ave been proposed in ICING <ref type="bibr" target="#b2">[3]</ref>, the Origin and Path Trace (OPT) <ref type="bibr" target="#b1">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type="bibr" tar
gn is inspired by <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, some of which were actually used. Fig. <ref type="figure"
pology analysis <ref type="bibr" target="#b13">[14]</ref>, obtained from some BGP related protocols <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, or employing t
sion, numerous network attack surfaces are opened up today <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. For example, an attacker may try to flood arbitrary packets unctions from routing nodes, and its design is inspired by <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, some of which we nd Path Trace (OPT) <ref type="bibr" target="#b1">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type="bibr" target="#b3">[4]</ref>, and PPV <ref type="bibr" target="#b27">[29]</ref>. In ICING
rototype source and routing node of PSVM on a terminal computer and a modular software router Click <ref type="bibr" target="#b10">[11]</ref>, respectively. The source node system has one Intel(R) Cor "table" xml:id="tab_0"><head></head><label></label><figDesc>We implement our PSVM prototype on Click<ref type="bibr" target="#b10">[11]</ref> and evaluate basic and dynamic PSVM protocols in different
metric key (Key Session N i</p><p>) from the node's master key (without seeking help from node N i) <ref type="bibr" target="#b12">[13]</ref> to calculate the authentication structure for source and p ref type="bibr" target="#b15">[16]</ref>, or employing the existing control plane routing protocols <ref type="bibr" target="#b12">[13]</ref> (especially Pathlet <ref type="bibr" target="#b16">[17]</r metric key of N i (Key Session N i</p><p>) can be derived through pseudo-random operation functions <ref type="bibr" target="#b12">[13]</ref> by N i and its CGA, respectively, without long-term storag
ches for IP path tracking fail to solve the above problems, being unable to identify path deviation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Recently, there a
>[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type="bibr" target="#b3">[4]</ref>, and PPV <ref type="bibr" target="#b27">[29]</ref>. In ICING, it requires each router to verify the optimized
Furthermore, we suppose that node N i and its CGA would use secret methods (such as Diffie-Hellman <ref type="bibr" target="#b18">[19]</ref>) to share the master key (Key N i ), which may be replaced
sion, numerous network attack surfaces are opened up today <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. For example, an attacker may try to flood arbitrary packets unctions from routing nodes, and its design is inspired by <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, some of which we nd Path Trace (OPT) <ref type="bibr" target="#b1">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type="bibr" target="#b3">[4]</ref>, and PPV <ref type="bibr" target="#b27">[29]</ref>. In ICING
pology analysis <ref type="bibr" target="#b13">[14]</ref>, obtained from some BGP related protocols <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, or employing t
the entities due to their typeagnostic nature <ref type="bibr" target="#b25">[Xie et al., 2016</ref><ref type="bibr" target="#b7">, Jain et al., 2018]</ref>. Since PSL-KGI is able to predict entity ty sion.</p><p>(ii) Explicit type supervised models also outperform the implict type supervised models <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>.</p><p>The margin of improvement is large wh on the use of ontological rules (exemplified by PSL-KGI) and embeddings (we use ComplEx, ConvE and <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>). Rule induction methods are orthogonal to o target="#b3">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a different approach <ref type="bibr" target="#b7">[Jain et al., 2018]</ref> propose extending standard KG embeddings wit es for entities generated by PSL-KGI in KG embeddings (the second stage), we modify the typed model <ref type="bibr" target="#b7">[Jain et al., 2018]</ref> as follows:</p><p>Instead of just using the are our explicitly supervised TypeE-X methods with the implicitly supervised embeddings proposed by <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>.</p><p>â¢ In Section 6.3, we analyse how our e supervision with the unsupervised type-compatible embeddings-based method proposed by Jain et al. <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>. As these results indicate, while explicitly nificantly improves the relation scores, improving weighted F1 up to 18% (over NELL).</p><p>Dataset <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>  </p></div> <div xmlns="http://www.tei-c.org ref type="bibr" target="#b18">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>. We showed their performance on existing dat d>Table 7 :</head><label>7</label><figDesc>Weighted F1 scores on relation triples in the test set by<ref type="bibr" target="#b7">[Jain et al., 2018]</ref> and TypeE-ComplEx.Anecdotes. Looking at the
ort, baseball is false since Matt Flynn is an NFL player), incompatible entity types, and many more <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref>. It has also been observed that such nois fectively, and specifically, the PSL-KGI implementation uses rules defined on schema-level features <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref>.</p></div> <div xmlns="http://www.tei-c.o exclusive (MUT and RMUT); and inverse relations (INV). We reproduce the list of information used in <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref> in tabular form in Table <ref type="table mal distributions: N (0.7, 0.2) for facts in the original KG and N (0.3, 0.2) for added noisy facts <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref>. The SAMEENT facts between entities are g hyper-parameter threshold as the cutoff for classifying a test triple based on the prediction score <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref>. Our experiments were run on Intel(R) Xeo the KG refinement task and methods for the same, from probabilistic rule based methods like PSL-KGI <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx e the probabilistic sources of information such as the confidence scores obtained during extraction <ref type="bibr" target="#b18">[Pujara et al., 2013</ref><ref type="bibr" target="#b8">, Jiang et al pe="bibr" target="#b8">, Jiang et al., 2012]</ref> from multiple sources. Of these methods, PSL-KGI <ref type="bibr" target="#b18">[Pujara et al., 2013</ref><ref type="bibr" target="#b19">[Pujara et a ref type="bibr" target="#b1">[Carlson et al., 2010]</ref>) has been used for the KG refinement task <ref type="bibr" target="#b18">[Pujara et al., 2013</ref><ref type="bibr" target="#b8">, Jiang et al
arget="#b16">[Nickel et al., 2012</ref><ref type="bibr" target="#b23">, Trouillon et al., 2016</ref><ref type="bibr" target="#b3">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, wi porate inference rules and ontologies, along with state-of-the-art KG embedding methods,viz., ConvE <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref> and ComplEx <ref type="bibr" target="#b2 tion). We work with ComplEx <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref> and ConvE <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref> embeddings which have shown state of the #b23">[Trouillon et al., 2016]</ref>, <ref type="bibr">SimplE [Kazemi and Poole, 2018]</ref>, ConvE <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a dif Then all facts upto length 3 in the hierarchy of taxonomy were included.</p><p>FB15K-237: FB15K-237 <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref>, another popular benchmark does not have compare them with Com-plEx <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref> and ConvE <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref>, two state-of-the-art KG embeddings meth g the same class balance, and use them as our validation and test split.</p><p>YAGO3-10: YAGO3-10 [ <ref type="bibr" target="#b3">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type="bibr"
ns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>Ontological Rule</p><p>Uncertain Extractions  <ref type="bibr" target="#b0">[Blum and Mitchell, 1998]</ref>, to combine the strengths of PSL-KGI a
ss in entity type and new fact predictions <ref type="bibr" target="#b16">[Nickel et al., 2012</ref><ref type="bibr" target="#b23">, Trouillon et al., 2016</ref><ref type="bibr" target="#b3">, Dettmer bedding methods,viz., ConvE <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref> and ComplEx <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref>, which do not make use of any ontologi raining can be done for the refinement task with a negative log-likelihood loss function as follows <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref>.</p><formula xml:id="formula_0">L(G) = ods can also be used to predict type labels of entities (the typeOf relation). We work with ComplEx <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref> and ConvE <ref type="bibr" target="#b3 ., subtype and subproperty information-and also shows that state-of-the-art embeddings like ComplEx <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref>, <ref type="bibr">SimplE [Kazemi and P valuate the performance of TypeE-X models in the KG refinement task, and compare them with Com-plEx <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref> and ConvE <ref type="bibr" target="#b3 " target="#b15">[Nickel et al., 2011</ref><ref type="bibr" target="#b21">, Socher et al., 2013</ref><ref type="bibr" target="#b23">, Trouillon et al., 2016]</ref>.</p><p>An important step in learning
t has also been observed that such noise can significantly degrade the performance of KG embeddings <ref type="bibr" target="#b19">[Pujara et al., 2017]</ref>.</p><p>The KG refinement task aims to red r subject, relation label or object. Note that this was the same model followed in an earlier study <ref type="bibr" target="#b19">[Pujara et al., 2017]</ref>.</p><p>â¢ We further refine the noise mode ultiple sources. Of these methods, PSL-KGI <ref type="bibr" target="#b18">[Pujara et al., 2013</ref><ref type="bibr" target="#b19">[Pujara et al., , 2017] ]</ref> is shown not only to perform better w

ion in KG embeddings -e.g., TKRL <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> and TransC <ref type="bibr" target="#b10">[Lv et al., 2018]</ref>. Recently, SimplE + <ref type="bibr" target="
-10: YAGO3-10 [ <ref type="bibr" target="#b3">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type="bibr" target="#b22">[Suchanek et al., 2007]</ref> knowledge graph. It is often used for e
that embeddings, with a few recent exceptions <ref type="bibr" target="#b6">[Guo et al., 2016</ref><ref type="bibr" target="#b12">, Minervini et al., 2017</ref><ref type="bibr">, 2018</ref><ref type=
ngs in such a way as to maximise the plausibility of the triples that are already present in the KG <ref type="bibr" target="#b15">[Nickel et al., 2011</ref><ref type="bibr" target="#b21">, Socher et
work in modeling structural as well as uncertainty information of relations in the embedding space. <ref type="bibr" target="#b2">[Chen et al., 2019]</ref> uses Probabilistic Soft Logic to come up wit

e extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type="bibr" target="#b14">[Nakashole et al., 2011</ref>] and many more. A detailed survey of ap
e often due to the lack of type compatibility between the entities due to their typeagnostic nature <ref type="bibr" target="#b25">[Xie et al., 2016</ref><ref type="bibr" target="#b7">, Jain et al., 2 here are some recent efforts to incorporate type hierarchy information in KG embeddings -e.g., TKRL <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> and TransC <ref type="bibr" target="#b10">[L ot have ontological and type label information. Therefore, we use the type labels for entities from <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> which also provides the domain and range inf
n contrast to the two stages with iterations of our method). We use the setting introduced in R-GCN <ref type="bibr" target="#b20">(Schlichtkrull et al. [2018]</ref>) to combine scores of KG embedding


"bibr" target="#b11">[Ma et al., 2014]</ref>; reconciling diverse evidence from multiple extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type="bi tion <ref type="bibr" target="#b11">[Ma et al., 2014]</ref>, classification with diverse extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type="bibr" ent or replace the set of rules we use. Further, evidence from diverse extractors as in the case of <ref type="bibr" target="#b4">[Dong et al., 2014]</ref> can be incorporated into the PSL-KGI framewo
t al., 2011</ref>] and many more. A detailed survey of approaches for KG refinement is available in <ref type="bibr" target="#b17">[Paulheim, 2017]</ref>. On the other hand, neural and tensor-based em verse extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type="bibr" target="#b17">[Paulheim, 2017]</ref> for an overview). While these works have their
work in modeling structural as well as uncertainty information of relations in the embedding space. <ref type="bibr" target="#b2">[Chen et al., 2019]</ref> uses Probabilistic Soft Logic to come up wit
"bibr" target="#b11">[Ma et al., 2014]</ref>; reconciling diverse evidence from multiple extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type="bi tion <ref type="bibr" target="#b11">[Ma et al., 2014]</ref>, classification with diverse extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type="bibr" ent or replace the set of rules we use. Further, evidence from diverse extractors as in the case of <ref type="bibr" target="#b4">[Dong et al., 2014]</ref> can be incorporated into the PSL-KGI framewo
e extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type="bibr" target="#b14">[Nakashole et al., 2011</ref>] and many more. A detailed survey of ap
3">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, with a few recent exceptions <ref type="bibr" target="#b6">[Guo et al., 2016</ref><ref type="bibr" target="#b12">, Minervini et a

e often due to the lack of type compatibility between the entities due to their typeagnostic nature <ref type="bibr" target="#b25">[Xie et al., 2016</ref><ref type="bibr" target="#b7">, Jain et al., 2 here are some recent efforts to incorporate type hierarchy information in KG embeddings -e.g., TKRL <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> and TransC <ref type="bibr" target="#b10">[L ot have ontological and type label information. Therefore, we use the type labels for entities from <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> which also provides the domain and range inf
t has also been observed that such noise can significantly degrade the performance of KG embeddings <ref type="bibr" target="#b19">[Pujara et al., 2017]</ref>.</p><p>The KG refinement task aims to red r subject, relation label or object. Note that this was the same model followed in an earlier study <ref type="bibr" target="#b19">[Pujara et al., 2017]</ref>.</p><p>â¢ We further refine the noise mode ultiple sources. Of these methods, PSL-KGI <ref type="bibr" target="#b18">[Pujara et al., 2013</ref><ref type="bibr" target="#b19">[Pujara et al., , 2017] ]</ref> is shown not only to perform better w
2016</ref><ref type="bibr" target="#b12">, Minervini et al., 2017</ref><ref type="bibr">, 2018</ref><ref type="bibr" target="#b5">, Fatemi et al., 2019]</ref>, do not make use of rich taxonomic/ontolo , 2016]</ref> and TransC <ref type="bibr" target="#b10">[Lv et al., 2018]</ref>. Recently, SimplE + <ref type="bibr" target="#b5">[Fatemi et al., 2019]</ref> includes taxonomic information -i.e., subt
en some research in using rule-based reasoning and KG embeddings together in an iterative manner in <ref type="bibr" target="#b26">[Zhang et al., 2019]</ref>.</p><p>They achieve improvements in the pe
that embeddings, with a few recent exceptions <ref type="bibr" target="#b6">[Guo et al., 2016</ref><ref type="bibr" target="#b12">, Minervini et al., 2017</ref><ref type="bibr">, 2018</ref><ref type=

en some research in using rule-based reasoning and KG embeddings together in an iterative manner in <ref type="bibr" target="#b26">[Zhang et al., 2019]</ref>.</p><p>They achieve improvements in the pe
3">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, with a few recent exceptions <ref type="bibr" target="#b6">[Guo et al., 2016</ref><ref type="bibr" target="#b12">, Minervini et a
ciation rule mining over the noisy KG to induce rules which can help in eliminating incorrect facts <ref type="bibr" target="#b11">[Ma et al., 2014]</ref>; reconciling diverse evidence from multiple e other research directions for (partially) solving the KG refinement problem such as rule induction <ref type="bibr" target="#b11">[Ma et al., 2014]</ref>, classification with diverse extractors <ref
ngs in such a way as to maximise the plausibility of the triples that are already present in the KG <ref type="bibr" target="#b15">[Nickel et al., 2011</ref><ref type="bibr" target="#b21">, Socher et
-10: YAGO3-10 [ <ref type="bibr" target="#b3">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type="bibr" target="#b22">[Suchanek et al., 2007]</ref> knowledge graph. It is often used for e
ion in KG embeddings -e.g., TKRL <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> and TransC <ref type="bibr" target="#b10">[Lv et al., 2018]</ref>. Recently, SimplE + <ref type="bibr" target="
-10: YAGO3-10 [ <ref type="bibr" target="#b3">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type="bibr" target="#b22">[Suchanek et al., 2007]</ref> knowledge graph. It is often used for e
t al., 2011</ref>] and many more. A detailed survey of approaches for KG refinement is available in <ref type="bibr" target="#b17">[Paulheim, 2017]</ref>. On the other hand, neural and tensor-based em verse extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type="bibr" target="#b17">[Paulheim, 2017]</ref> for an overview). While these works have their
en some research in using rule-based reasoning and KG embeddings together in an iterative manner in <ref type="bibr" target="#b26">[Zhang et al., 2019]</ref>.</p><p>They achieve improvements in the pe
e often due to the lack of type compatibility between the entities due to their typeagnostic nature <ref type="bibr" target="#b25">[Xie et al., 2016</ref><ref type="bibr" target="#b7">, Jain et al., 2 here are some recent efforts to incorporate type hierarchy information in KG embeddings -e.g., TKRL <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> and TransC <ref type="bibr" target="#b10">[L ot have ontological and type label information. Therefore, we use the type labels for entities from <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> which also provides the domain and range inf
triples that are already present in the KG <ref type="bibr" target="#b15">[Nickel et al., 2011</ref><ref type="bibr" target="#b21">, Socher et al., 2013</ref><ref type="bibr" target="#b23">, Trouillon
n contrast to the two stages with iterations of our method). We use the setting introduced in R-GCN <ref type="bibr" target="#b20">(Schlichtkrull et al. [2018]</ref>) to combine scores of KG embedding
2016</ref><ref type="bibr" target="#b12">, Minervini et al., 2017</ref><ref type="bibr">, 2018</ref><ref type="bibr" target="#b5">, Fatemi et al., 2019]</ref>, do not make use of rich taxonomic/ontolo , 2016]</ref> and TransC <ref type="bibr" target="#b10">[Lv et al., 2018]</ref>. Recently, SimplE + <ref type="bibr" target="#b5">[Fatemi et al., 2019]</ref> includes taxonomic information -i.e., subt
e extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type="bibr" target="#b14">[Nakashole et al., 2011</ref>] and many more. A detailed survey of ap
e presence of noise from the automatic extraction methods used to populate them. For instance, NELL <ref type="bibr" target="#b1">[Carlson et al., 2010]</ref> is known to contain various kinds of erro e part of the original benchmark test collection.   The NELL subset taken from its 165 th iteration <ref type="bibr" target="#b1">[Carlson et al., 2010]</ref>) has been used for the KG refinement task
that embeddings, with a few recent exceptions <ref type="bibr" target="#b6">[Guo et al., 2016</ref><ref type="bibr" target="#b12">, Minervini et al., 2017</ref><ref type="bibr">, 2018</ref><ref type=
ion in KG embeddings -e.g., TKRL <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> and TransC <ref type="bibr" target="#b10">[Lv et al., 2018]</ref>. Recently, SimplE + <ref type="bibr" target="
work in modeling structural as well as uncertainty information of relations in the embedding space. <ref type="bibr" target="#b2">[Chen et al., 2019]</ref> uses Probabilistic Soft Logic to come up wit
en some research in using rule-based reasoning and KG embeddings together in an iterative manner in <ref type="bibr" target="#b26">[Zhang et al., 2019]</ref>.</p><p>They achieve improvements in the pe
work in modeling structural as well as uncertainty information of relations in the embedding space. <ref type="bibr" target="#b2">[Chen et al., 2019]</ref> uses Probabilistic Soft Logic to come up wit

"bibr" target="#b11">[Ma et al., 2014]</ref>; reconciling diverse evidence from multiple extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type="bi tion <ref type="bibr" target="#b11">[Ma et al., 2014]</ref>, classification with diverse extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type="bibr" ent or replace the set of rules we use. Further, evidence from diverse extractors as in the case of <ref type="bibr" target="#b4">[Dong et al., 2014]</ref> can be incorporated into the PSL-KGI framewo

3">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, with a few recent exceptions <ref type="bibr" target="#b6">[Guo et al., 2016</ref><ref type="bibr" target="#b12">, Minervini et a
ifferent domains require specialized normalization methods. In computer vision, batch normalization <ref type="bibr" target="#b15">[16]</ref> is a standard component. While in natural language process h set of feature values the normalization is applied to. For example, in computer vision, BatchNorm <ref type="bibr" target="#b15">[16]</ref> is the de facto method that normalizes the feature values ring testing, the estimated dataset-level statistics are used instead of the batch-level statistics <ref type="bibr" target="#b15">[16]</ref>.</p><p>In GNNs, for each feature dimension, the BatchNorm -invariant" property <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. In comparison, BatchNorm in the lower branch suffers from e-invariant" property<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. In comparison, BatchNorm in the lower branch suffers from ed to improve the training process in different applications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" tar h of the parameters; <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref> show that the norma rom BatchNorm layers under different settings of batch sizes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32</ref>, 64], as in Figure < matrix in layer k. We apply the normalization after the linear transformation as in previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" ta hich try to provide accurate estimations for the mean and standard deviation over the whole dataset <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" ta
alization ability <ref type="bibr" target="#b38">[39]</ref>, and infinite-width asymptotic behavior <ref type="bibr" target="#b6">[7]</ref>. These theoretical understandings lead to GNN architectures he same. We study the output of the standard shift operation in the first layer, i.e., k = 1 in Eq. <ref type="bibr" target="#b6">(7)</ref>. From the following proposition, we can see that when the st
as been a surge of interest in Graph Neural Networks (GNNs) for learning with graph-structured data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta arn the representations of nodes and graphs. Modern GNNs follow a neighborhood aggregation strategy <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" ta
GNNs learn node and graph features by following a neighbor aggregation (or message passing) scheme <ref type="bibr" target="#b9">[10]</ref>, where node features are recursively aggregated from their
NN architecture variants, e.g., neighbor aggregation modules, that learn good graph representations <ref type="bibr" target="#b36">[37]</ref>. To that end, many theoretical aspects of GNNs have been s that end, many theoretical aspects of GNNs have been studied, including their representation power <ref type="bibr" target="#b36">[37]</ref>, generalization ability <ref type="bibr" target="#b38">[39 important problem remains: the optimization of GNNs is often unstable, and the convergence is slow <ref type="bibr" target="#b36">[37]</ref>. This raises the question:</p><p>Can we provably improve t tional Networks (GCN) <ref type="bibr" target="#b18">[19]</ref> and Graph Isomorphism Network (GIN) <ref type="bibr" target="#b36">[37]</ref>. In GCN, the AGGREGATE function is defined as:</p><formula , as the aggregation function is similar to the convolutional operation, BatchNorm is usually used. <ref type="bibr" target="#b36">[37]</ref> uses BatchNorm in the GIN model, where the BatchNorm is ap es in a single graph. Then a natural question is whether using a batch-level normalization for GNNs <ref type="bibr" target="#b36">[37]</ref> can lead to similar advantages. In batch normalization (Ba ion holds for batch normalization.</p><p>To study this, we train a 5-layer GIN with BatchNorm as in <ref type="bibr" target="#b36">[37]</ref> on the PROTEINS dataset and train a ResNet18 <ref type="bi there are no available node features, then X i is set to be the one-hot encoding of the node degree <ref type="bibr" target="#b36">[37]</ref>. In a r-regular graph, all nodes have the same encoding, a arget="#tab_1">1</ref>. We evaluate our proposed GraphNorm on two typical graph neural networks GIN <ref type="bibr" target="#b36">[37]</ref> and GCN <ref type="bibr" target="#b18">[19]</ref> and comp readout for MUTAG, PTC, PROTEINS and NCI1 datasets, and use MEAN readout for other datasets, as in <ref type="bibr" target="#b36">[37]</ref>. Details of the experimental settings are presented in App mmarized in Table <ref type="table" target="#tab_3">2</ref>. Those information can be also found in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b14">[15]</ref>. Social netw d social network datasets, we use 5-layer GIN/GCN with a linear output head for prediction followed <ref type="bibr" target="#b36">[37]</ref> with residual connection. The hidden dimension of GIN/GCN ion of GIN/GCN is set to be 64. For the large-scale ogbgmolhiv dataset, we also use 5-layer GIN/GCN <ref type="bibr" target="#b36">[37]</ref> architecture with residual connection. Following <ref type â 3, 5e â 4, 5e â 5} âª {0.0}, the learning rate â {1e â 4, 1e â 3, 1e â 2}. We follow previous work <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b14">[15]</ref> to select th <ref type="bibr" target="#b16">[17]</ref>. We report the accuracies reported in the original paper <ref type="bibr" target="#b36">[37]</ref>. For the large-scale ogbg-molhiv dataset, we use the basel ref>, including the Graph-agnostic MLP model, GCN <ref type="bibr" target="#b18">[19]</ref> and GIN <ref type="bibr" target="#b36">[37]</ref>. We also report the roc-auc values reported in the origina ormance over different random seeds (or cross-validation). For the medium-scale datasets, following <ref type="bibr" target="#b36">[37]</ref>, we perform a 10-fold cross-validation as these datasets d sks (PROTEINS, PTC, NCI1, MUTAG, IMDB-BINARY datasets), we train a 5-layer GIN with BatchNorm as in <ref type="bibr" target="#b36">[37]</ref> and the number of sub-layers in MLP is set to 2. For image in previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. We can instantiate Eq. ( <ref type="formula" target="#form ings</head><p>We use eight popularly used benchmark datasets of different scales in the experiments <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40]</ref>, including four me
the whole dataset <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref>. During testing, the estimated dataset-level statistics are tings of batch sizes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32</ref>, 64], as in Figure <ref type="figure">9</ref>. We can see th
sdom in optimization shows that preconditioning can accelerate the convergence of iterative methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, and similar ideas are
benchmark datasets of different scales in the experiments <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40]</ref>, including four medium-scale bioinformatics datasets (MUTAG
et="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref>. GNNs learn node and graph features by following a neighbor
type="bibr" target="#b20">21]</ref> show that the normalization implicitly tunes the learning rate. <ref type="bibr" target="#b27">[28]</ref> reveals that normalization smooths the optimization landsc
rget="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" tar
ssage passing, in order to capture richer topological properties. Our method draws inspiration from <ref type="bibr" target="#b32">[33]</ref>, where it was shown that GNNs become universal when the ve target="#b35">[36]</ref>.</p><p>It is important to note here that contrary to identifierbased GNNs <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bib rovide a unique identification of the vertices, then universality will also hold (Corollary 3.1. in <ref type="bibr" target="#b32">[33]</ref>).</p><p>We conjecture, that in real-world scenarios the nu ><p>Unique identifiers. From a different perspective, <ref type="bibr" target="#b67">[68]</ref> and <ref type="bibr" target="#b32">[33]</ref> showed the connections between GNNs and distributed local
l neural networks, transportation networks and food webs <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>.</p><p>In Figur
and community structure in social networks, respectively <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Therefore, three major questions arise when designing GNN
ogP" (see <ref type="bibr" target="#b110">[111]</ref>, <ref type="bibr" target="#b111">[112]</ref>, <ref type="bibr" target="#b112">[113]</ref> for details) of molecules from the ZINC database <ref ty
="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref> and diffusion operators <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bib
]</ref> for details) of molecules from the ZINC database <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b113">[114]</ref>. We use structural features obtained with kcycle countin
n is widely studied, and many general-purpose algorithms <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bib
/ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and physics <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, to name a few. Mos
and community structure in social networks, respectively <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Therefore, three major questions arise when designing GNN
<ref type="bibr" target="#b0">[1]</ref> to bioinformatics <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, chemistry <ref type="bibr" target="#b3">[4]</ref>, <ref typ
">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bib
<ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> or feature matrix <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>) are not compatibl denoising autoencoder to disturb the structure information. To build a symmetric graph autoencoder, <ref type="bibr" target="#b23">[24]</ref> proposes Laplacian sharpening as the counterpart of Laplac ing the latent representations to match a prior distribution for robust node embeddings.</p><p>GALA <ref type="bibr" target="#b23">[24]</ref> proposes a symmetric graph convolutional autoencoder recov
he learned node embeddings, we visualize the node representations in 2D space using t-SNE algorithm <ref type="bibr" target="#b28">[29]</ref>. The figures are shown in Figure <ref type="figure" target
>27,</ref><ref type="bibr" target="#b32">33]</ref> can be concluded by an encoder-decoder framework <ref type="bibr" target="#b10">[11]</ref>, while they differ from model structure and training objec
><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>, and random walks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. However, these met . Another line of researches manages to learn node embeddings with a particular objective function. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref> learn node embeddin
rget="#b16">[17]</ref>, assuming that similar nodes tend to cooccur in same sequences. Other models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" targ
attributes/features and are widely applied to represent network-structured data in social networks <ref type="bibr" target="#b11">[12]</ref>, citation networks <ref type="bibr" target="#b15">[16]</re
ode graph structural information. Early graph embedding approaches are based on Laplacian eigenmaps <ref type="bibr" target="#b20">[21]</ref>, matrix factorization <ref type="bibr" target="#b2">[3,</r he high-dimensional adjacency matrix to low-dimensional latent embedding space. Laplacian eigenmaps <ref type="bibr" target="#b20">[21]</ref> and matrix factorization <ref type="bibr" target="#b2">[3]
><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>, and random walks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. However, these met . Another line of researches manages to learn node embeddings with a particular objective function. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref> learn node embeddin
imilar nodes tend to cooccur in same sequences. Other models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref> can be concluded by
also argue that training objectives of these algorithms (either reconstructing the adjacency matrix <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> or feature matrix CN as the encoder, then decode by inner product with cross-entropy loss. As variants of GAE (VGAE), <ref type="bibr" target="#b22">[23]</ref> exploits adversarially regularized method to learn more ro ional networks with the (variational) autoencoder for representation learning.</p><p>ARGA and ARVGA <ref type="bibr" target="#b22">[23]</ref> add adversarial constraints to GAE and VGAE respectively,
ng marginalized graph autoencoder. Its training objective is reconstructing the feature matrix. AGC <ref type="bibr" target="#b37">[38]</ref> exploits high-order graph convolution to filter node featu
bute this performance degradation to the oversmoothing issue <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref>, which states that descriptions of the over-smoothing issue simplify the assumption of non-linear activation function <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref> or make approximat , which means that representations of nodes converge to indistinguishable limits. To our knowledge, <ref type="bibr" target="#b14">[15]</ref> is the first attempt to demystify the over-smoothing issue whole graph when the number of training nodes is limited under a semi-supervised learning setting. <ref type="bibr" target="#b14">[15]</ref> applies co-training and self-training to overcome the limi tations has a slight downward trend as the number of propagation iterations increases. According to <ref type="bibr" target="#b14">[15]</ref>, the node representations suffering from the oversmoothing ervation when building very deep graph neural networks, which aligns with the over-smoothing issue. <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b32">[33]</ref> study the ov layers, are very difficult to be separated.  Several studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref> attribute this performance degradation phenomenon to the ov
tive fields. Several recent works attribute this performance degradation to the oversmoothing issue <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" targ nerated by multiple GCN layers, like 6 layers, are very difficult to be separated.  Several studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref> attribute this perfo filter on the spectral domain, thus deriving smoothing features across a graph. Another recent work <ref type="bibr" target="#b2">[3]</ref> verify that smoothing is the nature of most typical graph co shallow architectures. A smoothness regularizer term and adaptive edge optimization are proposed in <ref type="bibr" target="#b2">[3]</ref> to relieve the over-smoothing problem. Jumping Knowledge Net
MLP network for feature transformation. Theoretically, MLP can approximate any measurable function <ref type="bibr" target="#b9">[10]</ref>. Obviously, Z only contains the information of individual n
<ref type="bibr" target="#b37">38]</ref> and link prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. Graph convolutions
target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar
rget="#b32">[33]</ref> by analyzing the connection of nodes' influence distribution and random walk <ref type="bibr" target="#b16">[17]</ref>. Recently, SGC <ref type="bibr" target="#b30">[31]</ref> i
target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" tar
, graph classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" tar
achieved for many applications, such as node classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" targe eature x i . Most graph convolutions, like GCN <ref type="bibr" target="#b10">[11]</ref>, GraphSAGE <ref type="bibr" target="#b8">[9]</ref>, GAT <ref type="bibr" target="#b29">[30]</ref>, and GIN <ref uently utilized propagation mechanisms. The row-averaging normalization A â is adopted in GraphSAGE <ref type="bibr" target="#b8">[9]</ref> and DGCNN <ref type="bibr" target="#b37">[38]</ref>. The sym 29">[30]</ref>, Mixture Model Network (MoNet) <ref type="bibr" target="#b20">[21]</ref>, Graph-SAGE <ref type="bibr" target="#b8">[9]</ref>, APPNP <ref type="bibr" target="#b11">[12]</ref>, and SGC <r
f type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, graph classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
r proposed metric from the quantitative perspective, we employ a data visualization technique t-SNE <ref type="bibr" target="#b18">[19]</ref>. t-SNE provides an interpretable visualization, especially
arget="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" tar aph Attention Network(GAT) <ref type="bibr" target="#b29">[30]</ref>, Mixture Model Network (MoNet) <ref type="bibr" target="#b20">[21]</ref>, Graph-SAGE <ref type="bibr" target="#b8">[9]</ref>, APPNP
Amazon Photo <ref type="bibr" target="#b27">[28]</ref> are segments of the Amazon co-purchase graph <ref type="bibr" target="#b19">[20]</ref> where nodes are goods and edges denote that two goods are
mula><p>is a layer-specific trainable weight matrix. Ï is a nonlinear activation function like ReLU <ref type="bibr" target="#b21">[22]</ref>. Intuitively, GCN learns representation for each node by p
arget="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" tar AGE <ref type="bibr" target="#b8">[9]</ref>, GAT <ref type="bibr" target="#b29">[30]</ref>, and GIN <ref type="bibr" target="#b31">[32]</ref>, can be obtained under this framework by deploying differe
get="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, graph classification <ref type="bibr" target="#b5">[6,</re oversmoothing issue <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref>, which states that representations from different classes b simplify the assumption of non-linear activation function <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref> or make approximations of different probabilities <ref type 15,</ref><ref type="bibr" target="#b32">33]</ref> or make approximations of different probabilities <ref type="bibr" target="#b32">[33]</ref>. Our theoretical analysis can serve as a more rigorous and e representations of nodes from different classes indistinguishable. The same problem is studied in <ref type="bibr" target="#b32">[33]</ref> by analyzing the connection of nodes' influence distributi type="bibr" target="#b2">[3]</ref> to relieve the over-smoothing problem. Jumping Knowledge Network <ref type="bibr" target="#b32">[33]</ref> deploys a layer-aggregation mechanism to adaptively select networks, which aligns with the over-smoothing issue. <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b32">[33]</ref> study the over-smoothing issue from the perspective of Lap
MLP network for feature transformation. Theoretically, MLP can approximate any measurable function <ref type="bibr" target="#b9">[10]</ref>. Obviously, Z only contains the information of individual n
get="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" tar
get="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, graph classification <ref type="bibr" target="#b5">[6,</re oversmoothing issue <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref>, which states that representations from different classes b simplify the assumption of non-linear activation function <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref> or make approximations of different probabilities <ref type 15,</ref><ref type="bibr" target="#b32">33]</ref> or make approximations of different probabilities <ref type="bibr" target="#b32">[33]</ref>. Our theoretical analysis can serve as a more rigorous and e representations of nodes from different classes indistinguishable. The same problem is studied in <ref type="bibr" target="#b32">[33]</ref> by analyzing the connection of nodes' influence distributi type="bibr" target="#b2">[3]</ref> to relieve the over-smoothing problem. Jumping Knowledge Network <ref type="bibr" target="#b32">[33]</ref> deploys a layer-aggregation mechanism to adaptively select networks, which aligns with the over-smoothing issue. <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b32">[33]</ref> study the over-smoothing issue from the perspective of Lap
alized Laplacian Label Propagation (LabelProp NL) <ref type="bibr" target="#b1">[2]</ref>, Cheb-Net <ref type="bibr" target="#b3">[4]</ref>, Graph Convolutional Network (GCN) <ref type="bibr" target="
get="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, graph classification <ref type="bibr" target="#b5">[6,</re oversmoothing issue <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref>, which states that representations from different classes b simplify the assumption of non-linear activation function <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref> or make approximations of different probabilities <ref type 15,</ref><ref type="bibr" target="#b32">33]</ref> or make approximations of different probabilities <ref type="bibr" target="#b32">[33]</ref>. Our theoretical analysis can serve as a more rigorous and e representations of nodes from different classes indistinguishable. The same problem is studied in <ref type="bibr" target="#b32">[33]</ref> by analyzing the connection of nodes' influence distributi type="bibr" target="#b2">[3]</ref> to relieve the over-smoothing problem. Jumping Knowledge Network <ref type="bibr" target="#b32">[33]</ref> deploys a layer-aggregation mechanism to adaptively select networks, which aligns with the over-smoothing issue. <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b32">[33]</ref> study the over-smoothing issue from the perspective of Lap
f type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, graph classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target
icit data augmentation. Moreover, to supplement the input graph with more global information, MVGRL <ref type="bibr" target="#b14">[15]</ref> proposes to augment the input graph using graph diffusion. asure MI between input and representations of both nodes and edges without data augmentation; MVGRL <ref type="bibr" target="#b14">[15]</ref> proposes to learn both node-level and graph-level represen <ref type="bibr" target="#b43">[44]</ref>, GMI <ref type="bibr" target="#b29">[30]</ref>, and MVGRL <ref type="bibr" target="#b14">[15]</ref> in Table <ref type="table" target="#tab_0">1</ref>, where MI) <ref type="bibr" target="#b29">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref type="bibr" target="#b14">[15]</ref>. Furthermore, we report the performance obtained using a l
posed to maximize the MI between node embeddings and a global summary embedding. Following DGI, GMI <ref type="bibr" target="#b29">[30]</ref> proposes two node-level contrastive objectives to directly the agreement of node embeddings across two corrupted views of the graph.</p><p>Following DGI, GMI <ref type="bibr" target="#b29">[30]</ref> employs two discriminators to directly measure MI between summary, we provide a brief comparison between the  <ref type="bibr" target="#b43">[44]</ref>, GMI <ref type="bibr" target="#b29">[30]</ref>, and MVGRL <ref type="bibr" target="#b14">[15]</ref> in Ta ax (DGI) <ref type="bibr" target="#b43">[44]</ref>, Graphical Mutual Information Maximization (GMI) <ref type="bibr" target="#b29">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref
representing a branch of the field. Node features are calculated as the average of pretrained GloVe <ref type="bibr" target="#b30">[31]</ref> word embeddings of the words in each article.</p><p>â¢ Amaz
arget="#b17">[18]</ref>.</p><p>Theoretical analysis sheds light on the reasons behind their success <ref type="bibr" target="#b32">[33]</ref>. Objectives used in these methods can be seen as maximizin lower bound of the InfoNCE objective <ref type="bibr" target="#b41">[42]</ref>, which is defined as <ref type="bibr" target="#b32">[33]</ref>. According to van den Oord et al. <ref type="bibr" target= ive J and the InfoNCE objective <ref type="bibr" target="#b41">[42]</ref> , which can be defined as <ref type="bibr" target="#b32">[33]</ref> ð¼ NCE (U; V) â E   </p><p>Thus, we arrive at 2J â¤ ð¼ (U; V) -c.org/ns/1.0" xml:id="fig_7"><head>( 17 )</head><label>17</label><figDesc>According to Poole et al.<ref type="bibr" target="#b32">[33]</ref>, the InfoNCE estimator is a lower bound of the true MI, i. een widely applied in the representation learning literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" tar
ional methods on unsupervised graph representation learning employ the contrastive paradigm as well <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" ta ar embeddings. Positive samples under this circumstance are nodes appearing in the same random walk <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>. For example, the datasets. Also, these methods are known to be error-prone with inappropriate hyperparameter tuning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Recent work , (1) traditional methods including DeepWalk <ref type="bibr" target="#b31">[32]</ref> and node2vec <ref type="bibr" target="#b10">[11]</ref> and (2) deep learning methods including Graph Autoencoders
random flip, cropping, resizing, rotation <ref type="bibr" target="#b8">[9]</ref>, color distortion <ref type="bibr" target="#b22">[23]</ref>, etc. Existing work <ref type="bibr" target="#b15">[16,</r
ibr" target="#b38">39]</ref> and natural language processing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref>. These CL methods see
g GNN models are mostly established in a supervised manner <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref>, which require abun literature has grown up around the theme of supervised GNN <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" tar rts, we also report the performance of two representative models Graph Convolutional Networks (GCN) <ref type="bibr" target="#b21">[22]</ref> and Graph Attention Networks (GAT) <ref type="bibr" target ://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation details.</head><p>We employ a two-layer GCN <ref type="bibr" target="#b21">[22]</ref> as the encoder for all deep learning baselines due to its ref type="formula" target="#formula_22">21</ref>), we finally arrive at inequality J â¤ ð¼ (X; U, V), <ref type="bibr" target="#b21">(22)</ref> which concludes the proof. â¡ </p></div><figure xmlns="http
et="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, which requires labeled datasets that may not be accessible
al language processing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref>. These CL methods seek to maximize the Mutual Information (
representing a branch of the field. Node features are calculated as the average of pretrained GloVe <ref type="bibr" target="#b30">[31]</ref> word embeddings of the words in each article.</p><p>â¢ Amaz
uctural augmentation schemes, we calculate edge centrality scores of the famous Karate club dataset <ref type="bibr" target="#b49">[50]</ref>, containing two groups of students leading by two coaches
sion and contrasting node representations to augmented graph summary representations. Moreover, GCC <ref type="bibr" target="#b33">[34]</ref> proposes a pretraining framework based on contrastive lear
with negative-sampled counterparts.</p><p>Inspired by previous CL methods, Deep Graph InfoMax (DGI) <ref type="bibr" target="#b43">[44]</ref> marries the power of GNN into InfoMax-based methods. DGI f gmentation in either the structural domain or the attribute domain, such as feature shifting in DGI <ref type="bibr" target="#b43">[44]</ref>, is not sufficient for generating diverse neighborhoods (i type="bibr" target="#b13">[14]</ref>, which incorporates DeepWalk-like objectives. Recent work DGI <ref type="bibr" target="#b43">[44]</ref> marries the power of GNN and contrastive learning, which f related graph contrastive learning methods. In summary, we provide a brief comparison between the  <ref type="bibr" target="#b43">[44]</ref>, GMI <ref type="bibr" target="#b29">[30]</ref>, and MVGRL ><p>For every experiment, we follow the linear evaluation scheme as introduced in VeliÄkoviÄ et al. <ref type="bibr" target="#b43">[44]</ref>, where each model is firstly trained in an unsupervised ma Graph Autoencoders (GAE, VGAE) <ref type="bibr" target="#b20">[21]</ref>, Deep Graph Infomax (DGI) <ref type="bibr" target="#b43">[44]</ref>, Graphical Mutual Information Maximization (GMI) <ref type
an be performed in parallel. In contrast, the triplet loss is known to be computationally expensive <ref type="bibr" target="#b35">[36]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
success in many fields, e.g., visual representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39]</ref> and natural languag get="#b8">[9]</ref>, color distortion <ref type="bibr" target="#b22">[23]</ref>, etc. Existing work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" ta
etc. Existing work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref> employs a memory bank for storing negative samples. Other w
random flip, cropping, resizing, rotation <ref type="bibr" target="#b8">[9]</ref>, color distortion <ref type="bibr" target="#b22">[23]</ref>, etc. Existing work <ref type="bibr" target="#b15">[16,</r
e paradigm as well <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>. Prior work on unsu br" target="#b10">[11]</ref> and (2) deep learning methods including Graph Autoencoders (GAE, VGAE) <ref type="bibr" target="#b20">[21]</ref>, Deep Graph Infomax (DGI) <ref type="bibr" target="#b43">[
e paradigm as well <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>. Prior work on unsu br" target="#b10">[11]</ref> and (2) deep learning methods including Graph Autoencoders (GAE, VGAE) <ref type="bibr" target="#b20">[21]</ref>, Deep Graph Infomax (DGI) <ref type="bibr" target="#b43">[
an be performed in parallel. In contrast, the triplet loss is known to be computationally expensive <ref type="bibr" target="#b35">[36]</ref>.</p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head
sion and contrasting node representations to augmented graph summary representations. Moreover, GCC <ref type="bibr" target="#b33">[34]</ref> proposes a pretraining framework based on contrastive lear
ature exploration, we first describe the recently proposed contextual stochastic block model (cSBM) <ref type="bibr" target="#b10">(Deshpande et al., 2018)</ref>. The cSBM allows for smoothly controll rning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs <ref type="bibr" target="#b10">(Deshpande et al., 2018)</ref> to generate synthetic graphs. We consi d have similar performances for Ï and âÏ. Due to space limitation we refer the interested reader to <ref type="bibr" target="#b10">(Deshpande et al., 2018)</ref> for a review of all formal theoretical philic graphs. The information-theoretic limits of reconstruction for the cSBM are characterized in <ref type="bibr" target="#b10">Deshpande et al. (2018)</ref>. The results show that, asymptotically, ate synthetic data is that the information-theoretic limit of the model is already characterized in <ref type="bibr" target="#b10">Deshpande et al. (2018)</ref>. This result is summarized below.</p><p de et al. (2018)</ref>. This result is summarized below.</p><p>Theorem A.7 (Informal main result in <ref type="bibr" target="#b10">Deshpande et al. (2018)</ref>). Assume that n, f â â, n f â Î¾ and d â

a running time similar to that of APPNP. It is nevertheless worth pointing out that the authors of <ref type="bibr" target="#b5">Bojchevski et al. (2020)</ref> successfully scaled APPNP to operate on
on domains. Many GNNs use message passing <ref type="bibr" target="#b12">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b3">Battaglia et al., 2018)</ref> to manipulate node features and graph to

puts of different GCN layers to arrive at the final output. On the other hand, the GCN-Cheby method <ref type="bibr" target="#b9">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b18">Kipf &am kovic et al., 2018)</ref>, JK-Net <ref type="bibr" target="#b37">(Xu et al., 2018)</ref>, GCN-Cheby <ref type="bibr" target="#b9">(Defferrard et al., 2016)</ref>, APPNP <ref type="bibr" target="#b19">
ignals. In contrast, recent GNN models that utilize Personalized PageRanks (PPR) with fixed weights <ref type="bibr" target="#b36">(Wu et al., 2019;</ref><ref type="bibr" target="#b19">Klicpera et al. heart GNNs that is related to our GPR-GNN approach. It can be easily seen that APPNP as well as SGC <ref type="bibr" target="#b36">(Wu et al., 2019)</ref> are special cases of our model since APPNP fi errard et al., 2016)</ref>, APPNP <ref type="bibr" target="#b19">(Klicpera et al., 2018)</ref>, SGC <ref type="bibr" target="#b36">(Wu et al., 2019)</ref>, SAGE <ref type="bibr" target="#b14">(Hamilto
s from the same class tend to form edges. Homophily is also a common assumption in graph clustering <ref type="bibr" target="#b35">(Von Luxburg, 2007;</ref><ref type="bibr" target="#b33">Tsourakakis, ss filter case.</p><p>Proof. We start with the low pass filter result. From basic spectral analysis <ref type="bibr" target="#b35">(Von Luxburg, 2007)</ref> we know that Î» 1 = 1 and |Î» i | &lt; 1, âi

puts of different GCN layers to arrive at the final output. On the other hand, the GCN-Cheby method <ref type="bibr" target="#b9">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b18">Kipf &am kovic et al., 2018)</ref>, JK-Net <ref type="bibr" target="#b37">(Xu et al., 2018)</ref>, GCN-Cheby <ref type="bibr" target="#b9">(Defferrard et al., 2016)</ref>, APPNP <ref type="bibr" target="#b19">

e challenging. A number of GPM frameworks have been proposed to reduce the burden on the programmer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" ta lgorithms. Low-level systems such as RStream <ref type="bibr" target="#b55">[56]</ref> and Pangolin <ref type="bibr" target="#b11">[12]</ref> provide low-level API functions for the user to control th form the state-of-the-art GPM systems, AutoMine <ref type="bibr" target="#b38">[39]</ref>, Pangolin <ref type="bibr" target="#b11">[12]</ref>, and Peregrine <ref type="bibr" target="#b28">[29]</ref> b e an example in Appendix B.6). This technique is called Memoization of Embedding Connectivity (MEC) <ref type="bibr" target="#b11">[12]</ref>. â¢ For edge-induced extension, a set of edges instead of v tern Classification (CP): FP and CP are low-level optimizations enabled in a prior system, Pangolin <ref type="bibr" target="#b11">[12]</ref>, so we describe them in Appendices B.4 and B.5. To support type="foot" target="#foot_0">3</ref> : AutoMine <ref type="bibr" target="#b38">[39]</ref>, Pangolin <ref type="bibr" target="#b11">[12]</ref>, and Peregrine <ref type="bibr" target="#b28">[29]</ref>. "#b10">[11]</ref> is a distributed GPM system which incorporates task-parallel processing. Pangolin <ref type="bibr" target="#b11">[12]</ref> is a shared-memory GPM system targeting both CPU and GPU. ferent patterns. For small implicit patterns, Sandslash uses customized pattern classification (CP) <ref type="bibr" target="#b11">[12]</ref>. For example, in FSM, the labeled wedge patterns can be di
stom aggregation-keys like Fractal <ref type="bibr" target="#b18">[19]</ref>.</p><p>Some algorithms <ref type="bibr" target="#b2">[3]</ref> do local counting for each vertex or edge instead of global on, kClist <ref type="bibr" target="#b15">[16]</ref>, and a hand-optimized ð-MC implementation, PGD <ref type="bibr" target="#b2">[3]</ref>. Sandslash enables this optimization for implicit-pattern pr is typically requires a local count <ref type="bibr" target="#b27">[28]</ref> (or micro-level count <ref type="bibr" target="#b2">[3]</ref>) of embeddings associated with a single vertex or edge inste able). For fair comparison, we modified DistGraph <ref type="bibr" target="#b51">[52]</ref> and PGD <ref type="bibr" target="#b2">[3]</ref> so that they produce the same output as Sandslash. We added ="#b13">[14]</ref>. It constructs DAG using a core value based ordering to reduce search space. PGD <ref type="bibr" target="#b2">[3]</ref> counts 3 and 4-motifs by leveraging proven formulas to reduc tp://www.tei-c.org/ns/1.0"><head n="5">Low-Level Sandslash</head><p>Hand-optimized GPM applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" targ le" target="#tab_3">2</ref>. We also evaluate the state-of-the-art expertoptimized GPM applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target

Listings 2 and 3 in the Appendix show 3-MC and 4-MC using this low-level API.</p><p>Some algorithms <ref type="bibr" target="#b15">[16]</ref> search local (sub-)graphs instead of the (global) input gr ></formula><p>This optimization (MNC) has been used in a hand-optimized ð-CL implementation, kClist <ref type="bibr" target="#b15">[16]</ref>, and a hand-optimized ð-MC implementation, PGD <ref type=" </head><p>In some problems like ð-CL, pattern invariants can be exploited to prune the search space <ref type="bibr" target="#b15">[16]</ref>. To extend an embedding {ð£ 1 ...ð£ ð } at level ð â 1 for ð GPM systems. Moreover, Sandslash-Lo achieves better performance than even expert-implemented kClist <ref type="bibr" target="#b15">[16]</ref> by enabling search on a local graph (LG). There are some c >[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43]</ref>. kClist <ref type="bibr" target="#b15">[16]</ref> is a parallel ð-CL algorithm derived from <ref type="bibr" rtex ID. For core value based orientation, ordering is established using the core value of vertices <ref type="bibr" target="#b15">[16]</ref>, </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head embedding {111101}. With this code we can rebuild the exact structure of the embedding on the left. <ref type="bibr" target="#b15">[16]</ref>, since an ð-clique can only be extended from an (ð-1)-cliq Low-Level Sandslash</head><p>Hand-optimized GPM applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" tar mized GPM applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b51">52]</ref> listed in Table <re
et="#b17">[18,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref>, distributed CPUs <ref type="bibr">[22,</ref><ref type="bib
on multicore CPUs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref>, distributed CPUs <
GPM applications targeting various platforms. For TC, there are parallel solvers on multicore CPUs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" ta
1]</ref>, and GPUs <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43]</ref>. kClist <ref type="bibr" target="#b15">[16]</ref> is a para
have been proposed to reduce the burden on the programmer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" tar .</p><p>High-level systems such as AutoMine <ref type="bibr" target="#b38">[39]</ref> and Peregrine <ref type="bibr" target="#b28">[29]</ref> take specifications of patterns as input and leverage stat ="bibr" target="#b38">[39]</ref>, Pangolin <ref type="bibr" target="#b11">[12]</ref>, and Peregrine <ref type="bibr" target="#b28">[29]</ref> by 7.7Ã, 6.2Ã and 3.9Ã on average, respectively. Applicati ="bibr" target="#b38">[39]</ref>, Pangolin <ref type="bibr" target="#b11">[12]</ref>, and Peregrine <ref type="bibr" target="#b28">[29]</ref>. We use the five applications (also used in previous syste metry breaking. A widely used approach is to apply partial orders between vertices in the embedding <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref>. For special patte
orms. For TC, there are parallel solvers on multicore CPUs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" tar

ormally, given the reference frame I t and each support frame I t+Ï , Feature Pyramid Network (FPN) <ref type="bibr" target="#b55">[56]</ref> is leveraged to extract multi-scale pyramidal feature maps ad><p>Feature Pyramid Network. FPN is built at the top of ResNet-101 pre-trained on ImageNet. As in <ref type="bibr" target="#b55">[56]</ref>, P3, P4, Algorithm 2 Inference Algorithm of our SSVD </p><
="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr rkable progresses have been witnessed for object detection <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr
capitalize on FlowNet-s <ref type="bibr" target="#b58">[59]</ref> to produce optical flow, PWC-Net <ref type="bibr" target="#b59">[60]</ref> is particularly remould in our motion stream. Compared to 4 2 , and 7 2 , respectively. Two-stream Feature Aggregation. For motion stream, we utilize PWC-Net <ref type="bibr" target="#b59">[60]</ref> pre-trained on Flying Chairs dataset for optical flow esti t that the receptive field in sampling stream for offset prediction is smaller than that in PWC-Net <ref type="bibr" target="#b59">[60]</ref> for optical flow generation. As such, the range of estimat
="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> have successfully achieved remarkable improvements on object ="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, remarkable progresses have been witnessed for object detect
18]</ref> and another branch is featurelevel aggregation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bib
understanding <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bib
#b14">[15]</ref> extends R-FCN with a tracking module for simultaneous detection and tracking. DorT <ref type="bibr" target="#b42">[43]</ref>  </p></div> <div xmlns="http://www.tei-c.org/ns/1.0"><head objects Methods Backbone mAP (%) R-FCN <ref type="bibr" target="#b7">[8]</ref> ResNet-101 73.6 DorT <ref type="bibr" target="#b42">[43]</ref> ResNet-101 73.9 Faster R-CNN <ref type="bibr" target="#b12 VD + Viterbi <ref type="bibr" target="#b66">[67]</ref> is 60.2%, which is higher than 57.0% of DorT <ref type="bibr" target="#b42">[43]</ref>. The results further demonstrate the effectiveness of our
he temporal coherence exploration in video understanding <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bib
">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bib ructs multi-scale sliding windows to jointly classify and localize objects in deep architecture. In <ref type="bibr" target="#b27">[28]</ref>, YOLO divides feature maps into rigid grids, and objects a
understanding <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bib
rmable convolution over the feature map of support frame and the predicted offsets.</p><p>structure <ref type="bibr" target="#b61">[62]</ref> design to process input feature maps across multiple scale
gure" target="#fig_3">4</ref>. The philosophy is originated from the idea of deformable convolution <ref type="bibr" target="#b60">[61]</ref> which performs non-rigid spatial sampling with self-learnt case, we extend the augmentation of spatial sampling locations in standard deformation convolution <ref type="bibr" target="#b60">[61]</ref> which is only conditioned on one feature map to the measur achieves comparable performance with FGFA under the same backbone of Deformable Convolution Network <ref type="bibr" target="#b60">[61]</ref> (DCN). Note that as reported in <ref type="bibr" target="#
">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bib ased on a two-stage paradigm, i.e., first perform region proposal and then do classification. R-CNN <ref type="bibr" target="#b25">[26]</ref> is one of first attempts that tackles object detection pro ype="bibr" target="#b32">[33]</ref> and Fast R-CNN <ref type="bibr" target="#b24">[25]</ref> extend <ref type="bibr" target="#b25">[26]</ref> by devising SPP pooling or ROI pooling to enable the shari
or object detection in videos. One common solution for video object detection is box-level tracking <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bib arch on object detection in videos has proceeded along two different directions: box-level tracking <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bib sals into tubelets for re-scoring, which further improves the robustness of video object detection. <ref type="bibr" target="#b14">[15]</ref> extends R-FCN with a tracking module for simultaneous dete annotations of testing videos are not publicly available, we follow the widely adopted protocols in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bib ataset are a subset of 200 classes of ImageNet object detection (DET) dataset. Therefore, we follow <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b23">[24]</ref> and train SSVD et-101 73.9 Faster R-CNN <ref type="bibr" target="#b12">[13]</ref> ResNet-101 75.4 D (&amp; T loss) <ref type="bibr" target="#b14">[15]</ref> ResNet-101 75.8 FGFA <ref type="bibr" target="#b23">[24]</ presentative twostage image object detectors only exploit single frame information. D (&amp;T loss) <ref type="bibr" target="#b14">[15]</ref> exhibits better performances than the two image object det f> DeepID+Craft 73.8 FGFA <ref type="bibr" target="#b23">[24]</ref> ResNet-101 78.4 D&amp;T (Ï = 1) <ref type="bibr" target="#b14">[15]</ref> ResNet-101 79.8 STSN <ref type="bibr" target="#b18">[19]</
">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bib
ROI pooling. In addition, inspired by domain adaptation <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> for recognition, <ref type="bibr" target="#b35">[36]</ref>
">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. In particular, one common deep solution for object detect
e="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and video translation <ref type="bibr" target="#b57">[58]</ref>. As a result, we follow this elegant recipe and design the
">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. The former oft b64">[65]</ref> ResNet-101 77.1 MANet <ref type="bibr" target="#b20">[21]</ref> ResNet-101 78.1 THP <ref type="bibr" target="#b22">[23]</ref> ResNet-101+DCN 78.6 STSN <ref type="bibr" target="#b18">[1 mable Convolution Network <ref type="bibr" target="#b60">[61]</ref> (DCN). Note that as reported in <ref type="bibr" target="#b22">[23]</ref>, the performance of FGFA with DCN is 78.8%. For fair compa s, e.g., R-FCN and Faster R-CNN. Note that here we exclude several video object detection methods ( <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b44">[45]</ref>) which are addi
">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bib " target="#b16">[17]</ref> and feature-level aggregation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Box-level trac tion in a frame by learning to spatially sample features from adjacent frames for aggregation. STMN <ref type="bibr" target="#b21">[22]</ref> adopts spatiotemporal memory module with spatial alignment >[15]</ref> ResNet-101 79.8 STSN <ref type="bibr" target="#b18">[19]</ref> ResNet-101+DCN 80.4 STMN <ref type="bibr" target="#b21">[22]</ref> ResNet-101 80.5 HQ-link <ref type="bibr" target="#b43">[44
tion boxes between each two adjacent frames by predicted tracking boxes and Viterbi Algorithm as in <ref type="bibr" target="#b66">[67]</ref>, which boosts the performance from 75.8% to 79.8%. STMN fi m to link the per-frame detection boxes of our SSVD into tracklets. The mAP track of SSVD + Viterbi <ref type="bibr" target="#b66">[67]</ref> is 60.2%, which is higher than 57.0% of DorT <ref type="bi
ef type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> for recognition, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> focus on learni
" target="#b23">[24]</ref>, <ref type="bibr" target="#b44">[45]</ref> which capitalize on FlowNet-s <ref type="bibr" target="#b58">[59]</ref> to produce optical flow, PWC-Net <ref type="bibr" target="
" target="#b23">[24]</ref>, <ref type="bibr" target="#b44">[45]</ref> which capitalize on FlowNet-s <ref type="bibr" target="#b58">[59]</ref> to produce optical flow, PWC-Net <ref type="bibr" target="
tection in images <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" t type="bibr" target="#b4">[5]</ref>, remarkable progresses have been witnessed for object detection <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" at the first stage. Compared to the costly per-region classification subnet in Faster R-CNN, R-FCN <ref type="bibr" target="#b7">[8]</ref> capitalizes on the fully convolutional network with position ailure case is due to that SSVD leaves the relations between objects Methods Backbone mAP (%) R-FCN <ref type="bibr" target="#b7">[8]</ref> ResNet-101 73.6 DorT <ref type="bibr" target="#b42">[43]</re n end-to-end fashion without any postprocessing. Among them, the former two approaches (i.e., R-FCN <ref type="bibr" target="#b7">[8]</ref> and Faster R-CNN <ref type="bibr" target="#b12">[13]</ref>)
ef type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> for recognition, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> focus on learni
">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. In particular, one common deep solution for object detect
ef type="bibr" target="#b34">[35]</ref> for recognition, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> focus on learning robust and domaininvariant detectors bas
l aggregation, enabling an end-toend detection paradigm. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b44">[45]</ref> calibrate a sequence of per-frame feature maps with the gu ike the works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b44">[45]</ref> which capitalize on FlowNet-s <ref type="bibr" target="#b5 here we exclude several video object detection methods ( <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b44">[45]</ref>) which are additionally equipped with the acceleration tec
">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bib ased on a two-stage paradigm, i.e., first perform region proposal and then do classification. R-CNN <ref type="bibr" target="#b25">[26]</ref> is one of first attempts that tackles object detection pro ype="bibr" target="#b32">[33]</ref> and Fast R-CNN <ref type="bibr" target="#b24">[25]</ref> extend <ref type="bibr" target="#b25">[26]</ref> by devising SPP pooling or ROI pooling to enable the shari
b23">[24]</ref> ResNet-101 76.3 LWDN <ref type="bibr" target="#b63">[64]</ref> ResNet-101 76.3 PSLA <ref type="bibr" target="#b64">[65]</ref> ResNet-101 77.1 MANet <ref type="bibr" target="#b20">[21]<
or object detection in videos. One common solution for video object detection is box-level tracking <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bib arch on object detection in videos has proceeded along two different directions: box-level tracking <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bib sals into tubelets for re-scoring, which further improves the robustness of video object detection. <ref type="bibr" target="#b14">[15]</ref> extends R-FCN with a tracking module for simultaneous dete annotations of testing videos are not publicly available, we follow the widely adopted protocols in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bib ataset are a subset of 200 classes of ImageNet object detection (DET) dataset. Therefore, we follow <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b23">[24]</ref> and train SSVD et-101 73.9 Faster R-CNN <ref type="bibr" target="#b12">[13]</ref> ResNet-101 75.4 D (&amp; T loss) <ref type="bibr" target="#b14">[15]</ref> ResNet-101 75.8 FGFA <ref type="bibr" target="#b23">[24]</ presentative twostage image object detectors only exploit single frame information. D (&amp;T loss) <ref type="bibr" target="#b14">[15]</ref> exhibits better performances than the two image object det f> DeepID+Craft 73.8 FGFA <ref type="bibr" target="#b23">[24]</ref> ResNet-101 78.4 D&amp;T (Ï = 1) <ref type="bibr" target="#b14">[15]</ref> ResNet-101 79.8 STSN <ref type="bibr" target="#b18">[19]</
"#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bib object detection <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bib r, they trail in accuracy compared to two-stage ones. To match the performance of two-stage models, <ref type="bibr" target="#b10">[11]</ref> presents an effective dense one-stage detector, RetinaNet, ream is injected into class/box subnets, which simultaneously classify anchor boxes with Focal Loss <ref type="bibr" target="#b10">[11]</ref> (L FL ) and regress from anchor boxes to ground-truth obje t anchors are assigned to P3, P4, P5, P6 with anchor areas ranging from 32 2 to 256 2 . As in [56], <ref type="bibr" target="#b10">[11]</ref>, each pyramid layer is associated with anchors at three as
ave tried to predicate only those instances of H2P branches which have low confidence of prediction <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" >[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Policies like Diverge Merge Processor (DMP) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref> use careful compi control flow convergence using generic patterns of convergence. This is unlike previous approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" paths can converge to some later point in the program (using the same convergence criterion as DMP <ref type="bibr" target="#b6">[7]</ref>). Loops are naturally converging and contribute to another 1 namically applied predication only on branch instances having low confidence from branch prediction <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" ofitably) and fetches both the directions of the hammock in hardware. Diverge Merge Processor (DMP) <ref type="bibr" target="#b6">[7]</ref> improves upon both Wish Branches and DHP. DMP uses compiler ce register or flags (like stores or branches), instantly releases its resources.</p><p>Prior works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref> have relied on se er transparency without resorting to complex RAT recovery mechanisms or re-execution as proposed in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>3) Predica evaluate this trade-off for ACB.  In this section, we compare against Diverge-Merge Processor (DMP) <ref type="bibr" target="#b6">[7]</ref>, which relies on changes to the compiler, ISA and micro-arch e="bibr" target="#b17">[18]</ref> but due to large overheads, the realistic benefits are diminished <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Wish Branches <r target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref>. Diverge-Merge Processor (DMP) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref> uses branch predi
tion of the control flow removing the need for branch prediction. Store-Load-Branch (SLB) Predictor <ref type="bibr" target="#b42">[43]</ref> is an adjunct branch predictor which improves accuracy by
ing to predicate frequently occurring basic blocks. Generalized multipath execution was proposed in <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref>. Diverge-Merge
rends with scaling of OOO processor. The 1X point is similar in parameters to the Skylake processor <ref type="bibr" target="#b0">[1]</ref>. Performance potential for future processors is bound by the d width 1 . For these results, the baseline is similar in parameters to the Intel Skylake processor <ref type="bibr" target="#b0">[1]</ref> and uses a branch predictor similar to TAGE <ref type="bibr" storage, ACB delivers 8% performance improvement over a baseline processor similar to Intel Skylake <ref type="bibr" target="#b0">[1]</ref>. Since ACB requires little additional hardware and saves 22% ions. Simulated core runs at 3.2 GHz and micro-architecture parameters are similar to Intel Skylake <ref type="bibr" target="#b0">[1]</ref> configuration. Detailed parameters enlisted in Table <ref ty h is implementable in modern OOO processors. Specifically, we make the following new contributions. <ref type="bibr" target="#b0">1)</ref> We present an analysis of the fundamental cost-benefit trade-
rlbench, bzip2, gcc, mcf, gobmk, hmmer, sjeng, libquantum, h264ref, omnetpp, astar, xalancbmk ISPEC <ref type="bibr" target="#b20">[21]</ref> bwaves, gamess, milc, zeusmp soplex, povray, calculix, gem vray, calculix, gemsfdtd, tonto, lbm, wrf, sphinx3 gromacs, cactusADM, leslie3D, namd, deall, FSPEC <ref type="bibr" target="#b20">[21]</ref> catcubssn, lbm, cam4, pop2, imagick, nab, roms, perlbench,
es of H2P branches which have low confidence of prediction <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Policies like lly. However, this approach increases the compiled code footprint. Dynamic Hammock Predication (DHP <ref type="bibr" target="#b10">[11]</ref>) uses the compiler to identify simple, short hammocks whic ches), instantly releases its resources.</p><p>Prior works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref> have relied on select-micro-op based approaches to handle lex RAT recovery mechanisms or re-execution as proposed in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>3) Predicated-False Path Loads/Stores: All ACB body egory D, and reduces mispredictions over baseline. A similar observation was made by Klauser et al. <ref type="bibr" target="#b10">[11]</ref> for branch history update and dynamic predication. Interes tling mechanism like Dynamo is needed for such cases.</p><p>Comparison against DHP: Unlike DMP, DHP <ref type="bibr" target="#b10">[11]</ref> performs predication only on simple and short hammocks, ta but applies predication dynamically only on less predictable instances. Dynamic Hammock Predication <ref type="bibr" target="#b10">[11]</ref> targets only small, simple hammocks. Hyperblock predicatio
blocks. Generalized multipath execution was proposed in <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref>. Diverge-Merge Processor (DMP) <ref type="bibr" target="#b
rlbench, bzip2, gcc, mcf, gobmk, hmmer, sjeng, libquantum, h264ref, omnetpp, astar, xalancbmk ISPEC <ref type="bibr" target="#b20">[21]</ref> bwaves, gamess, milc, zeusmp soplex, povray, calculix, gem vray, calculix, gemsfdtd, tonto, lbm, wrf, sphinx3 gromacs, cactusADM, leslie3D, namd, deall, FSPEC <ref type="bibr" target="#b20">[21]</ref> catcubssn, lbm, cam4, pop2, imagick, nab, roms, perlbench,
ware predication has been studied extensively in the past <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Popular ISAs s
use program history to predict future outcomes of a branch <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Decades of researc
ibr" target="#b24">[25]</ref>, compression, 3dmark <ref type="bibr" target="#b25">[26]</ref>, eembc <ref type="bibr" target="#b26">[27]</ref>, chrome Client lammps <ref type="bibr" target="#b27">[28]<
bibr" target="#b17">18]</ref>. Learning on such data is possible using graph neural networks (GNNs) <ref type="bibr" target="#b25">[26]</ref> that typically operate by a message passing mechanism <ref
]</ref> to dynamic graphs by ignoring the temporal evolution, this has been shown to be sub-optimal <ref type="bibr" target="#b65">[66]</ref>, and in some cases, it is the dynamic structure that conta iN (t N ) ?(t -t N )].<label>(9)</label></formula><p>Here, ?(?) represents a generic time encoding <ref type="bibr" target="#b65">[66]</ref>, is the concatenation operator and</p><formula xml:id="for ated information. Differently from the original formulation of this layer (firstly proposed in TGAT <ref type="bibr" target="#b65">[66]</ref>) where no node-wise temporal features were used, in our ca arget="#b35">[36]</ref> uses the time projection embedding module emb(i, t) = (1+?tw)?s i (t). TGAT <ref type="bibr" target="#b65">[66]</ref> is a specific case of TGN when the memory and its related ils are provided in the supplementary material.</p><p>Tasks. Our experimental setup closely follows <ref type="bibr" target="#b65">[66]</ref> and focuses on the tasks of future edge prediction and dyn setting is used for node classification. We perform the same 70%-15%-15% chronological split as in <ref type="bibr" target="#b65">[66]</ref>.</p><p>Future Edge Prediction. The goal is to predict the ref type="bibr" target="#b46">[47]</ref>, Jodie <ref type="bibr" target="#b35">[36]</ref>, and TGAT <ref type="bibr" target="#b65">[66]</ref>) as well as state-of-the-art models for static graphs (GAE <ref type="bibr" target="#b26">[27]</ref>, CTDNE <ref type="bibr" target="#b46">[47]</ref> and TGAT <ref type="bibr" target="#b65">[66]</ref> are taken directly from the TGAT paper <ref type="bibr" ta 47]</ref> and TGAT <ref type="bibr" target="#b65">[66]</ref> are taken directly from the TGAT paper <ref type="bibr" target="#b65">[66]</ref>.</p><p>For Jodie <ref type="bibr" target="#b35">[36]</ref> and datasets we used the same hyperparameters, which had been found to work well in the TGAT paper <ref type="bibr" target="#b65">[66]</ref>.</p></div>			</div> 			<div type="references">  				<listB
, or learned by imposing a smoothness constraint over time <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" tar
br" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref>, in particular, social sciences <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b42">43]</ref> and biology <ref t
br" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref>, in particular, social sciences <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b42">43]</ref> and biology <ref t
="bibr" target="#b73">74]</ref>, fit to time series models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" tar
et="#b29">[30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">44]</ref>, used as components in RNNs <ref type="bibr" target="#b54">
tion, e.g. a recurrent neural network such as LSTM <ref type="bibr" target="#b28">[29]</ref> or GRU <ref type="bibr" target="#b8">[9]</ref>.</p><p>Embedding. The embedding module is used to generate t
seen during training <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b35">36]</ref>. Such approaches ar proaches for CTDGs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b37">38]</ref> use RNNs to update
ng. Graphs are ubiquitously used as models for systems of relations and interactions in many fields <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" targ
cal interactomes are dynamic. While it is often possible to apply static graph deep learning models <ref type="bibr" target="#b36">[37]</ref> to dynamic graphs by ignoring the temporal evolution, this tting of discrete-time dynamic graphs represented as a sequence of snapshots of the graph over time <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" ta amic Graphs (DTDG)s. Such approaches either aggregate graph snapshots and then apply static methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" ta
.org/ns/1.0"><head n="1">Introduction</head><p>In the past few years, graph representation learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" targ >For additional background, we refer the reader to surveys on general graph representation learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" targ
1">2]</ref>, assemble snapshots into tensors and factorize <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b38">39]</ref>, or encode each sna
els for systems of relations and interactions in many fields <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" tar
get="#b68">69,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b70">71]</ref>. Few approaches support the inductive setting of generalizi get="#b13">14,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b70">71]</ref>.</p><p>Only recently have Continuous Time Dynamic Graphs (C
</ref><ref type="bibr" target="#b42">43]</ref> and biology <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b17">18]</ref>. Learning on such d
bsequent snapshots <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" tar
type="bibr" target="#b33">[34]</ref>, DeepWalk <ref type="bibr" target="#b50">[51]</ref>, Node2Vec <ref type="bibr" target="#b22">[23]</ref>, GAT <ref type="bibr" target="#b60">[61]</ref> and GraphSA type="bibr" target="#b33">[34]</ref>, DeepWalk <ref type="bibr" target="#b50">[51]</ref>, Node2Vec <ref type="bibr" target="#b22">[23]</ref>, GAT <ref type="bibr" target="#b60">[61]</ref> and GraphSA
i (t) = emb(i, t) = h (L) i (t).</formula><p>Each layer amounts to performing multi-head-attention <ref type="bibr" target="#b59">[60]</ref> where the query (q (l) (t)) is a reference node (i.e. the
et="#b36">[37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" tar arget="#b40">41,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b47">48]</ref>, or learned by impo
rget="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref>, in particular, soc
et="#b29">[30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">44]</ref>, used as components in RNNs <ref type="bibr" target="#b54">
e.g., paper network <ref type="bibr" target="#b21">(Zhang et al. 2018)</ref>, paper-author network <ref type="bibr" target="#b20">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature
Some following literature modifies the framework for the purpose of the adversarial training. IRGAN <ref type="bibr" target="#b18">(Wang et al. 2017)</ref>  </p></div> <div xmlns="http://www.tei-c.org ive samples iteratively. And to make the generative module aware of relation information, following <ref type="bibr" target="#b18">(Wang et al. 2018a)</ref>, we design a random walk based generating s name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type="bibr" target="#b18">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655
thor. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type="bibr" target="#b21">(Zhang et al. 2018)</ref>, paper-author network <ref type="bibr" targ for experiments: â¢ AMiner-AND<ref type="foot" target="#foot_0">1</ref> . The dataset is released by <ref type="bibr" target="#b21">(Zhang et al. 2018)</ref>, which contains 500 author names for traini (Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type="bibr" target="#b21">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of me fully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type="bibr" target="#b21">(Zhang et al. 2018</ref>): This model designs a supervised global sta D, we use 100 names for testing and compare the result with the results of other models reported in <ref type="bibr" target="#b21">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample
resent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type="bibr" target="#b5">(Grover and Leskovec 2016)</ref> to represent these features by v i â
resent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type="bibr" target="#b5">(Grover and Leskovec 2016)</ref> to represent these features by v i â
Denton et al. 2015)</ref>, sequence <ref type="bibr" target="#b19">(Yu et al. 2017)</ref>, dialogue <ref type="bibr" target="#b11">(Li et al. 2017)</ref>. Some following literature modifies the framew
ion. Original purpose of GAN is to generate data from the underlying true distribution, e.g., image <ref type="bibr" target="#b2">(Denton et al. 2015)</ref>, sequence <ref type="bibr" target="#b19">(Y
me literature explores NRL on heterogeneous networks <ref type="bibr">(Tang, Qu, and Mei 2015;</ref><ref type="bibr" target="#b3">Dong, Chawla, and Swami 2017)</ref>. However, existing algorithms are
ncludes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type="bibr" target="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, a tegories according to the information they focus on. The first are based on the content information <ref type="bibr" target="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, a

g true distribution, e.g., image <ref type="bibr" target="#b2">(Denton et al. 2015)</ref>, sequence <ref type="bibr" target="#b19">(Yu et al. 2017)</ref>, dialogue <ref type="bibr" target="#b11">(Li e

resent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type="bibr" target="#b5">(Grover and Leskovec 2016)</ref> to represent these features by v i â
Denton et al. 2015)</ref>, sequence <ref type="bibr" target="#b19">(Yu et al. 2017)</ref>, dialogue <ref type="bibr" target="#b11">(Li et al. 2017)</ref>. Some following literature modifies the framew

les and complicated feature engineering to some extent. Inspired by generative adversarial networks <ref type="bibr" target="#b4">(Goodfellow et al. 2014)</ref>, we may combine the two categories in a adversarial framework. Generative Adversarial Networks. Recently, generative adversarial nets (GAN) <ref type="bibr" target="#b4">(Goodfellow et al. 2014</ref>) has attracted a great deal of attention





ey cannot measure the high-order connections among papers. Methods focusing on relation information <ref type="bibr" target="#b9">(Kanani, McCallum, and Pal 2007;</ref><ref type="bibr" target="#b0">Be
c. Methods focusing on the content information <ref type="bibr" target="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, and Giles 2006;</ref><ref type="bibr">Yoshida et al. 2 The first are based on the content information <ref type="bibr" target="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, and Giles 2006;</ref><ref type="bibr" target="#b12">Lo
les and complicated feature engineering to some extent. Inspired by generative adversarial networks <ref type="bibr" target="#b4">(Goodfellow et al. 2014)</ref>, we may combine the two categories in a adversarial framework. Generative Adversarial Networks. Recently, generative adversarial nets (GAN) <ref type="bibr" target="#b4">(Goodfellow et al. 2014</ref>) has attracted a great deal of attention
ocusing on relation information <ref type="bibr" target="#b9">(Kanani, McCallum, and Pal 2007;</ref><ref type="bibr" target="#b0">Bekkerman and McCallum 2005)</ref> usually solve the problem on the bi

g true distribution, e.g., image <ref type="bibr" target="#b2">(Denton et al. 2015)</ref>, sequence <ref type="bibr" target="#b19">(Yu et al. 2017)</ref>, dialogue <ref type="bibr" target="#b11">(Li e
les and complicated feature engineering to some extent. Inspired by generative adversarial networks <ref type="bibr" target="#b4">(Goodfellow et al. 2014)</ref>, we may combine the two categories in a adversarial framework. Generative Adversarial Networks. Recently, generative adversarial nets (GAN) <ref type="bibr" target="#b4">(Goodfellow et al. 2014</ref>) has attracted a great deal of attention
ncludes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type="bibr" target="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, a tegories according to the information they focus on. The first are based on the content information <ref type="bibr" target="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, a
get="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, and Giles 2006;</ref><ref type="bibr" target="#b12">Louppe et al. 2016;</ref><ref type="bibr">Yoshida et al. 2010)</ref>, network embedding is learned with an aim to preserve the connectivity of the constructed networks. <ref type="bibr" target="#b12">Louppe et al. (2016)</ref>: This model trains a function to measure t

ncludes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type="bibr" target="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, a tegories according to the information they focus on. The first are based on the content information <ref type="bibr" target="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, a

ssing pipeline implementations.</p><p>In recent years, the Halide image processing language [Ragan- <ref type="bibr" target="#b12">Kelley et al. 2012;</ref><ref type="bibr" target="#b12">Ragan-Kelley he Halide image processing language [Ragan- <ref type="bibr" target="#b12">Kelley et al. 2012;</ref><ref type="bibr" target="#b12">Ragan-Kelley et al. 2013</ref>] has proven to be an effective system nt efforts to automatically generate efficient image processing pipelines from high-level programs. <ref type="bibr" target="#b12">Ragan-Kelley et al. [2013]</ref> employed auto-tuning guided by genet uthoring efficient schedules. We assume familiarity with the Halide system, and refer the reader to <ref type="bibr" target="#b12">[Ragan-Kelley et al. 2012;</ref><ref type="bibr" target="#b12">Ragan- processing, and computer vision workloads. Eight of the benchmarks are drawn from public literature <ref type="bibr" target="#b12">[Ragan-Kelley et al. 2012;</ref><ref type="bibr" target="#b12">Ragan- lide system, and refer the reader to <ref type="bibr" target="#b12">[Ragan-Kelley et al. 2012;</ref><ref type="bibr" target="#b12">Ragan-Kelley et al. 2013]</ref> for a comprehensive description of th rks are drawn from public literature <ref type="bibr" target="#b12">[Ragan-Kelley et al. 2012;</ref><ref type="bibr" target="#b12">Ragan-Kelley et al. 2013;</ref><ref type="bibr" target="#b13">Ragan-K
ty schedules. A more general Halide auto-tuner was later implemented within the OpenTuner framework <ref type="bibr" target="#b2">[Ansel et al. 2014]</ref>. This system was able to find efficient sche


urther extend pipeline scheduling capabilities to multi-resolution operations and dynamic filtering <ref type="bibr" target="#b8">[Hegarty et al. 2016</ref>], but in doing so it sacrifices fully autom

//www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>in a deep neural network<ref type="bibr" target="#b10">[Krizhevsky et al. 2012</ref>] (DNN). The layer evaluates a large fil
blending, and multi-scale interpolation is five to ten times slower than hand-tuned implementations <ref type="bibr" target="#b11">[Mullapudi et al. 2015]</ref>.</p><p>While auto-tuning may seem like that stochastic auto-tuning systems struggled to converge quickly (or at all) on complex pipelines <ref type="bibr" target="#b11">[Mullapudi et al. 2015]</ref>, we implemented a simple, bruteforce au egarty et al. 2016</ref>], but in doing so it sacrifices fully automatic scheduling.</p><p>PolyMage <ref type="bibr" target="#b11">[Mullapudi et al. 2015</ref>] extends polyhedral analysis techniques or our more complex benchmarks.</p><p>PolyMage. We approximate the scheduling behavior of Poly-Mage <ref type="bibr" target="#b11">[Mullapudi et al. 2015</ref>] by restricting the auto-scheduler to ti orderings for a group, to make exploration of the optimization space tractable, our system follows <ref type="bibr" target="#b11">Mullapudi et al. [2015]</ref> and only considers a narrower space sch the loop nest of the output function. The iterative grouping process is similar to that employed by <ref type="bibr" target="#b11">Mullapudi et al. [2015]</ref>. However, while their work makes groupi
/p><p>? Dense matrix-matrix multiplication (MATMUL). Fast bilateral filter using the bilateral grid <ref type="bibr" target="#b3">[Chen et al. 2007</ref>]. Constructs the grid using a histogram reduct
blending, and multi-scale interpolation is five to ten times slower than hand-tuned implementations <ref type="bibr" target="#b11">[Mullapudi et al. 2015]</ref>.</p><p>While auto-tuning may seem like that stochastic auto-tuning systems struggled to converge quickly (or at all) on complex pipelines <ref type="bibr" target="#b11">[Mullapudi et al. 2015]</ref>, we implemented a simple, bruteforce au egarty et al. 2016</ref>], but in doing so it sacrifices fully automatic scheduling.</p><p>PolyMage <ref type="bibr" target="#b11">[Mullapudi et al. 2015</ref>] extends polyhedral analysis techniques or our more complex benchmarks.</p><p>PolyMage. We approximate the scheduling behavior of Poly-Mage <ref type="bibr" target="#b11">[Mullapudi et al. 2015</ref>] by restricting the auto-scheduler to ti orderings for a group, to make exploration of the optimization space tractable, our system follows <ref type="bibr" target="#b11">Mullapudi et al. [2015]</ref> and only considers a narrower space sch the loop nest of the output function. The iterative grouping process is similar to that employed by <ref type="bibr" target="#b11">Mullapudi et al. [2015]</ref>. However, while their work makes groupi

of a single device.</p><p>Recently, pipeline parallelism <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref> has been proposed as a promising approach for training lar een those nodes during backpropagation. The other category is asynchronous(async) pipeline training <ref type="bibr" target="#b11">[12]</ref>. This manner inserts mini-batches into pipeline continuous lism, and hybrid approaches combining both. Current state-of-theart pipeline partitioning algorithm <ref type="bibr" target="#b11">[12]</ref> is not able to be applied to synchronous training effectiv evice assignment affects communication efficiency and computing resource utilization. Previous work <ref type="bibr" target="#b11">[12]</ref> uses hierarchical planning and works well for asynchronous D. Contributions over previous work</head><p>Previous works on pipeline planning includes PipeDream <ref type="bibr" target="#b11">[12]</ref> (for asynchronous training) and torchgpipe <ref type="bibr it the micro-batch further into 2 even slices, and assign each to a device. An alternative approach <ref type="bibr" target="#b11">[12]</ref> (Fig. <ref type="figure" target="#fig_6">8(b)</ref>) is no
optimizing pipeline parallelism for synchronous training <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. This approach requires necessary gradients synchronizatio llelism.</p><p>Pipeline parallelism. Pipeline Parallelism <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bib
omputational intensive. It is a common practice to parallelize training by leveraging multiple GPUs <ref type="bibr" target="#b37">[39]</ref>- <ref type="bibr" target="#b40">[42]</ref>. Data paralleli buted training <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b37">[39]</ref>, <ref type="bibr" target="#b38">[40]</ref>, <ref type="bib
#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b19">[20]</ref> are unable to achieve satisfactory efficiency, or the mode teration computation/communication overlap between backward computation and gradients communication <ref type="bibr" target="#b19">[20]</ref>.</p><p>Overall analysis across these five models from Fig. ap, with promising results reported in some CNN benchmarks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Model Parallelism. Model Parallelism <ref type="bibr" tar
#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b19">[20]</ref> are unable to achieve satisfactory efficiency, or the mode teration computation/communication overlap between backward computation and gradients communication <ref type="bibr" target="#b19">[20]</ref>.</p><p>Overall analysis across these five models from Fig. ap, with promising results reported in some CNN benchmarks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Model Parallelism. Model Parallelism <ref type="bibr" tar
">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b37">[39]</ref>, <ref type="bibr" target="#b38">[40]</ref>, <ref type="bibr" target="#b49">[51]</ref>- <ref type="bib <ref type="bibr" target="#b54">[56]</ref> make further optimization based on PipeDream. Pal et al. <ref type="bibr" target="#b38">[40]</ref> evaluated the hybrid approach without thorough study. Some
tice to parallelize training by leveraging multiple GPUs <ref type="bibr" target="#b37">[39]</ref>- <ref type="bibr" target="#b40">[42]</ref>. Data parallelism, model parallelism and pipeline parallel
LP <ref type="bibr" target="#b1">[2]</ref>, Internet scale E-commerce search/recommendation systems <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>) have billions of p
">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b53">[55]</ref> has been recently proposed to train DNN in a pipelined man data and pipeline parallelism for asynchronous training. <ref type="bibr" target="#b51">[53]</ref>, <ref type="bibr" target="#b53">[55]</ref>, <ref type="bibr" target="#b54">[56]</ref> make further op
#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b19">[20]</ref> are unable to achieve satisfactory efficiency, or the mode teration computation/communication overlap between backward computation and gradients communication <ref type="bibr" target="#b19">[20]</ref>.</p><p>Overall analysis across these five models from Fig. ap, with promising results reported in some CNN benchmarks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Model Parallelism. Model Parallelism <ref type="bibr" tar
>, Internet scale E-commerce search/recommendation systems <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>) have billions of parameters, demanding tens to hundreds of
ing new samples, etc. Two of the most popular methods are Generative Adversarial Networks (GANs) by <ref type="bibr" target="#b2">Goodfellow et al. (2014)</ref> and Variational 2.z combines with x for

iques include Linear Regression by <ref type="bibr" target="#b6">Karlic et al. (2010)</ref>, SVM by <ref type="bibr" target="#b0">Cheng et al. (2011)</ref>, Random Forests by <ref type="bibr" target=" model the same problem as a regression task that includes SVR (Support Vector Regression) model by <ref type="bibr" target="#b0">Cheng et al. (2011)</ref> and DeepDIFF (Attention Based). These models
al. (2010)</ref>, SVM by <ref type="bibr" target="#b0">Cheng et al. (2011)</ref>, Random Forests by <ref type="bibr" target="#b1">Dong et al. (2012)</ref>, Rule-Based Learning by <ref type="bibr" targ

ion task have applied a slew of different techniques. These techniques include Linear Regression by <ref type="bibr" target="#b6">Karlic et al. (2010)</ref>, SVM by <ref type="bibr" target="#b0">Cheng

ion task have applied a slew of different techniques. These techniques include Linear Regression by <ref type="bibr" target="#b6">Karlic et al. (2010)</ref>, SVM by <ref type="bibr" target="#b0">Cheng
ion task have applied a slew of different techniques. These techniques include Linear Regression by <ref type="bibr" target="#b6">Karlic et al. (2010)</ref>, SVM by <ref type="bibr" target="#b0">Cheng
echniques in literature, specifically by <ref type="bibr" target="#b11">Scholz et al. (2008)</ref>; <ref type="bibr" target="#b7">Kasun et al. (2016)</ref>, such as Principle Component Analysis (PCA),
g and thus may converge to a better model. Beside, multiple embedding modules were also proposed by <ref type="bibr" target="#b4">Guo et al. (2020b)</ref> to learn DNA representations which was shown

research area, the studies using deep learning approaches mostly focus on dimensionality reduction <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b5">[6]</ref> and anomaly-based i
nown, multi-layer and radial basis function networks are used to classify the attack. Jadidi et al. <ref type="bibr" target="#b19">[20]</ref> proposed a method that was based on Multi-Layer Perceptron
Netflow records into kernel function and forwarding the calculated results to an OCSVM. Umer et al. <ref type="bibr" target="#b25">[26]</ref> proposed an intrusion detection model, which handled the f
ification, the results can be separated into four groups <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> </p><p>Receiver Operating Characteristics (ROC): The ROC c
which is a revised version of KDDCUP99, is used for evaluating the methods proposed in the studies <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" t
bsets of data in each sub-space, was employed to take apart the malicious flows. Hosseinpour et al. <ref type="bibr" target="#b30">[31]</ref> proposed a distributed IDS based on unsupervised clusterin
large datasets and real-valued outputs <ref type="bibr" target="#b63">[64]</ref>. Nesterovs updater <ref type="bibr" target="#b66">[67]</ref> is selected because it uses the momentum that supports the
located around hosts of the networks. The Ward Clustering approach was recommended by Satoh et al. <ref type="bibr" target="#b31">[32]</ref> to identify simple and malicious attacks from the SSH dict
f type="bibr" target="#b56">[57]</ref>, CTU-13 <ref type="bibr" target="#b57">[58]</ref>, UNSW-NB15 <ref type="bibr" target="#b58">[59]</ref>, CIDDS-001 <ref type="bibr" target="#b59">[60]</ref> and C
et="#b44">[45]</ref>.</p><p>The SVM method is modified into a One-Class SVM (OCSVM) as explained in <ref type="bibr" target="#b45">[46]</ref>. The dataset, which is given as an input to the algorithm tliers'', are employed as negative examples. The formulation of OCSVM can be carried out as follows <ref type="bibr" target="#b45">[46]</ref>:</p><formula xml:id="formula_5">f (x) = +1, if x ? S -1, i
ed application of different DT-based classification algorithms for intrusion detection. Zhao et al. <ref type="bibr" target="#b33">[34]</ref> proposed a simple and efficient flow-based approach to ide
nly used and constitute the basis of the studies such as <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b68">[69]</ref>, the sigmoid activation function is used, except for the o
is the method preferred as it provides results in lower false positive rates and higher accuracies <ref type="bibr" target="#b72">[74]</ref>. Winter et al. <ref type="bibr" target="#b23">[24]</ref> p
false positive rates and higher accuracies <ref type="bibr" target="#b72">[74]</ref>. Winter et al. <ref type="bibr" target="#b23">[24]</ref> proposed an inductive network IDS, which functioned on net
mmon flow attributes and TCF flags attributes, extracted from the datasets. Stevanovic and Pedersen <ref type="bibr" target="#b35">[36]</ref> presented an effective botnet detection approach using flo
ion <ref type="bibr" target="#b64">[65]</ref> is set to the commonly used value of 0.001 and Xavier <ref type="bibr" target="#b65">[66]</ref> is used as weight initialization method, which is the defa
g an appropriate metric. For a binary classification, the results can be separated into four groups <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> </p><p>Receiver
method.</p><p>The statistical methods used in literature can be summarized as follows. Kanda et al. <ref type="bibr" target="#b36">[37]</ref> suggested an anomaly detection method, called ADMIRE, base
-based intrusion detection can be found in studies of <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref>.</p><p>The objective of ANNs is to model the human brain,
tent-based features. b) Dataset used is very outdated and does not reflect the real network traffic <ref type="bibr" target="#b13">[14]</ref>.</p><p>As the flow-based data contains less information on not addressed by NSL-KDD, is that this dataset is not a realistic representation of network traffic <ref type="bibr" target="#b13">[14]</ref>. Additionally, although these datasets include some flow-b
false positive rates and higher accuracies <ref type="bibr" target="#b72">[74]</ref>. Winter et al. <ref type="bibr" target="#b23">[24]</ref> proposed an inductive network IDS, which functioned on net
uch as gender, race, and religion.</p><p>More recently, sentence-level representations such as ELMo <ref type="bibr" target="#b31">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b10">(Dev popular sentence encoders BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and ELMo <ref type="bibr" target="#b31">(Peters et al., 2018)</ref>, showing that our approach reduces the bi foot" target="#foot_2">2</ref>  <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and ELMo <ref type="bibr" target="#b31">(Peters et al., 2018)</ref>. Note that the pre-trained BERT encoder m
biasing and require changing the data or underlying word embeddings and retraining which is costly. <ref type="bibr" target="#b7">Bordia and Bowman (2019)</ref> only study word-level language models a
d Bowman (2019)</ref> only study word-level language models and also requires re-training. Finally, <ref type="bibr" target="#b16">Kurita et al. (2019)</ref> only measure bias on BERT by extending the
., 2019;</ref><ref type="bibr" target="#b42">Wang et al., 2019)</ref> and multimodal human language <ref type="bibr" target="#b20">(Liang et al., 2018</ref><ref type="bibr" target="#b19">(Liang et al.
pace V = {v 1 , ..., v k } is given by the first k components of principal component analysis (PCA) <ref type="bibr" target="#b0">(Abdi and Williams, 2010)</ref>:</p><formula xml:id="formula_6">V = PC

, 2019)</ref> and multimodal human language <ref type="bibr" target="#b20">(Liang et al., 2018</ref><ref type="bibr" target="#b19">(Liang et al., , 2019))</ref>. Table <ref type="table" target="#tab_4
presence of bias in sentence representations <ref type="bibr" target="#b24">(May et al., 2019;</ref><ref type="bibr" target="#b4">Basta et al., 2019)</ref>, none of them have been able to successfully
, 2019)</ref> and multimodal human language <ref type="bibr" target="#b20">(Liang et al., 2018</ref><ref type="bibr" target="#b19">(Liang et al., , 2019))</ref>. Table <ref type="table" target="#tab_4
and documents from large amounts of data <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b27">Mikolov et al., 2013)</ref>. Although word-level embeddings <ref type /ref>. Although word-level embeddings <ref type="bibr" target="#b30">(Pennington et al., 2014;</ref><ref type="bibr" target="#b27">Mikolov et al., 2013)</ref> are highly informative features useful fo mbeddings such as GloVe <ref type="bibr" target="#b30">(Pennington et al., 2014)</ref> and word2vec <ref type="bibr" target="#b27">(Mikolov et al., 2013)</ref> which can be retrained on a single machi
esentations of input information such as words, sentences, and documents from large amounts of data <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b27">Mikolov et level representations such as ELMo <ref type="bibr" target="#b31">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, and GPT <ref type="bibr" target="#b33">( ch can be retrained on a single machine within a few hours, the best sentence encoders such as BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, and GPT <ref type="bibr" target="#b33">( entence representations. Our experiments are performed on two widely popular sentence encoders BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and ELMo <ref type="bibr" target="#b31">( ng SENT-DEBIAS on two widely-used sentence encoders: BERT<ref type="foot" target="#foot_2">2</ref>  <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and ELMo <ref type="bibr" target="#b31">( ent <ref type="bibr" target="#b43">(Warstadt et al., 2018)</ref>. It is also possible to apply BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> on downstream tasks that involve two sent s="http://www.tei-c.org/ns/1.0"><head>Debiasing Method</head><p>Ave. Abs. Effect Size BERT original <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> +0.354 FastText <ref type="bibr" target="
pace V = {v 1 , ..., v k } is given by the first k components of principal component analysis (PCA) <ref type="bibr" target="#b0">(Abdi and Williams, 2010)</ref>:</p><formula xml:id="formula_6">V = PC
target="#b47">Zhao et al. (2019)</ref>, <ref type="bibr" target="#b28">Park et al. (2018), and</ref><ref type="bibr" target="#b11">Garg et al. (2019)</ref> are not able to perform post-hoc debiasing a

infeasible (in standard settings) to completely retrain these large sentence encoders for debiasing <ref type="bibr" target="#b49">(Zhao et al., 2018;</ref><ref type="bibr" target="#b46">Zhang et al.,
ltiple tasks in NLP <ref type="bibr" target="#b44">(Wu and Dredze, 2019)</ref>, multimodal learning <ref type="bibr" target="#b45">(Zellers et al., 2019;</ref><ref type="bibr">Sun et al., 2019a)</ref>
lysis datasets have labels that correlate with gender information and therefore contain gender bias <ref type="bibr" target="#b15">(Kiritchenko and Mohammad, 2018)</ref>. As a result, we do expect pos
., 2019;</ref><ref type="bibr" target="#b42">Wang et al., 2019)</ref> and multimodal human language <ref type="bibr" target="#b20">(Liang et al., 2018</ref><ref type="bibr" target="#b19">(Liang et al.
get="#b41">(Wang et al., 2018)</ref> which converts the Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b34">(Rajpurkar et al., 2016)</ref> into a binary classification task. The
, 2019)</ref> and multimodal human language <ref type="bibr" target="#b20">(Liang et al., 2018</ref><ref type="bibr" target="#b19">(Liang et al., , 2019))</ref>. Table <ref type="table" target="#tab_4

t al., 2013)</ref> and Corpus of Linguistic Acceptability (CoLA) grammatical acceptability judgment <ref type="bibr" target="#b43">(Warstadt et al., 2018)</ref>. It is also possible to apply BERT <ref
t="#b18">(Lauscher and Glava?, 2019;</ref><ref type="bibr" target="#b8">Caliskan et al., 2017;</ref><ref type="bibr" target="#b38">Swinger et al., 2019;</ref><ref type="bibr" target="#b6">Bolukbasi et
data collected from discussion forums related to politics, electronics, and relationships, 4) MELD <ref type="bibr" target="#b32">(Poria et al., 2019)</ref>, a large-scale multimodal multi-party emot
os such as healthcare <ref type="bibr" target="#b40">(Velupillai et al., 2018)</ref>, legal systems <ref type="bibr" target="#b9">(Dale, 2019)</ref>, and computational social science <ref type="bibr"
infeasible (in standard settings) to completely retrain these large sentence encoders for debiasing <ref type="bibr" target="#b49">(Zhao et al., 2018;</ref><ref type="bibr" target="#b46">Zhang et al.,

t="#b18">(Lauscher and Glava?, 2019;</ref><ref type="bibr" target="#b8">Caliskan et al., 2017;</ref><ref type="bibr" target="#b38">Swinger et al., 2019;</ref><ref type="bibr" target="#b6">Bolukbasi et
, 2019)</ref> and multimodal human language <ref type="bibr" target="#b20">(Liang et al., 2018</ref><ref type="bibr" target="#b19">(Liang et al., , 2019))</ref>. Table <ref type="table" target="#tab_4
word-level representations, these models have achieved better performance on multiple tasks in NLP <ref type="bibr" target="#b44">(Wu and Dredze, 2019)</ref>, multimodal learning <ref type="bibr" tar
d sentence representations. In particular, <ref type="bibr" target="#b47">Zhao et al. (2019)</ref>, <ref type="bibr" target="#b28">Park et al. (2018), and</ref><ref type="bibr" target="#b11">Garg et a

tations for both binary <ref type="bibr" target="#b6">(Bolukbasi et al., 2016)</ref> and multiclass <ref type="bibr" target="#b23">(Manzini et al., 2019)</ref> bias attributes such as gender, race, an , we use the Mean Average Cosine similarity (MAC) metric which extends SEAT to a multiclass setting <ref type="bibr" target="#b23">(Manzini et al., 2019)</ref>. For the binary gender setting, we use w "formula">2017</ref>) row N . The last row measures bias in a multiclass religion setting using MAC <ref type="bibr" target="#b23">(Manzini et al., 2019)</ref> before and after debiasing. MAC score ra efore they are used in downstream tasks <ref type="bibr" target="#b6">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b23">Manzini et al., 2019)</ref>. Secondly, sentences display large variet Caliskan Tests used in <ref type="bibr" target="#b24">May et al. (2019)</ref> with lexicons used by <ref type="bibr" target="#b23">Manzini et al. (2019)</ref>.</p></div> <div xmlns="http://www.tei-c.o
oding. They have been utilized to perform multi-purpose text generation with a single unified model <ref type="bibr" target="#b36">(Radford et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al bility present in large pretrained models <ref type="bibr" target="#b28">(McCann et al., 2018;</ref><ref type="bibr" target="#b36">Radford et al., 2019;</ref><ref type="bibr">Keskar et al., 2019;</ref ng with the vanilla BART model, we also include the zero-shot performance from GPT2 language models <ref type="bibr" target="#b36">(Radford et al., 2019)</ref> (without fine-tuning) as a reference poi

acts important portions of a document <ref type="bibr" target="#b2">(Cheng &amp; Lapata, 2016;</ref><ref type="bibr" target="#b31">Nallapati et al., 2017;</ref><ref type="bibr" target="#b32">Narayan e
"bibr" target="#b39">(Rush et al., 2015;</ref><ref type="bibr" target="#b42">See et al., 2017;</ref><ref type="bibr" target="#b34">Paulus et al., 2018)</ref> which can produce coherent and fluent summ


s we will show in this paper. In contrast, prior work primarily rely on pre-defined "control codes" <ref type="bibr" target="#b6">(Fan et al., 2018;</ref><ref type="bibr" target="#b27">Liu et al., 201 ntity or length as supervision to train the model conditioned on both the code and article together <ref type="bibr" target="#b6">(Fan et al., 2018;</ref><ref type="bibr" target="#b27">Liu et al., 201 100 documents and repeatedly acquire every entity in the document to generate summaries, following <ref type="bibr" target="#b6">Fan et al. (2018)</ref>. Then we compute Success Rate, the fraction of _2">3</ref> shows the Success Rate and factual correctness evaluations. We include the numbers from <ref type="bibr" target="#b6">Fan et al. (2018)</ref> (EntityCode) for reference point. We note that additional information to help generate the reference summary. We also run the LengthCode baseline <ref type="bibr" target="#b6">(Fan et al., 2018)</ref> based on BART, where the ground-truth length that CTRLsum achieves a very high success rate (â¼ 95%) of entity control, compared to previous work <ref type="bibr" target="#b6">(Fan et al., 2018)</ref> which can only succeed 61.2% and 33.8% of the 2seq <ref type="bibr" target="#b8">(Gehring et al., 2017)</ref> with the same hyperparameters as in <ref type="bibr" target="#b6">(Fan et al., 2018)</ref>, and (2) transformer seq2seq with the same hy BART numbers are in terms of unconstrained generated summaries. EntityCode numbers are directly from<ref type="bibr" target="#b6">(Fan et al., 2018)</ref>, which is obtained with a weaker convolutiona
in NewsQA <ref type="bibr" target="#b45">(Trischler et al., 2017)</ref> and out-of-domain SQuAD 1.1 <ref type="bibr" target="#b37">(Rajpurkar et al., 2016)</ref> respectively. We note that some NewsQA
"bibr" target="#b39">(Rush et al., 2015;</ref><ref type="bibr" target="#b42">See et al., 2017;</ref><ref type="bibr" target="#b34">Paulus et al., 2018)</ref> which can produce coherent and fluent summ
(2018)</ref> utilize copying words at test time to mask copying operations in a summarization task. <ref type="bibr" target="#b25">Li et al. (2018)</ref> and <ref type="bibr" target="#b41">Saito et al
news articles <ref type="bibr" target="#b14">(Hermann et al., 2015)</ref>, arXiv scientific papers <ref type="bibr" target="#b3">(Cohan et al., 2018)</ref>, and BIGPATENT patent documents <ref type=" n systems.</p><p>Summarizing Contributions. Existing datasets about scientific papers such as arXiv <ref type="bibr" target="#b3">(Cohan et al., 2018)</ref> collect paper abstracts as the summaries, w news articles <ref type="bibr" target="#b14">(Hermann et al., 2015)</ref>, arXiv scientific papers <ref type="bibr" target="#b3">(Cohan et al., 2018)</ref>, and BIGPATENT patent articles <ref type="b ><p>Here we show three random examples from the arXiv test set. Note that this is the test set from <ref type="bibr" target="#b3">(Cohan et al., 2018)</ref> instead of the contribution test data colle
arget="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar that causes the performance degradation in previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>In this pape state-of-the-arts for defending against adversarial attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" tar ing with random normal perturbations) or adversarial noise <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42]</ref>, fail to improve ac pe="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. Meanwhile, recent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" ta etter recognition than the vanilla training baseline. These results contradict previous conclusions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" ta -1 accuracy on ImageNet, which beats the vanilla training baseline by 0.6%. However, previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref> show adversarial t r" target="#b15">16]</ref> show adversarial training always degrades performance.</p><p>Compared to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref>, we make two chang ting noise. However, all previous attempts, by augmenting either with random noise (e.g., Tab. 5 in <ref type="bibr" target="#b17">[18]</ref> shows the result of training with random normal perturbati ojection step in PGD; or (2) we skip the random noise initialization step in PGD, turn it to I-FGSM <ref type="bibr" target="#b17">[18]</ref>. Other attack hyper-parameters are unchanged: the maximum Vanilla Training 81.7 83.7 84.5 PGD <ref type="bibr" target="#b22">[23]</ref> 81.8 84.3 85.2 I-FGSM <ref type="bibr" target="#b17">[18]</ref> 81.9 84. In Sec. 5.3, we show that adversarial training ca re of adversarial examples and clean images, as suggested in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>,</p><formula xml:id="formula_3">arg min Î¸ E (x,y)â¼D L(Î¸, x, al and clean domains. However, as observed in former studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>, directly optimizing Eq. ( <ref type="formula" target="#for stronger performance than the adversarial training baseline <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. Besides, compared to the fine-tuning strategy in Sec. 3, A
get="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" tar adversarial attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45]</ref>. Although adversari robustness. However, such trained models usually cannot generalize well to clean images as shown in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>. We validate this b46">47,</ref><ref type="bibr" target="#b42">43]</ref>, they cannot generalize well to clean images <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>. Unlike Madry's ad le model (EfficientNet-B7) on ImageNet using PGD attacker<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b22">[23]</ref>-both adversarially trained models obtain much lower accura and x is training sample with ground-truth label y. Consider Madry's adversarial training framework <ref type="bibr" target="#b22">[23]</ref>, instead of training with original samples, it trains netw Eq. ( <ref type="formula" target="#formula_3">3</ref>). We choose Projected Gradient Descent (PGD) <ref type="bibr" target="#b22">[23]</ref> under L â norm as the default attacker for generating adve ecognition models with different adversarial attacker. B3 B5 B7 Vanilla Training 81.7 83.7 84.5 PGD <ref type="bibr" target="#b22">[23]</ref> 81.8 84.3 85.2 I-FGSM <ref type="bibr" target="#b17">[18]<
lps EfficientNet-B7 <ref type="bibr" target="#b40">[41]</ref> to achieve 85.2% accuracy on ImageNet <ref type="bibr" target="#b32">[33]</ref>, 52.9% mCE (mean corruption error, lower is better) on Ima 4">45]</ref> also suggest that training with adversarial examples on large datasets, e.g., ImageNet <ref type="bibr" target="#b32">[33]</ref>, with supervised learning results in performance degradati et to 1. The attack step size is fixed to Î±=1.</p><p>Datasets. We use the standard ImageNet dataset <ref type="bibr" target="#b32">[33]</ref> to train all models. In addition to reporting performance helps EfficientNet-B7<ref type="bibr" target="#b40">[41]</ref> to achieve 85.2% accuracy on ImageNet<ref type="bibr" target="#b32">[33]</ref>, 52.9% mCE (mean corruption error, lower is better) on Ima
rformance for mobile networks. As a comparison, MobileNetv3 has 5.4M parameters with 75.2% accuracy <ref type="bibr" target="#b10">[11]</ref>. These results encourage the future investigation on more
mputer vision models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b38">39]</ref>. Specifically, BN normalizes input features by the mean and
get="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" tar ng baseline. These results contradict previous conclusions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b15">16]</ref> that the performanc adversarial noise <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42]</ref>, fail to improve accuracy on clean images. ial examples to
re much more robust to high frequency noise <ref type="bibr" target="#b46">[47]</ref>. Zhang et al. <ref type="bibr" target="#b50">[51]</ref> further suggest these adversarially learned feature repres allowed perturbation range. Though such trained models have several nice properties as described in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" ta
get="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref> have been made to i hile, recent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b44">45]</ref> also suggest that training with adversarial examples on lar in previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>In this paper, we propose AdvProp, short for Adversa rget="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45]</ref>. Although adversarial training significantly improves model usually cannot generalize well to clean images as shown in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>. We validate this result by training a medium-scale model ( 42">43]</ref>, they cannot generalize well to clean images <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>. Unlike Madry's adversarial training, our main goal is to i f type="bibr" target="#b31">[32]</ref>, e.g., â¼10% drop on CIFAR-10 [23] and â¼15% drop on Im-ageNet <ref type="bibr" target="#b44">[45]</ref>. Tsipras et al. <ref type="bibr" target="#b42">[43]</ref>
><ref type="bibr" target="#b19">20]</ref>, or on larger datasets but in the semi-supervised setting <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. Meanwhile, recent ustness, how to improve clean image accuracy with adversarial training is still under-explored. VAT <ref type="bibr" target="#b25">[26]</ref> and deep co-training <ref type="bibr" target="#b29">[30]</
get="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref> have been made to improve network robustness.</p><p>In this
-art 85.5% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in <ref type="bibr" target="#b23">[24]</ref> which is trained with 3.5B Instagram images (â¼3000Ã more t accuracy on ImageNet without any extra data. This result even surpasses the best model reported in <ref type="bibr" target="#b23">[24]</ref>, which is pretrained on 3.5B extra Instagram images (â¼3000 ccuracy on ImageNet without using extra data. This result even surpasses the best model reported in <ref type="bibr" target="#b23">[24]</ref>, which is pretrained on 3.5B extra Instagram images (â¼3000 lowed to train with corresponding distortions <ref type="bibr" target="#b5">[6]</ref> or extra data <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>To summariz
rations are used to model the sequential document selection process in search result diversi cation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref> and multi-page sea


er of documents for a given query, has played a vital role in the eld of information retrieval (IR) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. In recent years,
IV <ref type="bibr" target="#b37">[38]</ref>, R-LTR <ref type="bibr" target="#b45">[45]</ref>, PAMM <ref type="bibr" target="#b32">[33]</ref>, NTN-DIV <ref type="bibr" target="#b33">[34]</ref>, and MD

as a dual-agent stochastic game, on the basis of partially observed Markov decision process (POMDP) <ref type="bibr" target="#b21">[22]</ref>. In <ref type="bibr" target="#b41">[42]</ref>, a log-based
ec, a neural-optimized POMDP algorithm, for building a collaborative ltering recommender system. In <ref type="bibr" target="#b44">[44]</ref>, the recommendation is modeled with MDP and deep Q-learnin
as a dual-agent stochastic game, on the basis of partially observed Markov decision process (POMDP) <ref type="bibr" target="#b21">[22]</ref>. In <ref type="bibr" target="#b41">[42]</ref>, a log-based
b8">[9]</ref>; and the learning methods of SVM-DIV <ref type="bibr" target="#b37">[38]</ref>, R-LTR <ref type="bibr" target="#b45">[45]</ref>, PAMM <ref type="bibr" target="#b32">[33]</ref>, NTN-DIV <
">35]</ref> and multi-page search <ref type="bibr" target="#b40">[41]</ref>. The query change model <ref type="bibr" target="#b36">[37]</ref> formalizes the problem of session search as an MDP. The wi
